{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 “ihdp_npci_1-100.train.npz” 已经存在；不获取。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.layers as tfkl\n",
    "tfd,tfpl = tfp.distributions,tfp.layers\n",
    "import tensorflow.keras.backend as tfkb\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from evaluation import *\n",
    "from cevae_networks import *\n",
    "################################################\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='')\n",
    "parser.add_argument('--scale_penalize',    type = float, default = 0.001,  help = '')\n",
    "parser.add_argument('--learning_rate',     type = float, default = 0.001,  help = '')\n",
    "parser.add_argument('--default_y_scale',   type = float, default = 1.,  help = '')\n",
    "parser.add_argument('--t_dim',     type = int, default = 1,  help = '')\n",
    "parser.add_argument('--y_dim',     type = int, default = 1,  help = '')\n",
    "parser.add_argument('--x_dim',     type = int, default = 25, help = '')\n",
    "parser.add_argument('--z_dim',     type = int, default = 20, help = '')\n",
    "parser.add_argument('--x_num_dim', type = int, default = 6,  help = '')\n",
    "parser.add_argument('--x_bin_dim', type = int, default = 19, help = '')\n",
    "parser.add_argument('--val_split', type = float, default = 0.2, help = '')\n",
    "parser.add_argument('--batch_size', type = int, default = 256, help = '')\n",
    "parser.add_argument('--nh', type = int, default = 3, help = 'number of hidden layers')\n",
    "parser.add_argument('--h',  type = int, default = 200, help = 'number of hidden units')\n",
    "args = parser.parse_args([])\n",
    "################################################\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz \n",
    "\n",
    "def load_IHDP_data(training_data,testing_data,i):\n",
    "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "        train_data=np.load(trf); test_data=np.load(tef)\n",
    "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
    "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
    "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
    "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    "        ycf=np.concatenate((train_data['ycf'][:,i],  test_data['ycf'][:,i])).astype('float32')\n",
    "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "        data['y']=data['y'].reshape(-1,1)\n",
    "        data['ycf'] = ycf.reshape(-1,1)\n",
    "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "    return data\n",
    "\n",
    "ind = 7\n",
    "# rep = 5\n",
    "# rep = 1\n",
    "# data = load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.test.npz',i = ind)\n",
    "# for key in data:\n",
    "#     if key != 'y_scaler':\n",
    "#         data[key] = np.repeat(data[key],repeats = rep, axis = 0)\n",
    "# np.shape(data['x'])\n",
    "data_train = load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.train.npz',i = ind)\n",
    "data_valid = load_IHDP_data(training_data='./ihdp_npci_1-100.test.npz',testing_data='./ihdp_npci_1-100.test.npz',i = ind)\n",
    "np.shape(data_train['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonLayer(tfkl.Layer):\n",
    "    def __init__(self):\n",
    "        super(EpsilonLayer, self).__init__()\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.epsilon = self.add_weight(name='epsilon',\n",
    "                                       shape=[1, 1],\n",
    "                                       initializer='RandomNormal',\n",
    "                                       #  initializer='ones',\n",
    "                                       trainable=True)\n",
    "        super(EpsilonLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        #note there is only one epsilon were just duplicating it for conformability\n",
    "        return self.epsilon * tf.ones_like(inputs)[:, 0:1]\n",
    "\n",
    "class CEWAE(tf.keras.Model):\n",
    "    def __init__(self, kernel = \"IMQ\"):\n",
    "        super(CEWAE, self).__init__()\n",
    "        ########################################\n",
    "        # networks\n",
    "        self.activation = 'elu'\n",
    "        self.kernel = kernel\n",
    "        # CEVAE Model \n",
    "        ## (encoder)\n",
    "        self.q_y_tx = q_y_tx(args.x_bin_dim, args.x_num_dim, args.y_dim, args.t_dim, args.nh, args.h)\n",
    "        self.q_t_x = q_t_x(args.x_bin_dim, args.x_num_dim, args.t_dim, args.nh, args.h)\n",
    "        self.q_z_txy = q_z_txy(args.x_bin_dim, args.x_num_dim, args.y_dim, args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        ## (decoder)\n",
    "        self.p_x_z = p_x_z(args.x_bin_dim, args.x_num_dim, args.z_dim, args.nh, args.h)\n",
    "        self.p_t_z = p_t_z(args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        self.p_y_tz = p_y_tz(args.y_dim, args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        self.epsilon_layer = EpsilonLayer()\n",
    "        self.beta = 1\n",
    "        self.lmbda = 1\n",
    "\n",
    "    def call(self, data, training=False):\n",
    "        if training:\n",
    "            x_train,t_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "            \n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            \n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.sample()\n",
    "            # decoder\n",
    "            ## p(x|z)\n",
    "            x_num,x_bin = self.p_x_z(z_infer_sample)\n",
    "            ## p(t|z)\n",
    "            t = self.p_t_z(z_infer_sample)\n",
    "            ## p(y|t,z)\n",
    "            t0z = tf.concat([tf.zeros_like(t_train),z_infer_sample],-1)\n",
    "            t1z = tf.concat([tf.ones_like(t_train),z_infer_sample],-1)\n",
    "            y0 = self.p_y_tz(t0z)\n",
    "            y1 = self.p_y_tz(t1z)\n",
    "            y = [y0,y1]\n",
    "            epsilon = self.epsilon_layer(t_infer_sample)\n",
    "            \n",
    "            return y_infer,t_infer,z_infer,y,t,x_num,x_bin,epsilon\n",
    "        else:\n",
    "            x_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.loc\n",
    "            epsilon = self.epsilon_layer(t_infer_sample)\n",
    "            t1z = tf.concat([tf.ones_like(t_infer_sample),z_infer_sample],-1)\n",
    "            t0z = tf.concat([tf.zeros_like(t_infer_sample),z_infer_sample],-1)\n",
    "            y0 = self.p_y_tz(t0z)\n",
    "            y1 = self.p_y_tz(t1z)\n",
    "            y = [y0,y1]\n",
    "            return y,t_infer,epsilon,z_infer\n",
    "\n",
    "    def mmd_penalty(self, sample_qz, sample_pz, batch_size = args.batch_size):\n",
    "        opts = {'kernel': self.kernel, 'verbose':True, \"zdim\":20, \"pz\":\"normal\"} \n",
    "        sigma2_p = 1 ** 2\n",
    "        kernel = opts['kernel']\n",
    "        n = batch_size\n",
    "        n = tf.shape(sample_qz)[0]\n",
    "        n = tf.cast(n, tf.int32)\n",
    "        nf = tf.cast(n, tf.float32)\n",
    "        half_size = (n * n - n) / 2\n",
    "\n",
    "        norms_pz = tf.reduce_sum(tf.square(sample_pz), axis=1, keepdims=True)\n",
    "        dotprods_pz = tf.matmul(sample_pz, sample_pz, transpose_b=True)\n",
    "        distances_pz = norms_pz + tf.transpose(norms_pz) - 2. * dotprods_pz\n",
    "\n",
    "        norms_qz = tf.reduce_sum(tf.square(sample_qz), axis=1, keepdims=True)\n",
    "        dotprods_qz = tf.matmul(sample_qz, sample_qz, transpose_b=True)\n",
    "        distances_qz = norms_qz + tf.transpose(norms_qz) - 2. * dotprods_qz\n",
    "\n",
    "        dotprods = tf.matmul(sample_qz, sample_pz, transpose_b=True)\n",
    "        distances = norms_qz + tf.transpose(norms_pz) - 2. * dotprods\n",
    "\n",
    "        if kernel == 'RBF':\n",
    "            # Median heuristic for the sigma^2 of Gaussian kernel\n",
    "            sigma2_k = tf.nn.top_k(\n",
    "                tf.reshape(distances, [-1]), half_size).values[half_size - 1]\n",
    "            sigma2_k += tf.nn.top_k(\n",
    "                tf.reshape(distances_qz, [-1]), half_size).values[half_size - 1]\n",
    "            # Maximal heuristic for the sigma^2 of Gaussian kernel\n",
    "            # sigma2_k = tf.nn.top_k(tf.reshape(distances_qz, [-1]), 1).values[0]\n",
    "            # sigma2_k += tf.nn.top_k(tf.reshape(distances, [-1]), 1).values[0]\n",
    "            # sigma2_k = opts['latent_space_dim'] * sigma2_p\n",
    "            if opts['verbose']:\n",
    "                sigma2_k = tf.Print(sigma2_k, [sigma2_k], 'Kernel width:')\n",
    "            res1 = tf.exp( - distances_qz / 2. / sigma2_k)\n",
    "            res1 += tf.exp( - distances_pz / 2. / sigma2_k)\n",
    "            res1 = tf.multiply(res1, 1. - tf.eye(n))\n",
    "            res1 = tf.reduce_sum(res1) / (nf * nf - nf)\n",
    "            res2 = tf.exp( - distances / 2. / sigma2_k)\n",
    "            res2 = tf.reduce_sum(res2) * 2. / (nf * nf)\n",
    "            stat = res1 - res2\n",
    "        elif kernel == 'IMQ':\n",
    "            # k(x, y) = C / (C + ||x - y||^2)\n",
    "            # C = tf.nn.top_k(tf.reshape(distances, [-1]), half_size).values[half_size - 1]\n",
    "            # C += tf.nn.top_k(tf.reshape(distances_qz, [-1]), half_size).values[half_size - 1]\n",
    "            if opts['pz'] == 'normal':\n",
    "                Cbase = 2. * opts['zdim'] * sigma2_p\n",
    "            elif opts['pz'] == 'sphere':\n",
    "                Cbase = 2.\n",
    "            elif opts['pz'] == 'uniform':\n",
    "                # E ||x - y||^2 = E[sum (xi - yi)^2]\n",
    "                #               = zdim E[(xi - yi)^2]\n",
    "                #               = const * zdim\n",
    "                Cbase = opts['zdim']\n",
    "            stat = 0.\n",
    "            for scale in [.1, .2, .5, 1., 2., 5., 10.]:\n",
    "                C = Cbase * scale\n",
    "                res1 = C / (C + distances_qz)\n",
    "                res1 += C / (C + distances_pz)\n",
    "                res1 = tf.multiply(res1, 1. - tf.eye(n))\n",
    "                res1 = tf.reduce_sum(res1) / (nf * nf - nf)\n",
    "                res2 = C / (C + distances)\n",
    "                res2 = tf.reduce_sum(res2) * 2. / (nf * nf)\n",
    "                stat += res1 - res2\n",
    "        return stat\n",
    "\n",
    "    def cewae_loss(self, data, pred, training = False):\n",
    "        x_train, t_train, y_train = data[0],data[1],data[2]\n",
    "        x_train_num, x_train_bin = x_train[:,:args.x_num_dim],x_train[:,args.x_num_dim:]\n",
    "        y_infer,t_infer,z_infer,y,t,x_num,x_bin,epsilon = pred\n",
    "        y0_infer,y1_infer = y_infer\n",
    "        y0,y1 = y\n",
    "        # reconstruct loss\n",
    "        rec_x_num = tfkb.mean(tf.math.square(x_train_num - x_num.sample()))\n",
    "        rec_x_bin = tf.reduce_sum(\n",
    "            tfk.losses.binary_crossentropy(\n",
    "                x_train_bin,\n",
    "                tf.cast(x_bin.sample(),tf.float32),\n",
    "                from_logits=False))\n",
    "        rec_t_bin = tf.reduce_sum(\n",
    "            tfk.losses.binary_crossentropy(\n",
    "                t_train,\n",
    "                tf.cast(t.sample(),tf.float32),\n",
    "                from_logits=False))\n",
    "        rec_y0 = tf.math.square(y0.sample() - y_train)\n",
    "        rec_y1 = tf.math.square(y1.sample() - y_train)\n",
    "        rec_y = tfkb.mean(t_train * rec_y1 + (1-t_train)* rec_y0)\n",
    "        # regularization\n",
    "        # q dist\n",
    "        reg_y0 = tf.math.square(y0_infer.sample() - y_train)\n",
    "        reg_y1 = tf.math.square(y1_infer.sample() - y_train)\n",
    "        reg_y = tfkb.mean(t_train * reg_y1 + (1-t_train)* reg_y0)\n",
    "        reg_t = tf.reduce_sum(\n",
    "            tfk.losses.binary_crossentropy(\n",
    "                t_train,\n",
    "                tf.cast(t_infer.sample(),tf.float32),\n",
    "                from_logits=False))\n",
    "        rec_loss =  rec_x_num + rec_x_bin + rec_t_bin + rec_y\n",
    "        rec_loss =  rec_x_num + rec_x_bin + rec_t_bin + rec_y + reg_y + reg_t\n",
    "        # mmd penalty\n",
    "        pz = tfd.Normal(loc = tf.zeros_like(z_infer.sample()), scale = tf.ones_like(z_infer.sample()))\n",
    "        reg_mmd = self.mmd_penalty(z_infer.sample(), pz.sample())\n",
    "        # target regularization\n",
    "        y_pred = y0.loc * (1-t_train) + y1.loc * t_train\n",
    "        t_pred = tf.math.sigmoid(t.logits)\n",
    "        cc = t_train/t_pred - (1-t_train) / (1-t_pred)\n",
    "        t_reg = tf.math.square(y_pred + epsilon * cc - y_train)\n",
    "        reg_loss = reg_mmd + self.beta * tfkb.mean(t_reg)\n",
    "        loss = rec_loss + reg_loss\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, data):\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "            loss = self.cewae_loss(data = data, pred = pred, training = True)\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        metrics = {\"loss\": loss}\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "        y_infer = pred[0]\n",
    "        loss = self.cewae_loss(data = data, pred = pred, training = False)\n",
    "        y0, y1 = y_infer[0].sample(),y_infer[1].sample()\n",
    "        ate = tfkb.mean(y1) - tfkb.mean(y0)\n",
    "        metrics = {\"loss\":loss,\"y0\": tfkb.mean(y0),\"y1\": tfkb.mean(y1),'ate_afte_scaled': ate}\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_117/kernel:0', 'dense_117/bias:0', 'dense_118/kernel:0', 'dense_118/bias:0', 'dense_119/kernel:0', 'dense_119/bias:0', 'dense_120/kernel:0', 'dense_120/bias:0', 'dense_131/kernel:0', 'dense_131/bias:0', 'dense_132/kernel:0', 'dense_132/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_117/kernel:0', 'dense_117/bias:0', 'dense_118/kernel:0', 'dense_118/bias:0', 'dense_119/kernel:0', 'dense_119/bias:0', 'dense_120/kernel:0', 'dense_120/bias:0', 'dense_131/kernel:0', 'dense_131/bias:0', 'dense_132/kernel:0', 'dense_132/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "6/6 [==============================] - 9s 584ms/step - loss: 3083.8700 - val_loss: 2224.0161 - val_y0: 1.0866 - val_y1: -0.4049 - val_ate_afte_scaled: -1.4915 - lr: 5.0000e-04\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 2941.1108 - val_loss: 2075.3694 - val_y0: 1.0557 - val_y1: -0.1583 - val_ate_afte_scaled: -1.2140 - lr: 5.0000e-04\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 2942.5261 - val_loss: 2112.7607 - val_y0: 0.4456 - val_y1: -7.0078e-04 - val_ate_afte_scaled: -0.4463 - lr: 5.0000e-04\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 1s 162ms/step - loss: 2926.5995 - val_loss: 2202.0415 - val_y0: 0.2849 - val_y1: -0.1734 - val_ate_afte_scaled: -0.4583 - lr: 5.0000e-04\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 1s 171ms/step - loss: 2911.1958 - val_loss: 2254.1836 - val_y0: 0.1135 - val_y1: -0.1261 - val_ate_afte_scaled: -0.2396 - lr: 5.0000e-04\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 1s 225ms/step - loss: 2851.7300 - val_loss: 2043.3782 - val_y0: 0.2384 - val_y1: -0.4072 - val_ate_afte_scaled: -0.6456 - lr: 5.0000e-04\n",
      "Epoch 7/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 3709.0354\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "6/6 [==============================] - 1s 146ms/step - loss: 2925.4107 - val_loss: 2302.9688 - val_y0: 0.0187 - val_y1: -0.3864 - val_ate_afte_scaled: -0.4051 - lr: 5.0000e-04\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 2886.7488 - val_loss: 2181.4570 - val_y0: -0.0873 - val_y1: -0.4250 - val_ate_afte_scaled: -0.3377 - lr: 2.5000e-04\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 2845.8744 - val_loss: 2087.5229 - val_y0: -0.0401 - val_y1: -0.5946 - val_ate_afte_scaled: -0.5546 - lr: 2.5000e-04\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 2922.5220 - val_loss: 2159.3484 - val_y0: 0.2347 - val_y1: -0.6140 - val_ate_afte_scaled: -0.8486 - lr: 2.5000e-04\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 2833.9209 - val_loss: 2153.4800 - val_y0: 0.0908 - val_y1: -0.7333 - val_ate_afte_scaled: -0.8241 - lr: 2.5000e-04\n",
      "Epoch 12/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 3758.9865\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 2949.1335 - val_loss: 2165.9575 - val_y0: -0.0151 - val_y1: -0.8066 - val_ate_afte_scaled: -0.7914 - lr: 2.5000e-04\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 2912.5642 - val_loss: 2074.5361 - val_y0: 0.0035 - val_y1: -0.6593 - val_ate_afte_scaled: -0.6627 - lr: 1.2500e-04\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 2847.2279 - val_loss: 2029.7063 - val_y0: -2.0558e-04 - val_y1: -0.8067 - val_ate_afte_scaled: -0.8065 - lr: 1.2500e-04\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 1s 140ms/step - loss: 2873.0179 - val_loss: 2059.2805 - val_y0: 0.1318 - val_y1: -0.7008 - val_ate_afte_scaled: -0.8326 - lr: 1.2500e-04\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 2807.4308 - val_loss: 2098.3667 - val_y0: -0.0533 - val_y1: -0.7594 - val_ate_afte_scaled: -0.7062 - lr: 1.2500e-04\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 3087.5359\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 2776.8887 - val_loss: 2110.7842 - val_y0: 0.0973 - val_y1: -0.8671 - val_ate_afte_scaled: -0.9644 - lr: 1.2500e-04\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 2912.4832 - val_loss: 2052.9365 - val_y0: 0.0201 - val_y1: -0.8775 - val_ate_afte_scaled: -0.8975 - lr: 6.2500e-05\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 1s 141ms/step - loss: 2864.3129 - val_loss: 2018.8575 - val_y0: 0.0398 - val_y1: -1.1741 - val_ate_afte_scaled: -1.2140 - lr: 6.2500e-05\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 2874.8131 - val_loss: 2086.3660 - val_y0: 0.0085 - val_y1: -1.0915 - val_ate_afte_scaled: -1.1000 - lr: 6.2500e-05\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 1s 152ms/step - loss: 2892.8203 - val_loss: 2160.6692 - val_y0: 0.0247 - val_y1: -1.0158 - val_ate_afte_scaled: -1.0404 - lr: 6.2500e-05\n",
      "Epoch 22/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 3568.6591\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 2821.9429 - val_loss: 2076.6821 - val_y0: 0.1395 - val_y1: -1.1119 - val_ate_afte_scaled: -1.2514 - lr: 6.2500e-05\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 2912.0015 - val_loss: 2151.0093 - val_y0: 0.0788 - val_y1: -1.1339 - val_ate_afte_scaled: -1.2127 - lr: 3.1250e-05\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 2860.5254 - val_loss: 2125.0376 - val_y0: 0.0621 - val_y1: -1.0375 - val_ate_afte_scaled: -1.0996 - lr: 3.1250e-05\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 2849.9431 - val_loss: 2235.3311 - val_y0: 0.2314 - val_y1: -1.1175 - val_ate_afte_scaled: -1.3489 - lr: 3.1250e-05\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 2775.9673 - val_loss: 2048.6602 - val_y0: 0.0863 - val_y1: -1.2676 - val_ate_afte_scaled: -1.3539 - lr: 3.1250e-05\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 1s 142ms/step - loss: 2898.0663 - val_loss: 2260.9067 - val_y0: 0.1018 - val_y1: -1.2152 - val_ate_afte_scaled: -1.3170 - lr: 3.1250e-05\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 1s 148ms/step - loss: 2810.8643 - val_loss: 1998.9537 - val_y0: 0.1674 - val_y1: -1.2624 - val_ate_afte_scaled: -1.4299 - lr: 3.1250e-05\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 1s 125ms/step - loss: 2838.4042 - val_loss: 2051.5347 - val_y0: 0.1867 - val_y1: -1.1676 - val_ate_afte_scaled: -1.3543 - lr: 3.1250e-05\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 2819.3635 - val_loss: 2016.0237 - val_y0: 0.1343 - val_y1: -1.3936 - val_ate_afte_scaled: -1.5279 - lr: 3.1250e-05\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 3223.2163\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 2902.0667 - val_loss: 2076.9150 - val_y0: 0.0055 - val_y1: -1.3946 - val_ate_afte_scaled: -1.4001 - lr: 3.1250e-05\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 1s 130ms/step - loss: 2846.3907 - val_loss: 2179.9814 - val_y0: 0.1925 - val_y1: -1.3741 - val_ate_afte_scaled: -1.5666 - lr: 1.5625e-05\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 2830.4479 - val_loss: 2115.0103 - val_y0: -0.1209 - val_y1: -1.4563 - val_ate_afte_scaled: -1.3354 - lr: 1.5625e-05\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 2715.0342 - val_loss: 2191.5713 - val_y0: -0.0563 - val_y1: -1.4640 - val_ate_afte_scaled: -1.4076 - lr: 1.5625e-05\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 2775.4607 - val_loss: 1913.1637 - val_y0: 0.0700 - val_y1: -1.5404 - val_ate_afte_scaled: -1.6105 - lr: 1.5625e-05\n",
      "Epoch 36/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 3541.3058\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 2772.3922 - val_loss: 2248.6899 - val_y0: 0.1299 - val_y1: -1.5575 - val_ate_afte_scaled: -1.6874 - lr: 1.5625e-05\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 2736.0138 - val_loss: 2202.0347 - val_y0: 0.2145 - val_y1: -1.5140 - val_ate_afte_scaled: -1.7285 - lr: 7.8125e-06\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 2777.8033 - val_loss: 1961.9062 - val_y0: 0.0801 - val_y1: -1.6540 - val_ate_afte_scaled: -1.7341 - lr: 7.8125e-06\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 2779.1542 - val_loss: 2076.4280 - val_y0: -0.0206 - val_y1: -1.5482 - val_ate_afte_scaled: -1.5275 - lr: 7.8125e-06\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 1s 141ms/step - loss: 2873.9200 - val_loss: 2042.6746 - val_y0: 0.1060 - val_y1: -1.3202 - val_ate_afte_scaled: -1.4262 - lr: 7.8125e-06\n",
      "Epoch 41/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 3626.1379\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 2873.1848 - val_loss: 2210.9119 - val_y0: -0.0908 - val_y1: -1.6482 - val_ate_afte_scaled: -1.5574 - lr: 7.8125e-06\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 2858.4332 - val_loss: 1976.0171 - val_y0: -0.0578 - val_y1: -1.6417 - val_ate_afte_scaled: -1.5839 - lr: 3.9063e-06\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 2913.6649 - val_loss: 2144.0249 - val_y0: 0.0090 - val_y1: -1.5652 - val_ate_afte_scaled: -1.5742 - lr: 3.9063e-06\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 2772.9919 - val_loss: 2039.5962 - val_y0: 0.0088 - val_y1: -1.6211 - val_ate_afte_scaled: -1.6299 - lr: 3.9063e-06\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 1s 122ms/step - loss: 2821.1478 - val_loss: 1882.4121 - val_y0: -0.0151 - val_y1: -1.4940 - val_ate_afte_scaled: -1.4789 - lr: 3.9063e-06\n",
      "Epoch 46/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 3589.7526\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 2803.4696 - val_loss: 2074.9316 - val_y0: -0.0718 - val_y1: -1.4354 - val_ate_afte_scaled: -1.3635 - lr: 3.9063e-06\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 2896.6384 - val_loss: 2091.9746 - val_y0: -0.0551 - val_y1: -1.5948 - val_ate_afte_scaled: -1.5397 - lr: 1.9531e-06\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 2865.9999 - val_loss: 2008.5303 - val_y0: 0.0578 - val_y1: -1.4168 - val_ate_afte_scaled: -1.4746 - lr: 1.9531e-06\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 2760.6840 - val_loss: 2167.2891 - val_y0: 0.0674 - val_y1: -1.7251 - val_ate_afte_scaled: -1.7925 - lr: 1.9531e-06\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 1s 127ms/step - loss: 2828.8154 - val_loss: 2153.8184 - val_y0: -0.0513 - val_y1: -1.4042 - val_ate_afte_scaled: -1.3529 - lr: 1.9531e-06\n",
      "Epoch 51/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 3559.7590\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 2791.4424 - val_loss: 1845.7668 - val_y0: -0.0894 - val_y1: -1.6232 - val_ate_afte_scaled: -1.5338 - lr: 1.9531e-06\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 1s 126ms/step - loss: 2819.7283 - val_loss: 2058.2178 - val_y0: -0.0590 - val_y1: -1.5018 - val_ate_afte_scaled: -1.4428 - lr: 9.7656e-07\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 2817.6711 - val_loss: 2019.8723 - val_y0: -0.0545 - val_y1: -1.5359 - val_ate_afte_scaled: -1.4814 - lr: 9.7656e-07\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 2796.1271 - val_loss: 2248.0713 - val_y0: 0.0515 - val_y1: -1.3998 - val_ate_afte_scaled: -1.4513 - lr: 9.7656e-07\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 2760.4384 - val_loss: 2151.9478 - val_y0: 0.0416 - val_y1: -1.5156 - val_ate_afte_scaled: -1.5572 - lr: 9.7656e-07\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 3156.9713\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 2830.9274 - val_loss: 1933.5647 - val_y0: 0.0584 - val_y1: -1.4922 - val_ate_afte_scaled: -1.5506 - lr: 9.7656e-07\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 2850.6086 - val_loss: 2185.7549 - val_y0: 0.1437 - val_y1: -1.6023 - val_ate_afte_scaled: -1.7461 - lr: 4.8828e-07\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 1s 135ms/step - loss: 2802.1071 - val_loss: 1991.2244 - val_y0: 0.0054 - val_y1: -1.5697 - val_ate_afte_scaled: -1.5750 - lr: 4.8828e-07\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 2816.5931 - val_loss: 2021.7456 - val_y0: 0.0726 - val_y1: -1.6052 - val_ate_afte_scaled: -1.6778 - lr: 4.8828e-07\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 2859.6706 - val_loss: 2183.8716 - val_y0: 0.0032 - val_y1: -1.5419 - val_ate_afte_scaled: -1.5451 - lr: 4.8828e-07\n",
      "Epoch 61/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 3530.2166\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "6/6 [==============================] - 1s 141ms/step - loss: 2782.8752 - val_loss: 2006.7375 - val_y0: -0.0953 - val_y1: -1.6369 - val_ate_afte_scaled: -1.5416 - lr: 4.8828e-07\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 2788.1275 - val_loss: 2122.3018 - val_y0: -0.1472 - val_y1: -1.6611 - val_ate_afte_scaled: -1.5139 - lr: 2.4414e-07\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 2790.7070 - val_loss: 2109.3000 - val_y0: -0.1111 - val_y1: -1.6636 - val_ate_afte_scaled: -1.5525 - lr: 2.4414e-07\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 1s 169ms/step - loss: 2788.5986 - val_loss: 2044.1823 - val_y0: -0.0275 - val_y1: -1.5149 - val_ate_afte_scaled: -1.4874 - lr: 2.4414e-07\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 1s 169ms/step - loss: 2783.2260 - val_loss: 2020.4493 - val_y0: -0.0977 - val_y1: -1.4361 - val_ate_afte_scaled: -1.3384 - lr: 2.4414e-07\n",
      "Epoch 66/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 3534.8366\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "6/6 [==============================] - 1s 142ms/step - loss: 2800.6961 - val_loss: 2203.3467 - val_y0: -0.0173 - val_y1: -1.6062 - val_ate_afte_scaled: -1.5889 - lr: 2.4414e-07\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 2934.4712 - val_loss: 2151.5139 - val_y0: -0.1353 - val_y1: -1.6210 - val_ate_afte_scaled: -1.4856 - lr: 1.2207e-07\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 2829.1820 - val_loss: 2154.9468 - val_y0: -0.0100 - val_y1: -1.6950 - val_ate_afte_scaled: -1.6850 - lr: 1.2207e-07\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 1s 139ms/step - loss: 2800.3896 - val_loss: 2235.3110 - val_y0: 0.0729 - val_y1: -1.5197 - val_ate_afte_scaled: -1.5926 - lr: 1.2207e-07\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 2821.0314 - val_loss: 2213.8413 - val_y0: 0.0206 - val_y1: -1.5003 - val_ate_afte_scaled: -1.5209 - lr: 1.2207e-07\n",
      "Epoch 71/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 3499.7975\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "6/6 [==============================] - 1s 136ms/step - loss: 2758.7040 - val_loss: 2099.5737 - val_y0: -0.0013 - val_y1: -1.5778 - val_ate_afte_scaled: -1.5765 - lr: 1.2207e-07\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 2919.1777 - val_loss: 2046.0359 - val_y0: 0.0359 - val_y1: -1.5832 - val_ate_afte_scaled: -1.6191 - lr: 6.1035e-08\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 2836.6560 - val_loss: 2255.4990 - val_y0: 0.0879 - val_y1: -1.6160 - val_ate_afte_scaled: -1.7039 - lr: 6.1035e-08\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 2813.8991 - val_loss: 2043.8120 - val_y0: -0.0511 - val_y1: -1.4871 - val_ate_afte_scaled: -1.4360 - lr: 6.1035e-08\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 2794.5027 - val_loss: 2094.8032 - val_y0: -0.0584 - val_y1: -1.5448 - val_ate_afte_scaled: -1.4865 - lr: 6.1035e-08\n",
      "Epoch 76/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 3585.5624\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 2794.6324 - val_loss: 2188.4541 - val_y0: 0.0583 - val_y1: -1.3692 - val_ate_afte_scaled: -1.4275 - lr: 6.1035e-08\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 2751.2001 - val_loss: 2202.0322 - val_y0: -0.0435 - val_y1: -1.6238 - val_ate_afte_scaled: -1.5803 - lr: 3.0518e-08\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 1s 140ms/step - loss: 2849.3984 - val_loss: 1968.0126 - val_y0: 0.0263 - val_y1: -1.6345 - val_ate_afte_scaled: -1.6607 - lr: 3.0518e-08\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 1s 140ms/step - loss: 2680.8948 - val_loss: 1927.5347 - val_y0: -0.0311 - val_y1: -1.7033 - val_ate_afte_scaled: -1.6722 - lr: 3.0518e-08\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 1s 140ms/step - loss: 2910.5623 - val_loss: 2139.7075 - val_y0: 0.0233 - val_y1: -1.5427 - val_ate_afte_scaled: -1.5660 - lr: 3.0518e-08\n",
      "Epoch 81/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 3611.6833\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 2895.5344 - val_loss: 1996.6523 - val_y0: 0.0389 - val_y1: -1.5804 - val_ate_afte_scaled: -1.6193 - lr: 3.0518e-08\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 1s 142ms/step - loss: 2888.0414 - val_loss: 2084.5269 - val_y0: -0.0143 - val_y1: -1.6267 - val_ate_afte_scaled: -1.6124 - lr: 1.5259e-08\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 1s 145ms/step - loss: 2836.9496 - val_loss: 2085.0928 - val_y0: 0.0750 - val_y1: -1.5040 - val_ate_afte_scaled: -1.5790 - lr: 1.5259e-08\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 1s 142ms/step - loss: 2771.7242 - val_loss: 2039.1594 - val_y0: 0.0480 - val_y1: -1.6484 - val_ate_afte_scaled: -1.6964 - lr: 1.5259e-08\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 2779.0587 - val_loss: 2102.7402 - val_y0: 0.0908 - val_y1: -1.6222 - val_ate_afte_scaled: -1.7130 - lr: 1.5259e-08\n",
      "Epoch 86/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 3672.4645\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
      "6/6 [==============================] - 1s 138ms/step - loss: 2895.0936 - val_loss: 2219.2300 - val_y0: -0.1559 - val_y1: -1.5728 - val_ate_afte_scaled: -1.4169 - lr: 1.5259e-08\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 2771.5028 - val_loss: 2065.5312 - val_y0: 0.1188 - val_y1: -1.5717 - val_ate_afte_scaled: -1.6904 - lr: 7.6294e-09\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 2750.5113 - val_loss: 2260.2939 - val_y0: 0.0528 - val_y1: -1.5247 - val_ate_afte_scaled: -1.5774 - lr: 7.6294e-09\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 1s 129ms/step - loss: 2819.9517 - val_loss: 2113.9399 - val_y0: -0.0649 - val_y1: -1.5234 - val_ate_afte_scaled: -1.4585 - lr: 7.6294e-09\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 1s 140ms/step - loss: 2870.0237 - val_loss: 2054.2935 - val_y0: 0.0353 - val_y1: -1.5682 - val_ate_afte_scaled: -1.6034 - lr: 7.6294e-09\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 3163.2738\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 2837.2224 - val_loss: 2065.2241 - val_y0: 0.1497 - val_y1: -1.5796 - val_ate_afte_scaled: -1.7293 - lr: 7.6294e-09\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Colab command to allow us to run Colab in TF2\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "%reload_ext tensorboard \n",
    "\n",
    "model = CEWAE()\n",
    "### MAIN CODE ####\n",
    "verbose=True\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    " \n",
    "callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='val_loss', patience=40, min_delta=0), \n",
    "        #40 is Shi's recommendation patience for this dataset, but you should tune for your data \n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0, cooldown=0, min_lr=0),\n",
    "        #This learning rate scheduling is quite agressive which seems good for this dataset\n",
    "        metrics_for_cevae_target(data_train,'train',verbose),\n",
    "        metrics_for_cevae_target(data_valid,'valid',verbose),\n",
    "        tensorboard_callback\n",
    "    ]\n",
    "    \n",
    "#optimizer hyperparameters\n",
    "learning_rate = 5e-4\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate))\n",
    "\n",
    "model.fit(\n",
    "    [data_train['x'],data_train['t'],data_train['ys']],\n",
    "    callbacks=callbacks,\n",
    "    validation_data=[[data_valid['x'],data_valid['t'],data_valid['ys']]],\n",
    "    epochs=300,\n",
    "    batch_size=256,\n",
    "    verbose=verbose\n",
    "    )\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 45269), started 2 days, 3:53:25 ago. (Use '!kill 45269' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-bee3806f203f8e2\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-bee3806f203f8e2\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
