{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 “ihdp_npci_1-100.train.npz” 已经存在；不获取。\n",
      "\n",
      "文件 “ihdp_npci_1-100.test.npz” 已经存在；不获取。\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([5.26691518]), array([2.59847927]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.layers as tfkl\n",
    "tfd,tfpl = tfp.distributions,tfp.layers\n",
    "import tensorflow.keras.backend as tfkb\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from evaluation import *\n",
    "from cevae_networks import *\n",
    "################################################\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='')\n",
    "parser.add_argument('--scale_penalize',    type = float, default = 0.001,  help = '')\n",
    "parser.add_argument('--learning_rate',     type = float, default = 0.001,  help = '')\n",
    "parser.add_argument('--default_y_scale',   type = float, default = 1.,  help = '')\n",
    "parser.add_argument('--t_dim',     type = int, default = 1,  help = '')\n",
    "parser.add_argument('--y_dim',     type = int, default = 1,  help = '')\n",
    "parser.add_argument('--x_dim',     type = int, default = 25, help = '')\n",
    "parser.add_argument('--z_dim',     type = int, default = 20, help = '')\n",
    "parser.add_argument('--x_num_dim', type = int, default = 6,  help = '')\n",
    "parser.add_argument('--x_bin_dim', type = int, default = 19, help = '')\n",
    "parser.add_argument('--val_split', type = float, default = 0.2, help = '')\n",
    "parser.add_argument('--batch_size', type = int, default = 256, help = '')\n",
    "parser.add_argument('--nh', type = int, default = 3, help = 'number of hidden layers')\n",
    "parser.add_argument('--h',  type = int, default = 200, help = 'number of hidden units')\n",
    "args = parser.parse_args([])\n",
    "################################################\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz \n",
    "\n",
    "def load_IHDP_data(training_data,testing_data,i):\n",
    "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "        train_data=np.load(trf); test_data=np.load(tef)\n",
    "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
    "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
    "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
    "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    "        ycf=np.concatenate((train_data['ycf'][:,i],  test_data['ycf'][:,i])).astype('float32')\n",
    "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "        data['y']=data['y'].reshape(-1,1)\n",
    "        data['ycf'] = ycf.reshape(-1,1)\n",
    "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "    return data\n",
    "\n",
    "ind = 7\n",
    "rep = 1\n",
    "data = load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.test.npz',i = ind)\n",
    "for key in data:\n",
    "    if key != 'y_scaler':\n",
    "        data[key] = np.repeat(data[key],repeats = rep, axis = 0)\n",
    "data['y_scaler'].mean_, data['y_scaler'].scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonLayer(tfkl.Layer):\n",
    "    def __init__(self):\n",
    "        super(EpsilonLayer, self).__init__()\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.epsilon = self.add_weight(name='epsilon',\n",
    "                                       shape=[1, 1],\n",
    "                                       initializer='RandomNormal',\n",
    "                                       #  initializer='ones',\n",
    "                                       trainable=True)\n",
    "        super(EpsilonLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        #note there is only one epsilon were just duplicating it for conformability\n",
    "        return self.epsilon * tf.ones_like(inputs)[:, 0:1]\n",
    "\n",
    "class CEWAE(tf.keras.Model):\n",
    "    def __init__(self, kernel = \"IMQ\"):\n",
    "        super(CEWAE, self).__init__()\n",
    "        ########################################\n",
    "        # networks\n",
    "        self.activation = 'elu'\n",
    "        self.kernel = kernel\n",
    "        # CEVAE Model \n",
    "        ## (encoder)\n",
    "        self.q_y_tx = q_y_tx(args.x_bin_dim, args.x_num_dim, args.y_dim, args.t_dim, args.nh, args.h)\n",
    "        self.q_t_x = q_t_x(args.x_bin_dim, args.x_num_dim, args.t_dim, args.nh, args.h)\n",
    "        self.q_z_txy = q_z_txy(args.x_bin_dim, args.x_num_dim, args.y_dim, args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        ## (decoder)\n",
    "        self.p_x_z = p_x_z(args.x_bin_dim, args.x_num_dim, args.z_dim, args.nh, args.h)\n",
    "        self.p_t_z = p_t_z(args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        self.p_y_tz = p_y_tz(args.y_dim, args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        self.epsilon_layer = EpsilonLayer()\n",
    "        self.beta = 1\n",
    "        self.lmbda = 1\n",
    "\n",
    "    def call(self, data, training=False):\n",
    "        if training:\n",
    "            x_train,t_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "            \n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            \n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.sample()\n",
    "            # decoder\n",
    "            ## p(x|z)\n",
    "            x_num,x_bin = self.p_x_z(z_infer_sample)\n",
    "            ## p(t|z)\n",
    "            t = self.p_t_z(z_infer_sample)\n",
    "            ## p(y|t,z)\n",
    "            t0z = tf.concat([tf.zeros_like(t_train),z_infer_sample],-1)\n",
    "            t1z = tf.concat([tf.ones_like(t_train),z_infer_sample],-1)\n",
    "            y0 = self.p_y_tz(t0z)\n",
    "            y1 = self.p_y_tz(t1z)\n",
    "            y = [y0,y1]\n",
    "            epsilon = self.epsilon_layer(t_infer_sample)\n",
    "            \n",
    "            return y_infer,t_infer,z_infer,y,t,x_num,x_bin,epsilon\n",
    "        else:\n",
    "            x_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.loc\n",
    "\n",
    "            t1z = tf.concat([tf.ones_like(t_infer_sample),z_infer_sample],-1)\n",
    "            t0z = tf.concat([tf.zeros_like(t_infer_sample),z_infer_sample],-1)\n",
    "            y0 = self.p_y_tz(t0z)\n",
    "            y1 = self.p_y_tz(t1z)\n",
    "            y = [y0,y1]\n",
    "            return y,t_infer,z_infer\n",
    "\n",
    "    def mmd_penalty(self, sample_qz, sample_pz, batch_size = args.batch_size):\n",
    "        opts = {'kernel': self.kernel, 'verbose':True, \"zdim\":20, \"pz\":\"normal\"} \n",
    "        sigma2_p = 1 ** 2\n",
    "        kernel = opts['kernel']\n",
    "        n = batch_size\n",
    "        n = tf.shape(sample_qz)[0]\n",
    "        n = tf.cast(n, tf.int32)\n",
    "        nf = tf.cast(n, tf.float32)\n",
    "        half_size = (n * n - n) / 2\n",
    "\n",
    "        norms_pz = tf.reduce_sum(tf.square(sample_pz), axis=1, keepdims=True)\n",
    "        dotprods_pz = tf.matmul(sample_pz, sample_pz, transpose_b=True)\n",
    "        distances_pz = norms_pz + tf.transpose(norms_pz) - 2. * dotprods_pz\n",
    "\n",
    "        norms_qz = tf.reduce_sum(tf.square(sample_qz), axis=1, keepdims=True)\n",
    "        dotprods_qz = tf.matmul(sample_qz, sample_qz, transpose_b=True)\n",
    "        distances_qz = norms_qz + tf.transpose(norms_qz) - 2. * dotprods_qz\n",
    "\n",
    "        dotprods = tf.matmul(sample_qz, sample_pz, transpose_b=True)\n",
    "        distances = norms_qz + tf.transpose(norms_pz) - 2. * dotprods\n",
    "\n",
    "        if kernel == 'RBF':\n",
    "            # Median heuristic for the sigma^2 of Gaussian kernel\n",
    "            sigma2_k = tf.nn.top_k(\n",
    "                tf.reshape(distances, [-1]), half_size).values[half_size - 1]\n",
    "            sigma2_k += tf.nn.top_k(\n",
    "                tf.reshape(distances_qz, [-1]), half_size).values[half_size - 1]\n",
    "            # Maximal heuristic for the sigma^2 of Gaussian kernel\n",
    "            # sigma2_k = tf.nn.top_k(tf.reshape(distances_qz, [-1]), 1).values[0]\n",
    "            # sigma2_k += tf.nn.top_k(tf.reshape(distances, [-1]), 1).values[0]\n",
    "            # sigma2_k = opts['latent_space_dim'] * sigma2_p\n",
    "            if opts['verbose']:\n",
    "                sigma2_k = tf.Print(sigma2_k, [sigma2_k], 'Kernel width:')\n",
    "            res1 = tf.exp( - distances_qz / 2. / sigma2_k)\n",
    "            res1 += tf.exp( - distances_pz / 2. / sigma2_k)\n",
    "            res1 = tf.multiply(res1, 1. - tf.eye(n))\n",
    "            res1 = tf.reduce_sum(res1) / (nf * nf - nf)\n",
    "            res2 = tf.exp( - distances / 2. / sigma2_k)\n",
    "            res2 = tf.reduce_sum(res2) * 2. / (nf * nf)\n",
    "            stat = res1 - res2\n",
    "        elif kernel == 'IMQ':\n",
    "            # k(x, y) = C / (C + ||x - y||^2)\n",
    "            # C = tf.nn.top_k(tf.reshape(distances, [-1]), half_size).values[half_size - 1]\n",
    "            # C += tf.nn.top_k(tf.reshape(distances_qz, [-1]), half_size).values[half_size - 1]\n",
    "            if opts['pz'] == 'normal':\n",
    "                Cbase = 2. * opts['zdim'] * sigma2_p\n",
    "            elif opts['pz'] == 'sphere':\n",
    "                Cbase = 2.\n",
    "            elif opts['pz'] == 'uniform':\n",
    "                # E ||x - y||^2 = E[sum (xi - yi)^2]\n",
    "                #               = zdim E[(xi - yi)^2]\n",
    "                #               = const * zdim\n",
    "                Cbase = opts['zdim']\n",
    "            stat = 0.\n",
    "            for scale in [.1, .2, .5, 1., 2., 5., 10.]:\n",
    "                C = Cbase * scale\n",
    "                res1 = C / (C + distances_qz)\n",
    "                res1 += C / (C + distances_pz)\n",
    "                res1 = tf.multiply(res1, 1. - tf.eye(n))\n",
    "                res1 = tf.reduce_sum(res1) / (nf * nf - nf)\n",
    "                res2 = C / (C + distances)\n",
    "                res2 = tf.reduce_sum(res2) * 2. / (nf * nf)\n",
    "                stat += res1 - res2\n",
    "        return stat\n",
    "\n",
    "    def cewae_loss(self, data, pred, training = False):\n",
    "        x_train, t_train, y_train = data[0],data[1],data[2]\n",
    "        x_train_num, x_train_bin = x_train[:,:args.x_num_dim],x_train[:,args.x_num_dim:]\n",
    "        y_infer,t_infer,z_infer,y,t,x_num,x_bin,epsilon = pred\n",
    "        y0_infer,y1_infer = y_infer\n",
    "        y0,y1 = y\n",
    "        # reconstruct loss\n",
    "        rec_x_num = tfkb.mean(tf.math.square(x_train_num - x_num.sample()))\n",
    "        rec_x_bin = tf.reduce_sum(\n",
    "            tfk.losses.binary_crossentropy(\n",
    "                x_train_bin,\n",
    "                tf.cast(x_bin.sample(),tf.float32),\n",
    "                from_logits=False))\n",
    "        rec_t_bin = tf.reduce_sum(\n",
    "            tfk.losses.binary_crossentropy(\n",
    "                t_train,\n",
    "                tf.cast(t.sample(),tf.float32),\n",
    "                from_logits=False))\n",
    "        rec_y0 = tf.math.square(y0.sample() - y_train)\n",
    "        rec_y1 = tf.math.square(y1.sample() - y_train)\n",
    "        rec_y = tfkb.mean(t_train * rec_y1 + (1-t_train)* rec_y0)\n",
    "        # regularization\n",
    "        # q dist\n",
    "        reg_y0 = tf.math.square(y0_infer.sample() - y_train)\n",
    "        reg_y1 = tf.math.square(y1_infer.sample() - y_train)\n",
    "        reg_y = tfkb.mean(t_train * reg_y1 + (1-t_train)* reg_y0)\n",
    "        reg_t = tf.reduce_sum(\n",
    "            tfk.losses.binary_crossentropy(\n",
    "                t_train,\n",
    "                tf.cast(t_infer.sample(),tf.float32),\n",
    "                from_logits=False))\n",
    "        rec_loss =  rec_x_num + rec_x_bin + rec_t_bin + rec_y\n",
    "        # rec_loss =  rec_x_num + rec_x_bin + rec_t_bin + rec_y + reg_y + reg_t\n",
    "        # mmd penalty\n",
    "        pz = tfd.Normal(loc = tf.zeros_like(z_infer.sample()), scale = tf.ones_like(z_infer.sample()))\n",
    "        reg_mmd = self.mmd_penalty(z_infer.sample(), pz.sample())\n",
    "        # target regularization\n",
    "        y_pred = y0.loc * (1-t_train) + y1.loc * t_train\n",
    "        t_pred = tf.math.sigmoid(t.logits)\n",
    "        cc = t_train/t_pred - (1-t_train) / (1-t_pred)\n",
    "        t_reg = tf.math.square(y_pred + epsilon * cc - y_train)\n",
    "        reg_loss = reg_mmd + self.beta * tfkb.mean(t_reg)\n",
    "        loss = rec_loss + reg_loss * 0.5\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, data):\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "            loss = self.cewae_loss(data = data, pred = pred, training = True)\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        metrics = {\"loss\": loss}\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "        y_infer = pred[0]\n",
    "        loss = self.cewae_loss(data = data, pred = pred, training = False)\n",
    "        y0, y1 = y_infer[0].sample(),y_infer[1].sample()\n",
    "        ate = tfkb.mean(y1) - tfkb.mean(y0)\n",
    "        metrics = {\"loss\":loss,\"y0\": tfkb.mean(y0),\"y1\": tfkb.mean(y1),'ate_afte_scaled': ate}\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_265/kernel:0', 'dense_265/bias:0', 'dense_266/kernel:0', 'dense_266/bias:0', 'dense_267/kernel:0', 'dense_267/bias:0', 'dense_268/kernel:0', 'dense_268/bias:0', 'dense_279/kernel:0', 'dense_279/bias:0', 'dense_280/kernel:0', 'dense_280/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_265/kernel:0', 'dense_265/bias:0', 'dense_266/kernel:0', 'dense_266/bias:0', 'dense_267/kernel:0', 'dense_267/bias:0', 'dense_268/kernel:0', 'dense_268/bias:0', 'dense_279/kernel:0', 'dense_279/bias:0', 'dense_280/kernel:0', 'dense_280/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "3/3 [==============================] - ETA: 0s - loss: 2996.5883 — ite: 4.4970  — ate: 3.0422 — pehe: 5.1162 \n",
      "3/3 [==============================] - 6s 737ms/step - loss: 2584.9362 - val_loss: 2476.8335 - val_y0: 0.5002 - val_y1: -0.6072 - val_ate_afte_scaled: -1.1074 - lr: 5.0000e-04\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 3053.1532 — ite: 4.0322  — ate: 1.7083 — pehe: 4.1466 \n",
      "3/3 [==============================] - 1s 267ms/step - loss: 2596.8843 - val_loss: 2280.5898 - val_y0: 0.9767 - val_y1: -1.1429 - val_ate_afte_scaled: -2.1197 - lr: 5.0000e-04\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2857.3838 — ite: 3.7603  — ate: 0.2529 — pehe: 3.9960 \n",
      "3/3 [==============================] - 1s 242ms/step - loss: 2433.5866 - val_loss: 2033.9564 - val_y0: 1.2161 - val_y1: -1.6230 - val_ate_afte_scaled: -2.8391 - lr: 5.0000e-04\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2771.6842 — ite: 3.6699  — ate: 0.1615 — pehe: 3.8636 \n",
      "3/3 [==============================] - 1s 244ms/step - loss: 2338.4109 - val_loss: 2173.6797 - val_y0: 0.9589 - val_y1: -1.7132 - val_ate_afte_scaled: -2.6721 - lr: 5.0000e-04\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2832.7381 — ite: 3.6577  — ate: 0.9001 — pehe: 3.9766 \n",
      "3/3 [==============================] - 1s 258ms/step - loss: 2443.7351 - val_loss: 2402.0918 - val_y0: 0.6172 - val_y1: -1.2983 - val_ate_afte_scaled: -1.9154 - lr: 5.0000e-04\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2895.8421 — ite: 3.8191  — ate: 1.1748 — pehe: 4.0734 \n",
      "3/3 [==============================] - 1s 268ms/step - loss: 2479.8816 - val_loss: 2205.1768 - val_y0: 0.7036 - val_y1: -0.9158 - val_ate_afte_scaled: -1.6194 - lr: 5.0000e-04\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2907.2920 — ite: 3.8090  — ate: 1.1824 — pehe: 3.8337 \n",
      "3/3 [==============================] - 1s 258ms/step - loss: 2499.6470 - val_loss: 2208.7915 - val_y0: 0.7987 - val_y1: -0.7818 - val_ate_afte_scaled: -1.5805 - lr: 5.0000e-04\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2996.7396 — ite: 3.9511  — ate: 1.2144 — pehe: 4.0676 \n",
      "3/3 [==============================] - 1s 233ms/step - loss: 2571.6403 - val_loss: 2238.5151 - val_y0: 0.8526 - val_y1: -0.9160 - val_ate_afte_scaled: -1.7687 - lr: 5.0000e-04\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 3004.9391\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      " — ite: 3.8859  — ate: 0.9058 — pehe: 3.8431 \n",
      "3/3 [==============================] - 1s 260ms/step - loss: 2587.0428 - val_loss: 2189.0647 - val_y0: 0.9234 - val_y1: -1.1559 - val_ate_afte_scaled: -2.0793 - lr: 5.0000e-04\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2814.9676 — ite: 3.9887  — ate: 0.7020 — pehe: 3.8877 \n",
      "3/3 [==============================] - 1s 233ms/step - loss: 2411.9572 - val_loss: 2155.6848 - val_y0: 1.1675 - val_y1: -1.2831 - val_ate_afte_scaled: -2.4506 - lr: 2.5000e-04\n",
      "Epoch 11/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3839.3167 — ite: 3.9884  — ate: 0.6237 — pehe: 3.9815 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: 2523.4899 - val_loss: 2345.5510 - val_y0: 1.0470 - val_y1: -1.2949 - val_ate_afte_scaled: -2.3419 - lr: 2.5000e-04\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2938.2576 — ite: 3.8072  — ate: 0.5062 — pehe: 3.8428 \n",
      "3/3 [==============================] - 1s 236ms/step - loss: 2527.0392 - val_loss: 2120.8596 - val_y0: 0.9416 - val_y1: -1.1289 - val_ate_afte_scaled: -2.0705 - lr: 2.5000e-04\n",
      "Epoch 13/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3952.0410 — ite: 3.7810  — ate: 0.7587 — pehe: 3.9870 \n",
      "3/3 [==============================] - 0s 220ms/step - loss: 2505.4427 - val_loss: 2195.2336 - val_y0: 0.9228 - val_y1: -0.9472 - val_ate_afte_scaled: -1.8700 - lr: 2.5000e-04\n",
      "Epoch 14/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3682.4680\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      " — ite: 3.8122  — ate: 0.6016 — pehe: 4.0234 \n",
      "3/3 [==============================] - 0s 218ms/step - loss: 2498.9354 - val_loss: 2146.0237 - val_y0: 0.9407 - val_y1: -1.1097 - val_ate_afte_scaled: -2.0505 - lr: 2.5000e-04\n",
      "Epoch 15/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3683.5034 — ite: 3.8062  — ate: 0.4928 — pehe: 3.8676 \n",
      "3/3 [==============================] - 0s 232ms/step - loss: 2436.5673 - val_loss: 2090.3013 - val_y0: 1.0523 - val_y1: -0.9825 - val_ate_afte_scaled: -2.0348 - lr: 1.2500e-04\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2849.0266 — ite: 3.7886  — ate: 0.2017 — pehe: 3.8102 \n",
      "3/3 [==============================] - 1s 236ms/step - loss: 2458.9670 - val_loss: 2084.0430 - val_y0: 0.8864 - val_y1: -0.9619 - val_ate_afte_scaled: -1.8483 - lr: 1.2500e-04\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2855.2046 — ite: 3.8698  — ate: 0.3789 — pehe: 3.9088 \n",
      "3/3 [==============================] - 0s 228ms/step - loss: 2453.4324 - val_loss: 2158.0869 - val_y0: 1.0851 - val_y1: -0.9303 - val_ate_afte_scaled: -2.0154 - lr: 1.2500e-04\n",
      "Epoch 18/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3651.8625 — ite: 3.7365  — ate: 0.6112 — pehe: 3.6894 \n",
      "3/3 [==============================] - 0s 206ms/step - loss: 2481.5718 - val_loss: 2196.1609 - val_y0: 1.0448 - val_y1: -0.8924 - val_ate_afte_scaled: -1.9372 - lr: 1.2500e-04\n",
      "Epoch 19/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3740.5337\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      " — ite: 3.8257  — ate: 0.5029 — pehe: 3.7226 \n",
      "3/3 [==============================] - 0s 223ms/step - loss: 2490.2691 - val_loss: 2115.5498 - val_y0: 1.1133 - val_y1: -1.1670 - val_ate_afte_scaled: -2.2803 - lr: 1.2500e-04\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2896.3163 — ite: 3.9296  — ate: 0.2904 — pehe: 3.7580 \n",
      "3/3 [==============================] - 1s 248ms/step - loss: 2505.8652 - val_loss: 2093.8076 - val_y0: 1.1143 - val_y1: -1.0832 - val_ate_afte_scaled: -2.1975 - lr: 6.2500e-05\n",
      "Epoch 21/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3655.1345 — ite: 3.9141  — ate: 0.6261 — pehe: 3.9036 \n",
      "3/3 [==============================] - 0s 229ms/step - loss: 2491.6681 - val_loss: 2289.9753 - val_y0: 1.1609 - val_y1: -0.9778 - val_ate_afte_scaled: -2.1387 - lr: 6.2500e-05\n",
      "Epoch 22/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3505.9578 — ite: 3.8118  — ate: 0.6003 — pehe: 3.9574 \n",
      "3/3 [==============================] - 0s 214ms/step - loss: 2393.8885 - val_loss: 2211.1304 - val_y0: 1.3103 - val_y1: -1.0234 - val_ate_afte_scaled: -2.3336 - lr: 6.2500e-05\n",
      "Epoch 23/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3968.5071 — ite: 3.8072  — ate: 0.7980 — pehe: 3.8905 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: 2535.5164 - val_loss: 2258.8713 - val_y0: 1.2713 - val_y1: -1.0407 - val_ate_afte_scaled: -2.3120 - lr: 6.2500e-05\n",
      "Epoch 24/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3747.8184\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      " — ite: 3.8518  — ate: 0.4267 — pehe: 3.8751 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: 2502.0826 - val_loss: 2249.7522 - val_y0: 1.2787 - val_y1: -0.9758 - val_ate_afte_scaled: -2.2545 - lr: 6.2500e-05\n",
      "Epoch 25/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3955.1628 — ite: 3.8161  — ate: 0.3619 — pehe: 3.7415 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: 2540.0996 - val_loss: 2313.2141 - val_y0: 1.4713 - val_y1: -1.0755 - val_ate_afte_scaled: -2.5468 - lr: 3.1250e-05\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2888.2537 — ite: 3.7339  — ate: 0.4783 — pehe: 3.6647 \n",
      "3/3 [==============================] - 0s 222ms/step - loss: 2449.7322 - val_loss: 2195.3594 - val_y0: 1.3509 - val_y1: -1.2181 - val_ate_afte_scaled: -2.5690 - lr: 3.1250e-05\n",
      "Epoch 27/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3836.6409 — ite: 3.8072  — ate: 0.3624 — pehe: 3.7934 \n",
      "3/3 [==============================] - 0s 218ms/step - loss: 2466.5665 - val_loss: 2297.5596 - val_y0: 1.3940 - val_y1: -1.1386 - val_ate_afte_scaled: -2.5326 - lr: 3.1250e-05\n",
      "Epoch 28/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3794.1133 — ite: 3.7495  — ate: 0.6314 — pehe: 3.7583 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: 2475.7780 - val_loss: 2085.2043 - val_y0: 1.4864 - val_y1: -1.1358 - val_ate_afte_scaled: -2.6223 - lr: 3.1250e-05\n",
      "Epoch 29/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3482.1965\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      " — ite: 3.8740  — ate: 0.5954 — pehe: 3.9002 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: 2310.1924 - val_loss: 2105.5640 - val_y0: 1.5406 - val_y1: -0.9927 - val_ate_afte_scaled: -2.5333 - lr: 3.1250e-05\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2892.0505 — ite: 3.7926  — ate: 0.4181 — pehe: 3.7696 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: 2496.8911 - val_loss: 2200.6345 - val_y0: 1.5029 - val_y1: -1.1942 - val_ate_afte_scaled: -2.6970 - lr: 1.5625e-05\n",
      "Epoch 31/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3608.5339 — ite: 3.8147  — ate: 0.6667 — pehe: 3.8821 \n",
      "3/3 [==============================] - 0s 196ms/step - loss: 2446.0508 - val_loss: 2238.9014 - val_y0: 1.3948 - val_y1: -1.1713 - val_ate_afte_scaled: -2.5662 - lr: 1.5625e-05\n",
      "Epoch 32/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3470.3533 — ite: 3.7299  — ate: 0.4268 — pehe: 3.7631 \n",
      "3/3 [==============================] - 0s 197ms/step - loss: 2404.3827 - val_loss: 2276.0388 - val_y0: 1.6037 - val_y1: -1.1410 - val_ate_afte_scaled: -2.7447 - lr: 1.5625e-05\n",
      "Epoch 33/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3557.1787 — ite: 3.7285  — ate: 0.5398 — pehe: 3.7923 \n",
      "3/3 [==============================] - 0s 192ms/step - loss: 2420.0861 - val_loss: 2056.0542 - val_y0: 1.3022 - val_y1: -1.2094 - val_ate_afte_scaled: -2.5116 - lr: 1.5625e-05\n",
      "Epoch 34/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3775.7100\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      " — ite: 3.8243  — ate: 0.4898 — pehe: 3.7161 \n",
      "3/3 [==============================] - 0s 191ms/step - loss: 2458.1122 - val_loss: 2242.9419 - val_y0: 1.3727 - val_y1: -1.2108 - val_ate_afte_scaled: -2.5835 - lr: 1.5625e-05\n",
      "Epoch 35/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3512.1367 — ite: 3.7934  — ate: 0.3089 — pehe: 3.8571 \n",
      "3/3 [==============================] - 0s 194ms/step - loss: 2414.4054 - val_loss: 1962.8285 - val_y0: 1.5048 - val_y1: -1.2744 - val_ate_afte_scaled: -2.7792 - lr: 7.8125e-06\n",
      "Epoch 36/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3648.0081 — ite: 3.7996  — ate: 0.2687 — pehe: 3.7354 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: 2504.1609 - val_loss: 2302.2747 - val_y0: 1.5729 - val_y1: -1.2683 - val_ate_afte_scaled: -2.8412 - lr: 7.8125e-06\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2874.1336 — ite: 3.7702  — ate: 0.4595 — pehe: 3.7958 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: 2461.7177 - val_loss: 2205.3579 - val_y0: 1.6658 - val_y1: -1.2237 - val_ate_afte_scaled: -2.8896 - lr: 7.8125e-06\n",
      "Epoch 38/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3747.2727 — ite: 3.8126  — ate: 0.5423 — pehe: 3.7706 \n",
      "3/3 [==============================] - 0s 185ms/step - loss: 2442.8406 - val_loss: 2054.3838 - val_y0: 1.5366 - val_y1: -1.3631 - val_ate_afte_scaled: -2.8996 - lr: 7.8125e-06\n",
      "Epoch 39/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3713.2468\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      " — ite: 3.7964  — ate: 0.5526 — pehe: 3.7221 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: 2373.1689 - val_loss: 2197.6384 - val_y0: 1.4390 - val_y1: -1.2566 - val_ate_afte_scaled: -2.6957 - lr: 7.8125e-06\n",
      "Epoch 40/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3496.9009 — ite: 3.8881  — ate: 0.3920 — pehe: 3.8463 \n",
      "3/3 [==============================] - 0s 192ms/step - loss: 2425.0796 - val_loss: 2164.0886 - val_y0: 1.5654 - val_y1: -1.0235 - val_ate_afte_scaled: -2.5888 - lr: 3.9063e-06\n",
      "Epoch 41/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3655.1208 — ite: 3.9264  — ate: 0.5810 — pehe: 3.7486 \n",
      "3/3 [==============================] - 0s 188ms/step - loss: 2423.3494 - val_loss: 2266.5569 - val_y0: 1.3728 - val_y1: -1.3444 - val_ate_afte_scaled: -2.7172 - lr: 3.9063e-06\n",
      "Epoch 42/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3594.3611 — ite: 3.8870  — ate: 0.3645 — pehe: 3.6970 \n",
      "3/3 [==============================] - 0s 187ms/step - loss: 2441.2318 - val_loss: 2142.7090 - val_y0: 1.4097 - val_y1: -1.3431 - val_ate_afte_scaled: -2.7528 - lr: 3.9063e-06\n",
      "Epoch 43/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3739.7776 — ite: 3.8372  — ate: 0.6157 — pehe: 3.9654 \n",
      "3/3 [==============================] - 0s 188ms/step - loss: 2482.7534 - val_loss: 2272.8088 - val_y0: 1.4794 - val_y1: -1.2737 - val_ate_afte_scaled: -2.7531 - lr: 3.9063e-06\n",
      "Epoch 44/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3538.6782\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      " — ite: 3.7762  — ate: 0.4246 — pehe: 3.7297 \n",
      "3/3 [==============================] - 0s 204ms/step - loss: 2459.5465 - val_loss: 2236.3584 - val_y0: 1.4835 - val_y1: -1.3367 - val_ate_afte_scaled: -2.8201 - lr: 3.9063e-06\n",
      "Epoch 45/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3704.0044 — ite: 3.9011  — ate: 0.6489 — pehe: 3.8195 \n",
      "3/3 [==============================] - 0s 196ms/step - loss: 2487.5466 - val_loss: 2030.0585 - val_y0: 1.4636 - val_y1: -1.2116 - val_ate_afte_scaled: -2.6752 - lr: 1.9531e-06\n",
      "Epoch 46/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3904.4487 — ite: 3.6711  — ate: 0.4382 — pehe: 3.7188 \n",
      "3/3 [==============================] - 0s 198ms/step - loss: 2441.0486 - val_loss: 2098.6335 - val_y0: 1.4115 - val_y1: -1.1499 - val_ate_afte_scaled: -2.5613 - lr: 1.9531e-06\n",
      "Epoch 47/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3625.0061 — ite: 3.7795  — ate: 0.4579 — pehe: 3.7470 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: 2426.3032 - val_loss: 2222.0752 - val_y0: 1.4310 - val_y1: -1.3066 - val_ate_afte_scaled: -2.7376 - lr: 1.9531e-06\n",
      "Epoch 48/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3503.5820 — ite: 3.8534  — ate: 0.6806 — pehe: 3.8292 \n",
      "3/3 [==============================] - 0s 188ms/step - loss: 2497.5468 - val_loss: 2156.8472 - val_y0: 1.5465 - val_y1: -1.1259 - val_ate_afte_scaled: -2.6724 - lr: 1.9531e-06\n",
      "Epoch 49/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3911.2932\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      " — ite: 3.8025  — ate: 0.3743 — pehe: 3.7920 \n",
      "3/3 [==============================] - 0s 204ms/step - loss: 2530.5827 - val_loss: 2250.7061 - val_y0: 1.5580 - val_y1: -1.4319 - val_ate_afte_scaled: -2.9899 - lr: 1.9531e-06\n",
      "Epoch 50/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3549.4131 — ite: 3.8212  — ate: 0.5661 — pehe: 3.6516 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: 2429.0250 - val_loss: 2171.7686 - val_y0: 1.4401 - val_y1: -1.1083 - val_ate_afte_scaled: -2.5484 - lr: 9.7656e-07\n",
      "Epoch 51/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3857.1365 — ite: 3.8579  — ate: 0.3741 — pehe: 3.8119 \n",
      "3/3 [==============================] - 0s 197ms/step - loss: 2506.7819 - val_loss: 2008.7299 - val_y0: 1.4040 - val_y1: -1.3262 - val_ate_afte_scaled: -2.7302 - lr: 9.7656e-07\n",
      "Epoch 52/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3557.6194 — ite: 3.8140  — ate: 0.5986 — pehe: 3.7758 \n",
      "3/3 [==============================] - 0s 198ms/step - loss: 2372.9195 - val_loss: 2117.2070 - val_y0: 1.4355 - val_y1: -1.2050 - val_ate_afte_scaled: -2.6405 - lr: 9.7656e-07\n",
      "Epoch 53/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3902.3484 — ite: 3.8662  — ate: 0.4249 — pehe: 3.8326 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: 2537.1307 - val_loss: 1961.3115 - val_y0: 1.4410 - val_y1: -1.2387 - val_ate_afte_scaled: -2.6797 - lr: 9.7656e-07\n",
      "Epoch 54/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3742.4978\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      " — ite: 3.7758  — ate: 0.4808 — pehe: 3.8003 \n",
      "3/3 [==============================] - 0s 205ms/step - loss: 2530.9547 - val_loss: 2140.7820 - val_y0: 1.5479 - val_y1: -1.1021 - val_ate_afte_scaled: -2.6500 - lr: 9.7656e-07\n",
      "Epoch 55/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3678.3467 — ite: 3.8247  — ate: 0.6576 — pehe: 3.7453 \n",
      "3/3 [==============================] - 0s 195ms/step - loss: 2457.3408 - val_loss: 2187.1633 - val_y0: 1.5388 - val_y1: -1.2164 - val_ate_afte_scaled: -2.7552 - lr: 4.8828e-07\n",
      "Epoch 56/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3756.7998 — ite: 3.7846  — ate: 0.4391 — pehe: 3.9127 \n",
      "3/3 [==============================] - 0s 196ms/step - loss: 2512.8062 - val_loss: 2113.7810 - val_y0: 1.5566 - val_y1: -1.1910 - val_ate_afte_scaled: -2.7476 - lr: 4.8828e-07\n",
      "Epoch 57/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3695.7310 — ite: 3.8046  — ate: 0.5433 — pehe: 3.8893 \n",
      "3/3 [==============================] - 0s 194ms/step - loss: 2397.7423 - val_loss: 2285.2065 - val_y0: 1.6422 - val_y1: -1.3006 - val_ate_afte_scaled: -2.9428 - lr: 4.8828e-07\n",
      "Epoch 58/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3653.3464 — ite: 3.8012  — ate: 0.5064 — pehe: 3.8034 \n",
      "3/3 [==============================] - 0s 204ms/step - loss: 2406.4072 - val_loss: 2137.7200 - val_y0: 1.5042 - val_y1: -1.2679 - val_ate_afte_scaled: -2.7721 - lr: 4.8828e-07\n",
      "Epoch 59/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3688.9365\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      " — ite: 3.8518  — ate: 0.5340 — pehe: 3.7120 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: 2489.9083 - val_loss: 2081.5916 - val_y0: 1.5717 - val_y1: -1.3034 - val_ate_afte_scaled: -2.8751 - lr: 4.8828e-07\n",
      "Epoch 60/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3805.0947 — ite: 3.7868  — ate: 0.4530 — pehe: 3.8050 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: 2409.9839 - val_loss: 2309.1392 - val_y0: 1.5026 - val_y1: -1.2400 - val_ate_afte_scaled: -2.7427 - lr: 2.4414e-07\n",
      "Epoch 61/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3750.7031 — ite: 3.8451  — ate: 0.7811 — pehe: 3.7401 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: 2410.9357 - val_loss: 2163.7063 - val_y0: 1.4048 - val_y1: -1.3346 - val_ate_afte_scaled: -2.7395 - lr: 2.4414e-07\n",
      "Epoch 62/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3830.9067 — ite: 3.7144  — ate: 0.3275 — pehe: 3.6996 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: 2534.4393 - val_loss: 2270.6704 - val_y0: 1.3531 - val_y1: -1.3588 - val_ate_afte_scaled: -2.7119 - lr: 2.4414e-07\n",
      "Epoch 63/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3789.0779 — ite: 3.7742  — ate: 0.6507 — pehe: 3.6940 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: 2486.0895 - val_loss: 2152.1733 - val_y0: 1.3895 - val_y1: -1.3611 - val_ate_afte_scaled: -2.7505 - lr: 2.4414e-07\n",
      "Epoch 64/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3589.1541\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      " — ite: 3.8073  — ate: 0.5199 — pehe: 3.7726 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: 2481.5340 - val_loss: 2186.0925 - val_y0: 1.4733 - val_y1: -1.2122 - val_ate_afte_scaled: -2.6856 - lr: 2.4414e-07\n",
      "Epoch 65/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3855.3269 — ite: 3.8280  — ate: 0.4754 — pehe: 3.6999 \n",
      "3/3 [==============================] - 0s 193ms/step - loss: 2524.8196 - val_loss: 2074.2405 - val_y0: 1.4034 - val_y1: -1.1333 - val_ate_afte_scaled: -2.5367 - lr: 1.2207e-07\n",
      "Epoch 66/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3631.1565 — ite: 3.8357  — ate: 0.5266 — pehe: 3.8697 \n",
      "3/3 [==============================] - 0s 196ms/step - loss: 2445.3501 - val_loss: 2315.5435 - val_y0: 1.4840 - val_y1: -1.3031 - val_ate_afte_scaled: -2.7872 - lr: 1.2207e-07\n",
      "Epoch 67/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3635.1733 — ite: 3.8337  — ate: 0.5683 — pehe: 3.8528 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: 2408.0082 - val_loss: 2195.9836 - val_y0: 1.3660 - val_y1: -1.3178 - val_ate_afte_scaled: -2.6838 - lr: 1.2207e-07\n",
      "Epoch 68/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3646.4453 — ite: 3.7444  — ate: 0.2789 — pehe: 3.6145 \n",
      "3/3 [==============================] - 0s 196ms/step - loss: 2492.1439 - val_loss: 2265.7422 - val_y0: 1.4914 - val_y1: -1.3916 - val_ate_afte_scaled: -2.8830 - lr: 1.2207e-07\n",
      "Epoch 69/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3626.7651\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      " — ite: 3.8321  — ate: 0.6568 — pehe: 3.8542 \n",
      "3/3 [==============================] - 0s 193ms/step - loss: 2439.0994 - val_loss: 2326.3994 - val_y0: 1.5745 - val_y1: -1.2161 - val_ate_afte_scaled: -2.7906 - lr: 1.2207e-07\n",
      "Epoch 70/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3393.8210 — ite: 3.7709  — ate: 0.5026 — pehe: 3.6864 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: 2404.8849 - val_loss: 2102.2820 - val_y0: 1.5222 - val_y1: -1.1967 - val_ate_afte_scaled: -2.7189 - lr: 6.1035e-08\n",
      "Epoch 71/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3975.0869 — ite: 3.7197  — ate: 0.8086 — pehe: 3.6402 \n",
      "3/3 [==============================] - 0s 190ms/step - loss: 2579.2137 - val_loss: 2235.6216 - val_y0: 1.5004 - val_y1: -1.2742 - val_ate_afte_scaled: -2.7746 - lr: 6.1035e-08\n",
      "Epoch 72/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3846.2080 — ite: 3.7905  — ate: 0.4031 — pehe: 3.7152 \n",
      "3/3 [==============================] - 0s 193ms/step - loss: 2445.7939 - val_loss: 2170.6165 - val_y0: 1.5376 - val_y1: -1.2796 - val_ate_afte_scaled: -2.8172 - lr: 6.1035e-08\n",
      "Epoch 73/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3539.1892 — ite: 3.8605  — ate: 0.4940 — pehe: 3.8991 \n",
      "3/3 [==============================] - 0s 195ms/step - loss: 2409.1879 - val_loss: 2191.2334 - val_y0: 1.5896 - val_y1: -1.3124 - val_ate_afte_scaled: -2.9021 - lr: 6.1035e-08\n",
      "Epoch 74/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3537.7954\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      " — ite: 3.8236  — ate: 0.3074 — pehe: 3.7615 \n",
      "3/3 [==============================] - 0s 196ms/step - loss: 2476.3614 - val_loss: 2122.2039 - val_y0: 1.4508 - val_y1: -1.1835 - val_ate_afte_scaled: -2.6343 - lr: 6.1035e-08\n",
      "Epoch 75/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3673.3630 — ite: 3.8049  — ate: 0.8812 — pehe: 3.7465 \n",
      "3/3 [==============================] - 0s 204ms/step - loss: 2506.8497 - val_loss: 2142.4998 - val_y0: 1.4436 - val_y1: -1.2412 - val_ate_afte_scaled: -2.6848 - lr: 3.0518e-08\n",
      "Epoch 76/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3512.3972 — ite: 3.7441  — ate: 0.2564 — pehe: 3.6129 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: 2285.1299 - val_loss: 2170.9949 - val_y0: 1.5603 - val_y1: -1.0656 - val_ate_afte_scaled: -2.6259 - lr: 3.0518e-08\n",
      "Epoch 77/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3433.7266 — ite: 3.9152  — ate: 0.6193 — pehe: 3.9335 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: 2394.6450 - val_loss: 2305.4878 - val_y0: 1.4586 - val_y1: -1.3201 - val_ate_afte_scaled: -2.7787 - lr: 3.0518e-08\n",
      "Epoch 78/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3759.3855 — ite: 3.9090  — ate: 0.5431 — pehe: 3.8293 \n",
      "3/3 [==============================] - 0s 197ms/step - loss: 2521.1141 - val_loss: 2219.9473 - val_y0: 1.5284 - val_y1: -1.3308 - val_ate_afte_scaled: -2.8592 - lr: 3.0518e-08\n",
      "Epoch 79/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3714.5413\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      " — ite: 3.9032  — ate: 0.5269 — pehe: 3.8461 \n",
      "3/3 [==============================] - 0s 206ms/step - loss: 2399.8791 - val_loss: 2061.4790 - val_y0: 1.4711 - val_y1: -1.3996 - val_ate_afte_scaled: -2.8706 - lr: 3.0518e-08\n",
      "Epoch 80/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3739.6633 — ite: 3.8131  — ate: 0.4599 — pehe: 3.6585 \n",
      "3/3 [==============================] - 0s 193ms/step - loss: 2466.0899 - val_loss: 2127.1023 - val_y0: 1.5254 - val_y1: -1.2390 - val_ate_afte_scaled: -2.7645 - lr: 1.5259e-08\n",
      "Epoch 81/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3772.9639 — ite: 3.9143  — ate: 0.4111 — pehe: 3.9314 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: 2475.3337 - val_loss: 2095.2310 - val_y0: 1.5410 - val_y1: -1.2767 - val_ate_afte_scaled: -2.8178 - lr: 1.5259e-08\n",
      "Epoch 82/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3556.1152 — ite: 3.8321  — ate: 0.6721 — pehe: 3.8811 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: 2424.0790 - val_loss: 2206.7263 - val_y0: 1.4879 - val_y1: -1.3230 - val_ate_afte_scaled: -2.8109 - lr: 1.5259e-08\n",
      "Epoch 83/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3633.3687 — ite: 3.8196  — ate: 0.5631 — pehe: 3.7897 \n",
      "3/3 [==============================] - 0s 214ms/step - loss: 2439.6451 - val_loss: 2140.0344 - val_y0: 1.5772 - val_y1: -1.2002 - val_ate_afte_scaled: -2.7774 - lr: 1.5259e-08\n",
      "Epoch 84/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3683.6914\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
      " — ite: 3.9128  — ate: 0.6689 — pehe: 3.8330 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: 2396.0737 - val_loss: 2123.1516 - val_y0: 1.5502 - val_y1: -1.3447 - val_ate_afte_scaled: -2.8948 - lr: 1.5259e-08\n",
      "Epoch 85/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3739.2351 — ite: 3.7443  — ate: 0.2536 — pehe: 3.7546 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: 2464.4443 - val_loss: 2061.3608 - val_y0: 1.5930 - val_y1: -1.3184 - val_ate_afte_scaled: -2.9114 - lr: 7.6294e-09\n",
      "Epoch 86/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3591.7202 — ite: 3.8455  — ate: 0.4837 — pehe: 3.9940 \n",
      "3/3 [==============================] - 0s 206ms/step - loss: 2400.2809 - val_loss: 2396.7732 - val_y0: 1.3463 - val_y1: -1.2690 - val_ate_afte_scaled: -2.6153 - lr: 7.6294e-09\n",
      "Epoch 87/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3821.3105 — ite: 3.8802  — ate: 0.5226 — pehe: 3.8543 \n",
      "3/3 [==============================] - 0s 207ms/step - loss: 2418.2008 - val_loss: 2172.8870 - val_y0: 1.6210 - val_y1: -1.2679 - val_ate_afte_scaled: -2.8888 - lr: 7.6294e-09\n",
      "Epoch 88/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3734.5933 — ite: 3.8183  — ate: 0.3902 — pehe: 3.8157 \n",
      "3/3 [==============================] - 0s 196ms/step - loss: 2415.4609 - val_loss: 2296.4656 - val_y0: 1.5549 - val_y1: -1.2209 - val_ate_afte_scaled: -2.7758 - lr: 7.6294e-09\n",
      "Epoch 89/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3722.9973\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
      " — ite: 3.8405  — ate: 0.6410 — pehe: 3.7777 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: 2535.4525 - val_loss: 2224.5149 - val_y0: 1.4373 - val_y1: -1.2196 - val_ate_afte_scaled: -2.6569 - lr: 7.6294e-09\n",
      "Epoch 90/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3908.5715 — ite: 3.8339  — ate: 0.3531 — pehe: 3.8023 \n",
      "3/3 [==============================] - 0s 210ms/step - loss: 2552.6775 - val_loss: 2130.4678 - val_y0: 1.5374 - val_y1: -1.2644 - val_ate_afte_scaled: -2.8018 - lr: 3.8147e-09\n",
      "Epoch 91/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3654.5918 — ite: 3.8409  — ate: 0.3336 — pehe: 3.7595 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: 2498.1345 - val_loss: 2192.5813 - val_y0: 1.6519 - val_y1: -1.2758 - val_ate_afte_scaled: -2.9277 - lr: 3.8147e-09\n",
      "Epoch 92/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2866.1983 — ite: 3.8535  — ate: 0.7936 — pehe: 3.8752 \n",
      "3/3 [==============================] - 0s 223ms/step - loss: 2444.4474 - val_loss: 2214.2827 - val_y0: 1.5373 - val_y1: -1.3433 - val_ate_afte_scaled: -2.8806 - lr: 3.8147e-09\n",
      "Epoch 93/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3719.4048 — ite: 3.7718  — ate: 0.4768 — pehe: 3.7887 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: 2454.7049 - val_loss: 2273.8452 - val_y0: 1.4775 - val_y1: -1.3884 - val_ate_afte_scaled: -2.8659 - lr: 3.8147e-09\n",
      "Epoch 94/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3671.7930\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
      " — ite: 3.8946  — ate: 0.5593 — pehe: 3.9183 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: 2461.8371 - val_loss: 1939.0409 - val_y0: 1.5913 - val_y1: -1.1301 - val_ate_afte_scaled: -2.7214 - lr: 3.8147e-09\n",
      "Epoch 95/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3634.5872 — ite: 3.7905  — ate: 0.3481 — pehe: 3.6852 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: 2426.6677 - val_loss: 1987.0421 - val_y0: 1.4185 - val_y1: -1.2772 - val_ate_afte_scaled: -2.6957 - lr: 1.9073e-09\n",
      "Epoch 96/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3532.9829 — ite: 3.8169  — ate: 0.6816 — pehe: 3.9347 \n",
      "3/3 [==============================] - 0s 210ms/step - loss: 2444.1314 - val_loss: 2269.3645 - val_y0: 1.5863 - val_y1: -1.2070 - val_ate_afte_scaled: -2.7934 - lr: 1.9073e-09\n",
      "Epoch 97/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3478.9304 — ite: 3.7562  — ate: 0.6122 — pehe: 3.8416 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: 2325.6427 - val_loss: 2079.9600 - val_y0: 1.3189 - val_y1: -1.1332 - val_ate_afte_scaled: -2.4521 - lr: 1.9073e-09\n",
      "Epoch 98/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3587.6470 — ite: 3.8752  — ate: 0.4256 — pehe: 3.7749 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: 2384.6326 - val_loss: 2160.6531 - val_y0: 1.5231 - val_y1: -1.4410 - val_ate_afte_scaled: -2.9641 - lr: 1.9073e-09\n",
      "Epoch 99/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3758.0056\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
      " — ite: 3.7753  — ate: 0.5339 — pehe: 3.7897 \n",
      "3/3 [==============================] - 0s 197ms/step - loss: 2487.6531 - val_loss: 2367.7778 - val_y0: 1.4891 - val_y1: -1.2212 - val_ate_afte_scaled: -2.7103 - lr: 1.9073e-09\n",
      "Epoch 100/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3654.8237 — ite: 3.8612  — ate: 0.6809 — pehe: 3.8754 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: 2484.4500 - val_loss: 2348.6997 - val_y0: 1.4236 - val_y1: -1.1408 - val_ate_afte_scaled: -2.5645 - lr: 9.5367e-10\n",
      "Epoch 101/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3621.3130 — ite: 3.8232  — ate: 0.6518 — pehe: 3.9245 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: 2438.6893 - val_loss: 2175.1628 - val_y0: 1.4392 - val_y1: -1.2994 - val_ate_afte_scaled: -2.7386 - lr: 9.5367e-10\n",
      "Epoch 102/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3485.8469 — ite: 3.8406  — ate: 0.4693 — pehe: 3.5318 \n",
      "3/3 [==============================] - 0s 219ms/step - loss: 2335.7875 - val_loss: 2158.6716 - val_y0: 1.5999 - val_y1: -1.1954 - val_ate_afte_scaled: -2.7953 - lr: 9.5367e-10\n",
      "Epoch 103/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3981.9319 — ite: 3.9445  — ate: 0.5182 — pehe: 3.8374 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: 2505.5099 - val_loss: 2074.9084 - val_y0: 1.5335 - val_y1: -1.3139 - val_ate_afte_scaled: -2.8474 - lr: 9.5367e-10\n",
      "Epoch 104/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3664.3237\n",
      "Epoch 00104: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
      " — ite: 3.8491  — ate: 0.2025 — pehe: 3.7859 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: 2509.7056 - val_loss: 2269.7261 - val_y0: 1.5338 - val_y1: -1.2273 - val_ate_afte_scaled: -2.7612 - lr: 9.5367e-10\n",
      "Epoch 105/300\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 3731.1663 — ite: 3.8333  — ate: 0.3940 — pehe: 3.9935 \n",
      "3/3 [==============================] - 1s 259ms/step - loss: 2517.9507 - val_loss: 2214.3477 - val_y0: 1.4369 - val_y1: -1.1984 - val_ate_afte_scaled: -2.6353 - lr: 4.7684e-10\n",
      "Epoch 106/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3802.9614 — ite: 3.9201  — ate: 0.7836 — pehe: 4.0836 \n",
      "3/3 [==============================] - 0s 214ms/step - loss: 2525.0314 - val_loss: 2026.0470 - val_y0: 1.4690 - val_y1: -1.2861 - val_ate_afte_scaled: -2.7551 - lr: 4.7684e-10\n",
      "Epoch 107/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3648.7493 — ite: 3.7272  — ate: 0.5571 — pehe: 3.6142 \n",
      "3/3 [==============================] - 0s 221ms/step - loss: 2494.2275 - val_loss: 2206.6589 - val_y0: 1.4345 - val_y1: -1.3267 - val_ate_afte_scaled: -2.7612 - lr: 4.7684e-10\n",
      "Epoch 108/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3747.9468 — ite: 3.8171  — ate: 0.4903 — pehe: 3.8063 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: 2522.9786 - val_loss: 2094.0576 - val_y0: 1.6159 - val_y1: -1.3047 - val_ate_afte_scaled: -2.9206 - lr: 4.7684e-10\n",
      "Epoch 109/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3661.3838\n",
      "Epoch 00109: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
      " — ite: 3.7879  — ate: 0.5489 — pehe: 3.8960 \n",
      "3/3 [==============================] - 0s 229ms/step - loss: 2423.3041 - val_loss: 2131.8933 - val_y0: 1.5813 - val_y1: -1.2851 - val_ate_afte_scaled: -2.8664 - lr: 4.7684e-10\n",
      "Epoch 110/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3581.2234 — ite: 3.7644  — ate: 0.6275 — pehe: 3.7711 \n",
      "3/3 [==============================] - 0s 205ms/step - loss: 2488.2813 - val_loss: 2148.8049 - val_y0: 1.4605 - val_y1: -1.0227 - val_ate_afte_scaled: -2.4832 - lr: 2.3842e-10\n",
      "Epoch 111/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3848.9360 — ite: 3.8403  — ate: 0.6115 — pehe: 3.9180 \n",
      "3/3 [==============================] - 0s 226ms/step - loss: 2450.4379 - val_loss: 2048.8901 - val_y0: 1.4820 - val_y1: -1.3593 - val_ate_afte_scaled: -2.8413 - lr: 2.3842e-10\n",
      "Epoch 112/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3510.0889 — ite: 3.8173  — ate: 0.3308 — pehe: 3.7535 \n",
      "3/3 [==============================] - 0s 210ms/step - loss: 2411.8499 - val_loss: 1954.7477 - val_y0: 1.5670 - val_y1: -1.3379 - val_ate_afte_scaled: -2.9049 - lr: 2.3842e-10\n",
      "Epoch 113/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3960.3904 — ite: 3.8497  — ate: 0.3345 — pehe: 3.7864 \n",
      "3/3 [==============================] - 0s 228ms/step - loss: 2485.6326 - val_loss: 2229.7068 - val_y0: 1.5912 - val_y1: -1.1787 - val_ate_afte_scaled: -2.7700 - lr: 2.3842e-10\n",
      "Epoch 114/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3736.9521\n",
      "Epoch 00114: ReduceLROnPlateau reducing learning rate to 1.1920929521291868e-10.\n",
      " — ite: 3.8321  — ate: 0.4799 — pehe: 3.9478 \n",
      "3/3 [==============================] - 0s 232ms/step - loss: 2493.4395 - val_loss: 2203.2727 - val_y0: 1.5908 - val_y1: -1.1782 - val_ate_afte_scaled: -2.7690 - lr: 2.3842e-10\n",
      "Epoch 115/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3626.4814 — ite: 4.0054  — ate: 0.6648 — pehe: 4.0367 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: 2428.4671 - val_loss: 2115.6553 - val_y0: 1.4641 - val_y1: -1.2429 - val_ate_afte_scaled: -2.7070 - lr: 1.1921e-10\n",
      "Epoch 116/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3687.5491 — ite: 3.9089  — ate: 0.6345 — pehe: 3.9657 \n",
      "3/3 [==============================] - 0s 210ms/step - loss: 2420.2991 - val_loss: 2247.8682 - val_y0: 1.5455 - val_y1: -1.2436 - val_ate_afte_scaled: -2.7891 - lr: 1.1921e-10\n",
      "Epoch 117/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3530.6138 — ite: 3.7470  — ate: 0.5412 — pehe: 3.7073 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: 2349.9210 - val_loss: 2186.0535 - val_y0: 1.5155 - val_y1: -1.2020 - val_ate_afte_scaled: -2.7174 - lr: 1.1921e-10\n",
      "Epoch 118/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3743.3120 — ite: 3.8715  — ate: 0.5323 — pehe: 3.8063 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: 2413.7624 - val_loss: 2266.6689 - val_y0: 1.4641 - val_y1: -1.1157 - val_ate_afte_scaled: -2.5798 - lr: 1.1921e-10\n",
      "Epoch 119/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3524.6912\n",
      "Epoch 00119: ReduceLROnPlateau reducing learning rate to 5.960464760645934e-11.\n",
      " — ite: 3.8701  — ate: 0.4191 — pehe: 3.8195 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: 2452.6516 - val_loss: 2034.6226 - val_y0: 1.5489 - val_y1: -1.0859 - val_ate_afte_scaled: -2.6348 - lr: 1.1921e-10\n",
      "Epoch 120/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3786.5571 — ite: 3.8149  — ate: 0.7084 — pehe: 3.8561 \n",
      "3/3 [==============================] - 0s 228ms/step - loss: 2550.9424 - val_loss: 2220.8855 - val_y0: 1.4982 - val_y1: -1.2602 - val_ate_afte_scaled: -2.7584 - lr: 5.9605e-11\n",
      "Epoch 121/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3827.8142 — ite: 3.8690  — ate: 0.6173 — pehe: 3.8585 \n",
      "3/3 [==============================] - 0s 222ms/step - loss: 2461.8674 - val_loss: 2100.3772 - val_y0: 1.4451 - val_y1: -1.4088 - val_ate_afte_scaled: -2.8539 - lr: 5.9605e-11\n",
      "Epoch 122/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3642.2476 — ite: 3.7643  — ate: 0.5946 — pehe: 3.8594 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: 2426.4545 - val_loss: 2225.0964 - val_y0: 1.5462 - val_y1: -1.2778 - val_ate_afte_scaled: -2.8240 - lr: 5.9605e-11\n",
      "Epoch 123/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3638.2441 — ite: 3.7664  — ate: 0.4019 — pehe: 3.5907 \n",
      "3/3 [==============================] - 0s 220ms/step - loss: 2432.2448 - val_loss: 2282.2427 - val_y0: 1.4479 - val_y1: -1.3372 - val_ate_afte_scaled: -2.7851 - lr: 5.9605e-11\n",
      "Epoch 124/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3572.0879\n",
      "Epoch 00124: ReduceLROnPlateau reducing learning rate to 2.980232380322967e-11.\n",
      " — ite: 3.7304  — ate: 0.5069 — pehe: 3.6419 \n",
      "3/3 [==============================] - 0s 223ms/step - loss: 2360.7025 - val_loss: 2145.1548 - val_y0: 1.4432 - val_y1: -1.1449 - val_ate_afte_scaled: -2.5881 - lr: 5.9605e-11\n",
      "Epoch 125/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3803.3916 — ite: 3.8688  — ate: 0.4649 — pehe: 3.8812 \n",
      "3/3 [==============================] - 0s 225ms/step - loss: 2506.8051 - val_loss: 2159.9478 - val_y0: 1.5069 - val_y1: -1.1946 - val_ate_afte_scaled: -2.7015 - lr: 2.9802e-11\n",
      "Epoch 126/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3644.6589 — ite: 3.7535  — ate: 0.4610 — pehe: 3.6494 \n",
      "3/3 [==============================] - 0s 225ms/step - loss: 2458.6168 - val_loss: 2234.7844 - val_y0: 1.5060 - val_y1: -1.1794 - val_ate_afte_scaled: -2.6854 - lr: 2.9802e-11\n",
      "Epoch 127/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3822.1042 — ite: 3.8730  — ate: 0.4755 — pehe: 3.9221 \n",
      "3/3 [==============================] - 0s 213ms/step - loss: 2533.1342 - val_loss: 2104.4106 - val_y0: 1.5745 - val_y1: -1.3512 - val_ate_afte_scaled: -2.9257 - lr: 2.9802e-11\n",
      "Epoch 128/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3758.9194 — ite: 3.8510  — ate: 0.2193 — pehe: 3.6819 \n",
      "3/3 [==============================] - 0s 221ms/step - loss: 2421.7352 - val_loss: 2177.5435 - val_y0: 1.5145 - val_y1: -1.2690 - val_ate_afte_scaled: -2.7835 - lr: 2.9802e-11\n",
      "Epoch 129/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3588.1885\n",
      "Epoch 00129: ReduceLROnPlateau reducing learning rate to 1.4901161901614834e-11.\n",
      " — ite: 3.9282  — ate: 0.3436 — pehe: 3.7859 \n",
      "3/3 [==============================] - 1s 246ms/step - loss: 2423.2177 - val_loss: 2168.0305 - val_y0: 1.6402 - val_y1: -1.2099 - val_ate_afte_scaled: -2.8501 - lr: 2.9802e-11\n",
      "Epoch 130/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3722.6162 — ite: 3.8150  — ate: 0.4839 — pehe: 3.7133 \n",
      "3/3 [==============================] - 0s 230ms/step - loss: 2418.5676 - val_loss: 2179.7319 - val_y0: 1.5756 - val_y1: -1.2890 - val_ate_afte_scaled: -2.8646 - lr: 1.4901e-11\n",
      "Epoch 131/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3497.9817 — ite: 3.7684  — ate: 0.6044 — pehe: 3.6865 \n",
      "3/3 [==============================] - 0s 221ms/step - loss: 2421.0776 - val_loss: 2161.0764 - val_y0: 1.4505 - val_y1: -1.2649 - val_ate_afte_scaled: -2.7154 - lr: 1.4901e-11\n",
      "Epoch 132/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2834.0056 — ite: 3.8392  — ate: 0.4953 — pehe: 3.8073 \n",
      "3/3 [==============================] - 1s 237ms/step - loss: 2446.6213 - val_loss: 2243.3838 - val_y0: 1.2887 - val_y1: -1.2517 - val_ate_afte_scaled: -2.5404 - lr: 1.4901e-11\n",
      "Epoch 133/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2821.4284 — ite: 3.8350  — ate: 0.7745 — pehe: 3.7789 \n",
      "3/3 [==============================] - 0s 237ms/step - loss: 2397.2004 - val_loss: 2041.7545 - val_y0: 1.4630 - val_y1: -1.2314 - val_ate_afte_scaled: -2.6943 - lr: 1.4901e-11\n",
      "Epoch 134/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3813.7798\n",
      "Epoch 00134: ReduceLROnPlateau reducing learning rate to 7.450580950807417e-12.\n",
      " — ite: 3.9806  — ate: 0.6472 — pehe: 3.8949 \n",
      "3/3 [==============================] - 0s 224ms/step - loss: 2550.9042 - val_loss: 2084.7102 - val_y0: 1.4236 - val_y1: -1.3000 - val_ate_afte_scaled: -2.7236 - lr: 1.4901e-11\n",
      "Epoch 135/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3641.2668 — ite: 3.9085  — ate: 0.6043 — pehe: 4.0018 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: 2489.0975 - val_loss: 2104.2231 - val_y0: 1.5343 - val_y1: -1.2700 - val_ate_afte_scaled: -2.8043 - lr: 7.4506e-12\n",
      "Epoch 136/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3893.4885 — ite: 3.8810  — ate: 0.5647 — pehe: 3.8024 \n",
      "3/3 [==============================] - 0s 222ms/step - loss: 2601.9569 - val_loss: 2082.7268 - val_y0: 1.5028 - val_y1: -1.2465 - val_ate_afte_scaled: -2.7493 - lr: 7.4506e-12\n",
      "Epoch 137/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3700.7634 — ite: 3.8084  — ate: 0.3324 — pehe: 3.7004 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: 2436.4747 - val_loss: 2111.1250 - val_y0: 1.3298 - val_y1: -1.0658 - val_ate_afte_scaled: -2.3956 - lr: 7.4506e-12\n",
      "Epoch 138/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3721.8369 — ite: 3.8292  — ate: 0.5485 — pehe: 3.7569 \n",
      "3/3 [==============================] - 0s 229ms/step - loss: 2477.3627 - val_loss: 2165.7178 - val_y0: 1.4242 - val_y1: -1.1797 - val_ate_afte_scaled: -2.6039 - lr: 7.4506e-12\n",
      "Epoch 139/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3726.7766\n",
      "Epoch 00139: ReduceLROnPlateau reducing learning rate to 3.725290475403709e-12.\n",
      " — ite: 3.9308  — ate: 0.8034 — pehe: 3.9102 \n",
      "3/3 [==============================] - 0s 221ms/step - loss: 2445.8624 - val_loss: 2090.1304 - val_y0: 1.3786 - val_y1: -1.4089 - val_ate_afte_scaled: -2.7875 - lr: 7.4506e-12\n",
      "Epoch 140/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2762.3929 — ite: 3.8046  — ate: 0.4588 — pehe: 3.7960 \n",
      "3/3 [==============================] - 1s 253ms/step - loss: 2358.8615 - val_loss: 2279.6301 - val_y0: 1.4224 - val_y1: -1.4453 - val_ate_afte_scaled: -2.8677 - lr: 3.7253e-12\n",
      "Epoch 141/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3704.1113 — ite: 3.8505  — ate: 0.6027 — pehe: 3.6543 \n",
      "3/3 [==============================] - 0s 232ms/step - loss: 2431.0504 - val_loss: 2075.6353 - val_y0: 1.5415 - val_y1: -1.3503 - val_ate_afte_scaled: -2.8919 - lr: 3.7253e-12\n",
      "Epoch 142/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3623.0320 — ite: 3.8419  — ate: 0.4201 — pehe: 3.8596 \n",
      "3/3 [==============================] - 0s 234ms/step - loss: 2327.7205 - val_loss: 2136.6743 - val_y0: 1.5472 - val_y1: -1.3298 - val_ate_afte_scaled: -2.8770 - lr: 3.7253e-12\n",
      "Epoch 143/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2835.5532 — ite: 3.8966  — ate: 0.3144 — pehe: 3.8074 \n",
      "3/3 [==============================] - 1s 244ms/step - loss: 2442.5785 - val_loss: 2191.4180 - val_y0: 1.4533 - val_y1: -1.2545 - val_ate_afte_scaled: -2.7078 - lr: 3.7253e-12\n",
      "Epoch 144/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2951.8207\n",
      "Epoch 00144: ReduceLROnPlateau reducing learning rate to 1.8626452377018543e-12.\n",
      " — ite: 3.8387  — ate: 0.3879 — pehe: 3.8499 \n",
      "3/3 [==============================] - 1s 243ms/step - loss: 2549.7370 - val_loss: 2229.4126 - val_y0: 1.4623 - val_y1: -1.1774 - val_ate_afte_scaled: -2.6397 - lr: 3.7253e-12\n",
      "Epoch 145/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3622.4431 — ite: 3.8801  — ate: 0.5607 — pehe: 3.6730 \n",
      "3/3 [==============================] - 0s 232ms/step - loss: 2434.7880 - val_loss: 2024.6305 - val_y0: 1.4745 - val_y1: -1.2164 - val_ate_afte_scaled: -2.6909 - lr: 1.8626e-12\n",
      "Epoch 146/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2876.2495 — ite: 3.9652  — ate: 0.3890 — pehe: 3.8471 \n",
      "3/3 [==============================] - 1s 241ms/step - loss: 2434.7115 - val_loss: 2081.3840 - val_y0: 1.4748 - val_y1: -1.2949 - val_ate_afte_scaled: -2.7697 - lr: 1.8626e-12\n",
      "Epoch 147/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2905.4797 — ite: 3.8207  — ate: 0.5516 — pehe: 3.8467 \n",
      "3/3 [==============================] - 1s 240ms/step - loss: 2526.3818 - val_loss: 2256.6787 - val_y0: 1.6234 - val_y1: -1.2744 - val_ate_afte_scaled: -2.8978 - lr: 1.8626e-12\n",
      "Epoch 148/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2929.7728 — ite: 3.8610  — ate: 0.4988 — pehe: 3.9125 \n",
      "3/3 [==============================] - 1s 245ms/step - loss: 2515.0524 - val_loss: 1884.4089 - val_y0: 1.4194 - val_y1: -1.2074 - val_ate_afte_scaled: -2.6269 - lr: 1.8626e-12\n",
      "Epoch 149/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2875.8777\n",
      "Epoch 00149: ReduceLROnPlateau reducing learning rate to 9.313226188509272e-13.\n",
      " — ite: 3.8141  — ate: 0.4590 — pehe: 3.6636 \n",
      "3/3 [==============================] - 1s 247ms/step - loss: 2454.5273 - val_loss: 2170.9126 - val_y0: 1.5783 - val_y1: -1.2960 - val_ate_afte_scaled: -2.8743 - lr: 1.8626e-12\n",
      "Epoch 150/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2908.4664 — ite: 3.8493  — ate: 0.3237 — pehe: 3.8714 \n",
      "3/3 [==============================] - 1s 262ms/step - loss: 2534.7269 - val_loss: 2225.4126 - val_y0: 1.4788 - val_y1: -1.2161 - val_ate_afte_scaled: -2.6950 - lr: 9.3132e-13\n",
      "Epoch 151/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3451.2847 — ite: 3.7346  — ate: 0.5336 — pehe: 3.7529 \n",
      "3/3 [==============================] - 0s 228ms/step - loss: 2349.5766 - val_loss: 2157.4800 - val_y0: 1.5533 - val_y1: -1.2432 - val_ate_afte_scaled: -2.7966 - lr: 9.3132e-13\n",
      "Epoch 152/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2819.2992 — ite: 3.8312  — ate: 0.4659 — pehe: 3.6246 \n",
      "3/3 [==============================] - 1s 237ms/step - loss: 2400.5540 - val_loss: 2271.4331 - val_y0: 1.5049 - val_y1: -1.3293 - val_ate_afte_scaled: -2.8342 - lr: 9.3132e-13\n",
      "Epoch 153/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3730.4968 — ite: 3.9204  — ate: 0.5558 — pehe: 3.8071 \n",
      "3/3 [==============================] - 0s 224ms/step - loss: 2316.9070 - val_loss: 2242.4250 - val_y0: 1.5397 - val_y1: -1.3172 - val_ate_afte_scaled: -2.8569 - lr: 9.3132e-13\n",
      "Epoch 154/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3440.1218\n",
      "Epoch 00154: ReduceLROnPlateau reducing learning rate to 4.656613094254636e-13.\n",
      " — ite: 3.9185  — ate: 0.6668 — pehe: 3.8668 \n",
      "3/3 [==============================] - 1s 264ms/step - loss: 2372.7607 - val_loss: 1936.7014 - val_y0: 1.6030 - val_y1: -1.2494 - val_ate_afte_scaled: -2.8523 - lr: 9.3132e-13\n",
      "Epoch 155/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3570.9424 — ite: 3.8240  — ate: 0.3694 — pehe: 3.8768 \n",
      "3/3 [==============================] - 1s 240ms/step - loss: 2382.1118 - val_loss: 2144.2034 - val_y0: 1.6439 - val_y1: -1.3419 - val_ate_afte_scaled: -2.9858 - lr: 4.6566e-13\n",
      "Epoch 156/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2814.8323 — ite: 3.7994  — ate: 0.4623 — pehe: 3.7463 \n",
      "3/3 [==============================] - 1s 244ms/step - loss: 2406.3726 - val_loss: 2056.4250 - val_y0: 1.5378 - val_y1: -1.2326 - val_ate_afte_scaled: -2.7704 - lr: 4.6566e-13\n",
      "Epoch 157/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3750.0024 — ite: 3.8086  — ate: 0.5977 — pehe: 3.8810 \n",
      "3/3 [==============================] - 1s 238ms/step - loss: 2394.6721 - val_loss: 2075.4360 - val_y0: 1.6123 - val_y1: -1.2927 - val_ate_afte_scaled: -2.9050 - lr: 4.6566e-13\n",
      "Epoch 158/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3662.9495 — ite: 3.8925  — ate: 0.7973 — pehe: 3.9498 \n",
      "3/3 [==============================] - 0s 225ms/step - loss: 2403.4523 - val_loss: 2175.0784 - val_y0: 1.6225 - val_y1: -1.2499 - val_ate_afte_scaled: -2.8724 - lr: 4.6566e-13\n",
      "Epoch 159/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3946.7253\n",
      "Epoch 00159: ReduceLROnPlateau reducing learning rate to 2.328306547127318e-13.\n",
      " — ite: 3.7714  — ate: 0.4794 — pehe: 3.8587 \n",
      "3/3 [==============================] - 1s 242ms/step - loss: 2508.8533 - val_loss: 2129.3677 - val_y0: 1.3810 - val_y1: -1.1573 - val_ate_afte_scaled: -2.5383 - lr: 4.6566e-13\n",
      "Epoch 160/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2797.0057 — ite: 3.9232  — ate: 0.4025 — pehe: 3.7969 \n",
      "3/3 [==============================] - 1s 244ms/step - loss: 2388.7833 - val_loss: 2215.1130 - val_y0: 1.5678 - val_y1: -1.3337 - val_ate_afte_scaled: -2.9016 - lr: 2.3283e-13\n",
      "Epoch 161/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3576.6455 — ite: 3.8657  — ate: 0.4555 — pehe: 3.7858 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: 2379.0522 - val_loss: 2195.9780 - val_y0: 1.5722 - val_y1: -1.4332 - val_ate_afte_scaled: -3.0053 - lr: 2.3283e-13\n",
      "Epoch 162/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3454.0435 — ite: 3.8180  — ate: 0.5894 — pehe: 3.8090 \n",
      "3/3 [==============================] - 0s 222ms/step - loss: 2452.6280 - val_loss: 1994.7394 - val_y0: 1.3365 - val_y1: -1.2060 - val_ate_afte_scaled: -2.5425 - lr: 2.3283e-13\n",
      "Epoch 163/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3750.7778 — ite: 3.8971  — ate: 0.5550 — pehe: 3.6740 \n",
      "3/3 [==============================] - 0s 221ms/step - loss: 2482.3883 - val_loss: 1931.8104 - val_y0: 1.4851 - val_y1: -1.3715 - val_ate_afte_scaled: -2.8566 - lr: 2.3283e-13\n",
      "Epoch 164/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2920.3248\n",
      "Epoch 00164: ReduceLROnPlateau reducing learning rate to 1.164153273563659e-13.\n",
      " — ite: 3.6974  — ate: 0.6340 — pehe: 3.7009 \n",
      "3/3 [==============================] - 1s 239ms/step - loss: 2490.9257 - val_loss: 2066.8508 - val_y0: 1.3763 - val_y1: -1.3050 - val_ate_afte_scaled: -2.6813 - lr: 2.3283e-13\n",
      "Epoch 165/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3626.8174 — ite: 3.8956  — ate: 0.3721 — pehe: 3.9480 \n",
      "3/3 [==============================] - 1s 246ms/step - loss: 2497.7017 - val_loss: 2363.6709 - val_y0: 1.3940 - val_y1: -1.0500 - val_ate_afte_scaled: -2.4440 - lr: 1.1642e-13\n",
      "Epoch 166/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2784.2760 — ite: 3.8978  — ate: 0.3337 — pehe: 3.8665 \n",
      "3/3 [==============================] - 1s 242ms/step - loss: 2421.9202 - val_loss: 2138.3452 - val_y0: 1.4051 - val_y1: -1.3081 - val_ate_afte_scaled: -2.7132 - lr: 1.1642e-13\n",
      "Epoch 167/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2856.3646 — ite: 3.8700  — ate: 0.4816 — pehe: 3.9139 \n",
      "3/3 [==============================] - 1s 240ms/step - loss: 2437.5325 - val_loss: 2141.1064 - val_y0: 1.5104 - val_y1: -1.3097 - val_ate_afte_scaled: -2.8201 - lr: 1.1642e-13\n",
      "Epoch 168/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2923.5763 — ite: 3.8067  — ate: 0.4639 — pehe: 3.7329 \n",
      "3/3 [==============================] - 1s 254ms/step - loss: 2498.5915 - val_loss: 2264.4182 - val_y0: 1.5373 - val_y1: -1.3494 - val_ate_afte_scaled: -2.8867 - lr: 1.1642e-13\n",
      "Epoch 169/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2827.5509\n",
      "Epoch 00169: ReduceLROnPlateau reducing learning rate to 5.820766367818295e-14.\n",
      " — ite: 3.8799  — ate: 0.8239 — pehe: 3.8786 \n",
      "3/3 [==============================] - 1s 241ms/step - loss: 2425.4886 - val_loss: 2223.9951 - val_y0: 1.5436 - val_y1: -1.1405 - val_ate_afte_scaled: -2.6841 - lr: 1.1642e-13\n",
      "Epoch 170/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3405.9634 — ite: 3.8300  — ate: 0.6109 — pehe: 3.7928 \n",
      "3/3 [==============================] - 1s 237ms/step - loss: 2419.8773 - val_loss: 2265.0930 - val_y0: 1.4526 - val_y1: -1.1901 - val_ate_afte_scaled: -2.6428 - lr: 5.8208e-14\n",
      "Epoch 171/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3854.7395 — ite: 3.8320  — ate: 0.5088 — pehe: 3.7374 \n",
      "3/3 [==============================] - 0s 220ms/step - loss: 2532.2321 - val_loss: 2115.2656 - val_y0: 1.5290 - val_y1: -1.4024 - val_ate_afte_scaled: -2.9313 - lr: 5.8208e-14\n",
      "Epoch 172/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2800.4067 — ite: 3.8103  — ate: 0.3965 — pehe: 3.8429 \n",
      "3/3 [==============================] - 1s 269ms/step - loss: 2403.5696 - val_loss: 2331.4009 - val_y0: 1.6449 - val_y1: -1.3624 - val_ate_afte_scaled: -3.0073 - lr: 5.8208e-14\n",
      "Epoch 173/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3795.1458 — ite: 3.9158  — ate: 0.4946 — pehe: 3.8551 \n",
      "3/3 [==============================] - 0s 235ms/step - loss: 2503.9102 - val_loss: 1980.9220 - val_y0: 1.3952 - val_y1: -1.1700 - val_ate_afte_scaled: -2.5652 - lr: 5.8208e-14\n",
      "Epoch 174/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3754.8772\n",
      "Epoch 00174: ReduceLROnPlateau reducing learning rate to 2.9103831839091474e-14.\n",
      " — ite: 3.7206  — ate: 0.4403 — pehe: 3.7904 \n",
      "3/3 [==============================] - 0s 234ms/step - loss: 2544.2827 - val_loss: 2133.1023 - val_y0: 1.4573 - val_y1: -1.2526 - val_ate_afte_scaled: -2.7099 - lr: 5.8208e-14\n",
      "Epoch 175/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3902.1404 — ite: 3.9123  — ate: 0.5625 — pehe: 3.9979 \n",
      "3/3 [==============================] - 1s 251ms/step - loss: 2498.1755 - val_loss: 2124.6736 - val_y0: 1.4593 - val_y1: -1.3470 - val_ate_afte_scaled: -2.8063 - lr: 2.9104e-14\n",
      "Epoch 176/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2841.7941 — ite: 3.7962  — ate: 0.4331 — pehe: 3.7558 \n",
      "3/3 [==============================] - 1s 243ms/step - loss: 2439.6645 - val_loss: 2132.1975 - val_y0: 1.5131 - val_y1: -1.1182 - val_ate_afte_scaled: -2.6313 - lr: 2.9104e-14\n",
      "Epoch 177/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3683.6670 — ite: 3.7785  — ate: 0.4528 — pehe: 3.6797 \n",
      "3/3 [==============================] - 1s 237ms/step - loss: 2506.5910 - val_loss: 2202.0391 - val_y0: 1.5909 - val_y1: -1.4423 - val_ate_afte_scaled: -3.0333 - lr: 2.9104e-14\n",
      "Epoch 178/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2825.9019 — ite: 3.8074  — ate: 0.5294 — pehe: 3.9404 \n",
      "3/3 [==============================] - 1s 267ms/step - loss: 2424.8419 - val_loss: 2126.6985 - val_y0: 1.5133 - val_y1: -1.1926 - val_ate_afte_scaled: -2.7059 - lr: 2.9104e-14\n",
      "Epoch 179/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2814.5683\n",
      "Epoch 00179: ReduceLROnPlateau reducing learning rate to 1.4551915919545737e-14.\n",
      " — ite: 3.7518  — ate: 0.5023 — pehe: 3.7046 \n",
      "3/3 [==============================] - 1s 254ms/step - loss: 2404.3329 - val_loss: 2114.0750 - val_y0: 1.6092 - val_y1: -1.2069 - val_ate_afte_scaled: -2.8161 - lr: 2.9104e-14\n",
      "Epoch 180/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2955.8743 — ite: 3.8751  — ate: 0.5496 — pehe: 3.9001 \n",
      "3/3 [==============================] - 1s 248ms/step - loss: 2539.7945 - val_loss: 2196.8218 - val_y0: 1.4486 - val_y1: -1.2513 - val_ate_afte_scaled: -2.6999 - lr: 1.4552e-14\n",
      "Epoch 181/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2890.4683 — ite: 3.7983  — ate: 0.6011 — pehe: 3.6496 \n",
      "3/3 [==============================] - 1s 236ms/step - loss: 2486.3748 - val_loss: 2401.6086 - val_y0: 1.4176 - val_y1: -1.2075 - val_ate_afte_scaled: -2.6251 - lr: 1.4552e-14\n",
      "Epoch 182/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2870.9182 — ite: 3.8208  — ate: 0.5283 — pehe: 3.8613 \n",
      "3/3 [==============================] - 1s 259ms/step - loss: 2429.6360 - val_loss: 2048.9951 - val_y0: 1.3752 - val_y1: -1.2070 - val_ate_afte_scaled: -2.5822 - lr: 1.4552e-14\n",
      "Epoch 183/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3624.1101 — ite: 3.8528  — ate: 0.4911 — pehe: 3.7796 \n",
      "3/3 [==============================] - 1s 249ms/step - loss: 2398.7536 - val_loss: 2178.6179 - val_y0: 1.3679 - val_y1: -1.1621 - val_ate_afte_scaled: -2.5300 - lr: 1.4552e-14\n",
      "Epoch 184/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2820.8914\n",
      "Epoch 00184: ReduceLROnPlateau reducing learning rate to 7.275957959772868e-15.\n",
      " — ite: 3.8465  — ate: 0.5199 — pehe: 3.7044 \n",
      "3/3 [==============================] - 1s 252ms/step - loss: 2439.0223 - val_loss: 2153.9688 - val_y0: 1.5424 - val_y1: -1.2341 - val_ate_afte_scaled: -2.7766 - lr: 1.4552e-14\n",
      "Epoch 185/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3807.7969 — ite: 3.9152  — ate: 0.4025 — pehe: 3.8906 \n",
      "3/3 [==============================] - 1s 248ms/step - loss: 2467.1852 - val_loss: 2153.4819 - val_y0: 1.5627 - val_y1: -1.4473 - val_ate_afte_scaled: -3.0100 - lr: 7.2760e-15\n",
      "Epoch 186/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2885.9554 — ite: 3.7873  — ate: 0.3169 — pehe: 3.7601 \n",
      "3/3 [==============================] - 1s 241ms/step - loss: 2446.1670 - val_loss: 2156.0718 - val_y0: 1.6050 - val_y1: -1.3196 - val_ate_afte_scaled: -2.9246 - lr: 7.2760e-15\n",
      "Epoch 187/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2722.5793 — ite: 3.7643  — ate: 0.4265 — pehe: 3.7564 \n",
      "3/3 [==============================] - 1s 256ms/step - loss: 2344.7797 - val_loss: 2218.7451 - val_y0: 1.3945 - val_y1: -1.3368 - val_ate_afte_scaled: -2.7313 - lr: 7.2760e-15\n",
      "Epoch 188/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2822.3701 — ite: 4.0046  — ate: 0.3932 — pehe: 3.9200 \n",
      "3/3 [==============================] - 1s 254ms/step - loss: 2403.4694 - val_loss: 2101.3857 - val_y0: 1.4827 - val_y1: -1.3012 - val_ate_afte_scaled: -2.7839 - lr: 7.2760e-15\n",
      "Epoch 189/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3664.1167\n",
      "Epoch 00189: ReduceLROnPlateau reducing learning rate to 3.637978979886434e-15.\n",
      " — ite: 3.8566  — ate: 0.4029 — pehe: 3.8447 \n",
      "3/3 [==============================] - 0s 229ms/step - loss: 2497.8571 - val_loss: 2138.1770 - val_y0: 1.4791 - val_y1: -1.2561 - val_ate_afte_scaled: -2.7352 - lr: 7.2760e-15\n",
      "Epoch 190/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3699.2368 — ite: 3.8760  — ate: 0.5945 — pehe: 3.8787 \n",
      "3/3 [==============================] - 1s 243ms/step - loss: 2482.6488 - val_loss: 2228.4141 - val_y0: 1.6126 - val_y1: -1.2289 - val_ate_afte_scaled: -2.8415 - lr: 3.6380e-15\n",
      "Epoch 191/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2785.9978 — ite: 3.7718  — ate: 0.4493 — pehe: 3.7631 \n",
      "3/3 [==============================] - 1s 279ms/step - loss: 2358.6280 - val_loss: 2177.9331 - val_y0: 1.5009 - val_y1: -1.3632 - val_ate_afte_scaled: -2.8640 - lr: 3.6380e-15\n",
      "Epoch 192/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3725.7561 — ite: 3.7465  — ate: 0.5428 — pehe: 3.6592 \n",
      "3/3 [==============================] - 1s 272ms/step - loss: 2479.3168 - val_loss: 2217.1438 - val_y0: 1.6291 - val_y1: -1.2790 - val_ate_afte_scaled: -2.9081 - lr: 3.6380e-15\n",
      "Epoch 193/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2813.5227 — ite: 3.6820  — ate: 0.3018 — pehe: 3.6342 \n",
      "3/3 [==============================] - 1s 304ms/step - loss: 2385.9506 - val_loss: 2259.8567 - val_y0: 1.4686 - val_y1: -1.4003 - val_ate_afte_scaled: -2.8689 - lr: 3.6380e-15\n",
      "Epoch 194/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2889.1785\n",
      "Epoch 00194: ReduceLROnPlateau reducing learning rate to 1.818989489943217e-15.\n",
      " — ite: 3.8085  — ate: 0.3940 — pehe: 3.7811 \n",
      "3/3 [==============================] - 1s 324ms/step - loss: 2500.7736 - val_loss: 2103.1768 - val_y0: 1.4792 - val_y1: -1.3282 - val_ate_afte_scaled: -2.8074 - lr: 3.6380e-15\n",
      "Epoch 195/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2949.0882 — ite: 3.8476  — ate: 0.4642 — pehe: 3.6443 \n",
      "3/3 [==============================] - 1s 269ms/step - loss: 2545.7697 - val_loss: 2167.8464 - val_y0: 1.5073 - val_y1: -1.2353 - val_ate_afte_scaled: -2.7426 - lr: 1.8190e-15\n",
      "Epoch 196/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2898.1554 — ite: 3.8025  — ate: 0.5387 — pehe: 3.5691 \n",
      "3/3 [==============================] - 1s 249ms/step - loss: 2488.5386 - val_loss: 2117.8291 - val_y0: 1.5798 - val_y1: -1.1478 - val_ate_afte_scaled: -2.7277 - lr: 1.8190e-15\n",
      "Epoch 197/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2830.5828 — ite: 3.7068  — ate: 0.5626 — pehe: 3.6918 \n",
      "3/3 [==============================] - 1s 320ms/step - loss: 2413.5994 - val_loss: 2136.8523 - val_y0: 1.4040 - val_y1: -1.2318 - val_ate_afte_scaled: -2.6358 - lr: 1.8190e-15\n",
      "Epoch 198/300\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 3625.9060 — ite: 3.9152  — ate: 0.7489 — pehe: 3.7016 \n",
      "3/3 [==============================] - 1s 285ms/step - loss: 2425.5510 - val_loss: 2105.6143 - val_y0: 1.5124 - val_y1: -1.2821 - val_ate_afte_scaled: -2.7945 - lr: 1.8190e-15\n",
      "Epoch 199/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3716.4900\n",
      "Epoch 00199: ReduceLROnPlateau reducing learning rate to 9.094947449716085e-16.\n",
      " — ite: 3.8733  — ate: 0.3615 — pehe: 3.8447 \n",
      "3/3 [==============================] - 1s 245ms/step - loss: 2454.0764 - val_loss: 2084.5659 - val_y0: 1.3603 - val_y1: -1.1408 - val_ate_afte_scaled: -2.5010 - lr: 1.8190e-15\n",
      "Epoch 200/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3664.8904 — ite: 3.8093  — ate: 0.4160 — pehe: 3.9014 \n",
      "3/3 [==============================] - 0s 223ms/step - loss: 2431.3010 - val_loss: 2044.9489 - val_y0: 1.5392 - val_y1: -1.1933 - val_ate_afte_scaled: -2.7325 - lr: 9.0949e-16\n",
      "Epoch 201/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3724.0352 — ite: 3.8238  — ate: 0.8205 — pehe: 3.7374 \n",
      "3/3 [==============================] - 1s 255ms/step - loss: 2483.9153 - val_loss: 2117.2861 - val_y0: 1.5650 - val_y1: -1.3848 - val_ate_afte_scaled: -2.9497 - lr: 9.0949e-16\n",
      "Epoch 202/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2877.7961 — ite: 3.8028  — ate: 0.4158 — pehe: 3.7352 \n",
      "3/3 [==============================] - 1s 262ms/step - loss: 2443.1739 - val_loss: 2097.8074 - val_y0: 1.4322 - val_y1: -1.3200 - val_ate_afte_scaled: -2.7522 - lr: 9.0949e-16\n",
      "Epoch 203/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2738.3954 — ite: 3.8762  — ate: 0.6104 — pehe: 3.9109 \n",
      "3/3 [==============================] - 1s 262ms/step - loss: 2343.9545 - val_loss: 2156.8713 - val_y0: 1.4732 - val_y1: -1.0554 - val_ate_afte_scaled: -2.5286 - lr: 9.0949e-16\n",
      "Epoch 204/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2740.4714\n",
      "Epoch 00204: ReduceLROnPlateau reducing learning rate to 4.547473724858043e-16.\n",
      " — ite: 3.8544  — ate: 0.4270 — pehe: 4.0093 \n",
      "3/3 [==============================] - 1s 254ms/step - loss: 2359.0128 - val_loss: 2266.8752 - val_y0: 1.5602 - val_y1: -1.3143 - val_ate_afte_scaled: -2.8746 - lr: 9.0949e-16\n",
      "Epoch 205/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2951.3210 — ite: 3.8338  — ate: 0.5121 — pehe: 3.8055 \n",
      "3/3 [==============================] - 1s 330ms/step - loss: 2500.4307 - val_loss: 1938.1357 - val_y0: 1.5315 - val_y1: -1.2375 - val_ate_afte_scaled: -2.7690 - lr: 4.5475e-16\n",
      "Epoch 206/300\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 3642.1429 — ite: 3.8741  — ate: 0.3135 — pehe: 3.7780 \n",
      "3/3 [==============================] - 1s 353ms/step - loss: 2512.0605 - val_loss: 2067.2134 - val_y0: 1.5658 - val_y1: -1.2366 - val_ate_afte_scaled: -2.8024 - lr: 4.5475e-16\n",
      "Epoch 207/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2873.3121 — ite: 3.9058  — ate: 0.7220 — pehe: 3.8833 \n",
      "3/3 [==============================] - 1s 284ms/step - loss: 2468.1858 - val_loss: 2052.5173 - val_y0: 1.5445 - val_y1: -1.1222 - val_ate_afte_scaled: -2.6667 - lr: 4.5475e-16\n",
      "Epoch 208/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2818.7567 — ite: 3.7638  — ate: 0.6544 — pehe: 3.8020 \n",
      "3/3 [==============================] - 1s 291ms/step - loss: 2410.0619 - val_loss: 2024.8882 - val_y0: 1.6806 - val_y1: -1.3115 - val_ate_afte_scaled: -2.9921 - lr: 4.5475e-16\n",
      "Epoch 209/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3694.4338\n",
      "Epoch 00209: ReduceLROnPlateau reducing learning rate to 2.2737368624290214e-16.\n",
      " — ite: 3.8028  — ate: 0.1552 — pehe: 3.8446 \n",
      "3/3 [==============================] - 1s 274ms/step - loss: 2492.0229 - val_loss: 2227.6843 - val_y0: 1.6533 - val_y1: -1.2925 - val_ate_afte_scaled: -2.9458 - lr: 4.5475e-16\n",
      "Epoch 210/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2821.4122 — ite: 3.8745  — ate: 0.6121 — pehe: 3.8937 \n",
      "3/3 [==============================] - 1s 256ms/step - loss: 2443.7833 - val_loss: 2282.4902 - val_y0: 1.5131 - val_y1: -1.1552 - val_ate_afte_scaled: -2.6683 - lr: 2.2737e-16\n",
      "Epoch 211/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2935.8097 — ite: 3.9021  — ate: 0.6190 — pehe: 3.8773 \n",
      "3/3 [==============================] - 1s 265ms/step - loss: 2481.5319 - val_loss: 2238.5347 - val_y0: 1.6217 - val_y1: -1.2890 - val_ate_afte_scaled: -2.9107 - lr: 2.2737e-16\n",
      "Epoch 212/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2877.3874 — ite: 3.8541  — ate: 0.6110 — pehe: 3.7167 \n",
      "3/3 [==============================] - 1s 236ms/step - loss: 2455.1860 - val_loss: 2284.0105 - val_y0: 1.5264 - val_y1: -1.3025 - val_ate_afte_scaled: -2.8289 - lr: 2.2737e-16\n",
      "Epoch 213/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2788.8347 — ite: 3.8068  — ate: 0.2574 — pehe: 3.6952 \n",
      "3/3 [==============================] - 1s 258ms/step - loss: 2401.0698 - val_loss: 2252.9634 - val_y0: 1.5165 - val_y1: -1.2936 - val_ate_afte_scaled: -2.8101 - lr: 2.2737e-16\n",
      "Epoch 214/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3860.5906\n",
      "Epoch 00214: ReduceLROnPlateau reducing learning rate to 1.1368684312145107e-16.\n",
      " — ite: 3.8077  — ate: 0.6084 — pehe: 3.7735 \n",
      "3/3 [==============================] - 1s 246ms/step - loss: 2458.4195 - val_loss: 2193.7571 - val_y0: 1.5157 - val_y1: -1.3584 - val_ate_afte_scaled: -2.8742 - lr: 2.2737e-16\n",
      "Epoch 215/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2889.2779 — ite: 3.7942  — ate: 0.6238 — pehe: 3.6635 \n",
      "3/3 [==============================] - 1s 278ms/step - loss: 2452.6906 - val_loss: 2128.0029 - val_y0: 1.4996 - val_y1: -1.2387 - val_ate_afte_scaled: -2.7382 - lr: 1.1369e-16\n",
      "Epoch 216/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3437.2534 — ite: 3.9588  — ate: 0.4446 — pehe: 4.0197 \n",
      "3/3 [==============================] - 1s 237ms/step - loss: 2409.1382 - val_loss: 2212.9333 - val_y0: 1.3138 - val_y1: -1.2379 - val_ate_afte_scaled: -2.5517 - lr: 1.1369e-16\n",
      "Epoch 217/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2765.9691 — ite: 3.8661  — ate: 0.6911 — pehe: 3.8041 \n",
      "3/3 [==============================] - 1s 254ms/step - loss: 2382.1969 - val_loss: 2161.4141 - val_y0: 1.4172 - val_y1: -1.2030 - val_ate_afte_scaled: -2.6202 - lr: 1.1369e-16\n",
      "Epoch 218/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2857.0913 — ite: 3.8889  — ate: 0.4296 — pehe: 3.7509 \n",
      "3/3 [==============================] - 1s 254ms/step - loss: 2448.4501 - val_loss: 2152.0454 - val_y0: 1.5644 - val_y1: -1.3053 - val_ate_afte_scaled: -2.8697 - lr: 1.1369e-16\n",
      "Epoch 219/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3456.0955\n",
      "Epoch 00219: ReduceLROnPlateau reducing learning rate to 5.684342156072553e-17.\n",
      " — ite: 3.7627  — ate: 0.5534 — pehe: 3.6857 \n",
      "3/3 [==============================] - 0s 220ms/step - loss: 2378.5109 - val_loss: 2360.3840 - val_y0: 1.5368 - val_y1: -1.1825 - val_ate_afte_scaled: -2.7193 - lr: 1.1369e-16\n",
      "Epoch 220/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3662.0037 — ite: 3.8672  — ate: 0.4995 — pehe: 3.9314 \n",
      "3/3 [==============================] - 1s 297ms/step - loss: 2427.4477 - val_loss: 2198.2017 - val_y0: 1.5186 - val_y1: -1.3548 - val_ate_afte_scaled: -2.8734 - lr: 5.6843e-17\n",
      "Epoch 221/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3748.4900 — ite: 3.8241  — ate: 0.4394 — pehe: 3.7805 \n",
      "3/3 [==============================] - 1s 270ms/step - loss: 2441.1672 - val_loss: 2187.5083 - val_y0: 1.4547 - val_y1: -1.2727 - val_ate_afte_scaled: -2.7274 - lr: 5.6843e-17\n",
      "Epoch 222/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2949.8661 — ite: 3.7995  — ate: 0.5876 — pehe: 3.6883 \n",
      "3/3 [==============================] - 1s 365ms/step - loss: 2520.4851 - val_loss: 2186.0227 - val_y0: 1.5640 - val_y1: -1.2410 - val_ate_afte_scaled: -2.8050 - lr: 5.6843e-17\n",
      "Epoch 223/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3828.9143 — ite: 3.7057  — ate: 0.6591 — pehe: 3.8085 \n",
      "3/3 [==============================] - 1s 334ms/step - loss: 2457.9564 - val_loss: 2174.6079 - val_y0: 1.4772 - val_y1: -1.1741 - val_ate_afte_scaled: -2.6513 - lr: 5.6843e-17\n",
      "Epoch 224/300\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 3693.3087\n",
      "Epoch 00224: ReduceLROnPlateau reducing learning rate to 2.842171078036277e-17.\n",
      " — ite: 3.8926  — ate: 0.5550 — pehe: 3.8047 \n",
      "3/3 [==============================] - 1s 353ms/step - loss: 2486.4301 - val_loss: 2271.8076 - val_y0: 1.5882 - val_y1: -1.1844 - val_ate_afte_scaled: -2.7726 - lr: 5.6843e-17\n",
      "Epoch 225/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2794.0580 — ite: 3.8215  — ate: 0.2743 — pehe: 4.0482 \n",
      "3/3 [==============================] - 1s 248ms/step - loss: 2355.3659 - val_loss: 2129.2327 - val_y0: 1.5120 - val_y1: -1.3654 - val_ate_afte_scaled: -2.8774 - lr: 2.8422e-17\n",
      "Epoch 226/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 3003.5749 — ite: 3.7910  — ate: 0.5447 — pehe: 3.8357 \n",
      "3/3 [==============================] - 1s 252ms/step - loss: 2572.5120 - val_loss: 2170.8174 - val_y0: 1.4467 - val_y1: -1.2952 - val_ate_afte_scaled: -2.7419 - lr: 2.8422e-17\n",
      "Epoch 227/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2797.5005 — ite: 3.7429  — ate: 0.3436 — pehe: 3.6434 \n",
      "3/3 [==============================] - 1s 236ms/step - loss: 2373.7126 - val_loss: 1919.7228 - val_y0: 1.5244 - val_y1: -1.2268 - val_ate_afte_scaled: -2.7512 - lr: 2.8422e-17\n",
      "Epoch 228/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2860.8252 — ite: 3.7828  — ate: 0.4693 — pehe: 3.6768 \n",
      "3/3 [==============================] - 1s 237ms/step - loss: 2407.1037 - val_loss: 2064.3259 - val_y0: 1.4438 - val_y1: -1.2756 - val_ate_afte_scaled: -2.7194 - lr: 2.8422e-17\n",
      "Epoch 229/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2936.0860\n",
      "Epoch 00229: ReduceLROnPlateau reducing learning rate to 1.4210855390181384e-17.\n",
      " — ite: 3.8492  — ate: 0.4109 — pehe: 3.8227 \n",
      "3/3 [==============================] - 1s 252ms/step - loss: 2539.8323 - val_loss: 2108.7246 - val_y0: 1.5166 - val_y1: -1.2424 - val_ate_afte_scaled: -2.7590 - lr: 2.8422e-17\n",
      "Epoch 230/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3890.1545 — ite: 3.8368  — ate: 0.5936 — pehe: 3.9933 \n",
      "3/3 [==============================] - 1s 251ms/step - loss: 2451.9543 - val_loss: 2164.8618 - val_y0: 1.5420 - val_y1: -1.0474 - val_ate_afte_scaled: -2.5894 - lr: 1.4211e-17\n",
      "Epoch 231/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2871.9851 — ite: 3.8546  — ate: 0.5347 — pehe: 3.7019 \n",
      "3/3 [==============================] - 1s 273ms/step - loss: 2457.9328 - val_loss: 2186.8999 - val_y0: 1.5870 - val_y1: -1.1646 - val_ate_afte_scaled: -2.7516 - lr: 1.4211e-17\n",
      "Epoch 232/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2809.5379 — ite: 3.8625  — ate: 0.5087 — pehe: 3.7158 \n",
      "3/3 [==============================] - 1s 286ms/step - loss: 2384.1041 - val_loss: 2223.1311 - val_y0: 1.3773 - val_y1: -1.1543 - val_ate_afte_scaled: -2.5316 - lr: 1.4211e-17\n",
      "Epoch 233/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3562.2881 — ite: 3.8354  — ate: 0.5735 — pehe: 3.7295 \n",
      "3/3 [==============================] - 1s 246ms/step - loss: 2407.7774 - val_loss: 2022.4725 - val_y0: 1.6329 - val_y1: -1.2684 - val_ate_afte_scaled: -2.9012 - lr: 1.4211e-17\n",
      "Epoch 234/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2874.6073\n",
      "Epoch 00234: ReduceLROnPlateau reducing learning rate to 7.105427695090692e-18.\n",
      " — ite: 3.8232  — ate: 0.3240 — pehe: 3.8238 \n",
      "3/3 [==============================] - 1s 258ms/step - loss: 2435.4523 - val_loss: 2243.2563 - val_y0: 1.5940 - val_y1: -1.1651 - val_ate_afte_scaled: -2.7591 - lr: 1.4211e-17\n",
      "Epoch 235/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2884.6672 — ite: 3.7754  — ate: 0.7443 — pehe: 3.8816 \n",
      "3/3 [==============================] - 1s 259ms/step - loss: 2498.1016 - val_loss: 2180.0583 - val_y0: 1.4759 - val_y1: -1.1446 - val_ate_afte_scaled: -2.6205 - lr: 7.1054e-18\n",
      "Epoch 236/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3788.4395 — ite: 3.8063  — ate: 0.4308 — pehe: 3.8048 \n",
      "3/3 [==============================] - 0s 214ms/step - loss: 2521.9755 - val_loss: 2179.0881 - val_y0: 1.5744 - val_y1: -1.2136 - val_ate_afte_scaled: -2.7881 - lr: 7.1054e-18\n",
      "Epoch 237/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2786.7928 — ite: 3.8509  — ate: 0.5834 — pehe: 3.7424 \n",
      "3/3 [==============================] - 1s 257ms/step - loss: 2386.8342 - val_loss: 2253.0771 - val_y0: 1.3982 - val_y1: -1.2514 - val_ate_afte_scaled: -2.6496 - lr: 7.1054e-18\n",
      "Epoch 238/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3500.0815 — ite: 3.9384  — ate: 0.6616 — pehe: 3.8504 \n",
      "3/3 [==============================] - 1s 235ms/step - loss: 2392.1089 - val_loss: 2192.0906 - val_y0: 1.6729 - val_y1: -1.2561 - val_ate_afte_scaled: -2.9290 - lr: 7.1054e-18\n",
      "Epoch 239/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2906.9072\n",
      "Epoch 00239: ReduceLROnPlateau reducing learning rate to 3.552713847545346e-18.\n",
      " — ite: 3.8108  — ate: 0.5034 — pehe: 3.7920 \n",
      "3/3 [==============================] - 1s 257ms/step - loss: 2463.2632 - val_loss: 2184.9619 - val_y0: 1.3960 - val_y1: -1.2158 - val_ate_afte_scaled: -2.6117 - lr: 7.1054e-18\n",
      "Epoch 240/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3799.0920 — ite: 3.8338  — ate: 0.2850 — pehe: 3.8997 \n",
      "3/3 [==============================] - 0s 222ms/step - loss: 2444.7128 - val_loss: 2162.8245 - val_y0: 1.4343 - val_y1: -1.2826 - val_ate_afte_scaled: -2.7169 - lr: 3.5527e-18\n",
      "Epoch 241/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3823.8899 — ite: 3.7655  — ate: 0.5154 — pehe: 3.6699 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: 2595.8872 - val_loss: 2269.2771 - val_y0: 1.4970 - val_y1: -1.3191 - val_ate_afte_scaled: -2.8161 - lr: 3.5527e-18\n",
      "Epoch 242/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3707.2444 — ite: 3.8121  — ate: 0.4446 — pehe: 3.7403 \n",
      "3/3 [==============================] - 0s 213ms/step - loss: 2439.0276 - val_loss: 2224.0613 - val_y0: 1.5461 - val_y1: -1.3695 - val_ate_afte_scaled: -2.9156 - lr: 3.5527e-18\n",
      "Epoch 243/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3850.8596 — ite: 3.8995  — ate: 0.6363 — pehe: 3.7778 \n",
      "3/3 [==============================] - 1s 251ms/step - loss: 2529.3162 - val_loss: 2305.4778 - val_y0: 1.4903 - val_y1: -1.3135 - val_ate_afte_scaled: -2.8037 - lr: 3.5527e-18\n",
      "Epoch 244/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2899.6013\n",
      "Epoch 00244: ReduceLROnPlateau reducing learning rate to 1.776356923772673e-18.\n",
      " — ite: 3.8308  — ate: 0.2742 — pehe: 3.8551 \n",
      "3/3 [==============================] - 1s 257ms/step - loss: 2497.5375 - val_loss: 2271.6113 - val_y0: 1.4941 - val_y1: -1.2690 - val_ate_afte_scaled: -2.7631 - lr: 3.5527e-18\n",
      "Epoch 245/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2946.4598 — ite: 3.8371  — ate: 0.4561 — pehe: 3.8762 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: 2530.7206 - val_loss: 2140.0359 - val_y0: 1.5805 - val_y1: -1.3051 - val_ate_afte_scaled: -2.8856 - lr: 1.7764e-18\n",
      "Epoch 246/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2879.5419 — ite: 3.8926  — ate: 0.5658 — pehe: 3.7645 \n",
      "3/3 [==============================] - 0s 235ms/step - loss: 2485.7643 - val_loss: 2087.1111 - val_y0: 1.5384 - val_y1: -1.2783 - val_ate_afte_scaled: -2.8167 - lr: 1.7764e-18\n",
      "Epoch 247/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3642.8696 — ite: 3.7404  — ate: 0.2932 — pehe: 3.5461 \n",
      "3/3 [==============================] - 0s 214ms/step - loss: 2412.0728 - val_loss: 2167.2297 - val_y0: 1.4320 - val_y1: -1.2109 - val_ate_afte_scaled: -2.6429 - lr: 1.7764e-18\n",
      "Epoch 248/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3636.5825 — ite: 3.7704  — ate: 0.4406 — pehe: 3.7410 \n",
      "3/3 [==============================] - 0s 232ms/step - loss: 2506.1488 - val_loss: 2283.9919 - val_y0: 1.4661 - val_y1: -1.3381 - val_ate_afte_scaled: -2.8042 - lr: 1.7764e-18\n",
      "Epoch 249/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3621.5764\n",
      "Epoch 00249: ReduceLROnPlateau reducing learning rate to 8.881784618863365e-19.\n",
      " — ite: 3.8160  — ate: 0.3896 — pehe: 3.6855 \n",
      "3/3 [==============================] - 1s 241ms/step - loss: 2503.4546 - val_loss: 2154.4749 - val_y0: 1.5287 - val_y1: -1.2077 - val_ate_afte_scaled: -2.7365 - lr: 1.7764e-18\n",
      "Epoch 250/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2792.2054 — ite: 3.7682  — ate: 0.8809 — pehe: 3.8838 \n",
      "3/3 [==============================] - 1s 242ms/step - loss: 2389.1990 - val_loss: 1949.9132 - val_y0: 1.5434 - val_y1: -1.2151 - val_ate_afte_scaled: -2.7585 - lr: 8.8818e-19\n",
      "Epoch 251/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2823.5134 — ite: 3.7634  — ate: 0.5358 — pehe: 3.7198 \n",
      "3/3 [==============================] - 1s 242ms/step - loss: 2429.4528 - val_loss: 2215.6770 - val_y0: 1.4716 - val_y1: -1.2226 - val_ate_afte_scaled: -2.6941 - lr: 8.8818e-19\n",
      "Epoch 252/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3600.4336 — ite: 3.8432  — ate: 0.4707 — pehe: 3.7302 \n",
      "3/3 [==============================] - 0s 226ms/step - loss: 2458.4296 - val_loss: 2127.1843 - val_y0: 1.4118 - val_y1: -1.2698 - val_ate_afte_scaled: -2.6816 - lr: 8.8818e-19\n",
      "Epoch 253/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2807.2070 — ite: 3.8270  — ate: 0.5953 — pehe: 3.8610 \n",
      "3/3 [==============================] - 1s 247ms/step - loss: 2405.3120 - val_loss: 2125.7927 - val_y0: 1.4913 - val_y1: -1.1055 - val_ate_afte_scaled: -2.5968 - lr: 8.8818e-19\n",
      "Epoch 254/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2867.4914\n",
      "Epoch 00254: ReduceLROnPlateau reducing learning rate to 4.440892309431682e-19.\n",
      " — ite: 3.7632  — ate: 0.5139 — pehe: 3.8010 \n",
      "3/3 [==============================] - 1s 245ms/step - loss: 2462.5914 - val_loss: 2072.9543 - val_y0: 1.6429 - val_y1: -1.2848 - val_ate_afte_scaled: -2.9277 - lr: 8.8818e-19\n",
      "Epoch 255/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3741.5784 — ite: 3.8340  — ate: 0.5710 — pehe: 3.7384 \n",
      "3/3 [==============================] - 0s 225ms/step - loss: 2459.0236 - val_loss: 2042.4771 - val_y0: 1.5860 - val_y1: -1.2325 - val_ate_afte_scaled: -2.8186 - lr: 4.4409e-19\n",
      "Epoch 256/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3560.1396 — ite: 3.8009  — ate: 0.3321 — pehe: 3.8789 \n",
      "3/3 [==============================] - 0s 239ms/step - loss: 2451.1667 - val_loss: 2119.3296 - val_y0: 1.5707 - val_y1: -1.2176 - val_ate_afte_scaled: -2.7883 - lr: 4.4409e-19\n",
      "Epoch 257/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3741.0408 — ite: 3.7426  — ate: 0.2114 — pehe: 3.6764 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: 2454.4412 - val_loss: 2063.1426 - val_y0: 1.4407 - val_y1: -1.2186 - val_ate_afte_scaled: -2.6593 - lr: 4.4409e-19\n",
      "Epoch 258/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3813.8784 — ite: 3.9271  — ate: 0.5683 — pehe: 3.9734 \n",
      "3/3 [==============================] - 0s 217ms/step - loss: 2363.5088 - val_loss: 2218.9177 - val_y0: 1.4107 - val_y1: -1.1594 - val_ate_afte_scaled: -2.5701 - lr: 4.4409e-19\n",
      "Epoch 259/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3613.2712\n",
      "Epoch 00259: ReduceLROnPlateau reducing learning rate to 2.220446154715841e-19.\n",
      " — ite: 3.7999  — ate: 0.3734 — pehe: 3.6558 \n",
      "3/3 [==============================] - 0s 204ms/step - loss: 2416.3549 - val_loss: 2276.8086 - val_y0: 1.4747 - val_y1: -1.3798 - val_ate_afte_scaled: -2.8545 - lr: 4.4409e-19\n",
      "Epoch 260/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 2803.3324 — ite: 3.8002  — ate: 0.3065 — pehe: 3.7533 \n",
      "3/3 [==============================] - 0s 232ms/step - loss: 2400.7611 - val_loss: 2123.9514 - val_y0: 1.5153 - val_y1: -1.1520 - val_ate_afte_scaled: -2.6673 - lr: 2.2204e-19\n",
      "Epoch 261/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3518.1855 — ite: 3.8310  — ate: 0.6703 — pehe: 3.7052 \n",
      "3/3 [==============================] - 0s 193ms/step - loss: 2374.0444 - val_loss: 2090.9131 - val_y0: 1.3964 - val_y1: -1.2709 - val_ate_afte_scaled: -2.6672 - lr: 2.2204e-19\n",
      "Epoch 262/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3767.9849 — ite: 3.8187  — ate: 0.4391 — pehe: 3.6878 \n",
      "3/3 [==============================] - 0s 195ms/step - loss: 2442.0844 - val_loss: 2175.4006 - val_y0: 1.3979 - val_y1: -1.0621 - val_ate_afte_scaled: -2.4599 - lr: 2.2204e-19\n",
      "Epoch 263/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3704.9780 — ite: 3.7900  — ate: 0.4534 — pehe: 3.6580 \n",
      "3/3 [==============================] - 0s 205ms/step - loss: 2435.6087 - val_loss: 2058.4956 - val_y0: 1.3974 - val_y1: -1.1921 - val_ate_afte_scaled: -2.5895 - lr: 2.2204e-19\n",
      "Epoch 264/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4003.1133\n",
      "Epoch 00264: ReduceLROnPlateau reducing learning rate to 1.1102230773579206e-19.\n",
      " — ite: 3.8208  — ate: 0.5818 — pehe: 3.8170 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: 2601.2374 - val_loss: 1965.4076 - val_y0: 1.4340 - val_y1: -1.2376 - val_ate_afte_scaled: -2.6716 - lr: 2.2204e-19\n",
      "Epoch 265/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3680.2395 — ite: 3.8435  — ate: 0.3075 — pehe: 3.8492 \n",
      "3/3 [==============================] - 0s 190ms/step - loss: 2418.9892 - val_loss: 2242.2773 - val_y0: 1.4559 - val_y1: -1.2245 - val_ate_afte_scaled: -2.6804 - lr: 1.1102e-19\n",
      "Epoch 266/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3618.1968 — ite: 3.7790  — ate: 0.4197 — pehe: 3.8184 \n",
      "3/3 [==============================] - 0s 198ms/step - loss: 2534.4825 - val_loss: 2232.6379 - val_y0: 1.5601 - val_y1: -1.2896 - val_ate_afte_scaled: -2.8497 - lr: 1.1102e-19\n",
      "Epoch 267/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3806.9578 — ite: 3.7360  — ate: 0.4699 — pehe: 3.6146 \n",
      "3/3 [==============================] - 0s 189ms/step - loss: 2489.8480 - val_loss: 2203.9509 - val_y0: 1.6564 - val_y1: -1.3141 - val_ate_afte_scaled: -2.9706 - lr: 1.1102e-19\n",
      "Epoch 268/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3466.5366 — ite: 3.7910  — ate: 0.3819 — pehe: 3.8792 \n",
      "3/3 [==============================] - 0s 175ms/step - loss: 2393.1849 - val_loss: 2196.3418 - val_y0: 1.5016 - val_y1: -1.2332 - val_ate_afte_scaled: -2.7348 - lr: 1.1102e-19\n",
      "Epoch 269/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3646.5254\n",
      "Epoch 00269: ReduceLROnPlateau reducing learning rate to 5.551115386789603e-20.\n",
      " — ite: 3.8506  — ate: 0.8424 — pehe: 3.8233 \n",
      "3/3 [==============================] - 0s 192ms/step - loss: 2397.0587 - val_loss: 2184.8894 - val_y0: 1.6056 - val_y1: -1.3033 - val_ate_afte_scaled: -2.9088 - lr: 1.1102e-19\n",
      "Epoch 270/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3690.8044 — ite: 3.8213  — ate: 0.5552 — pehe: 3.7930 \n",
      "3/3 [==============================] - 0s 198ms/step - loss: 2463.5439 - val_loss: 2249.8899 - val_y0: 1.6391 - val_y1: -1.1452 - val_ate_afte_scaled: -2.7843 - lr: 5.5511e-20\n",
      "Epoch 271/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3643.6221 — ite: 3.9015  — ate: 0.6027 — pehe: 3.8087 \n",
      "3/3 [==============================] - 0s 178ms/step - loss: 2492.1398 - val_loss: 2100.0784 - val_y0: 1.3728 - val_y1: -1.2979 - val_ate_afte_scaled: -2.6708 - lr: 5.5511e-20\n",
      "Epoch 272/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3502.9377 — ite: 3.8796  — ate: 0.7746 — pehe: 3.7957 \n",
      "3/3 [==============================] - 0s 185ms/step - loss: 2400.4395 - val_loss: 2308.6338 - val_y0: 1.4758 - val_y1: -1.2018 - val_ate_afte_scaled: -2.6777 - lr: 5.5511e-20\n",
      "Epoch 273/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3770.4656 — ite: 3.9315  — ate: 0.3824 — pehe: 3.7673 \n",
      "3/3 [==============================] - 0s 183ms/step - loss: 2394.3060 - val_loss: 2045.2960 - val_y0: 1.5631 - val_y1: -1.3665 - val_ate_afte_scaled: -2.9296 - lr: 5.5511e-20\n",
      "Epoch 274/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3589.7039\n",
      "Epoch 00274: ReduceLROnPlateau reducing learning rate to 2.7755576933948015e-20.\n",
      " — ite: 3.8526  — ate: 0.5374 — pehe: 3.8784 \n",
      "3/3 [==============================] - 0s 186ms/step - loss: 2428.6598 - val_loss: 2192.2407 - val_y0: 1.4716 - val_y1: -1.2184 - val_ate_afte_scaled: -2.6899 - lr: 5.5511e-20\n",
      "Epoch 275/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3668.1111 — ite: 3.8748  — ate: 0.4585 — pehe: 4.1010 \n",
      "3/3 [==============================] - 0s 192ms/step - loss: 2483.6066 - val_loss: 2207.6982 - val_y0: 1.5505 - val_y1: -1.2520 - val_ate_afte_scaled: -2.8025 - lr: 2.7756e-20\n",
      "Epoch 276/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3768.6392 — ite: 3.8291  — ate: 0.4905 — pehe: 3.9671 \n",
      "3/3 [==============================] - 0s 179ms/step - loss: 2475.7507 - val_loss: 2051.5986 - val_y0: 1.5588 - val_y1: -1.3810 - val_ate_afte_scaled: -2.9398 - lr: 2.7756e-20\n",
      "Epoch 277/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3804.0498 — ite: 3.8015  — ate: 0.4310 — pehe: 3.7761 \n",
      "3/3 [==============================] - 0s 183ms/step - loss: 2515.0093 - val_loss: 2071.9194 - val_y0: 1.5411 - val_y1: -1.3073 - val_ate_afte_scaled: -2.8483 - lr: 2.7756e-20\n",
      "Epoch 278/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3870.4297 — ite: 3.8081  — ate: 0.5699 — pehe: 3.6915 \n",
      "3/3 [==============================] - 0s 193ms/step - loss: 2519.0755 - val_loss: 2266.2625 - val_y0: 1.3328 - val_y1: -1.1803 - val_ate_afte_scaled: -2.5130 - lr: 2.7756e-20\n",
      "Epoch 279/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3690.3706\n",
      "Epoch 00279: ReduceLROnPlateau reducing learning rate to 1.3877788466974007e-20.\n",
      " — ite: 3.8602  — ate: 0.3021 — pehe: 3.9384 \n",
      "3/3 [==============================] - 0s 169ms/step - loss: 2481.2130 - val_loss: 2201.1025 - val_y0: 1.5457 - val_y1: -1.3381 - val_ate_afte_scaled: -2.8838 - lr: 2.7756e-20\n",
      "Epoch 280/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3700.6602 — ite: 3.8524  — ate: 0.5937 — pehe: 3.7214 \n",
      "3/3 [==============================] - 0s 178ms/step - loss: 2424.1006 - val_loss: 2122.3408 - val_y0: 1.3693 - val_y1: -1.1746 - val_ate_afte_scaled: -2.5439 - lr: 1.3878e-20\n",
      "Epoch 281/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3599.5820 — ite: 3.8828  — ate: 0.6770 — pehe: 3.8244 \n",
      "3/3 [==============================] - 0s 182ms/step - loss: 2460.7474 - val_loss: 2455.3132 - val_y0: 1.6431 - val_y1: -1.2999 - val_ate_afte_scaled: -2.9430 - lr: 1.3878e-20\n",
      "Epoch 282/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3677.8296 — ite: 3.8634  — ate: 0.3719 — pehe: 3.8409 \n",
      "3/3 [==============================] - 0s 213ms/step - loss: 2428.8553 - val_loss: 2234.5054 - val_y0: 1.6239 - val_y1: -1.3100 - val_ate_afte_scaled: -2.9339 - lr: 1.3878e-20\n",
      "Epoch 283/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3774.1389 — ite: 3.8133  — ate: 0.8158 — pehe: 3.8327 \n",
      "3/3 [==============================] - 0s 207ms/step - loss: 2487.9769 - val_loss: 2290.7231 - val_y0: 1.6092 - val_y1: -1.3523 - val_ate_afte_scaled: -2.9614 - lr: 1.3878e-20\n",
      "Epoch 284/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3587.1350\n",
      "Epoch 00284: ReduceLROnPlateau reducing learning rate to 6.938894233487004e-21.\n",
      " — ite: 3.9149  — ate: 0.5334 — pehe: 3.9046 \n",
      "3/3 [==============================] - 0s 188ms/step - loss: 2441.6919 - val_loss: 2205.8203 - val_y0: 1.5473 - val_y1: -1.3009 - val_ate_afte_scaled: -2.8482 - lr: 1.3878e-20\n",
      "Epoch 285/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3715.4177 — ite: 3.8487  — ate: 0.3029 — pehe: 3.7658 \n",
      "3/3 [==============================] - 0s 191ms/step - loss: 2503.4902 - val_loss: 2248.7280 - val_y0: 1.5479 - val_y1: -1.4271 - val_ate_afte_scaled: -2.9750 - lr: 6.9389e-21\n",
      "Epoch 286/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3566.0186 — ite: 3.8734  — ate: 0.5100 — pehe: 3.7725 \n",
      "3/3 [==============================] - 0s 198ms/step - loss: 2376.9362 - val_loss: 2120.2258 - val_y0: 1.5306 - val_y1: -1.1972 - val_ate_afte_scaled: -2.7278 - lr: 6.9389e-21\n",
      "Epoch 287/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3704.3616 — ite: 3.7584  — ate: 0.7858 — pehe: 3.8707 \n",
      "3/3 [==============================] - 0s 190ms/step - loss: 2467.4806 - val_loss: 2211.5237 - val_y0: 1.5052 - val_y1: -1.2169 - val_ate_afte_scaled: -2.7220 - lr: 6.9389e-21\n",
      "Epoch 288/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3766.6023 — ite: 3.7544  — ate: 0.3447 — pehe: 3.7910 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: 2517.1171 - val_loss: 2050.9849 - val_y0: 1.5233 - val_y1: -1.2058 - val_ate_afte_scaled: -2.7290 - lr: 6.9389e-21\n",
      "Epoch 289/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3792.8630\n",
      "Epoch 00289: ReduceLROnPlateau reducing learning rate to 3.469447116743502e-21.\n",
      " — ite: 3.8875  — ate: 0.3986 — pehe: 3.8432 \n",
      "3/3 [==============================] - 0s 196ms/step - loss: 2474.8943 - val_loss: 2032.1302 - val_y0: 1.4663 - val_y1: -1.1650 - val_ate_afte_scaled: -2.6313 - lr: 6.9389e-21\n",
      "Epoch 290/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3432.1592 — ite: 3.8639  — ate: 0.4660 — pehe: 3.8591 \n",
      "3/3 [==============================] - 0s 197ms/step - loss: 2438.7877 - val_loss: 2115.7090 - val_y0: 1.5421 - val_y1: -1.1826 - val_ate_afte_scaled: -2.7247 - lr: 3.4694e-21\n",
      "Epoch 291/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3394.5613 — ite: 3.8130  — ate: 0.9640 — pehe: 3.8966 \n",
      "3/3 [==============================] - 0s 186ms/step - loss: 2496.9379 - val_loss: 2119.2632 - val_y0: 1.5263 - val_y1: -1.3919 - val_ate_afte_scaled: -2.9183 - lr: 3.4694e-21\n",
      "Epoch 292/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3884.8889 — ite: 3.8591  — ate: 0.6264 — pehe: 3.7939 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: 2475.6667 - val_loss: 2158.4460 - val_y0: 1.4473 - val_y1: -1.3070 - val_ate_afte_scaled: -2.7543 - lr: 3.4694e-21\n",
      "Epoch 293/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3521.9487 — ite: 3.7840  — ate: 0.4888 — pehe: 3.8790 \n",
      "3/3 [==============================] - 0s 186ms/step - loss: 2496.1575 - val_loss: 2139.7502 - val_y0: 1.6935 - val_y1: -1.2565 - val_ate_afte_scaled: -2.9500 - lr: 3.4694e-21\n",
      "Epoch 294/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3587.7625\n",
      "Epoch 00294: ReduceLROnPlateau reducing learning rate to 1.734723558371751e-21.\n",
      " — ite: 3.7258  — ate: 0.2124 — pehe: 3.5560 \n",
      "3/3 [==============================] - 0s 183ms/step - loss: 2441.4930 - val_loss: 2006.1127 - val_y0: 1.5231 - val_y1: -1.1510 - val_ate_afte_scaled: -2.6741 - lr: 3.4694e-21\n",
      "Epoch 295/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3686.6982 — ite: 3.8344  — ate: 0.4221 — pehe: 3.8465 \n",
      "3/3 [==============================] - 0s 189ms/step - loss: 2480.9189 - val_loss: 2192.4607 - val_y0: 1.4104 - val_y1: -1.1732 - val_ate_afte_scaled: -2.5836 - lr: 1.7347e-21\n",
      "Epoch 296/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3607.2039 — ite: 3.9402  — ate: 0.6344 — pehe: 3.9525 \n",
      "3/3 [==============================] - 0s 196ms/step - loss: 2493.3889 - val_loss: 2186.8137 - val_y0: 1.4728 - val_y1: -1.2783 - val_ate_afte_scaled: -2.7511 - lr: 1.7347e-21\n",
      "Epoch 297/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3579.2549 — ite: 3.9084  — ate: 0.3659 — pehe: 3.8867 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: 2304.6470 - val_loss: 2126.2600 - val_y0: 1.4474 - val_y1: -1.3079 - val_ate_afte_scaled: -2.7553 - lr: 1.7347e-21\n",
      "Epoch 298/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3702.9014 — ite: 3.9183  — ate: 0.3582 — pehe: 3.9512 \n",
      "3/3 [==============================] - 0s 205ms/step - loss: 2397.5013 - val_loss: 2144.6128 - val_y0: 1.5315 - val_y1: -1.3018 - val_ate_afte_scaled: -2.8333 - lr: 1.7347e-21\n",
      "Epoch 299/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3670.6731\n",
      "Epoch 00299: ReduceLROnPlateau reducing learning rate to 8.673617791858755e-22.\n",
      " — ite: 3.7824  — ate: 0.3793 — pehe: 3.8162 \n",
      "3/3 [==============================] - 0s 179ms/step - loss: 2530.6651 - val_loss: 2298.8206 - val_y0: 1.5755 - val_y1: -1.3155 - val_ate_afte_scaled: -2.8911 - lr: 1.7347e-21\n",
      "Epoch 300/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3622.7410 — ite: 3.8711  — ate: 0.5861 — pehe: 3.7697 \n",
      "3/3 [==============================] - 0s 181ms/step - loss: 2425.8331 - val_loss: 2111.9937 - val_y0: 1.5331 - val_y1: -1.2420 - val_ate_afte_scaled: -2.7751 - lr: 8.6736e-22\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Colab command to allow us to run Colab in TF2\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "%reload_ext tensorboard \n",
    "\n",
    "model = CEWAE()\n",
    "### MAIN CODE ####\n",
    "verbose=True\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    " \n",
    "callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        # EarlyStopping(monitor='val_loss', patience=40, min_delta=0), \n",
    "        #40 is Shi's recommendation patience for this dataset, but you should tune for your data \n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0, cooldown=0, min_lr=0),\n",
    "        #This learning rate scheduling is quite agressive which seems good for this dataset\n",
    "        metrics_for_cevae(data,verbose),\n",
    "        tensorboard_callback\n",
    "    ]\n",
    "    \n",
    "#optimizer hyperparameters\n",
    "learning_rate = 5e-4\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate))\n",
    "\n",
    "model.fit(\n",
    "    [data['x'],data['t'],data['ys']],\n",
    "    callbacks=callbacks,\n",
    "    validation_split=args.val_split,\n",
    "    epochs=300,\n",
    "    batch_size=args.batch_size,\n",
    "    verbose=verbose\n",
    "    )\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 45269), started 1 day, 22:31:43 ago. (Use '!kill 45269' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ed9f3fb35a9b74fd\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ed9f3fb35a9b74fd\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
