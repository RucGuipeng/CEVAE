{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 “ihdp_npci_1-100.train.npz” 已经存在；不获取。\n",
      "\n",
      "文件 “ihdp_npci_1-100.test.npz” 已经存在；不获取。\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([5.26691518]), array([2.59847927]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.layers as tfkl\n",
    "tfd,tfpl = tfp.distributions,tfp.layers\n",
    "import tensorflow.keras.backend as tfkb\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from evaluation import *\n",
    "from cevae_networks import *\n",
    "################################################\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='')\n",
    "parser.add_argument('--scale_penalize',    type = float, default = 0.001,  help = '')\n",
    "parser.add_argument('--learning_rate',     type = float, default = 0.001,  help = '')\n",
    "parser.add_argument('--default_y_scale',   type = float, default = 1.,  help = '')\n",
    "parser.add_argument('--t_dim',     type = int, default = 1,  help = '')\n",
    "parser.add_argument('--y_dim',     type = int, default = 1,  help = '')\n",
    "parser.add_argument('--x_dim',     type = int, default = 25, help = '')\n",
    "parser.add_argument('--z_dim',     type = int, default = 20, help = '')\n",
    "parser.add_argument('--x_num_dim', type = int, default = 6,  help = '')\n",
    "parser.add_argument('--x_bin_dim', type = int, default = 19, help = '')\n",
    "parser.add_argument('--val_split', type = float, default = 0.2, help = '')\n",
    "parser.add_argument('--batch_size', type = int, default = 200, help = '')\n",
    "parser.add_argument('--nh', type = int, default = 3, help = 'number of hidden layers')\n",
    "parser.add_argument('--h',  type = int, default = 200, help = 'number of hidden units')\n",
    "args = parser.parse_args([])\n",
    "################################################\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz \n",
    "\n",
    "def load_IHDP_data(training_data,testing_data,i):\n",
    "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "        train_data=np.load(trf); test_data=np.load(tef)\n",
    "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
    "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
    "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
    "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    "        ycf=np.concatenate((train_data['ycf'][:,i],  test_data['ycf'][:,i])).astype('float32')\n",
    "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "        data['y']=data['y'].reshape(-1,1)\n",
    "        data['ycf'] = ycf.reshape(-1,1)\n",
    "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "    return data\n",
    "\n",
    "ind = 7\n",
    "rep = 1\n",
    "data = load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.test.npz',i = ind)\n",
    "for key in data:\n",
    "    if key != 'y_scaler':\n",
    "        data[key] = np.repeat(data[key],repeats = rep, axis = 0)\n",
    "data['y_scaler'].mean_, data['y_scaler'].scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonLayer(tfkl.Layer):\n",
    "    def __init__(self):\n",
    "        super(EpsilonLayer, self).__init__()\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.epsilon = self.add_weight(name='epsilon',\n",
    "                                       shape=[1, 1],\n",
    "                                       initializer='RandomNormal',\n",
    "                                       #  initializer='ones',\n",
    "                                       trainable=True)\n",
    "        super(EpsilonLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        #note there is only one epsilon were just duplicating it for conformability\n",
    "        return self.epsilon * tf.ones_like(inputs)[:, 0:1]\n",
    "\n",
    "class CEWAE(tf.keras.Model):\n",
    "    def __init__(self, kernel = \"IMQ\"):\n",
    "        super(CEWAE, self).__init__()\n",
    "        ########################################\n",
    "        # networks\n",
    "        self.activation = 'elu'\n",
    "        self.kernel = kernel\n",
    "        # CEVAE Model \n",
    "        ## (encoder)\n",
    "        self.q_y_tx = q_y_tx(args.x_bin_dim, args.x_num_dim, args.y_dim, args.t_dim, args.nh, args.h)\n",
    "        self.q_t_x = q_t_x(args.x_bin_dim, args.x_num_dim, args.t_dim, args.nh, args.h)\n",
    "        self.q_z_txy = q_z_txy(args.x_bin_dim, args.x_num_dim, args.y_dim, args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        ## (decoder)\n",
    "        self.p_x_z = p_x_z(args.x_bin_dim, args.x_num_dim, args.z_dim, args.nh, args.h)\n",
    "        self.p_t_z = p_t_z(args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        self.p_y_tz = p_y_tz(args.y_dim, args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        self.epsilon_layer = EpsilonLayer()\n",
    "        self.beta = 1\n",
    "        self.lmbda = 1\n",
    "\n",
    "    def call(self, data, training=False):\n",
    "        if training:\n",
    "            x_train,t_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "            \n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            \n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.sample()\n",
    "            # decoder\n",
    "            ## p(x|z)\n",
    "            x_num,x_bin = self.p_x_z(z_infer_sample)\n",
    "            ## p(t|z)\n",
    "            t = self.p_t_z(z_infer_sample)\n",
    "            ## p(y|t,z)\n",
    "            t0z = tf.concat([tf.zeros_like(t_train),z_infer_sample],-1)\n",
    "            t1z = tf.concat([tf.ones_like(t_train),z_infer_sample],-1)\n",
    "            y0 = self.p_y_tz(t0z)\n",
    "            y1 = self.p_y_tz(t1z)\n",
    "            y = [y0,y1]\n",
    "            epsilon = self.epsilon_layer(t_infer_sample)\n",
    "            \n",
    "            return y_infer,t_infer,z_infer,y,t,x_num,x_bin,epsilon\n",
    "        else:\n",
    "            x_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.loc\n",
    "\n",
    "            t1z = tf.concat([tf.ones_like(t_infer_sample),z_infer_sample],-1)\n",
    "            t0z = tf.concat([tf.zeros_like(t_infer_sample),z_infer_sample],-1)\n",
    "            y0 = self.p_y_tz(t0z)\n",
    "            y1 = self.p_y_tz(t1z)\n",
    "            y = [y0,y1]\n",
    "            return y,t_infer,z_infer\n",
    "\n",
    "    def mmd_penalty(self, sample_qz, sample_pz, batch_size = args.batch_size):\n",
    "        opts = {'kernel': self.kernel, 'verbose':True, \"zdim\":20, \"pz\":\"normal\"} \n",
    "        sigma2_p = 1 ** 2\n",
    "        kernel = opts['kernel']\n",
    "        n = batch_size\n",
    "        n = tf.shape(sample_qz)[0]\n",
    "        n = tf.cast(n, tf.int32)\n",
    "        nf = tf.cast(n, tf.float32)\n",
    "        half_size = (n * n - n) / 2\n",
    "\n",
    "        norms_pz = tf.reduce_sum(tf.square(sample_pz), axis=1, keepdims=True)\n",
    "        dotprods_pz = tf.matmul(sample_pz, sample_pz, transpose_b=True)\n",
    "        distances_pz = norms_pz + tf.transpose(norms_pz) - 2. * dotprods_pz\n",
    "\n",
    "        norms_qz = tf.reduce_sum(tf.square(sample_qz), axis=1, keepdims=True)\n",
    "        dotprods_qz = tf.matmul(sample_qz, sample_qz, transpose_b=True)\n",
    "        distances_qz = norms_qz + tf.transpose(norms_qz) - 2. * dotprods_qz\n",
    "\n",
    "        dotprods = tf.matmul(sample_qz, sample_pz, transpose_b=True)\n",
    "        distances = norms_qz + tf.transpose(norms_pz) - 2. * dotprods\n",
    "\n",
    "        if kernel == 'RBF':\n",
    "            # Median heuristic for the sigma^2 of Gaussian kernel\n",
    "            sigma2_k = tf.nn.top_k(\n",
    "                tf.reshape(distances, [-1]), half_size).values[half_size - 1]\n",
    "            sigma2_k += tf.nn.top_k(\n",
    "                tf.reshape(distances_qz, [-1]), half_size).values[half_size - 1]\n",
    "            # Maximal heuristic for the sigma^2 of Gaussian kernel\n",
    "            # sigma2_k = tf.nn.top_k(tf.reshape(distances_qz, [-1]), 1).values[0]\n",
    "            # sigma2_k += tf.nn.top_k(tf.reshape(distances, [-1]), 1).values[0]\n",
    "            # sigma2_k = opts['latent_space_dim'] * sigma2_p\n",
    "            if opts['verbose']:\n",
    "                sigma2_k = tf.Print(sigma2_k, [sigma2_k], 'Kernel width:')\n",
    "            res1 = tf.exp( - distances_qz / 2. / sigma2_k)\n",
    "            res1 += tf.exp( - distances_pz / 2. / sigma2_k)\n",
    "            res1 = tf.multiply(res1, 1. - tf.eye(n))\n",
    "            res1 = tf.reduce_sum(res1) / (nf * nf - nf)\n",
    "            res2 = tf.exp( - distances / 2. / sigma2_k)\n",
    "            res2 = tf.reduce_sum(res2) * 2. / (nf * nf)\n",
    "            stat = res1 - res2\n",
    "        elif kernel == 'IMQ':\n",
    "            # k(x, y) = C / (C + ||x - y||^2)\n",
    "            # C = tf.nn.top_k(tf.reshape(distances, [-1]), half_size).values[half_size - 1]\n",
    "            # C += tf.nn.top_k(tf.reshape(distances_qz, [-1]), half_size).values[half_size - 1]\n",
    "            if opts['pz'] == 'normal':\n",
    "                Cbase = 2. * opts['zdim'] * sigma2_p\n",
    "            elif opts['pz'] == 'sphere':\n",
    "                Cbase = 2.\n",
    "            elif opts['pz'] == 'uniform':\n",
    "                # E ||x - y||^2 = E[sum (xi - yi)^2]\n",
    "                #               = zdim E[(xi - yi)^2]\n",
    "                #               = const * zdim\n",
    "                Cbase = opts['zdim']\n",
    "            stat = 0.\n",
    "            for scale in [.1, .2, .5, 1., 2., 5., 10.]:\n",
    "                C = Cbase * scale\n",
    "                res1 = C / (C + distances_qz)\n",
    "                res1 += C / (C + distances_pz)\n",
    "                res1 = tf.multiply(res1, 1. - tf.eye(n))\n",
    "                res1 = tf.reduce_sum(res1) / (nf * nf - nf)\n",
    "                res2 = C / (C + distances)\n",
    "                res2 = tf.reduce_sum(res2) * 2. / (nf * nf)\n",
    "                stat += res1 - res2\n",
    "        return stat\n",
    "\n",
    "    def cewae_loss(self, data, pred, training = False):\n",
    "        x_train, t_train, y_train = data[0],data[1],data[2]\n",
    "        x_train_num, x_train_bin = x_train[:,:args.x_num_dim],x_train[:,args.x_num_dim:]\n",
    "        y_infer,t_infer,z_infer,y,t,x_num,x_bin,epsilon = pred\n",
    "        # y0_infer,y1_infer = y_infer\n",
    "        y0,y1 = y\n",
    "        # reconstruct loss\n",
    "        rec_x_num = tfkb.mean(tf.math.square(x_train_num - x_num.sample()))\n",
    "        rec_x_bin = tf.reduce_sum(\n",
    "            tfk.losses.binary_crossentropy(\n",
    "                x_train_bin,\n",
    "                tf.cast(x_bin.sample(),tf.float32),\n",
    "                from_logits=False))\n",
    "        rec_t_bin = tf.reduce_sum(\n",
    "            tfk.losses.binary_crossentropy(\n",
    "                t_train,\n",
    "                tf.cast(t_infer.sample(),tf.float32),\n",
    "                from_logits=False))\n",
    "        rec_y0 = tf.math.square(y0.sample() - y_train)\n",
    "        rec_y1 = tf.math.square(y1.sample() - y_train)\n",
    "        rec_y = tfkb.mean(t_train * rec_y1 + (1-t_train)* rec_y0)\n",
    "        # regularization\n",
    "        # mmd penalty\n",
    "        pz = tfd.Normal(loc = tf.zeros_like(z_infer.sample()), scale = tf.ones_like(z_infer.sample()))\n",
    "        reg_mmd = self.mmd_penalty(z_infer.sample(), pz.sample())\n",
    "        loss = rec_x_num + rec_x_bin + rec_t_bin + rec_y + reg_mmd * self.lmbda\n",
    "        loss = rec_x_num + rec_x_bin + rec_t_bin + rec_y + reg_mmd\n",
    "        # loss = rec_x_num + rec_x_bin + rec_t_bin + rec_y \n",
    "\n",
    "        # target regularization\n",
    "        y_pred = y0.loc * (1-t_train) + y1.loc * t_train\n",
    "        t_pred = tf.math.sigmoid(t.logits)\n",
    "        cc = t_train/t_pred - (1-t_train) / (1-t_pred)\n",
    "        t_reg = tf.math.square(y_pred + epsilon * cc - y_train)\n",
    "        loss += tfkb.mean(t_reg) * self.beta\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, data):\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "            loss = self.cewae_loss(data = data, pred = pred, training = True)\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        metrics = {\"loss\": loss}\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "        y_infer = pred[0]\n",
    "        loss = self.cewae_loss(data = data, pred = pred, training = False)\n",
    "        y0, y1 = y_infer[0].sample(),y_infer[1].sample()\n",
    "        ate = tfkb.mean(y1) - tfkb.mean(y0)\n",
    "        metrics = {\"loss\":loss,\"y0\": tfkb.mean(y0),\"y1\": tfkb.mean(y1),'ate_afte_scaled': ate}\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-26 16:05:43.976686: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_6/kernel:0', 'dense_6/bias:0', 'dense_7/kernel:0', 'dense_7/bias:0', 'dense_8/kernel:0', 'dense_8/bias:0', 'dense_9/kernel:0', 'dense_9/bias:0', 'dense_20/kernel:0', 'dense_20/bias:0', 'dense_21/kernel:0', 'dense_21/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_6/kernel:0', 'dense_6/bias:0', 'dense_7/kernel:0', 'dense_7/bias:0', 'dense_8/kernel:0', 'dense_8/bias:0', 'dense_9/kernel:0', 'dense_9/bias:0', 'dense_20/kernel:0', 'dense_20/bias:0', 'dense_21/kernel:0', 'dense_21/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "1/3 [=========>....................] - ETA: 13s - loss: 3033.5308 — ite: 4.4774  — ate: 3.1075 — pehe: 5.1418 \n",
      "3/3 [==============================] - 9s 935ms/step - loss: 3048.9556 - val_loss: 2464.4839 - val_y0: -0.1685 - val_y1: -0.3997 - val_ate_afte_scaled: -0.2311 - lr: 5.0000e-04\n",
      "Epoch 2/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3130.7668 — ite: 4.0488  — ate: 1.9800 — pehe: 4.2901 \n",
      "3/3 [==============================] - 1s 249ms/step - loss: 3075.3306 - val_loss: 2256.5337 - val_y0: -0.6991 - val_y1: -0.6682 - val_ate_afte_scaled: 0.0309 - lr: 5.0000e-04\n",
      "Epoch 3/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3105.1509WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0119s vs `on_train_batch_end` time: 0.0156s). Check your callbacks.\n",
      " — ite: 3.7584  — ate: 0.0893 — pehe: 4.0541 \n",
      "3/3 [==============================] - 1s 258ms/step - loss: 3029.3995 - val_loss: 2283.7085 - val_y0: -0.9646 - val_y1: -0.6888 - val_ate_afte_scaled: 0.2758 - lr: 5.0000e-04\n",
      "Epoch 4/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3130.0996 — ite: 3.6913  — ate: 0.0134 — pehe: 3.8497 \n",
      "3/3 [==============================] - 0s 206ms/step - loss: 3082.1568 - val_loss: 2354.1770 - val_y0: -0.6761 - val_y1: -0.5930 - val_ate_afte_scaled: 0.0831 - lr: 5.0000e-04\n",
      "Epoch 5/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2890.4663 — ite: 3.7888  — ate: 1.0903 — pehe: 4.0185 \n",
      "3/3 [==============================] - 0s 223ms/step - loss: 3107.7558 - val_loss: 2497.6338 - val_y0: -0.3487 - val_y1: -0.0040 - val_ate_afte_scaled: 0.3446 - lr: 5.0000e-04\n",
      "Epoch 6/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2981.7102 — ite: 4.0240  — ate: 1.5436 — pehe: 4.1666 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: 3008.0073 - val_loss: 2313.3105 - val_y0: 0.0062 - val_y1: 0.2292 - val_ate_afte_scaled: 0.2230 - lr: 5.0000e-04\n",
      "Epoch 7/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3053.0186 — ite: 4.0484  — ate: 1.4237 — pehe: 3.9136 \n",
      "3/3 [==============================] - 1s 254ms/step - loss: 2996.1248 - val_loss: 2346.7217 - val_y0: -0.0511 - val_y1: 0.2448 - val_ate_afte_scaled: 0.2960 - lr: 5.0000e-04\n",
      "Epoch 8/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3087.0776\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      " — ite: 4.0944  — ate: 1.3051 — pehe: 4.0872 \n",
      "3/3 [==============================] - 1s 269ms/step - loss: 3032.5139 - val_loss: 2257.9844 - val_y0: -0.0206 - val_y1: -0.0710 - val_ate_afte_scaled: -0.0504 - lr: 5.0000e-04\n",
      "Epoch 9/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3120.8254 — ite: 3.9247  — ate: 1.1021 — pehe: 3.9342 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: 2988.3796 - val_loss: 2296.7725 - val_y0: 0.1101 - val_y1: -0.0019 - val_ate_afte_scaled: -0.1120 - lr: 2.5000e-04\n",
      "Epoch 10/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3065.1292 — ite: 3.9207  — ate: 1.0398 — pehe: 3.9874 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: 2990.7859 - val_loss: 2367.7871 - val_y0: 0.1356 - val_y1: -0.0071 - val_ate_afte_scaled: -0.1427 - lr: 2.5000e-04\n",
      "Epoch 11/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3026.2019 — ite: 3.8861  — ate: 0.9687 — pehe: 4.0884 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: 3029.7858 - val_loss: 2508.1250 - val_y0: 0.1463 - val_y1: -0.0667 - val_ate_afte_scaled: -0.2130 - lr: 2.5000e-04\n",
      "Epoch 12/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3054.5222 — ite: 3.7615  — ate: 0.6828 — pehe: 3.9104 \n",
      "3/3 [==============================] - 0s 204ms/step - loss: 3039.5679 - val_loss: 2258.4036 - val_y0: -0.0943 - val_y1: -0.2264 - val_ate_afte_scaled: -0.1320 - lr: 2.5000e-04\n",
      "Epoch 13/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3346.4790\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      " — ite: 3.8136  — ate: 0.8955 — pehe: 4.0362 \n",
      "3/3 [==============================] - 0s 201ms/step - loss: 3160.6183 - val_loss: 2234.4751 - val_y0: -0.2988 - val_y1: -0.1641 - val_ate_afte_scaled: 0.1347 - lr: 2.5000e-04\n",
      "Epoch 14/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3073.9893 — ite: 3.8494  — ate: 0.8091 — pehe: 4.0717 \n",
      "3/3 [==============================] - 0s 197ms/step - loss: 3017.4423 - val_loss: 2339.2734 - val_y0: -0.3534 - val_y1: -0.3232 - val_ate_afte_scaled: 0.0302 - lr: 1.2500e-04\n",
      "Epoch 15/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3037.1626 — ite: 3.8439  — ate: 0.7463 — pehe: 3.8829 \n",
      "3/3 [==============================] - 0s 201ms/step - loss: 2988.6133 - val_loss: 2255.7229 - val_y0: -0.3743 - val_y1: -0.3385 - val_ate_afte_scaled: 0.0359 - lr: 1.2500e-04\n",
      "Epoch 16/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3003.1079 — ite: 3.8726  — ate: 0.5527 — pehe: 3.8231 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: 3006.5360 - val_loss: 2238.2637 - val_y0: -0.2564 - val_y1: -0.4331 - val_ate_afte_scaled: -0.1767 - lr: 1.2500e-04\n",
      "Epoch 17/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3069.4346 — ite: 3.9482  — ate: 0.7597 — pehe: 3.9499 \n",
      "3/3 [==============================] - 0s 201ms/step - loss: 2964.7597 - val_loss: 2338.0981 - val_y0: -0.2795 - val_y1: -0.4422 - val_ate_afte_scaled: -0.1627 - lr: 1.2500e-04\n",
      "Epoch 18/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2890.7437\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      " — ite: 3.8445  — ate: 1.0334 — pehe: 3.7938 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: 2969.5215 - val_loss: 2355.6191 - val_y0: -0.3299 - val_y1: -0.1991 - val_ate_afte_scaled: 0.1308 - lr: 1.2500e-04\n",
      "Epoch 19/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3243.6816 — ite: 3.8976  — ate: 0.9156 — pehe: 3.7966 \n",
      "3/3 [==============================] - 0s 197ms/step - loss: 3070.2838 - val_loss: 2309.1685 - val_y0: -0.3587 - val_y1: -0.3520 - val_ate_afte_scaled: 0.0067 - lr: 6.2500e-05\n",
      "Epoch 20/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3162.8230 — ite: 3.9537  — ate: 0.6831 — pehe: 3.7961 \n",
      "3/3 [==============================] - 0s 207ms/step - loss: 3102.0463 - val_loss: 2221.9839 - val_y0: -0.3769 - val_y1: -0.5305 - val_ate_afte_scaled: -0.1536 - lr: 6.2500e-05\n",
      "Epoch 21/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2929.9661 — ite: 3.9556  — ate: 1.0090 — pehe: 3.9888 \n",
      "3/3 [==============================] - 0s 198ms/step - loss: 3093.2407 - val_loss: 2456.4507 - val_y0: -0.3752 - val_y1: -0.4741 - val_ate_afte_scaled: -0.0989 - lr: 6.2500e-05\n",
      "Epoch 22/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3029.2495 — ite: 3.8505  — ate: 0.9125 — pehe: 4.0128 \n",
      "3/3 [==============================] - 0s 193ms/step - loss: 2971.5934 - val_loss: 2308.6768 - val_y0: -0.4563 - val_y1: -0.4905 - val_ate_afte_scaled: -0.0342 - lr: 6.2500e-05\n",
      "Epoch 23/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3274.7251\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      " — ite: 3.8475  — ate: 1.0470 — pehe: 3.9491 \n",
      "3/3 [==============================] - 0s 201ms/step - loss: 3150.0327 - val_loss: 2458.9048 - val_y0: -0.2685 - val_y1: -0.3719 - val_ate_afte_scaled: -0.1034 - lr: 6.2500e-05\n",
      "Epoch 24/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3186.0283 — ite: 3.8969  — ate: 0.6682 — pehe: 3.9182 \n",
      "3/3 [==============================] - 0s 201ms/step - loss: 3029.0961 - val_loss: 2343.5571 - val_y0: -0.4439 - val_y1: -0.4140 - val_ate_afte_scaled: 0.0299 - lr: 3.1250e-05\n",
      "Epoch 25/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3295.6807 — ite: 3.8752  — ate: 0.6428 — pehe: 3.8047 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: 2982.6053 - val_loss: 2427.4077 - val_y0: -0.4445 - val_y1: -0.4992 - val_ate_afte_scaled: -0.0548 - lr: 3.1250e-05\n",
      "Epoch 26/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3121.9280 — ite: 3.7998  — ate: 0.7266 — pehe: 3.6782 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: 3052.3304 - val_loss: 2428.9846 - val_y0: -0.4011 - val_y1: -0.3690 - val_ate_afte_scaled: 0.0322 - lr: 3.1250e-05\n",
      "Epoch 27/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3136.6646 — ite: 3.8619  — ate: 0.6227 — pehe: 3.8144 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: 3111.7151 - val_loss: 2420.5542 - val_y0: -0.4238 - val_y1: -0.6665 - val_ate_afte_scaled: -0.2427 - lr: 3.1250e-05\n",
      "Epoch 28/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3070.2771 — ite: 3.8203  — ate: 0.9091 — pehe: 3.8258 \n",
      "3/3 [==============================] - 0s 195ms/step - loss: 2957.0265 - val_loss: 2108.0115 - val_y0: -0.4975 - val_y1: -0.5753 - val_ate_afte_scaled: -0.0778 - lr: 3.1250e-05\n",
      "Epoch 29/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3006.0095 — ite: 3.9306  — ate: 0.9053 — pehe: 3.9741 \n",
      "3/3 [==============================] - 0s 193ms/step - loss: 3010.4130 - val_loss: 2209.8818 - val_y0: -0.4263 - val_y1: -0.4746 - val_ate_afte_scaled: -0.0483 - lr: 3.1250e-05\n",
      "Epoch 30/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3115.7512\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      " — ite: 3.8413  — ate: 0.7164 — pehe: 3.8275 \n",
      "3/3 [==============================] - 0s 192ms/step - loss: 3032.2707 - val_loss: 2401.4575 - val_y0: -0.4637 - val_y1: -0.3771 - val_ate_afte_scaled: 0.0866 - lr: 3.1250e-05\n",
      "Epoch 31/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3047.3672 — ite: 3.8756  — ate: 0.9701 — pehe: 3.9336 \n",
      "3/3 [==============================] - 0s 193ms/step - loss: 3106.1580 - val_loss: 2438.1541 - val_y0: -0.4619 - val_y1: -0.3986 - val_ate_afte_scaled: 0.0633 - lr: 1.5625e-05\n",
      "Epoch 32/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2904.1047 — ite: 3.7651  — ate: 0.7221 — pehe: 3.8165 \n",
      "3/3 [==============================] - 0s 198ms/step - loss: 3059.7725 - val_loss: 2352.7104 - val_y0: -0.5717 - val_y1: -0.6238 - val_ate_afte_scaled: -0.0521 - lr: 1.5625e-05\n",
      "Epoch 33/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3043.9160 — ite: 3.7604  — ate: 0.8194 — pehe: 3.8358 \n",
      "3/3 [==============================] - 0s 195ms/step - loss: 2894.6992 - val_loss: 2214.2581 - val_y0: -0.5003 - val_y1: -0.5276 - val_ate_afte_scaled: -0.0272 - lr: 1.5625e-05\n",
      "Epoch 34/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3192.8091 — ite: 3.8462  — ate: 0.7707 — pehe: 3.7708 \n",
      "3/3 [==============================] - 0s 192ms/step - loss: 3038.1125 - val_loss: 2424.3364 - val_y0: -0.5645 - val_y1: -0.3783 - val_ate_afte_scaled: 0.1863 - lr: 1.5625e-05\n",
      "Epoch 35/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2888.5464\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      " — ite: 3.8170  — ate: 0.5712 — pehe: 3.8723 \n",
      "3/3 [==============================] - 0s 195ms/step - loss: 3028.4457 - val_loss: 2105.2573 - val_y0: -0.4402 - val_y1: -0.4895 - val_ate_afte_scaled: -0.0494 - lr: 1.5625e-05\n",
      "Epoch 36/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3043.9116 — ite: 3.8077  — ate: 0.5248 — pehe: 3.7720 \n",
      "3/3 [==============================] - 0s 196ms/step - loss: 3050.2329 - val_loss: 2494.0886 - val_y0: -0.5379 - val_y1: -0.4269 - val_ate_afte_scaled: 0.1110 - lr: 7.8125e-06\n",
      "Epoch 37/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3020.6663 — ite: 3.7908  — ate: 0.7316 — pehe: 3.8302 \n",
      "3/3 [==============================] - 0s 193ms/step - loss: 3023.1509 - val_loss: 2443.1846 - val_y0: -0.4919 - val_y1: -0.4830 - val_ate_afte_scaled: 0.0089 - lr: 7.8125e-06\n",
      "Epoch 38/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3150.2727 — ite: 3.8426  — ate: 0.8332 — pehe: 3.8396 \n",
      "3/3 [==============================] - 0s 194ms/step - loss: 3035.2584 - val_loss: 2218.9167 - val_y0: -0.5913 - val_y1: -0.6417 - val_ate_afte_scaled: -0.0504 - lr: 7.8125e-06\n",
      "Epoch 39/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3127.8054 — ite: 3.8085  — ate: 0.8289 — pehe: 3.7936 \n",
      "3/3 [==============================] - 0s 194ms/step - loss: 3059.4413 - val_loss: 2351.9185 - val_y0: -0.5398 - val_y1: -0.5971 - val_ate_afte_scaled: -0.0573 - lr: 7.8125e-06\n",
      "Epoch 40/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3035.0356\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      " — ite: 3.9047  — ate: 0.6677 — pehe: 3.9016 \n",
      "3/3 [==============================] - 0s 195ms/step - loss: 3101.2059 - val_loss: 2270.5137 - val_y0: -0.5814 - val_y1: -0.5096 - val_ate_afte_scaled: 0.0717 - lr: 7.8125e-06\n",
      "Epoch 41/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2832.4741 — ite: 3.9395  — ate: 0.8445 — pehe: 3.8110 \n",
      "3/3 [==============================] - 0s 191ms/step - loss: 2979.3616 - val_loss: 2284.2395 - val_y0: -0.4461 - val_y1: -0.5909 - val_ate_afte_scaled: -0.1447 - lr: 3.9063e-06\n",
      "Epoch 42/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2821.1499 — ite: 3.8997  — ate: 0.6470 — pehe: 3.7520 \n",
      "3/3 [==============================] - 0s 196ms/step - loss: 2976.9111 - val_loss: 2261.8853 - val_y0: -0.5248 - val_y1: -0.5668 - val_ate_afte_scaled: -0.0421 - lr: 3.9063e-06\n",
      "Epoch 43/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3029.1992 — ite: 3.8593  — ate: 0.9128 — pehe: 4.0364 \n",
      "3/3 [==============================] - 0s 198ms/step - loss: 3036.0604 - val_loss: 2371.1169 - val_y0: -0.5591 - val_y1: -0.6844 - val_ate_afte_scaled: -0.1253 - lr: 3.9063e-06\n",
      "Epoch 44/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2974.0581 — ite: 3.7820  — ate: 0.7083 — pehe: 3.7585 \n",
      "3/3 [==============================] - 0s 196ms/step - loss: 3067.9759 - val_loss: 2362.9863 - val_y0: -0.5619 - val_y1: -0.5688 - val_ate_afte_scaled: -0.0069 - lr: 3.9063e-06\n",
      "Epoch 45/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3175.0322\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      " — ite: 3.9143  — ate: 0.9132 — pehe: 3.8761 \n",
      "3/3 [==============================] - 0s 198ms/step - loss: 3051.3025 - val_loss: 2339.3594 - val_y0: -0.4225 - val_y1: -0.4530 - val_ate_afte_scaled: -0.0305 - lr: 3.9063e-06\n",
      "Epoch 46/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3109.3037 — ite: 3.6868  — ate: 0.7219 — pehe: 3.7623 \n",
      "3/3 [==============================] - 0s 194ms/step - loss: 3053.8036 - val_loss: 2177.2114 - val_y0: -0.5522 - val_y1: -0.4313 - val_ate_afte_scaled: 0.1210 - lr: 1.9531e-06\n",
      "Epoch 47/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2942.8123 — ite: 3.7960  — ate: 0.7295 — pehe: 3.7854 \n",
      "3/3 [==============================] - 0s 195ms/step - loss: 2964.9040 - val_loss: 2382.6580 - val_y0: -0.4706 - val_y1: -0.5681 - val_ate_afte_scaled: -0.0975 - lr: 1.9531e-06\n",
      "Epoch 48/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2825.9165 — ite: 3.8641  — ate: 0.9535 — pehe: 3.8751 \n",
      "3/3 [==============================] - 0s 190ms/step - loss: 2984.4373 - val_loss: 2323.5869 - val_y0: -0.5873 - val_y1: -0.6006 - val_ate_afte_scaled: -0.0133 - lr: 1.9531e-06\n",
      "Epoch 49/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3244.5374 — ite: 3.8276  — ate: 0.6682 — pehe: 3.8386 \n",
      "3/3 [==============================] - 0s 189ms/step - loss: 3137.9243 - val_loss: 2516.5171 - val_y0: -0.5066 - val_y1: -0.5046 - val_ate_afte_scaled: 0.0020 - lr: 1.9531e-06\n",
      "Epoch 50/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3059.9172\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      " — ite: 3.8387  — ate: 0.8530 — pehe: 3.7029 \n",
      "3/3 [==============================] - 0s 193ms/step - loss: 3014.3939 - val_loss: 2267.6599 - val_y0: -0.6004 - val_y1: -0.6378 - val_ate_afte_scaled: -0.0374 - lr: 1.9531e-06\n",
      "Epoch 51/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3173.0979 — ite: 3.8765  — ate: 0.6546 — pehe: 3.8583 \n",
      "3/3 [==============================] - 0s 197ms/step - loss: 3111.3911 - val_loss: 2119.3584 - val_y0: -0.6273 - val_y1: -0.5260 - val_ate_afte_scaled: 0.1013 - lr: 9.7656e-07\n",
      "Epoch 52/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3019.0276 — ite: 3.8243  — ate: 0.8950 — pehe: 3.8444 \n",
      "3/3 [==============================] - 0s 204ms/step - loss: 2943.6306 - val_loss: 2421.2544 - val_y0: -0.4763 - val_y1: -0.6300 - val_ate_afte_scaled: -0.1537 - lr: 9.7656e-07\n",
      "Epoch 53/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3252.9563 — ite: 3.8853  — ate: 0.7095 — pehe: 3.8717 \n",
      "3/3 [==============================] - 0s 217ms/step - loss: 3033.2050 - val_loss: 2221.8062 - val_y0: -0.5424 - val_y1: -0.6031 - val_ate_afte_scaled: -0.0607 - lr: 9.7656e-07\n",
      "Epoch 54/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3225.6284 — ite: 3.7902  — ate: 0.7768 — pehe: 3.8551 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: 3229.6851 - val_loss: 2250.0044 - val_y0: -0.5489 - val_y1: -0.4971 - val_ate_afte_scaled: 0.0517 - lr: 9.7656e-07\n",
      "Epoch 55/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3120.3640\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      " — ite: 3.8407  — ate: 0.9472 — pehe: 3.8033 \n",
      "3/3 [==============================] - 0s 228ms/step - loss: 3067.0640 - val_loss: 2413.8940 - val_y0: -0.5137 - val_y1: -0.4723 - val_ate_afte_scaled: 0.0415 - lr: 9.7656e-07\n",
      "Epoch 56/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3119.6536 — ite: 3.7988  — ate: 0.7272 — pehe: 3.9393 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: 3110.5513 - val_loss: 2291.2903 - val_y0: -0.5102 - val_y1: -0.5905 - val_ate_afte_scaled: -0.0803 - lr: 4.8828e-07\n",
      "Epoch 57/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2995.5259 — ite: 3.8141  — ate: 0.8138 — pehe: 3.9237 \n",
      "3/3 [==============================] - 0s 201ms/step - loss: 2980.4399 - val_loss: 2389.3967 - val_y0: -0.6147 - val_y1: -0.4802 - val_ate_afte_scaled: 0.1345 - lr: 4.8828e-07\n",
      "Epoch 58/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3139.9292 — ite: 3.8233  — ate: 0.7909 — pehe: 3.8399 \n",
      "3/3 [==============================] - 0s 197ms/step - loss: 2951.1187 - val_loss: 2316.4448 - val_y0: -0.6328 - val_y1: -0.4764 - val_ate_afte_scaled: 0.1564 - lr: 4.8828e-07\n",
      "Epoch 59/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3057.3943 — ite: 3.8767  — ate: 0.8023 — pehe: 3.7337 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: 3211.2878 - val_loss: 2221.1621 - val_y0: -0.4245 - val_y1: -0.5070 - val_ate_afte_scaled: -0.0825 - lr: 4.8828e-07\n",
      "Epoch 60/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3039.3193\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      " — ite: 3.7874  — ate: 0.7450 — pehe: 3.8297 \n",
      "3/3 [==============================] - 0s 201ms/step - loss: 3081.5667 - val_loss: 2507.2808 - val_y0: -0.5354 - val_y1: -0.5392 - val_ate_afte_scaled: -0.0039 - lr: 4.8828e-07\n",
      "Epoch 61/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3209.7080 — ite: 3.8633  — ate: 1.0835 — pehe: 3.7890 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: 3085.7550 - val_loss: 2233.3931 - val_y0: -0.5575 - val_y1: -0.5573 - val_ate_afte_scaled: 2.1225e-04 - lr: 2.4414e-07\n",
      "Epoch 62/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3174.1001 — ite: 3.7308  — ate: 0.6079 — pehe: 3.7161 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: 3086.4959 - val_loss: 2386.7710 - val_y0: -0.5809 - val_y1: -0.6423 - val_ate_afte_scaled: -0.0614 - lr: 2.4414e-07\n",
      "Epoch 63/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2952.2144 — ite: 3.8005  — ate: 0.9211 — pehe: 3.7507 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: 3050.0475 - val_loss: 2333.5259 - val_y0: -0.4711 - val_y1: -0.6440 - val_ate_afte_scaled: -0.1729 - lr: 2.4414e-07\n",
      "Epoch 64/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3049.7524 — ite: 3.8331  — ate: 0.7874 — pehe: 3.8149 \n",
      "3/3 [==============================] - 0s 196ms/step - loss: 2990.9830 - val_loss: 2359.1172 - val_y0: -0.3885 - val_y1: -0.5343 - val_ate_afte_scaled: -0.1459 - lr: 2.4414e-07\n",
      "Epoch 65/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3175.7854\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      " — ite: 3.8497  — ate: 0.7425 — pehe: 3.7352 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: 3092.1719 - val_loss: 2146.3062 - val_y0: -0.5002 - val_y1: -0.5288 - val_ate_afte_scaled: -0.0286 - lr: 2.4414e-07\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Colab command to allow us to run Colab in TF2\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "%reload_ext tensorboard \n",
    "\n",
    "model = CEWAE()\n",
    "### MAIN CODE ####\n",
    "verbose=True\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    " \n",
    "callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='loss', patience=40, min_delta=0), \n",
    "        #40 is Shi's recommendation patience for this dataset, but you should tune for your data \n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0, cooldown=0, min_lr=0),\n",
    "        #This learning rate scheduling is quite agressive which seems good for this dataset\n",
    "        metrics_for_cevae(data,verbose),\n",
    "        tensorboard_callback\n",
    "    ]\n",
    "    \n",
    "#optimizer hyperparameters\n",
    "learning_rate = 5e-4\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate))\n",
    "\n",
    "model.fit(\n",
    "    [data['x'],data['t'],data['ys']],\n",
    "    callbacks=callbacks,\n",
    "    validation_split=args.val_split,\n",
    "    epochs=300,\n",
    "    batch_size=args.batch_size,\n",
    "    verbose=verbose\n",
    "    )\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 45269), started 1 day, 18:51:46 ago. (Use '!kill 45269' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-52862de939d57ec8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-52862de939d57ec8\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
