{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 “ihdp_npci_1-100.train.npz” 已经存在；不获取。\n",
      "\n",
      "文件 “ihdp_npci_1-100.test.npz” 已经存在；不获取。\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([5.26691518]), array([2.59847927]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.layers as tfkl\n",
    "tfd,tfpl = tfp.distributions,tfp.layers\n",
    "import tensorflow.keras.backend as tfkb\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from evaluation import *\n",
    "from cevae_networks import *\n",
    "################################################\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='')\n",
    "parser.add_argument('--scale_penalize',    type = float, default = 0.001,  help = '')\n",
    "parser.add_argument('--learning_rate',     type = float, default = 0.001,  help = '')\n",
    "parser.add_argument('--default_y_scale',   type = float, default = 1.,  help = '')\n",
    "parser.add_argument('--t_dim',     type = int, default = 1,  help = '')\n",
    "parser.add_argument('--y_dim',     type = int, default = 1,  help = '')\n",
    "parser.add_argument('--x_dim',     type = int, default = 25, help = '')\n",
    "parser.add_argument('--z_dim',     type = int, default = 20, help = '')\n",
    "parser.add_argument('--x_num_dim', type = int, default = 6,  help = '')\n",
    "parser.add_argument('--x_bin_dim', type = int, default = 19, help = '')\n",
    "parser.add_argument('--nh', type = int, default = 3, help = 'number of hidden layers')\n",
    "parser.add_argument('--h',  type = int, default = 200, help = 'number of hidden units')\n",
    "args = parser.parse_args([])\n",
    "################################################\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz \n",
    "\n",
    "def load_IHDP_data(training_data,testing_data,i):\n",
    "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "        train_data=np.load(trf); test_data=np.load(tef)\n",
    "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
    "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
    "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
    "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    "        ycf=np.concatenate((train_data['ycf'][:,i],  test_data['ycf'][:,i])).astype('float32')\n",
    "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "        data['y']=data['y'].reshape(-1,1)\n",
    "        data['ycf'] = ycf.reshape(-1,1)\n",
    "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "    return data\n",
    "\n",
    "ind = 7\n",
    "rep = 1\n",
    "data = load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.test.npz',i = ind)\n",
    "for key in data:\n",
    "    if key != 'y_scaler':\n",
    "        data[key] = np.repeat(data[key],repeats = rep, axis = 0)\n",
    "data['y_scaler'].mean_, data['y_scaler'].scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEVAE(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CEVAE, self).__init__()\n",
    "        ########################################\n",
    "        # networks\n",
    "        self.activation = 'elu'\n",
    "        # CEVAE Model \n",
    "        ## (encoder)\n",
    "        self.q_y_tx = q_y_tx(args.x_bin_dim, args.x_num_dim, args.y_dim, args.t_dim, args.nh, args.h)\n",
    "        self.q_t_x = q_t_x(args.x_bin_dim, args.x_num_dim, args.t_dim, args.nh, args.h)\n",
    "        self.q_z_txy = q_z_txy(args.x_bin_dim, args.x_num_dim, args.y_dim, args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        ## (decoder)\n",
    "        self.p_x_z = p_x_z(args.x_bin_dim, args.x_num_dim, args.z_dim, args.nh, args.h)\n",
    "        self.p_t_z = p_t_z(args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        self.p_y_tz = p_y_tz(args.y_dim, args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        \n",
    "\n",
    "    def call(self, data, training=False):\n",
    "        if training:\n",
    "            x_train,t_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "            \n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            \n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.sample()\n",
    "            # decoder\n",
    "            ## p(x|z)\n",
    "            x_num,x_bin = self.p_x_z(z_infer_sample)\n",
    "            ## p(t|z)\n",
    "            t = self.p_t_z(z_infer_sample)\n",
    "            ## p(y|t,z)\n",
    "            y = self.p_y_tz(tf.concat([t_train,z_infer_sample],-1) )\n",
    "            \n",
    "            return y_infer,t_infer,z_infer,y,t,x_num,x_bin\n",
    "        else:\n",
    "            x_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "        \n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            \n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.loc\n",
    "\n",
    "            t1z = tf.concat([tf.ones_like(t_infer_sample),z_infer_sample],-1)\n",
    "            t0z = tf.concat([tf.zeros_like(t_infer_sample),z_infer_sample],-1)\n",
    "            y0 = self.p_y_tz(t0z)\n",
    "            y1 = self.p_y_tz(t1z)\n",
    "            y = [y0,y1]\n",
    "            return y,t_infer,z_infer\n",
    "\n",
    "\n",
    "    def cevae_loss(self, data, pred, training = False):\n",
    "        x_train, t_train, y_train = data[0],data[1],data[2]\n",
    "        x_train_num, x_train_bin = x_train[:,:args.x_num_dim],x_train[:,args.x_num_dim:]\n",
    "        y_infer,t_infer,z_infer,y,t,x_num,x_bin = pred\n",
    "        y0,y1 = y_infer\n",
    "        # reconstruct loss\n",
    "        recon_x_num = tfkb.sum(x_num.log_prob(x_train_num), 1)\n",
    "        recon_x_bin = tfkb.sum(x_bin.log_prob(x_train_bin), 1)\n",
    "        recon_y = tfkb.sum(y.log_prob(y_train), 1)\n",
    "        recon_t = tfkb.sum(t.log_prob(t_train), 1)\n",
    "        # kl loss\n",
    "        z_infer_sample = z_infer.sample()\n",
    "        z = tfd.Normal(loc = [0] * 20, scale = [1]*20)\n",
    "        kl_z = tfkb.sum((z.log_prob(z_infer_sample) - z_infer.log_prob(z_infer_sample)), -1)\n",
    "        # aux loss\n",
    "        aux_y = tfkb.sum(y0.log_prob(y_train)*(1-t_train) + y1.log_prob(y_train)* t_train, 1)\n",
    "        aux_t = tfkb.sum(t_infer.log_prob(t_train), 1)\n",
    "        loss = -tfkb.mean(recon_x_bin + recon_x_num + recon_y + recon_t + aux_y + aux_t + kl_z)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, data):\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "            loss = self.cevae_loss(data = data, pred = pred, training = True)\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        metrics = {\"loss\": loss}\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "        y_infer = pred[0]\n",
    "        loss = self.cevae_loss(data = data, pred = pred, training = False)\n",
    "        y0, y1 = y_infer[0].sample(),y_infer[1].sample()\n",
    "        ate = tfkb.mean(y1) - tfkb.mean(y0)\n",
    "        metrics = {\"loss\":loss,\"y0\": tfkb.mean(y0),\"y1\": tfkb.mean(y1),'ate_afte_scaled': ate}\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/3 [=========>....................] - ETA: 6s - loss: 29.4146 — ite: 4.6857  — ate: 3.9176 — pehe: 5.5779 \n",
      "3/3 [==============================] - 4s 541ms/step - loss: 29.2084 - val_loss: 29.7594 - val_y0: -7.0111e-05 - val_y1: -0.1680 - val_ate_afte_scaled: -0.1679 - lr: 5.0000e-05\n",
      "Epoch 2/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 29.5119 — ite: 4.7706  — ate: 3.9333 — pehe: 5.7080 \n",
      "3/3 [==============================] - 0s 240ms/step - loss: 29.0976 - val_loss: 30.0018 - val_y0: 0.0706 - val_y1: -0.0619 - val_ate_afte_scaled: -0.1325 - lr: 5.0000e-05\n",
      "Epoch 3/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.4784 — ite: 4.6443  — ate: 3.6915 — pehe: 5.4111 \n",
      "3/3 [==============================] - 0s 235ms/step - loss: 28.9188 - val_loss: 29.6882 - val_y0: 0.0414 - val_y1: 0.0786 - val_ate_afte_scaled: 0.0372 - lr: 5.0000e-05\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 28.8762 — ite: 4.7073  — ate: 3.7556 — pehe: 5.4871 \n",
      "3/3 [==============================] - 1s 279ms/step - loss: 28.8426 - val_loss: 29.3965 - val_y0: -0.0425 - val_y1: 0.0944 - val_ate_afte_scaled: 0.1369 - lr: 5.0000e-05\n",
      "Epoch 5/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.7645 — ite: 4.6027  — ate: 3.7435 — pehe: 5.2952 \n",
      "3/3 [==============================] - 1s 253ms/step - loss: 28.4931 - val_loss: 29.5785 - val_y0: -0.2517 - val_y1: 0.0716 - val_ate_afte_scaled: 0.3233 - lr: 5.0000e-05\n",
      "Epoch 6/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.2157 — ite: 4.6667  — ate: 3.5741 — pehe: 5.2968 \n",
      "3/3 [==============================] - 0s 230ms/step - loss: 28.2942 - val_loss: 28.7394 - val_y0: -0.1401 - val_y1: 0.2321 - val_ate_afte_scaled: 0.3722 - lr: 5.0000e-05\n",
      "Epoch 7/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.3571 — ite: 4.7951  — ate: 3.7125 — pehe: 5.5072 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: 28.0876 - val_loss: 28.7317 - val_y0: -0.2001 - val_y1: 0.2653 - val_ate_afte_scaled: 0.4654 - lr: 5.0000e-05\n",
      "Epoch 8/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.3902 — ite: 4.6891  — ate: 3.4640 — pehe: 5.4145 \n",
      "3/3 [==============================] - 0s 210ms/step - loss: 27.9423 - val_loss: 28.8592 - val_y0: -0.0079 - val_y1: 0.3760 - val_ate_afte_scaled: 0.3839 - lr: 5.0000e-05\n",
      "Epoch 9/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.2699 — ite: 4.5409  — ate: 3.5847 — pehe: 5.3277 \n",
      "3/3 [==============================] - 0s 194ms/step - loss: 27.7343 - val_loss: 28.5884 - val_y0: -0.1377 - val_y1: 0.5013 - val_ate_afte_scaled: 0.6390 - lr: 5.0000e-05\n",
      "Epoch 10/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.3078 — ite: 4.5565  — ate: 3.5844 — pehe: 5.3481 \n",
      "3/3 [==============================] - 0s 194ms/step - loss: 27.6894 - val_loss: 28.4350 - val_y0: -0.2050 - val_y1: 0.4420 - val_ate_afte_scaled: 0.6470 - lr: 5.0000e-05\n",
      "Epoch 11/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.8687 — ite: 4.4853  — ate: 3.2632 — pehe: 5.0409 \n",
      "3/3 [==============================] - 0s 197ms/step - loss: 27.4287 - val_loss: 27.8650 - val_y0: -0.0144 - val_y1: 0.4889 - val_ate_afte_scaled: 0.5033 - lr: 5.0000e-05\n",
      "Epoch 12/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.5968 — ite: 4.5224  — ate: 3.5230 — pehe: 5.2611 \n",
      "3/3 [==============================] - 0s 190ms/step - loss: 27.4864 - val_loss: 28.0646 - val_y0: -0.1780 - val_y1: 0.5137 - val_ate_afte_scaled: 0.6917 - lr: 5.0000e-05\n",
      "Epoch 13/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.1747 — ite: 4.4392  — ate: 3.0860 — pehe: 4.9625 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: 27.0949 - val_loss: 27.8014 - val_y0: -0.2403 - val_y1: 0.6796 - val_ate_afte_scaled: 0.9199 - lr: 5.0000e-05\n",
      "Epoch 14/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.2398 — ite: 4.5799  — ate: 3.1490 — pehe: 5.1139 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: 26.8702 - val_loss: 27.6023 - val_y0: -0.1691 - val_y1: 0.6219 - val_ate_afte_scaled: 0.7911 - lr: 5.0000e-05\n",
      "Epoch 15/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.1306 — ite: 4.5183  — ate: 3.2269 — pehe: 5.2433 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: 26.6821 - val_loss: 27.3302 - val_y0: -0.1119 - val_y1: 0.7039 - val_ate_afte_scaled: 0.8157 - lr: 5.0000e-05\n",
      "Epoch 16/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.1285 — ite: 4.6272  — ate: 3.4150 — pehe: 5.2415 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: 26.5473 - val_loss: 27.3535 - val_y0: -0.0414 - val_y1: 0.7710 - val_ate_afte_scaled: 0.8124 - lr: 5.0000e-05\n",
      "Epoch 17/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 26.4049 — ite: 4.4416  — ate: 3.0975 — pehe: 5.0288 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: 26.4385 - val_loss: 27.0225 - val_y0: -0.2387 - val_y1: 0.7586 - val_ate_afte_scaled: 0.9973 - lr: 5.0000e-05\n",
      "Epoch 18/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 26.5921 — ite: 4.4183  — ate: 2.9937 — pehe: 4.9819 \n",
      "3/3 [==============================] - 0s 205ms/step - loss: 26.3114 - val_loss: 26.8012 - val_y0: -0.1669 - val_y1: 0.8233 - val_ate_afte_scaled: 0.9902 - lr: 5.0000e-05\n",
      "Epoch 19/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 26.3294 — ite: 4.3435  — ate: 2.8873 — pehe: 4.8176 \n",
      "3/3 [==============================] - 0s 198ms/step - loss: 26.2071 - val_loss: 26.4207 - val_y0: -0.1475 - val_y1: 0.8186 - val_ate_afte_scaled: 0.9660 - lr: 5.0000e-05\n",
      "Epoch 20/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 26.0636 — ite: 4.3720  — ate: 2.7256 — pehe: 4.7957 \n",
      "3/3 [==============================] - 0s 204ms/step - loss: 26.0385 - val_loss: 26.5125 - val_y0: -0.2869 - val_y1: 1.0234 - val_ate_afte_scaled: 1.3103 - lr: 5.0000e-05\n",
      "Epoch 21/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.6776 — ite: 4.2554  — ate: 2.6405 — pehe: 4.7161 \n",
      "3/3 [==============================] - 0s 217ms/step - loss: 25.7585 - val_loss: 26.3151 - val_y0: -0.3118 - val_y1: 0.7785 - val_ate_afte_scaled: 1.0903 - lr: 5.0000e-05\n",
      "Epoch 22/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.2821 — ite: 4.3284  — ate: 2.4575 — pehe: 4.8067 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: 25.4813 - val_loss: 26.2361 - val_y0: -0.2170 - val_y1: 0.9780 - val_ate_afte_scaled: 1.1950 - lr: 5.0000e-05\n",
      "Epoch 23/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.4563 — ite: 4.1273  — ate: 2.3081 — pehe: 4.5588 \n",
      "3/3 [==============================] - 0s 210ms/step - loss: 25.4412 - val_loss: 26.1345 - val_y0: -0.4477 - val_y1: 0.9332 - val_ate_afte_scaled: 1.3809 - lr: 5.0000e-05\n",
      "Epoch 24/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.0956 — ite: 4.2660  — ate: 2.3145 — pehe: 4.5631 \n",
      "3/3 [==============================] - 0s 210ms/step - loss: 25.3366 - val_loss: 25.6282 - val_y0: -0.0815 - val_y1: 0.9867 - val_ate_afte_scaled: 1.0681 - lr: 5.0000e-05\n",
      "Epoch 25/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.0940 — ite: 4.1296  — ate: 2.2261 — pehe: 4.5233 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: 24.7580 - val_loss: 25.4785 - val_y0: -0.2226 - val_y1: 1.1711 - val_ate_afte_scaled: 1.3937 - lr: 5.0000e-05\n",
      "Epoch 26/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.9493 — ite: 4.0633  — ate: 2.0565 — pehe: 4.4188 \n",
      "3/3 [==============================] - 0s 197ms/step - loss: 24.7619 - val_loss: 25.1906 - val_y0: -0.2878 - val_y1: 0.9891 - val_ate_afte_scaled: 1.2769 - lr: 5.0000e-05\n",
      "Epoch 27/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.0509 — ite: 4.0662  — ate: 1.9490 — pehe: 4.4798 \n",
      "3/3 [==============================] - 0s 204ms/step - loss: 24.7043 - val_loss: 25.0792 - val_y0: -0.3134 - val_y1: 0.9645 - val_ate_afte_scaled: 1.2779 - lr: 5.0000e-05\n",
      "Epoch 28/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.2109 — ite: 3.9767  — ate: 1.5773 — pehe: 4.3043 \n",
      "3/3 [==============================] - 0s 196ms/step - loss: 24.1323 - val_loss: 25.1347 - val_y0: -0.3085 - val_y1: 1.1261 - val_ate_afte_scaled: 1.4346 - lr: 5.0000e-05\n",
      "Epoch 29/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.5955 — ite: 3.9660  — ate: 1.4954 — pehe: 4.2045 \n",
      "3/3 [==============================] - 0s 213ms/step - loss: 24.2989 - val_loss: 24.7609 - val_y0: -0.1929 - val_y1: 0.9539 - val_ate_afte_scaled: 1.1468 - lr: 5.0000e-05\n",
      "Epoch 30/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.1720 — ite: 3.9714  — ate: 1.3684 — pehe: 4.4089 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: 24.1483 - val_loss: 24.9671 - val_y0: -0.2840 - val_y1: 1.1189 - val_ate_afte_scaled: 1.4029 - lr: 5.0000e-05\n",
      "Epoch 31/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.6082 — ite: 3.9369  — ate: 1.1707 — pehe: 4.1663 \n",
      "3/3 [==============================] - 0s 207ms/step - loss: 23.7536 - val_loss: 23.7308 - val_y0: -0.2048 - val_y1: 1.1617 - val_ate_afte_scaled: 1.3666 - lr: 5.0000e-05\n",
      "Epoch 32/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.8179 — ite: 3.7687  — ate: 1.0573 — pehe: 4.0153 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: 23.5289 - val_loss: 23.8128 - val_y0: -0.1926 - val_y1: 1.0800 - val_ate_afte_scaled: 1.2726 - lr: 5.0000e-05\n",
      "Epoch 33/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.8834 — ite: 3.8432  — ate: 0.9580 — pehe: 4.1544 \n",
      "3/3 [==============================] - 0s 217ms/step - loss: 23.3341 - val_loss: 23.6467 - val_y0: -0.2973 - val_y1: 0.9931 - val_ate_afte_scaled: 1.2904 - lr: 5.0000e-05\n",
      "Epoch 34/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.4449 — ite: 3.8194  — ate: 0.7988 — pehe: 4.1103 \n",
      "3/3 [==============================] - 0s 201ms/step - loss: 23.0195 - val_loss: 23.5915 - val_y0: -0.3112 - val_y1: 1.0992 - val_ate_afte_scaled: 1.4104 - lr: 5.0000e-05\n",
      "Epoch 35/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.1894 — ite: 3.8289  — ate: 0.8669 — pehe: 4.1149 \n",
      "3/3 [==============================] - 0s 206ms/step - loss: 22.6963 - val_loss: 23.4968 - val_y0: -0.1851 - val_y1: 0.9541 - val_ate_afte_scaled: 1.1392 - lr: 5.0000e-05\n",
      "Epoch 36/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.0804 — ite: 3.8181  — ate: 0.8177 — pehe: 4.1908 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: 22.9100 - val_loss: 23.2017 - val_y0: -0.1351 - val_y1: 1.1041 - val_ate_afte_scaled: 1.2392 - lr: 5.0000e-05\n",
      "Epoch 37/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.1632 — ite: 3.7413  — ate: 0.8692 — pehe: 3.9595 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: 22.8057 - val_loss: 23.1339 - val_y0: -0.3001 - val_y1: 1.1675 - val_ate_afte_scaled: 1.4676 - lr: 5.0000e-05\n",
      "Epoch 38/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.5123 — ite: 3.6952  — ate: 0.6660 — pehe: 3.9398 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: 22.3246 - val_loss: 22.5755 - val_y0: -0.3445 - val_y1: 1.1636 - val_ate_afte_scaled: 1.5081 - lr: 5.0000e-05\n",
      "Epoch 39/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.1933 — ite: 3.7499  — ate: 0.6609 — pehe: 4.0494 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: 22.1569 - val_loss: 22.3837 - val_y0: -0.3045 - val_y1: 1.1386 - val_ate_afte_scaled: 1.4431 - lr: 5.0000e-05\n",
      "Epoch 40/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.9406 — ite: 3.8003  — ate: 0.6613 — pehe: 4.0727 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: 21.9228 - val_loss: 22.8012 - val_y0: -0.2714 - val_y1: 1.0061 - val_ate_afte_scaled: 1.2775 - lr: 5.0000e-05\n",
      "Epoch 41/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.7214 — ite: 3.6288  — ate: 0.4221 — pehe: 3.8430 \n",
      "3/3 [==============================] - 0s 197ms/step - loss: 21.9840 - val_loss: 22.2128 - val_y0: -0.1926 - val_y1: 1.0643 - val_ate_afte_scaled: 1.2569 - lr: 5.0000e-05\n",
      "Epoch 42/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.1892 — ite: 3.7440  — ate: 0.7455 — pehe: 3.8457 \n",
      "3/3 [==============================] - 0s 195ms/step - loss: 21.5016 - val_loss: 21.8902 - val_y0: -0.3173 - val_y1: 1.0218 - val_ate_afte_scaled: 1.3392 - lr: 5.0000e-05\n",
      "Epoch 43/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.1816 — ite: 3.8241  — ate: 0.4196 — pehe: 3.8834 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: 21.4437 - val_loss: 22.1098 - val_y0: -0.2266 - val_y1: 1.0977 - val_ate_afte_scaled: 1.3244 - lr: 5.0000e-05\n",
      "Epoch 44/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.7834 — ite: 3.7388  — ate: 0.6539 — pehe: 4.1159 \n",
      "3/3 [==============================] - 0s 204ms/step - loss: 21.3636 - val_loss: 21.7777 - val_y0: -0.0742 - val_y1: 1.2308 - val_ate_afte_scaled: 1.3050 - lr: 5.0000e-05\n",
      "Epoch 45/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.8754 — ite: 3.7172  — ate: 0.3724 — pehe: 3.8781 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: 21.1770 - val_loss: 22.2642 - val_y0: -0.3189 - val_y1: 1.0005 - val_ate_afte_scaled: 1.3195 - lr: 5.0000e-05\n",
      "Epoch 46/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.3841 — ite: 3.7629  — ate: 0.7007 — pehe: 3.9805 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: 20.8244 - val_loss: 21.5083 - val_y0: -0.3187 - val_y1: 1.0792 - val_ate_afte_scaled: 1.3979 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.6622 — ite: 3.6933  — ate: 0.5737 — pehe: 3.9328 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: 20.8344 - val_loss: 21.5398 - val_y0: -0.1222 - val_y1: 1.1587 - val_ate_afte_scaled: 1.2810 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.5215 — ite: 3.7934  — ate: 0.5655 — pehe: 4.0549 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: 20.5648 - val_loss: 21.0487 - val_y0: -0.2985 - val_y1: 1.1441 - val_ate_afte_scaled: 1.4425 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.1846 — ite: 3.7797  — ate: 0.6689 — pehe: 4.0355 \n",
      "3/3 [==============================] - 0s 207ms/step - loss: 20.4103 - val_loss: 20.9531 - val_y0: -0.2545 - val_y1: 1.1238 - val_ate_afte_scaled: 1.3783 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.8549 — ite: 3.6991  — ate: 0.6774 — pehe: 4.0294 \n",
      "3/3 [==============================] - 0s 234ms/step - loss: 20.4386 - val_loss: 20.6080 - val_y0: -0.2763 - val_y1: 1.0830 - val_ate_afte_scaled: 1.3593 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.7578 — ite: 3.7818  — ate: 0.8731 — pehe: 3.9731 \n",
      "3/3 [==============================] - 0s 214ms/step - loss: 19.9498 - val_loss: 21.1759 - val_y0: -0.1723 - val_y1: 1.1224 - val_ate_afte_scaled: 1.2947 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.2494 — ite: 3.7767  — ate: 0.6413 — pehe: 4.0961 \n",
      "3/3 [==============================] - 0s 204ms/step - loss: 20.1027 - val_loss: 20.6792 - val_y0: -0.3146 - val_y1: 1.0256 - val_ate_afte_scaled: 1.3402 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.0465 — ite: 3.8317  — ate: 0.7099 — pehe: 4.1177 \n",
      "3/3 [==============================] - 0s 197ms/step - loss: 19.8780 - val_loss: 20.5425 - val_y0: -0.2965 - val_y1: 1.1088 - val_ate_afte_scaled: 1.4053 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.9137 — ite: 3.7501  — ate: 0.5620 — pehe: 4.0832 \n",
      "3/3 [==============================] - 0s 195ms/step - loss: 19.4279 - val_loss: 20.5887 - val_y0: -0.2280 - val_y1: 1.2892 - val_ate_afte_scaled: 1.5172 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.1820 — ite: 3.8446  — ate: 0.5719 — pehe: 4.1551 \n",
      "3/3 [==============================] - 0s 192ms/step - loss: 19.4750 - val_loss: 20.0582 - val_y0: -0.3891 - val_y1: 1.0381 - val_ate_afte_scaled: 1.4273 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.5807 — ite: 3.8575  — ate: 0.8604 — pehe: 4.1069 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: 19.2494 - val_loss: 20.0912 - val_y0: -0.1232 - val_y1: 0.9687 - val_ate_afte_scaled: 1.0919 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.4624 — ite: 3.7789  — ate: 0.5425 — pehe: 3.9640 \n",
      "3/3 [==============================] - 0s 210ms/step - loss: 19.4230 - val_loss: 19.5581 - val_y0: -0.1756 - val_y1: 1.0146 - val_ate_afte_scaled: 1.1901 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.5930 — ite: 3.7222  — ate: 0.7376 — pehe: 3.8774 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: 19.2656 - val_loss: 19.4728 - val_y0: -0.1772 - val_y1: 1.1360 - val_ate_afte_scaled: 1.3132 - lr: 5.0000e-05\n",
      "Epoch 59/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.6248 — ite: 3.6496  — ate: 0.8137 — pehe: 3.7944 \n",
      "3/3 [==============================] - 0s 213ms/step - loss: 19.1035 - val_loss: 19.6592 - val_y0: -0.2495 - val_y1: 1.0861 - val_ate_afte_scaled: 1.3356 - lr: 5.0000e-05\n",
      "Epoch 60/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.9836 — ite: 3.7346  — ate: 0.7839 — pehe: 4.0287 \n",
      "3/3 [==============================] - 0s 220ms/step - loss: 18.9855 - val_loss: 19.5476 - val_y0: -0.1994 - val_y1: 1.0981 - val_ate_afte_scaled: 1.2975 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17.8929 — ite: 3.8416  — ate: 0.5861 — pehe: 4.2602 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: 18.3777 - val_loss: 18.9595 - val_y0: -0.1231 - val_y1: 1.1012 - val_ate_afte_scaled: 1.2244 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.2745 — ite: 3.7911  — ate: 0.8494 — pehe: 4.1569 \n",
      "3/3 [==============================] - 0s 206ms/step - loss: 18.3236 - val_loss: 19.3461 - val_y0: -0.2332 - val_y1: 1.2621 - val_ate_afte_scaled: 1.4953 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.6071 — ite: 3.7401  — ate: 0.5390 — pehe: 3.9633 \n",
      "3/3 [==============================] - 0s 194ms/step - loss: 18.4015 - val_loss: 19.2881 - val_y0: -0.2771 - val_y1: 1.1354 - val_ate_afte_scaled: 1.4124 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17.8002 — ite: 3.7575  — ate: 0.6769 — pehe: 3.9137 \n",
      "3/3 [==============================] - 0s 193ms/step - loss: 18.3264 - val_loss: 19.0409 - val_y0: -0.1884 - val_y1: 1.2218 - val_ate_afte_scaled: 1.4102 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.4150 — ite: 3.6670  — ate: 0.5315 — pehe: 3.8224 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: 17.9903 - val_loss: 18.4959 - val_y0: -0.2276 - val_y1: 1.0674 - val_ate_afte_scaled: 1.2950 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.7598 — ite: 3.6926  — ate: 0.7239 — pehe: 3.9174 \n",
      "3/3 [==============================] - 0s 201ms/step - loss: 17.6142 - val_loss: 19.0393 - val_y0: -0.1995 - val_y1: 1.0640 - val_ate_afte_scaled: 1.2636 - lr: 5.0000e-05\n",
      "Epoch 67/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17.5228 — ite: 3.7052  — ate: 0.4809 — pehe: 3.9010 \n",
      "3/3 [==============================] - 0s 193ms/step - loss: 17.7907 - val_loss: 18.0231 - val_y0: -0.1335 - val_y1: 1.0896 - val_ate_afte_scaled: 1.2232 - lr: 5.0000e-05\n",
      "Epoch 68/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.7464 — ite: 3.7396  — ate: 0.5947 — pehe: 4.1355 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: 17.4062 - val_loss: 18.2431 - val_y0: -0.2306 - val_y1: 1.0971 - val_ate_afte_scaled: 1.3277 - lr: 5.0000e-05\n",
      "Epoch 69/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17.7812 — ite: 3.8437  — ate: 0.4248 — pehe: 4.1066 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: 16.9767 - val_loss: 17.8962 - val_y0: -0.1650 - val_y1: 1.1365 - val_ate_afte_scaled: 1.3015 - lr: 5.0000e-05\n",
      "Epoch 70/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16.8664 — ite: 3.8144  — ate: 0.6582 — pehe: 4.1037 \n",
      "3/3 [==============================] - 0s 195ms/step - loss: 17.3915 - val_loss: 17.5575 - val_y0: -0.2222 - val_y1: 1.1311 - val_ate_afte_scaled: 1.3532 - lr: 5.0000e-05\n",
      "Epoch 71/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16.8355 — ite: 3.7418  — ate: 0.5200 — pehe: 4.0393 \n",
      "3/3 [==============================] - 0s 194ms/step - loss: 16.3377 - val_loss: 17.1770 - val_y0: -0.1742 - val_y1: 1.1912 - val_ate_afte_scaled: 1.3653 - lr: 5.0000e-05\n",
      "Epoch 72/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17.0234 — ite: 3.7757  — ate: 0.6062 — pehe: 4.0510 \n",
      "3/3 [==============================] - 0s 184ms/step - loss: 16.1660 - val_loss: 17.5066 - val_y0: -0.2210 - val_y1: 1.1329 - val_ate_afte_scaled: 1.3539 - lr: 5.0000e-05\n",
      "Epoch 73/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17.0908 — ite: 3.7413  — ate: 0.5762 — pehe: 3.9993 \n",
      "3/3 [==============================] - 0s 217ms/step - loss: 16.0012 - val_loss: 17.2116 - val_y0: -0.2653 - val_y1: 1.1380 - val_ate_afte_scaled: 1.4033 - lr: 5.0000e-05\n",
      "Epoch 74/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16.5631 — ite: 3.8297  — ate: 0.7608 — pehe: 4.0564 \n",
      "3/3 [==============================] - 0s 210ms/step - loss: 15.8099 - val_loss: 16.7525 - val_y0: -0.2614 - val_y1: 1.1222 - val_ate_afte_scaled: 1.3837 - lr: 5.0000e-05\n",
      "Epoch 75/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15.8697 — ite: 3.7589  — ate: 0.6003 — pehe: 3.9532 \n",
      "3/3 [==============================] - 0s 218ms/step - loss: 15.6341 - val_loss: 16.0213 - val_y0: -0.4195 - val_y1: 1.1503 - val_ate_afte_scaled: 1.5698 - lr: 5.0000e-05\n",
      "Epoch 76/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15.9758 — ite: 3.8167  — ate: 1.0045 — pehe: 3.9927 \n",
      "3/3 [==============================] - 0s 224ms/step - loss: 15.7142 - val_loss: 16.1577 - val_y0: -0.4923 - val_y1: 1.1575 - val_ate_afte_scaled: 1.6498 - lr: 5.0000e-05\n",
      "Epoch 77/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15.9389 — ite: 3.7930  — ate: 0.7524 — pehe: 4.1734 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: 15.0578 - val_loss: 16.1651 - val_y0: -0.1962 - val_y1: 1.1003 - val_ate_afte_scaled: 1.2966 - lr: 5.0000e-05\n",
      "Epoch 78/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15.0911 — ite: 3.8006  — ate: 0.9322 — pehe: 4.1031 \n",
      "3/3 [==============================] - 0s 194ms/step - loss: 14.6047 - val_loss: 15.9362 - val_y0: -0.3925 - val_y1: 1.2438 - val_ate_afte_scaled: 1.6363 - lr: 5.0000e-05\n",
      "Epoch 79/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15.3247 — ite: 3.8109  — ate: 0.4985 — pehe: 3.9541 \n",
      "3/3 [==============================] - 0s 221ms/step - loss: 14.8425 - val_loss: 15.5474 - val_y0: -0.2331 - val_y1: 1.0701 - val_ate_afte_scaled: 1.3032 - lr: 5.0000e-05\n",
      "Epoch 80/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14.6428 — ite: 3.8796  — ate: 0.7342 — pehe: 4.1568 \n",
      "3/3 [==============================] - 0s 218ms/step - loss: 13.9159 - val_loss: 14.2944 - val_y0: -0.1461 - val_y1: 1.0448 - val_ate_afte_scaled: 1.1909 - lr: 5.0000e-05\n",
      "Epoch 81/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14.5402 — ite: 3.7542  — ate: 0.7317 — pehe: 4.0174 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: 13.3355 - val_loss: 14.4474 - val_y0: -0.1326 - val_y1: 1.2491 - val_ate_afte_scaled: 1.3817 - lr: 5.0000e-05\n",
      "Epoch 82/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13.3420 — ite: 3.7735  — ate: 0.5993 — pehe: 4.0193 \n",
      "3/3 [==============================] - 0s 201ms/step - loss: 13.5783 - val_loss: 13.3104 - val_y0: -0.3049 - val_y1: 1.0342 - val_ate_afte_scaled: 1.3391 - lr: 5.0000e-05\n",
      "Epoch 83/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11.8082 — ite: 3.7617  — ate: 0.5562 — pehe: 3.8157 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: 13.3643 - val_loss: 13.8369 - val_y0: -0.3617 - val_y1: 1.2127 - val_ate_afte_scaled: 1.5744 - lr: 5.0000e-05\n",
      "Epoch 84/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 12.2422 — ite: 3.7876  — ate: 0.4434 — pehe: 4.1046 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: 12.3105 - val_loss: 12.3689 - val_y0: -0.2514 - val_y1: 1.1833 - val_ate_afte_scaled: 1.4347 - lr: 5.0000e-05\n",
      "Epoch 85/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11.8430 — ite: 3.7305  — ate: 0.5069 — pehe: 3.9691 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: 12.2018 - val_loss: 12.6460 - val_y0: -0.2732 - val_y1: 1.0734 - val_ate_afte_scaled: 1.3467 - lr: 5.0000e-05\n",
      "Epoch 86/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11.6997 — ite: 3.8277  — ate: 0.5229 — pehe: 4.1212 \n",
      "3/3 [==============================] - 0s 201ms/step - loss: 11.6030 - val_loss: 12.2186 - val_y0: -0.2228 - val_y1: 1.1169 - val_ate_afte_scaled: 1.3397 - lr: 5.0000e-05\n",
      "Epoch 87/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10.9824 — ite: 3.8435  — ate: 0.6647 — pehe: 4.2764 \n",
      "3/3 [==============================] - 0s 230ms/step - loss: 10.6765 - val_loss: 10.8617 - val_y0: -0.2220 - val_y1: 1.0445 - val_ate_afte_scaled: 1.2664 - lr: 5.0000e-05\n",
      "Epoch 88/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10.8899 — ite: 3.8418  — ate: 0.7616 — pehe: 4.0825 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: 11.0615 - val_loss: 10.5788 - val_y0: -0.2154 - val_y1: 1.1924 - val_ate_afte_scaled: 1.4078 - lr: 5.0000e-05\n",
      "Epoch 89/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.3245 — ite: 3.7365  — ate: 0.5365 — pehe: 3.8889 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: 9.8838 - val_loss: 9.9575 - val_y0: -0.4784 - val_y1: 1.0538 - val_ate_afte_scaled: 1.5322 - lr: 5.0000e-05\n",
      "Epoch 90/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.5446 — ite: 3.8569  — ate: 1.0159 — pehe: 3.9915 \n",
      "3/3 [==============================] - 0s 198ms/step - loss: 9.1049 - val_loss: 8.8318 - val_y0: -0.1776 - val_y1: 1.1702 - val_ate_afte_scaled: 1.3479 - lr: 5.0000e-05\n",
      "Epoch 91/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.1917 — ite: 3.7661  — ate: 0.9793 — pehe: 4.0181 \n",
      "3/3 [==============================] - 0s 206ms/step - loss: 7.9792 - val_loss: 8.6965 - val_y0: -0.1670 - val_y1: 1.0454 - val_ate_afte_scaled: 1.2124 - lr: 5.0000e-05\n",
      "Epoch 92/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.8885 — ite: 3.8570  — ate: 0.6800 — pehe: 4.1215 \n",
      "3/3 [==============================] - 0s 218ms/step - loss: 7.5712 - val_loss: 7.3562 - val_y0: -0.2759 - val_y1: 1.2050 - val_ate_afte_scaled: 1.4809 - lr: 5.0000e-05\n",
      "Epoch 93/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.4451 — ite: 3.8478  — ate: 0.6375 — pehe: 4.2033 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: 5.9822 - val_loss: 6.7975 - val_y0: -0.3016 - val_y1: 1.2063 - val_ate_afte_scaled: 1.5079 - lr: 5.0000e-05\n",
      "Epoch 94/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.0154 — ite: 3.8651  — ate: 0.7138 — pehe: 4.1825 \n",
      "3/3 [==============================] - 0s 222ms/step - loss: 5.9861 - val_loss: 5.2436 - val_y0: -0.2878 - val_y1: 1.0730 - val_ate_afte_scaled: 1.3608 - lr: 5.0000e-05\n",
      "Epoch 95/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.7386 — ite: 3.8242  — ate: 0.8164 — pehe: 4.0850 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: 5.1910 - val_loss: 4.9644 - val_y0: -0.3573 - val_y1: 1.1897 - val_ate_afte_scaled: 1.5470 - lr: 5.0000e-05\n",
      "Epoch 96/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.2852 — ite: 3.8714  — ate: 0.5453 — pehe: 4.0726 \n",
      "3/3 [==============================] - 0s 220ms/step - loss: 2.8801 - val_loss: 4.0794 - val_y0: -0.2318 - val_y1: 1.2221 - val_ate_afte_scaled: 1.4539 - lr: 5.0000e-05\n",
      "Epoch 97/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.5686 — ite: 3.8115  — ate: 0.6132 — pehe: 4.0925 \n",
      "3/3 [==============================] - 0s 218ms/step - loss: 2.8814 - val_loss: 2.3457 - val_y0: -0.2607 - val_y1: 1.1451 - val_ate_afte_scaled: 1.4057 - lr: 5.0000e-05\n",
      "Epoch 98/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.6485 — ite: 3.9337  — ate: 0.9054 — pehe: 4.1433 \n",
      "3/3 [==============================] - 0s 214ms/step - loss: 1.6776 - val_loss: 1.8601 - val_y0: -0.2875 - val_y1: 1.2044 - val_ate_afte_scaled: 1.4918 - lr: 5.0000e-05\n",
      "Epoch 99/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.7334 — ite: 3.8868  — ate: 0.7812 — pehe: 4.1349 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: -0.7320 - val_loss: 0.0011 - val_y0: -0.3099 - val_y1: 0.9879 - val_ate_afte_scaled: 1.2978 - lr: 5.0000e-05\n",
      "Epoch 100/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.7411 — ite: 3.8991  — ate: 0.4502 — pehe: 4.2501 \n",
      "3/3 [==============================] - 0s 217ms/step - loss: -1.8149 - val_loss: -1.4292 - val_y0: -0.3058 - val_y1: 1.0801 - val_ate_afte_scaled: 1.3859 - lr: 5.0000e-05\n",
      "Epoch 101/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -0.2660 — ite: 3.8553  — ate: 0.5030 — pehe: 4.0325 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: -2.4453 - val_loss: -3.0875 - val_y0: -0.2600 - val_y1: 0.9865 - val_ate_afte_scaled: 1.2465 - lr: 5.0000e-05\n",
      "Epoch 102/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -4.7716 — ite: 3.8130  — ate: 0.8691 — pehe: 4.1613 \n",
      "3/3 [==============================] - 0s 204ms/step - loss: -4.5139 - val_loss: -4.4268 - val_y0: -0.3630 - val_y1: 1.1996 - val_ate_afte_scaled: 1.5627 - lr: 5.0000e-05\n",
      "Epoch 103/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -5.6322 — ite: 3.8498  — ate: 0.6586 — pehe: 4.0007 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: -6.2989 - val_loss: -6.0766 - val_y0: -0.1631 - val_y1: 1.2054 - val_ate_afte_scaled: 1.3685 - lr: 5.0000e-05\n",
      "Epoch 104/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -6.3392 — ite: 3.9180  — ate: 0.9161 — pehe: 4.0594 \n",
      "3/3 [==============================] - 0s 196ms/step - loss: -6.8567 - val_loss: -8.6426 - val_y0: -0.2254 - val_y1: 1.1693 - val_ate_afte_scaled: 1.3947 - lr: 5.0000e-05\n",
      "Epoch 105/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -7.1671 — ite: 4.0224  — ate: 0.8884 — pehe: 4.3017 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: -11.2079 - val_loss: -9.0072 - val_y0: -0.3165 - val_y1: 0.9818 - val_ate_afte_scaled: 1.2983 - lr: 5.0000e-05\n",
      "Epoch 106/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -13.7020 — ite: 3.8667  — ate: 0.6652 — pehe: 4.0358 \n",
      "3/3 [==============================] - 0s 205ms/step - loss: -11.9380 - val_loss: -12.3580 - val_y0: -0.1083 - val_y1: 1.1996 - val_ate_afte_scaled: 1.3079 - lr: 5.0000e-05\n",
      "Epoch 107/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -14.0360 — ite: 3.8637  — ate: 0.7697 — pehe: 4.2893 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: -14.3491 - val_loss: -16.2957 - val_y0: -0.1393 - val_y1: 1.2053 - val_ate_afte_scaled: 1.3447 - lr: 5.0000e-05\n",
      "Epoch 108/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -21.2276 — ite: 3.8041  — ate: 0.7087 — pehe: 4.1173 \n",
      "3/3 [==============================] - 0s 206ms/step - loss: -15.6332 - val_loss: -16.6037 - val_y0: -0.3493 - val_y1: 1.1028 - val_ate_afte_scaled: 1.4521 - lr: 5.0000e-05\n",
      "Epoch 109/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -21.3111 — ite: 3.8850  — ate: 0.6915 — pehe: 4.1689 \n",
      "3/3 [==============================] - 0s 214ms/step - loss: -19.1421 - val_loss: -18.5688 - val_y0: -0.3298 - val_y1: 1.0252 - val_ate_afte_scaled: 1.3550 - lr: 5.0000e-05\n",
      "Epoch 110/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -16.2606 — ite: 3.7930  — ate: 0.7751 — pehe: 4.1317 \n",
      "3/3 [==============================] - 0s 220ms/step - loss: -21.5020 - val_loss: -24.5395 - val_y0: -0.3302 - val_y1: 1.2333 - val_ate_afte_scaled: 1.5635 - lr: 5.0000e-05\n",
      "Epoch 111/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -24.9816 — ite: 3.8999  — ate: 1.0201 — pehe: 4.1761 \n",
      "3/3 [==============================] - 0s 213ms/step - loss: -26.0896 - val_loss: -26.3261 - val_y0: -0.3088 - val_y1: 1.1484 - val_ate_afte_scaled: 1.4572 - lr: 5.0000e-05\n",
      "Epoch 112/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -31.0152 — ite: 3.9266  — ate: 1.0577 — pehe: 4.1673 \n",
      "3/3 [==============================] - 0s 196ms/step - loss: -29.1105 - val_loss: -29.4622 - val_y0: -0.2823 - val_y1: 1.0910 - val_ate_afte_scaled: 1.3733 - lr: 5.0000e-05\n",
      "Epoch 113/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -31.2025 — ite: 3.9105  — ate: 0.5764 — pehe: 4.2174 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: -33.0596 - val_loss: -33.6737 - val_y0: -0.2259 - val_y1: 1.2314 - val_ate_afte_scaled: 1.4574 - lr: 5.0000e-05\n",
      "Epoch 114/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -27.1806 — ite: 3.9407  — ate: 0.6655 — pehe: 4.1923 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: -37.2106 - val_loss: -38.3114 - val_y0: -0.3244 - val_y1: 1.1457 - val_ate_afte_scaled: 1.4701 - lr: 5.0000e-05\n",
      "Epoch 115/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -36.0847 — ite: 3.7082  — ate: 0.7370 — pehe: 3.8857 \n",
      "3/3 [==============================] - 0s 201ms/step - loss: -41.8509 - val_loss: -42.4229 - val_y0: -0.3068 - val_y1: 1.1798 - val_ate_afte_scaled: 1.4866 - lr: 5.0000e-05\n",
      "Epoch 116/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -43.5407 — ite: 3.8193  — ate: 0.7415 — pehe: 3.9379 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: -46.6530 - val_loss: -46.5859 - val_y0: -0.1819 - val_y1: 1.1225 - val_ate_afte_scaled: 1.3043 - lr: 5.0000e-05\n",
      "Epoch 117/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -54.7839 — ite: 3.9034  — ate: 0.8141 — pehe: 4.1035 \n",
      "3/3 [==============================] - 0s 214ms/step - loss: -49.2850 - val_loss: -51.0362 - val_y0: -0.4022 - val_y1: 1.2469 - val_ate_afte_scaled: 1.6491 - lr: 5.0000e-05\n",
      "Epoch 118/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -50.3353 — ite: 3.8287  — ate: 0.8311 — pehe: 4.0840 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: -54.4128 - val_loss: -58.2762 - val_y0: -0.2287 - val_y1: 1.0634 - val_ate_afte_scaled: 1.2921 - lr: 5.0000e-05\n",
      "Epoch 119/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -58.6001 — ite: 3.8808  — ate: 0.8564 — pehe: 3.9568 \n",
      "3/3 [==============================] - 0s 220ms/step - loss: -62.3733 - val_loss: -64.9212 - val_y0: -0.2996 - val_y1: 0.9661 - val_ate_afte_scaled: 1.2656 - lr: 5.0000e-05\n",
      "Epoch 120/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -72.6035 — ite: 3.7503  — ate: 0.3908 — pehe: 3.9662 \n",
      "3/3 [==============================] - 0s 206ms/step - loss: -66.3638 - val_loss: -69.2653 - val_y0: -0.4138 - val_y1: 1.0070 - val_ate_afte_scaled: 1.4208 - lr: 5.0000e-05\n",
      "Epoch 121/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -71.3211 — ite: 3.8715  — ate: 0.6115 — pehe: 4.0911 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: -76.0310 - val_loss: -76.2774 - val_y0: -0.2163 - val_y1: 1.0365 - val_ate_afte_scaled: 1.2528 - lr: 5.0000e-05\n",
      "Epoch 122/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -78.1360 — ite: 3.8314  — ate: 1.0037 — pehe: 4.0020 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: -80.3192 - val_loss: -85.3209 - val_y0: -0.3335 - val_y1: 0.9761 - val_ate_afte_scaled: 1.3096 - lr: 5.0000e-05\n",
      "Epoch 123/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -95.7296 — ite: 3.8480  — ate: 0.9597 — pehe: 4.0873 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: -88.6974 - val_loss: -89.7243 - val_y0: -0.1929 - val_y1: 1.0983 - val_ate_afte_scaled: 1.2912 - lr: 5.0000e-05\n",
      "Epoch 124/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -93.7813 — ite: 3.8582  — ate: 0.6116 — pehe: 4.1324 \n",
      "3/3 [==============================] - 0s 205ms/step - loss: -99.9415 - val_loss: -100.7165 - val_y0: -0.0445 - val_y1: 1.1490 - val_ate_afte_scaled: 1.1935 - lr: 5.0000e-05\n",
      "Epoch 125/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -108.6850 — ite: 3.8888  — ate: 0.8724 — pehe: 4.2326 \n",
      "3/3 [==============================] - 0s 223ms/step - loss: -106.2225 - val_loss: -109.0624 - val_y0: -0.2350 - val_y1: 1.0736 - val_ate_afte_scaled: 1.3086 - lr: 5.0000e-05\n",
      "Epoch 126/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -113.1213 — ite: 3.8723  — ate: 0.6121 — pehe: 4.0824 \n",
      "3/3 [==============================] - 0s 229ms/step - loss: -117.8330 - val_loss: -123.0336 - val_y0: 0.0380 - val_y1: 1.1906 - val_ate_afte_scaled: 1.1526 - lr: 5.0000e-05\n",
      "Epoch 127/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -112.9524 — ite: 3.8448  — ate: 0.6759 — pehe: 3.9339 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: -125.1874 - val_loss: -130.8911 - val_y0: -0.1141 - val_y1: 0.9861 - val_ate_afte_scaled: 1.1001 - lr: 5.0000e-05\n",
      "Epoch 128/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -144.8824 — ite: 3.7907  — ate: 0.7849 — pehe: 4.0759 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: -139.5499 - val_loss: -141.3642 - val_y0: -0.1757 - val_y1: 1.0171 - val_ate_afte_scaled: 1.1928 - lr: 5.0000e-05\n",
      "Epoch 129/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -147.5318 — ite: 3.8703  — ate: 0.6918 — pehe: 4.1607 \n",
      "3/3 [==============================] - 0s 213ms/step - loss: -146.2830 - val_loss: -150.3763 - val_y0: -0.2819 - val_y1: 0.9469 - val_ate_afte_scaled: 1.2288 - lr: 5.0000e-05\n",
      "Epoch 130/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -159.7588 — ite: 3.9381  — ate: 0.6471 — pehe: 4.1321 \n",
      "3/3 [==============================] - 0s 214ms/step - loss: -158.5480 - val_loss: -169.0686 - val_y0: -0.3655 - val_y1: 0.9836 - val_ate_afte_scaled: 1.3491 - lr: 5.0000e-05\n",
      "Epoch 131/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -168.3187 — ite: 3.7919  — ate: 0.6249 — pehe: 3.9956 \n",
      "3/3 [==============================] - 0s 207ms/step - loss: -177.6609 - val_loss: -182.4354 - val_y0: -0.1500 - val_y1: 0.8704 - val_ate_afte_scaled: 1.0204 - lr: 5.0000e-05\n",
      "Epoch 132/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -189.8261 — ite: 3.8809  — ate: 0.6952 — pehe: 4.1258 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: -191.3084 - val_loss: -197.2729 - val_y0: -0.1172 - val_y1: 1.0873 - val_ate_afte_scaled: 1.2046 - lr: 5.0000e-05\n",
      "Epoch 133/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -182.5751 — ite: 3.8460  — ate: 0.5110 — pehe: 4.0868 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: -200.1776 - val_loss: -212.8078 - val_y0: -0.1989 - val_y1: 1.0836 - val_ate_afte_scaled: 1.2825 - lr: 5.0000e-05\n",
      "Epoch 134/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -201.3248 — ite: 3.8025  — ate: 0.7562 — pehe: 4.0287 \n",
      "3/3 [==============================] - 0s 223ms/step - loss: -215.0454 - val_loss: -230.9885 - val_y0: -0.2054 - val_y1: 0.8689 - val_ate_afte_scaled: 1.0743 - lr: 5.0000e-05\n",
      "Epoch 135/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -248.3020 — ite: 3.8937  — ate: 0.5327 — pehe: 4.3865 \n",
      "3/3 [==============================] - 0s 220ms/step - loss: -231.8631 - val_loss: -245.0810 - val_y0: -0.2997 - val_y1: 0.9010 - val_ate_afte_scaled: 1.2007 - lr: 5.0000e-05\n",
      "Epoch 136/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -273.8002 — ite: 3.8400  — ate: 0.8672 — pehe: 4.0184 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: -259.8816 - val_loss: -262.9172 - val_y0: -0.3408 - val_y1: 0.8385 - val_ate_afte_scaled: 1.1793 - lr: 5.0000e-05\n",
      "Epoch 137/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -242.8770 — ite: 3.8473  — ate: 1.0021 — pehe: 4.2185 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: -278.2376 - val_loss: -288.4840 - val_y0: -0.3333 - val_y1: 1.1029 - val_ate_afte_scaled: 1.4363 - lr: 5.0000e-05\n",
      "Epoch 138/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -275.6235 — ite: 3.8305  — ate: 0.1750 — pehe: 4.0723 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: -307.4870 - val_loss: -314.0799 - val_y0: -0.3099 - val_y1: 0.9893 - val_ate_afte_scaled: 1.2992 - lr: 5.0000e-05\n",
      "Epoch 139/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -285.6364 — ite: 3.7856  — ate: 0.6755 — pehe: 4.0245 \n",
      "3/3 [==============================] - 0s 201ms/step - loss: -320.5573 - val_loss: -340.7895 - val_y0: -0.1223 - val_y1: 1.0576 - val_ate_afte_scaled: 1.1799 - lr: 5.0000e-05\n",
      "Epoch 140/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -383.3079 — ite: 3.9189  — ate: 1.2128 — pehe: 4.2326 \n",
      "3/3 [==============================] - 0s 196ms/step - loss: -351.6367 - val_loss: -363.5731 - val_y0: 0.0138 - val_y1: 0.9674 - val_ate_afte_scaled: 0.9535 - lr: 5.0000e-05\n",
      "Epoch 141/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -425.5166 — ite: 3.9309  — ate: 0.4347 — pehe: 4.1432 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: -381.6534 - val_loss: -404.3392 - val_y0: -0.1976 - val_y1: 0.8829 - val_ate_afte_scaled: 1.0806 - lr: 5.0000e-05\n",
      "Epoch 142/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -384.0534 — ite: 4.0098  — ate: 1.0898 — pehe: 4.3693 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: -408.1055 - val_loss: -438.5123 - val_y0: -0.1997 - val_y1: 0.7179 - val_ate_afte_scaled: 0.9176 - lr: 5.0000e-05\n",
      "Epoch 143/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -434.2119 — ite: 4.0751  — ate: 1.3523 — pehe: 4.4003 \n",
      "3/3 [==============================] - 0s 192ms/step - loss: -441.6552 - val_loss: -476.4592 - val_y0: -0.0714 - val_y1: 0.7082 - val_ate_afte_scaled: 0.7795 - lr: 5.0000e-05\n",
      "Epoch 144/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -493.4397 — ite: 3.8038  — ate: 0.8617 — pehe: 4.1200 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: -512.2220 - val_loss: -515.8120 - val_y0: -0.1710 - val_y1: 0.6056 - val_ate_afte_scaled: 0.7766 - lr: 5.0000e-05\n",
      "Epoch 145/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -493.6804 — ite: 3.9209  — ate: 1.1670 — pehe: 4.2645 \n",
      "3/3 [==============================] - 0s 221ms/step - loss: -543.3181 - val_loss: -557.5504 - val_y0: -0.0902 - val_y1: 0.6409 - val_ate_afte_scaled: 0.7311 - lr: 5.0000e-05\n",
      "Epoch 146/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -585.4211 — ite: 4.0392  — ate: 1.3656 — pehe: 4.3345 \n",
      "3/3 [==============================] - 0s 201ms/step - loss: -585.6324 - val_loss: -607.8029 - val_y0: -0.0580 - val_y1: 0.5936 - val_ate_afte_scaled: 0.6516 - lr: 5.0000e-05\n",
      "Epoch 147/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -541.2158 — ite: 3.9966  — ate: 0.8854 — pehe: 4.1906 \n",
      "3/3 [==============================] - 0s 205ms/step - loss: -662.1269 - val_loss: -660.1086 - val_y0: 0.1148 - val_y1: 0.5045 - val_ate_afte_scaled: 0.3897 - lr: 5.0000e-05\n",
      "Epoch 148/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -622.4729 — ite: 4.0418  — ate: 0.9918 — pehe: 4.3495 \n",
      "3/3 [==============================] - 0s 206ms/step - loss: -700.7034 - val_loss: -718.9030 - val_y0: 0.2126 - val_y1: 0.6829 - val_ate_afte_scaled: 0.4703 - lr: 5.0000e-05\n",
      "Epoch 149/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -736.4260 — ite: 4.0022  — ate: 1.5482 — pehe: 4.3192 \n",
      "3/3 [==============================] - 0s 223ms/step - loss: -761.6638 - val_loss: -784.3575 - val_y0: 0.1491 - val_y1: 0.7508 - val_ate_afte_scaled: 0.6017 - lr: 5.0000e-05\n",
      "Epoch 150/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -809.3479 — ite: 4.0091  — ate: 1.1454 — pehe: 4.2433 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: -834.0648 - val_loss: -851.7803 - val_y0: 0.3173 - val_y1: 0.8685 - val_ate_afte_scaled: 0.5512 - lr: 5.0000e-05\n",
      "Epoch 151/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -838.5988 — ite: 3.8927  — ate: 1.0075 — pehe: 4.1228 \n",
      "3/3 [==============================] - 0s 239ms/step - loss: -867.7103 - val_loss: -915.6740 - val_y0: 0.2534 - val_y1: 0.7843 - val_ate_afte_scaled: 0.5308 - lr: 5.0000e-05\n",
      "Epoch 152/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -915.3448 — ite: 3.8964  — ate: 0.9713 — pehe: 4.1729 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: -933.6310 - val_loss: -1004.3907 - val_y0: 0.3365 - val_y1: 0.9798 - val_ate_afte_scaled: 0.6434 - lr: 5.0000e-05\n",
      "Epoch 153/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -996.2505 — ite: 3.9829  — ate: 1.0323 — pehe: 4.2461 \n",
      "3/3 [==============================] - 0s 206ms/step - loss: -1018.1865 - val_loss: -1084.1700 - val_y0: 0.4081 - val_y1: 0.8217 - val_ate_afte_scaled: 0.4136 - lr: 5.0000e-05\n",
      "Epoch 154/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1048.1177 — ite: 3.9326  — ate: 1.1205 — pehe: 4.0557 \n",
      "3/3 [==============================] - 0s 219ms/step - loss: -1144.3952 - val_loss: -1170.3171 - val_y0: 0.5681 - val_y1: 1.0122 - val_ate_afte_scaled: 0.4440 - lr: 5.0000e-05\n",
      "Epoch 155/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1130.1530 — ite: 3.8488  — ate: 0.8643 — pehe: 4.0417 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: -1187.0051 - val_loss: -1246.7411 - val_y0: 0.7537 - val_y1: 0.9295 - val_ate_afte_scaled: 0.1758 - lr: 5.0000e-05\n",
      "Epoch 156/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1179.9131 — ite: 3.8291  — ate: 0.7047 — pehe: 4.0290 \n",
      "3/3 [==============================] - 0s 223ms/step - loss: -1285.2720 - val_loss: -1322.5178 - val_y0: 0.5632 - val_y1: 0.6735 - val_ate_afte_scaled: 0.1103 - lr: 5.0000e-05\n",
      "Epoch 157/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1263.3701 — ite: 3.8792  — ate: 0.9870 — pehe: 4.1956 \n",
      "3/3 [==============================] - 0s 206ms/step - loss: -1352.6877 - val_loss: -1419.9209 - val_y0: 0.7396 - val_y1: 1.0254 - val_ate_afte_scaled: 0.2858 - lr: 5.0000e-05\n",
      "Epoch 158/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1234.2832 — ite: 3.8804  — ate: 0.6386 — pehe: 4.1331 \n",
      "3/3 [==============================] - 0s 207ms/step - loss: -1412.8620 - val_loss: -1497.8263 - val_y0: 0.9746 - val_y1: 1.0247 - val_ate_afte_scaled: 0.0500 - lr: 5.0000e-05\n",
      "Epoch 159/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1422.6033 — ite: 3.8911  — ate: 0.9379 — pehe: 4.1542 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: -1547.9093 - val_loss: -1584.9379 - val_y0: 1.0060 - val_y1: 1.1657 - val_ate_afte_scaled: 0.1597 - lr: 5.0000e-05\n",
      "Epoch 160/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1553.3997 — ite: 3.8483  — ate: 0.8283 — pehe: 4.0659 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: -1682.2738 - val_loss: -1680.7759 - val_y0: 0.7249 - val_y1: 1.0241 - val_ate_afte_scaled: 0.2992 - lr: 5.0000e-05\n",
      "Epoch 161/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1805.0947 — ite: 3.9452  — ate: 0.7313 — pehe: 4.1845 \n",
      "3/3 [==============================] - 0s 207ms/step - loss: -1673.6542 - val_loss: -1790.6704 - val_y0: 0.8022 - val_y1: 0.8769 - val_ate_afte_scaled: 0.0747 - lr: 5.0000e-05\n",
      "Epoch 162/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1857.0171 — ite: 3.8868  — ate: 1.0605 — pehe: 4.0586 \n",
      "3/3 [==============================] - 0s 195ms/step - loss: -1841.8940 - val_loss: -1910.4398 - val_y0: 1.2487 - val_y1: 1.1766 - val_ate_afte_scaled: -0.0721 - lr: 5.0000e-05\n",
      "Epoch 163/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1727.2656 — ite: 3.8484  — ate: 0.6595 — pehe: 4.2381 \n",
      "3/3 [==============================] - 0s 196ms/step - loss: -1959.3610 - val_loss: -2023.1539 - val_y0: 1.1622 - val_y1: 1.3078 - val_ate_afte_scaled: 0.1456 - lr: 5.0000e-05\n",
      "Epoch 164/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -2010.1367 — ite: 3.7869  — ate: 0.9593 — pehe: 4.0933 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: -2060.0841 - val_loss: -2156.8188 - val_y0: 1.0580 - val_y1: 0.9782 - val_ate_afte_scaled: -0.0798 - lr: 5.0000e-05\n",
      "Epoch 165/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -2081.9099 — ite: 3.7396  — ate: 0.6374 — pehe: 4.0097 \n",
      "3/3 [==============================] - 0s 223ms/step - loss: -2205.9879 - val_loss: -2286.4380 - val_y0: 0.8897 - val_y1: 1.2019 - val_ate_afte_scaled: 0.3121 - lr: 5.0000e-05\n",
      "Epoch 166/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -2420.8530 — ite: 3.6869  — ate: 0.6315 — pehe: 3.9087 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: -2368.2581 - val_loss: -2425.5330 - val_y0: 1.0903 - val_y1: 1.4393 - val_ate_afte_scaled: 0.3490 - lr: 5.0000e-05\n",
      "Epoch 167/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -2483.1091 — ite: 3.8834  — ate: 1.0180 — pehe: 4.3422 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: -2454.0226 - val_loss: -2591.2561 - val_y0: 1.1439 - val_y1: 1.5985 - val_ate_afte_scaled: 0.4546 - lr: 5.0000e-05\n",
      "Epoch 168/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -2598.4214 — ite: 3.7912  — ate: 0.6014 — pehe: 3.9828 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: -2640.8914 - val_loss: -2733.7395 - val_y0: 1.3706 - val_y1: 1.6851 - val_ate_afte_scaled: 0.3145 - lr: 5.0000e-05\n",
      "Epoch 169/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -2825.1208 — ite: 3.7351  — ate: 0.6931 — pehe: 3.9102 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: -2770.1730 - val_loss: -2906.5247 - val_y0: 1.4745 - val_y1: 1.4838 - val_ate_afte_scaled: 0.0093 - lr: 5.0000e-05\n",
      "Epoch 170/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -2790.9268 — ite: 3.8363  — ate: 0.5450 — pehe: 4.1563 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: -2971.4043 - val_loss: -3086.6533 - val_y0: 1.5064 - val_y1: 1.6879 - val_ate_afte_scaled: 0.1815 - lr: 5.0000e-05\n",
      "Epoch 171/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -3007.5188 — ite: 3.7647  — ate: 0.6654 — pehe: 4.0852 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: -3191.1673 - val_loss: -3289.9636 - val_y0: 1.4897 - val_y1: 1.9001 - val_ate_afte_scaled: 0.4104 - lr: 5.0000e-05\n",
      "Epoch 172/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -3498.5793 — ite: 3.8955  — ate: 0.6858 — pehe: 4.0460 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: -3258.3656 - val_loss: -3459.8850 - val_y0: 1.4413 - val_y1: 2.0581 - val_ate_afte_scaled: 0.6167 - lr: 5.0000e-05\n",
      "Epoch 173/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -2956.5024 — ite: 3.8931  — ate: 0.9199 — pehe: 4.2631 \n",
      "3/3 [==============================] - 0s 206ms/step - loss: -3612.8266 - val_loss: -3675.2434 - val_y0: 1.4074 - val_y1: 1.7733 - val_ate_afte_scaled: 0.3659 - lr: 5.0000e-05\n",
      "Epoch 174/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -3900.4863 — ite: 3.8551  — ate: 0.3204 — pehe: 4.1819 \n",
      "3/3 [==============================] - 0s 210ms/step - loss: -3747.9390 - val_loss: -3861.8821 - val_y0: 1.5196 - val_y1: 2.0486 - val_ate_afte_scaled: 0.5290 - lr: 5.0000e-05\n",
      "Epoch 175/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -3795.1599 — ite: 3.8159  — ate: 0.8013 — pehe: 3.9568 \n",
      "3/3 [==============================] - 0s 206ms/step - loss: -3901.8721 - val_loss: -4118.1577 - val_y0: 1.4620 - val_y1: 2.1166 - val_ate_afte_scaled: 0.6546 - lr: 5.0000e-05\n",
      "Epoch 176/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -4631.8936 — ite: 3.6897  — ate: 0.5411 — pehe: 3.8823 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: -4076.0529 - val_loss: -4355.0205 - val_y0: 1.4463 - val_y1: 1.9812 - val_ate_afte_scaled: 0.5349 - lr: 5.0000e-05\n",
      "Epoch 177/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -4440.6143 — ite: 3.8131  — ate: 0.8211 — pehe: 4.0579 \n",
      "3/3 [==============================] - 0s 207ms/step - loss: -4382.7527 - val_loss: -4599.4248 - val_y0: 1.3764 - val_y1: 2.0273 - val_ate_afte_scaled: 0.6509 - lr: 5.0000e-05\n",
      "Epoch 178/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -4805.8218 — ite: 3.8581  — ate: 0.5755 — pehe: 4.1954 \n",
      "3/3 [==============================] - 0s 210ms/step - loss: -4622.1641 - val_loss: -4866.0625 - val_y0: 1.2333 - val_y1: 2.0478 - val_ate_afte_scaled: 0.8145 - lr: 5.0000e-05\n",
      "Epoch 179/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -5447.7031 — ite: 3.7882  — ate: 0.8353 — pehe: 4.0882 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: -4814.0784 - val_loss: -5145.0522 - val_y0: 1.6045 - val_y1: 2.1391 - val_ate_afte_scaled: 0.5346 - lr: 5.0000e-05\n",
      "Epoch 180/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -4869.2769 — ite: 3.8016  — ate: 0.0140 — pehe: 4.1243 \n",
      "3/3 [==============================] - 0s 206ms/step - loss: -5243.0947 - val_loss: -5417.1323 - val_y0: 1.7502 - val_y1: 2.0773 - val_ate_afte_scaled: 0.3270 - lr: 5.0000e-05\n",
      "Epoch 181/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -5781.7939 — ite: 3.7552  — ate: 0.7302 — pehe: 4.0802 \n",
      "3/3 [==============================] - 0s 214ms/step - loss: -5652.9916 - val_loss: -5754.2559 - val_y0: 1.5164 - val_y1: 2.0945 - val_ate_afte_scaled: 0.5782 - lr: 5.0000e-05\n",
      "Epoch 182/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -5804.6201 — ite: 3.7885  — ate: 0.5229 — pehe: 4.0658 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: -5790.9980 - val_loss: -6056.3125 - val_y0: 1.5576 - val_y1: 1.9581 - val_ate_afte_scaled: 0.4004 - lr: 5.0000e-05\n",
      "Epoch 183/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -5394.2500 — ite: 3.7908  — ate: 0.6159 — pehe: 4.0242 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: -6279.6936 - val_loss: -6384.3770 - val_y0: 1.7082 - val_y1: 1.8926 - val_ate_afte_scaled: 0.1843 - lr: 5.0000e-05\n",
      "Epoch 184/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -6320.2202 — ite: 3.7344  — ate: 0.4445 — pehe: 3.9153 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: -6592.1992 - val_loss: -6756.5938 - val_y0: 1.7124 - val_y1: 2.0348 - val_ate_afte_scaled: 0.3224 - lr: 5.0000e-05\n",
      "Epoch 185/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -7281.0962 — ite: 3.7492  — ate: 0.4617 — pehe: 4.0097 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: -6651.9382 - val_loss: -7118.6016 - val_y0: 1.6808 - val_y1: 2.0821 - val_ate_afte_scaled: 0.4012 - lr: 5.0000e-05\n",
      "Epoch 186/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -7113.5723 — ite: 3.7581  — ate: 0.6433 — pehe: 4.0135 \n",
      "3/3 [==============================] - 0s 204ms/step - loss: -7112.9285 - val_loss: -7550.0376 - val_y0: 1.6477 - val_y1: 2.0044 - val_ate_afte_scaled: 0.3568 - lr: 5.0000e-05\n",
      "Epoch 187/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -7448.5127 — ite: 3.7978  — ate: 0.5749 — pehe: 4.0860 \n",
      "3/3 [==============================] - 0s 217ms/step - loss: -7595.1075 - val_loss: -7939.2373 - val_y0: 1.8207 - val_y1: 2.1421 - val_ate_afte_scaled: 0.3214 - lr: 5.0000e-05\n",
      "Epoch 188/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -7526.7368 — ite: 3.8664  — ate: 0.1992 — pehe: 4.1541 \n",
      "3/3 [==============================] - 0s 210ms/step - loss: -8080.5341 - val_loss: -8352.2471 - val_y0: 1.7605 - val_y1: 2.4425 - val_ate_afte_scaled: 0.6820 - lr: 5.0000e-05\n",
      "Epoch 189/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -8616.3350 — ite: 3.8924  — ate: 0.8540 — pehe: 4.0460 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: -8594.3071 - val_loss: -8858.6367 - val_y0: 1.7736 - val_y1: 2.2664 - val_ate_afte_scaled: 0.4928 - lr: 5.0000e-05\n",
      "Epoch 190/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -8625.0146 — ite: 3.7382  — ate: 0.5287 — pehe: 4.0067 \n",
      "3/3 [==============================] - 0s 207ms/step - loss: -8756.4819 - val_loss: -9308.7109 - val_y0: 1.6354 - val_y1: 2.3187 - val_ate_afte_scaled: 0.6833 - lr: 5.0000e-05\n",
      "Epoch 191/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -9594.3555 — ite: 3.7231  — ate: 0.4554 — pehe: 4.0645 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: -9452.0889 - val_loss: -9778.2686 - val_y0: 1.9130 - val_y1: 2.4686 - val_ate_afte_scaled: 0.5556 - lr: 5.0000e-05\n",
      "Epoch 192/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -9889.1279 — ite: 3.8447  — ate: 1.1820 — pehe: 4.0742 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: -10100.2708 - val_loss: -10291.8789 - val_y0: 1.8343 - val_y1: 2.2771 - val_ate_afte_scaled: 0.4428 - lr: 5.0000e-05\n",
      "Epoch 193/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -9933.9717 — ite: 3.9548  — ate: 0.2561 — pehe: 4.2421 \n",
      "3/3 [==============================] - 0s 230ms/step - loss: -9673.5701 - val_loss: -10708.0537 - val_y0: 1.4483 - val_y1: 2.1406 - val_ate_afte_scaled: 0.6923 - lr: 5.0000e-05\n",
      "Epoch 194/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -10463.6514 — ite: 3.9077  — ate: 1.2159 — pehe: 4.3917 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: -10744.3030 - val_loss: -10889.0107 - val_y0: 1.0095 - val_y1: 1.5888 - val_ate_afte_scaled: 0.5793 - lr: 5.0000e-05\n",
      "Epoch 195/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -10423.5625 — ite: 3.8014  — ate: 0.3561 — pehe: 4.1616 \n",
      "3/3 [==============================] - 0s 218ms/step - loss: -10863.9563 - val_loss: -11116.0977 - val_y0: 0.7940 - val_y1: 1.7342 - val_ate_afte_scaled: 0.9402 - lr: 5.0000e-05\n",
      "Epoch 196/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -11206.6035 — ite: 3.6779  — ate: 0.8636 — pehe: 3.9453 \n",
      "3/3 [==============================] - 0s 205ms/step - loss: -11414.5459 - val_loss: -11411.8447 - val_y0: 1.2305 - val_y1: 1.9095 - val_ate_afte_scaled: 0.6790 - lr: 5.0000e-05\n",
      "Epoch 197/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -10105.6377 — ite: 3.7256  — ate: 0.5353 — pehe: 3.9224 \n",
      "3/3 [==============================] - 0s 230ms/step - loss: -11579.1511 - val_loss: -11652.8164 - val_y0: 1.5869 - val_y1: 2.2515 - val_ate_afte_scaled: 0.6646 - lr: 5.0000e-05\n",
      "Epoch 198/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -11686.1035 — ite: 3.7984  — ate: 0.1222 — pehe: 4.0906 \n",
      "3/3 [==============================] - 0s 228ms/step - loss: -11904.4958 - val_loss: -11999.0049 - val_y0: 2.1226 - val_y1: 2.7384 - val_ate_afte_scaled: 0.6158 - lr: 5.0000e-05\n",
      "Epoch 199/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -11044.1846 — ite: 3.8552  — ate: 0.9667 — pehe: 4.0923 \n",
      "3/3 [==============================] - 0s 224ms/step - loss: -12012.3948 - val_loss: -12232.2354 - val_y0: 2.4037 - val_y1: 2.9747 - val_ate_afte_scaled: 0.5710 - lr: 5.0000e-05\n",
      "Epoch 200/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -12848.7998 — ite: 3.8682  — ate: 0.3782 — pehe: 4.0186 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: -12239.7319 - val_loss: -12537.5166 - val_y0: 2.6532 - val_y1: 3.0739 - val_ate_afte_scaled: 0.4208 - lr: 5.0000e-05\n",
      "Epoch 201/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -13314.6172 — ite: 3.7925  — ate: 0.5598 — pehe: 4.0203 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: -12255.3733 - val_loss: -12814.1445 - val_y0: 2.6150 - val_y1: 3.0382 - val_ate_afte_scaled: 0.4232 - lr: 5.0000e-05\n",
      "Epoch 202/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -12093.6377 — ite: 3.8565  — ate: 0.2815 — pehe: 4.0707 \n",
      "3/3 [==============================] - 0s 217ms/step - loss: -12657.4314 - val_loss: -13081.8311 - val_y0: 2.5744 - val_y1: 2.9745 - val_ate_afte_scaled: 0.4001 - lr: 5.0000e-05\n",
      "Epoch 203/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -11752.4121 — ite: 3.7713  — ate: 0.5046 — pehe: 4.0076 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: -13599.1055 - val_loss: -13311.7744 - val_y0: 2.4851 - val_y1: 2.8552 - val_ate_afte_scaled: 0.3702 - lr: 5.0000e-05\n",
      "Epoch 204/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -14324.9541 — ite: 3.8898  — ate: 1.0159 — pehe: 4.3640 \n",
      "3/3 [==============================] - 0s 194ms/step - loss: -13054.8130 - val_loss: -13591.3604 - val_y0: 2.1612 - val_y1: 2.7617 - val_ate_afte_scaled: 0.6005 - lr: 5.0000e-05\n",
      "Epoch 205/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -13179.9746 — ite: 3.7203  — ate: 0.1757 — pehe: 4.1460 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: -13299.5032 - val_loss: -13932.3281 - val_y0: 2.2083 - val_y1: 2.6718 - val_ate_afte_scaled: 0.4635 - lr: 5.0000e-05\n",
      "Epoch 206/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -13451.0977 — ite: 3.8523  — ate: 0.9034 — pehe: 4.0395 \n",
      "3/3 [==============================] - 0s 219ms/step - loss: -14280.7170 - val_loss: -14238.6787 - val_y0: 2.1527 - val_y1: 2.6491 - val_ate_afte_scaled: 0.4963 - lr: 5.0000e-05\n",
      "Epoch 207/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -15015.3096 — ite: 3.8138  — ate: 0.1701 — pehe: 4.0027 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: -14227.7678 - val_loss: -14522.9150 - val_y0: 2.3520 - val_y1: 2.7234 - val_ate_afte_scaled: 0.3714 - lr: 5.0000e-05\n",
      "Epoch 208/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -13908.6367 — ite: 3.7832  — ate: 0.3529 — pehe: 4.1424 \n",
      "3/3 [==============================] - 0s 222ms/step - loss: -14546.6548 - val_loss: -14849.8848 - val_y0: 2.3833 - val_y1: 2.9380 - val_ate_afte_scaled: 0.5547 - lr: 5.0000e-05\n",
      "Epoch 209/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -13830.4297 — ite: 3.7402  — ate: 0.6575 — pehe: 4.0523 \n",
      "3/3 [==============================] - 0s 220ms/step - loss: -14931.6599 - val_loss: -15172.0918 - val_y0: 2.6426 - val_y1: 2.9842 - val_ate_afte_scaled: 0.3416 - lr: 5.0000e-05\n",
      "Epoch 210/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -15311.7227 — ite: 3.8240  — ate: 0.3694 — pehe: 4.1505 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: -15086.5161 - val_loss: -15469.3701 - val_y0: 2.6599 - val_y1: 3.1073 - val_ate_afte_scaled: 0.4474 - lr: 5.0000e-05\n",
      "Epoch 211/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -15426.5010\n",
      "Epoch 00211: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      " — ite: 3.7975  — ate: 0.6167 — pehe: 4.0773 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: -15336.0178 - val_loss: -15796.3252 - val_y0: 2.5751 - val_y1: 3.1234 - val_ate_afte_scaled: 0.5483 - lr: 5.0000e-05\n",
      "Epoch 212/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -15759.1748 — ite: 3.7563  — ate: 0.6628 — pehe: 4.0139 \n",
      "3/3 [==============================] - 0s 228ms/step - loss: -15946.4646 - val_loss: -15949.4932 - val_y0: 2.6064 - val_y1: 3.1248 - val_ate_afte_scaled: 0.5185 - lr: 2.5000e-05\n",
      "Epoch 213/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -17066.7031 — ite: 3.7721  — ate: 0.8011 — pehe: 4.0945 \n",
      "3/3 [==============================] - 1s 242ms/step - loss: -15468.6470 - val_loss: -16168.1787 - val_y0: 2.6271 - val_y1: 3.1883 - val_ate_afte_scaled: 0.5612 - lr: 2.5000e-05\n",
      "Epoch 214/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -15395.2773 — ite: 3.7116  — ate: 0.4393 — pehe: 3.9619 \n",
      "3/3 [==============================] - 0s 210ms/step - loss: -16114.2666 - val_loss: -16271.8633 - val_y0: 2.6692 - val_y1: 3.2620 - val_ate_afte_scaled: 0.5928 - lr: 2.5000e-05\n",
      "Epoch 215/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -16412.3359 — ite: 3.7389  — ate: 0.3896 — pehe: 4.0945 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: -16018.1519 - val_loss: -16474.5859 - val_y0: 2.5963 - val_y1: 3.2346 - val_ate_afte_scaled: 0.6383 - lr: 2.5000e-05\n",
      "Epoch 216/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -15328.8252 — ite: 3.7526  — ate: 0.5808 — pehe: 4.0405 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: -16491.7854 - val_loss: -16614.2598 - val_y0: 2.6230 - val_y1: 3.2592 - val_ate_afte_scaled: 0.6363 - lr: 2.5000e-05\n",
      "Epoch 217/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -15324.5039 — ite: 3.8113  — ate: 0.7260 — pehe: 4.0845 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: -16740.3999 - val_loss: -16832.7910 - val_y0: 2.6257 - val_y1: 2.9034 - val_ate_afte_scaled: 0.2777 - lr: 2.5000e-05\n",
      "Epoch 218/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -16935.2520 — ite: 3.7650  — ate: 0.6198 — pehe: 4.0146 \n",
      "3/3 [==============================] - 0s 210ms/step - loss: -16432.7603 - val_loss: -16946.9707 - val_y0: 2.5168 - val_y1: 3.3108 - val_ate_afte_scaled: 0.7940 - lr: 2.5000e-05\n",
      "Epoch 219/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -18018.5469 — ite: 3.8308  — ate: 0.4570 — pehe: 4.2298 \n",
      "3/3 [==============================] - 0s 217ms/step - loss: -16683.6963 - val_loss: -17077.9629 - val_y0: 2.5798 - val_y1: 3.1271 - val_ate_afte_scaled: 0.5473 - lr: 2.5000e-05\n",
      "Epoch 220/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -15748.8223 — ite: 3.8194  — ate: 0.9974 — pehe: 4.1443 \n",
      "3/3 [==============================] - 0s 206ms/step - loss: -17085.3833 - val_loss: -17270.0664 - val_y0: 2.6478 - val_y1: 3.1718 - val_ate_afte_scaled: 0.5240 - lr: 2.5000e-05\n",
      "Epoch 221/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -16503.9590 — ite: 3.7370  — ate: 0.4563 — pehe: 3.8898 \n",
      "3/3 [==============================] - 0s 204ms/step - loss: -16896.4922 - val_loss: -17479.5742 - val_y0: 2.6537 - val_y1: 3.1964 - val_ate_afte_scaled: 0.5427 - lr: 2.5000e-05\n",
      "Epoch 222/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -17355.9043 — ite: 3.8298  — ate: 0.3603 — pehe: 4.2032 \n",
      "3/3 [==============================] - 1s 253ms/step - loss: -17666.6824 - val_loss: -17585.4727 - val_y0: 2.7698 - val_y1: 3.3702 - val_ate_afte_scaled: 0.6004 - lr: 2.5000e-05\n",
      "Epoch 223/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -17224.4453 — ite: 3.7979  — ate: 0.5032 — pehe: 4.0852 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: -17570.7349 - val_loss: -17753.0859 - val_y0: 2.8421 - val_y1: 3.4284 - val_ate_afte_scaled: 0.5864 - lr: 2.5000e-05\n",
      "Epoch 224/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -17952.4863 — ite: 3.6935  — ate: 0.4485 — pehe: 3.8930 \n",
      "3/3 [==============================] - 0s 201ms/step - loss: -17636.7402 - val_loss: -17957.3906 - val_y0: 2.9865 - val_y1: 3.3612 - val_ate_afte_scaled: 0.3747 - lr: 2.5000e-05\n",
      "Epoch 225/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -17773.0957 — ite: 3.9210  — ate: 0.8300 — pehe: 4.2866 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: -18037.0508 - val_loss: -18148.5293 - val_y0: 2.7196 - val_y1: 3.3664 - val_ate_afte_scaled: 0.6467 - lr: 2.5000e-05\n",
      "Epoch 226/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -16116.0977 — ite: 3.8295  — ate: 0.3058 — pehe: 4.2025 \n",
      "3/3 [==============================] - 0s 207ms/step - loss: -17988.4219 - val_loss: -18315.8242 - val_y0: 2.8158 - val_y1: 3.2741 - val_ate_afte_scaled: 0.4584 - lr: 2.5000e-05\n",
      "Epoch 227/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -19653.5859 — ite: 3.7762  — ate: 0.7792 — pehe: 3.9041 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: -18430.3186 - val_loss: -18482.5195 - val_y0: 2.7782 - val_y1: 3.3793 - val_ate_afte_scaled: 0.6010 - lr: 2.5000e-05\n",
      "Epoch 228/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -19197.0703 — ite: 3.7538  — ate: 0.5028 — pehe: 3.9008 \n",
      "3/3 [==============================] - 0s 207ms/step - loss: -18110.4902 - val_loss: -18688.6348 - val_y0: 2.8529 - val_y1: 3.2333 - val_ate_afte_scaled: 0.3804 - lr: 2.5000e-05\n",
      "Epoch 229/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -18015.2500 — ite: 3.7419  — ate: 0.6062 — pehe: 3.9123 \n",
      "3/3 [==============================] - 0s 225ms/step - loss: -18946.9966 - val_loss: -18862.9043 - val_y0: 2.8168 - val_y1: 3.2290 - val_ate_afte_scaled: 0.4122 - lr: 2.5000e-05\n",
      "Epoch 230/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -16141.5312 — ite: 3.8021  — ate: 0.5323 — pehe: 4.2384 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: -19076.9653 - val_loss: -19113.2891 - val_y0: 2.9200 - val_y1: 3.3690 - val_ate_afte_scaled: 0.4490 - lr: 2.5000e-05\n",
      "Epoch 231/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -18954.3730 — ite: 3.8330  — ate: 0.6384 — pehe: 4.1264 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: -19182.5186 - val_loss: -19159.6172 - val_y0: 2.8810 - val_y1: 3.3495 - val_ate_afte_scaled: 0.4684 - lr: 2.5000e-05\n",
      "Epoch 232/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20409.1934 — ite: 3.7643  — ate: 0.2298 — pehe: 4.1594 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: -19109.9409 - val_loss: -19367.8945 - val_y0: 2.7960 - val_y1: 3.3897 - val_ate_afte_scaled: 0.5937 - lr: 2.5000e-05\n",
      "Epoch 233/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -18469.3281 — ite: 3.7194  — ate: 0.2540 — pehe: 4.0529 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: -19591.1094 - val_loss: -19568.9902 - val_y0: 2.7960 - val_y1: 3.3161 - val_ate_afte_scaled: 0.5200 - lr: 2.5000e-05\n",
      "Epoch 234/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22262.4746 — ite: 3.6659  — ate: 0.7501 — pehe: 3.9558 \n",
      "3/3 [==============================] - 0s 230ms/step - loss: -19163.6079 - val_loss: -19732.7598 - val_y0: 2.7969 - val_y1: 3.4526 - val_ate_afte_scaled: 0.6557 - lr: 2.5000e-05\n",
      "Epoch 235/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -19051.4121 — ite: 3.7221  — ate: 0.3447 — pehe: 3.9253 \n",
      "3/3 [==============================] - 0s 221ms/step - loss: -19811.7808 - val_loss: -19928.9023 - val_y0: 2.7966 - val_y1: 3.3276 - val_ate_afte_scaled: 0.5309 - lr: 2.5000e-05\n",
      "Epoch 236/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22228.0391 — ite: 3.7486  — ate: 0.2710 — pehe: 3.9875 \n",
      "3/3 [==============================] - 0s 214ms/step - loss: -19880.9395 - val_loss: -20108.5977 - val_y0: 2.8371 - val_y1: 3.2554 - val_ate_afte_scaled: 0.4183 - lr: 2.5000e-05\n",
      "Epoch 237/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -19987.1315 — ite: 3.7807  — ate: 0.6542 — pehe: 4.0238 \n",
      "3/3 [==============================] - 1s 243ms/step - loss: -19936.7593 - val_loss: -20258.2188 - val_y0: 2.9080 - val_y1: 3.5276 - val_ate_afte_scaled: 0.6196 - lr: 2.5000e-05\n",
      "Epoch 238/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -19608.6719\n",
      "Epoch 00238: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      " — ite: 3.7848  — ate: 0.5706 — pehe: 4.0914 \n",
      "3/3 [==============================] - 0s 223ms/step - loss: -20175.9160 - val_loss: -20469.2207 - val_y0: 2.9908 - val_y1: 3.4958 - val_ate_afte_scaled: 0.5050 - lr: 2.5000e-05\n",
      "Epoch 239/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20404.7070 — ite: 3.8479  — ate: 0.4811 — pehe: 4.0573 \n",
      "3/3 [==============================] - 0s 214ms/step - loss: -20488.7812 - val_loss: -20579.9902 - val_y0: 2.8627 - val_y1: 3.5067 - val_ate_afte_scaled: 0.6440 - lr: 1.2500e-05\n",
      "Epoch 240/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20914.8730 — ite: 3.7379  — ate: 0.5821 — pehe: 3.9166 \n",
      "3/3 [==============================] - 0s 218ms/step - loss: -19966.7476 - val_loss: -20687.3809 - val_y0: 3.0881 - val_y1: 3.4461 - val_ate_afte_scaled: 0.3580 - lr: 1.2500e-05\n",
      "Epoch 241/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -21471.7441 — ite: 3.7594  — ate: 0.5751 — pehe: 3.9725 \n",
      "3/3 [==============================] - 0s 198ms/step - loss: -20160.0825 - val_loss: -20715.7422 - val_y0: 2.9920 - val_y1: 3.4828 - val_ate_afte_scaled: 0.4907 - lr: 1.2500e-05\n",
      "Epoch 242/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20690.3848 — ite: 3.7733  — ate: 0.5568 — pehe: 4.0418 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: -20780.5220 - val_loss: -20800.7266 - val_y0: 3.0006 - val_y1: 3.4815 - val_ate_afte_scaled: 0.4809 - lr: 1.2500e-05\n",
      "Epoch 243/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20568.0684 — ite: 3.7081  — ate: 0.4834 — pehe: 3.8408 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: -20582.8340 - val_loss: -20919.9551 - val_y0: 2.8661 - val_y1: 3.7054 - val_ate_afte_scaled: 0.8393 - lr: 1.2500e-05\n",
      "Epoch 244/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -19341.8477 — ite: 3.7626  — ate: 0.5615 — pehe: 3.9299 \n",
      "3/3 [==============================] - 1s 258ms/step - loss: -20765.1240 - val_loss: -21038.0586 - val_y0: 2.8519 - val_y1: 3.3684 - val_ate_afte_scaled: 0.5165 - lr: 1.2500e-05\n",
      "Epoch 245/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20527.3789 — ite: 3.8137  — ate: 0.2492 — pehe: 4.1412 \n",
      "3/3 [==============================] - 0s 227ms/step - loss: -21181.9180 - val_loss: -21078.3008 - val_y0: 2.9422 - val_y1: 3.6278 - val_ate_afte_scaled: 0.6857 - lr: 1.2500e-05\n",
      "Epoch 246/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20344.6055 — ite: 3.7877  — ate: 0.6051 — pehe: 4.0630 \n",
      "3/3 [==============================] - 0s 232ms/step - loss: -21114.2817 - val_loss: -21276.0176 - val_y0: 3.1356 - val_y1: 3.4967 - val_ate_afte_scaled: 0.3610 - lr: 1.2500e-05\n",
      "Epoch 247/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20019.5156 — ite: 3.7771  — ate: 0.5236 — pehe: 3.9489 \n",
      "3/3 [==============================] - 0s 224ms/step - loss: -21358.4956 - val_loss: -21354.6855 - val_y0: 3.1456 - val_y1: 3.3810 - val_ate_afte_scaled: 0.2354 - lr: 1.2500e-05\n",
      "Epoch 248/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -21527.6582 — ite: 3.8179  — ate: 0.6135 — pehe: 4.2136 \n",
      "3/3 [==============================] - 0s 229ms/step - loss: -21770.4824 - val_loss: -21410.1992 - val_y0: 2.9376 - val_y1: 3.3982 - val_ate_afte_scaled: 0.4605 - lr: 1.2500e-05\n",
      "Epoch 249/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -19750.8984 — ite: 3.6912  — ate: 0.5158 — pehe: 3.9198 \n",
      "3/3 [==============================] - 0s 225ms/step - loss: -21278.3423 - val_loss: -21538.3242 - val_y0: 3.0394 - val_y1: 3.5183 - val_ate_afte_scaled: 0.4789 - lr: 1.2500e-05\n",
      "Epoch 250/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20796.9727 — ite: 3.7776  — ate: 0.4980 — pehe: 3.9485 \n",
      "3/3 [==============================] - 0s 226ms/step - loss: -21440.4351 - val_loss: -21554.4707 - val_y0: 3.0350 - val_y1: 3.4904 - val_ate_afte_scaled: 0.4554 - lr: 1.2500e-05\n",
      "Epoch 251/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -21586.7754 — ite: 3.7633  — ate: 0.8087 — pehe: 4.1507 \n",
      "3/3 [==============================] - 0s 223ms/step - loss: -21447.9536 - val_loss: -21670.6309 - val_y0: 2.9868 - val_y1: 3.5011 - val_ate_afte_scaled: 0.5143 - lr: 1.2500e-05\n",
      "Epoch 252/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22794.1953 — ite: 3.8806  — ate: 0.4095 — pehe: 4.2284 \n",
      "3/3 [==============================] - 0s 231ms/step - loss: -22017.0454 - val_loss: -21773.1348 - val_y0: 3.1145 - val_y1: 3.5541 - val_ate_afte_scaled: 0.4395 - lr: 1.2500e-05\n",
      "Epoch 253/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -21242.3008 — ite: 3.8149  — ate: 0.5106 — pehe: 4.1426 \n",
      "3/3 [==============================] - 0s 221ms/step - loss: -22198.8232 - val_loss: -21866.0293 - val_y0: 3.0251 - val_y1: 3.5429 - val_ate_afte_scaled: 0.5178 - lr: 1.2500e-05\n",
      "Epoch 254/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22799.1152 — ite: 3.8817  — ate: 0.6654 — pehe: 4.2528 \n",
      "3/3 [==============================] - 0s 224ms/step - loss: -21922.9287 - val_loss: -21943.4375 - val_y0: 3.0177 - val_y1: 3.5555 - val_ate_afte_scaled: 0.5378 - lr: 1.2500e-05\n",
      "Epoch 255/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20987.4355 — ite: 3.8019  — ate: 0.5120 — pehe: 4.0195 \n",
      "3/3 [==============================] - 0s 228ms/step - loss: -22133.0854 - val_loss: -22133.3965 - val_y0: 3.1210 - val_y1: 3.6121 - val_ate_afte_scaled: 0.4911 - lr: 1.2500e-05\n",
      "Epoch 256/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22982.9355 — ite: 3.8053  — ate: 0.6071 — pehe: 4.1143 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: -21921.4775 - val_loss: -22177.8613 - val_y0: 3.0239 - val_y1: 3.5513 - val_ate_afte_scaled: 0.5274 - lr: 1.2500e-05\n",
      "Epoch 257/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22318.7422 — ite: 3.8010  — ate: 0.6863 — pehe: 4.1746 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: -22417.5967 - val_loss: -22271.6641 - val_y0: 3.0770 - val_y1: 3.5738 - val_ate_afte_scaled: 0.4968 - lr: 1.2500e-05\n",
      "Epoch 258/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -21482.5332\n",
      "Epoch 00258: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      " — ite: 3.8290  — ate: 0.6508 — pehe: 4.2745 \n",
      "3/3 [==============================] - 1s 241ms/step - loss: -22313.2397 - val_loss: -22338.2734 - val_y0: 2.9646 - val_y1: 3.6425 - val_ate_afte_scaled: 0.6779 - lr: 1.2500e-05\n",
      "Epoch 259/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -21552.1582 — ite: 3.8069  — ate: 0.3110 — pehe: 4.0309 \n",
      "3/3 [==============================] - 0s 214ms/step - loss: -22548.4243 - val_loss: -22420.6348 - val_y0: 3.1753 - val_y1: 3.5988 - val_ate_afte_scaled: 0.4236 - lr: 6.2500e-06\n",
      "Epoch 260/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -24064.7969 — ite: 3.8423  — ate: 0.6066 — pehe: 4.1947 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: -22190.3750 - val_loss: -22487.2500 - val_y0: 3.2301 - val_y1: 3.4988 - val_ate_afte_scaled: 0.2687 - lr: 6.2500e-06\n",
      "Epoch 261/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22857.1133 — ite: 3.7872  — ate: 0.5401 — pehe: 4.1618 \n",
      "3/3 [==============================] - 0s 213ms/step - loss: -22120.6646 - val_loss: -22472.6992 - val_y0: 3.0004 - val_y1: 3.6555 - val_ate_afte_scaled: 0.6552 - lr: 6.2500e-06\n",
      "Epoch 262/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22429.3145 — ite: 3.7731  — ate: 0.4726 — pehe: 4.0118 \n",
      "3/3 [==============================] - 0s 224ms/step - loss: -22459.2822 - val_loss: -22601.6680 - val_y0: 3.1394 - val_y1: 3.4537 - val_ate_afte_scaled: 0.3143 - lr: 6.2500e-06\n",
      "Epoch 263/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22964.4551\n",
      "Epoch 00263: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      " — ite: 3.7719  — ate: 0.6686 — pehe: 4.0390 \n",
      "3/3 [==============================] - 0s 217ms/step - loss: -22325.0898 - val_loss: -22678.0547 - val_y0: 3.1159 - val_y1: 3.8154 - val_ate_afte_scaled: 0.6995 - lr: 6.2500e-06\n",
      "Epoch 264/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -23020.9902 — ite: 3.7809  — ate: 0.3713 — pehe: 3.8864 \n",
      "3/3 [==============================] - 0s 219ms/step - loss: -22357.7290 - val_loss: -22643.6055 - val_y0: 2.9985 - val_y1: 3.5547 - val_ate_afte_scaled: 0.5563 - lr: 3.1250e-06\n",
      "Epoch 265/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -23077.9395 — ite: 3.8432  — ate: 0.6607 — pehe: 4.2209 \n",
      "3/3 [==============================] - 0s 228ms/step - loss: -22251.9683 - val_loss: -22670.9941 - val_y0: 2.9980 - val_y1: 3.6726 - val_ate_afte_scaled: 0.6747 - lr: 3.1250e-06\n",
      "Epoch 266/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20887.3496 — ite: 3.8447  — ate: 0.5678 — pehe: 4.1092 \n",
      "3/3 [==============================] - 0s 231ms/step - loss: -23104.0454 - val_loss: -22645.6406 - val_y0: 3.2216 - val_y1: 3.4935 - val_ate_afte_scaled: 0.2720 - lr: 3.1250e-06\n",
      "Epoch 267/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -21182.3496 — ite: 3.7614  — ate: 0.6679 — pehe: 4.0735 \n",
      "3/3 [==============================] - 0s 235ms/step - loss: -22946.5356 - val_loss: -22657.9004 - val_y0: 3.1938 - val_y1: 3.5766 - val_ate_afte_scaled: 0.3828 - lr: 3.1250e-06\n",
      "Epoch 268/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22140.1309 — ite: 3.7463  — ate: 0.4000 — pehe: 3.8954 \n",
      "3/3 [==============================] - 0s 225ms/step - loss: -22379.2520 - val_loss: -22706.7090 - val_y0: 3.0149 - val_y1: 3.5947 - val_ate_afte_scaled: 0.5797 - lr: 3.1250e-06\n",
      "Epoch 269/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -24381.9277 — ite: 3.8273  — ate: 0.3504 — pehe: 4.2203 \n",
      "3/3 [==============================] - 0s 232ms/step - loss: -22299.0532 - val_loss: -22753.7793 - val_y0: 3.1953 - val_y1: 3.5380 - val_ate_afte_scaled: 0.3427 - lr: 3.1250e-06\n",
      "Epoch 270/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -25635.4473 — ite: 3.8215  — ate: 0.4508 — pehe: 4.1684 \n",
      "3/3 [==============================] - 1s 244ms/step - loss: -22461.6240 - val_loss: -22827.6992 - val_y0: 3.0085 - val_y1: 3.6166 - val_ate_afte_scaled: 0.6081 - lr: 3.1250e-06\n",
      "Epoch 271/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -23528.7266\n",
      "Epoch 00271: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      " — ite: 3.7771  — ate: 0.3437 — pehe: 4.0645 \n",
      "3/3 [==============================] - 0s 232ms/step - loss: -22677.8032 - val_loss: -22825.6035 - val_y0: 3.0650 - val_y1: 3.5483 - val_ate_afte_scaled: 0.4832 - lr: 3.1250e-06\n",
      "Epoch 272/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22975.3906 — ite: 3.7756  — ate: 0.3834 — pehe: 4.1758 \n",
      "3/3 [==============================] - 0s 231ms/step - loss: -22926.0996 - val_loss: -22826.6797 - val_y0: 3.1159 - val_y1: 3.5317 - val_ate_afte_scaled: 0.4158 - lr: 1.5625e-06\n",
      "Epoch 273/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22516.2129 — ite: 3.7964  — ate: 0.5130 — pehe: 4.1554 \n",
      "3/3 [==============================] - 0s 217ms/step - loss: -22373.5171 - val_loss: -22824.0059 - val_y0: 3.0856 - val_y1: 3.5239 - val_ate_afte_scaled: 0.4383 - lr: 1.5625e-06\n",
      "Epoch 274/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -24001.5293 — ite: 3.7701  — ate: 0.5419 — pehe: 4.0749 \n",
      "3/3 [==============================] - 0s 241ms/step - loss: -22436.0615 - val_loss: -22858.2734 - val_y0: 3.2088 - val_y1: 3.5974 - val_ate_afte_scaled: 0.3886 - lr: 1.5625e-06\n",
      "Epoch 275/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22297.2324 — ite: 3.8937  — ate: 0.4210 — pehe: 4.1369 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: -22511.5698 - val_loss: -22874.9688 - val_y0: 3.0258 - val_y1: 3.6607 - val_ate_afte_scaled: 0.6348 - lr: 1.5625e-06\n",
      "Epoch 276/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -25021.0801\n",
      "Epoch 00276: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      " — ite: 3.7931  — ate: 0.3881 — pehe: 4.0808 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: -22653.1821 - val_loss: -22832.5801 - val_y0: 3.1024 - val_y1: 3.5034 - val_ate_afte_scaled: 0.4010 - lr: 1.5625e-06\n",
      "Epoch 277/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22069.1543 — ite: 3.7908  — ate: 0.5542 — pehe: 4.0837 \n",
      "3/3 [==============================] - 0s 225ms/step - loss: -22851.5356 - val_loss: -22852.2539 - val_y0: 3.1716 - val_y1: 3.6390 - val_ate_afte_scaled: 0.4674 - lr: 7.8125e-07\n",
      "Epoch 278/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22742.8203 — ite: 3.8338  — ate: 0.3447 — pehe: 4.1173 \n",
      "3/3 [==============================] - 0s 223ms/step - loss: -22786.9243 - val_loss: -22859.8008 - val_y0: 3.1226 - val_y1: 3.5360 - val_ate_afte_scaled: 0.4134 - lr: 7.8125e-07\n",
      "Epoch 279/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22047.1680 — ite: 3.7897  — ate: 0.5175 — pehe: 4.0429 \n",
      "3/3 [==============================] - 0s 221ms/step - loss: -23262.0498 - val_loss: -22896.0957 - val_y0: 3.1525 - val_y1: 3.4865 - val_ate_afte_scaled: 0.3340 - lr: 7.8125e-07\n",
      "Epoch 280/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22811.6855 — ite: 3.7765  — ate: 0.3743 — pehe: 4.1371 \n",
      "3/3 [==============================] - 0s 225ms/step - loss: -22573.9897 - val_loss: -22856.7695 - val_y0: 3.1683 - val_y1: 3.5600 - val_ate_afte_scaled: 0.3917 - lr: 7.8125e-07\n",
      "Epoch 281/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -24553.5547 — ite: 3.8105  — ate: 0.5464 — pehe: 3.9159 \n",
      "3/3 [==============================] - 0s 224ms/step - loss: -21966.2734 - val_loss: -22872.2891 - val_y0: 3.0812 - val_y1: 3.4912 - val_ate_afte_scaled: 0.4101 - lr: 7.8125e-07\n",
      "Epoch 282/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -23327.6953 — ite: 3.8710  — ate: 0.7167 — pehe: 4.2047 \n",
      "3/3 [==============================] - 0s 232ms/step - loss: -22464.4507 - val_loss: -22872.3125 - val_y0: 3.0817 - val_y1: 3.4701 - val_ate_afte_scaled: 0.3884 - lr: 7.8125e-07\n",
      "Epoch 283/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -21860.7656 — ite: 3.7556  — ate: 0.5431 — pehe: 4.0032 \n",
      "3/3 [==============================] - 0s 218ms/step - loss: -22767.8887 - val_loss: -22935.6426 - val_y0: 3.2494 - val_y1: 3.6005 - val_ate_afte_scaled: 0.3511 - lr: 7.8125e-07\n",
      "Epoch 284/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -23581.9668\n",
      "Epoch 00284: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      " — ite: 3.8433  — ate: 0.6305 — pehe: 4.1660 \n",
      "3/3 [==============================] - 0s 231ms/step - loss: -23008.6831 - val_loss: -22893.7930 - val_y0: 3.1524 - val_y1: 3.5913 - val_ate_afte_scaled: 0.4389 - lr: 7.8125e-07\n",
      "Epoch 285/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20619.8145 — ite: 3.7873  — ate: 0.8085 — pehe: 4.0454 \n",
      "3/3 [==============================] - 0s 213ms/step - loss: -22887.8418 - val_loss: -22912.3633 - val_y0: 3.0629 - val_y1: 3.7053 - val_ate_afte_scaled: 0.6425 - lr: 3.9062e-07\n",
      "Epoch 286/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -23852.2402 — ite: 3.7551  — ate: 0.7986 — pehe: 3.9958 \n",
      "3/3 [==============================] - 0s 221ms/step - loss: -23123.7773 - val_loss: -22880.8047 - val_y0: 3.1916 - val_y1: 3.6160 - val_ate_afte_scaled: 0.4244 - lr: 3.9062e-07\n",
      "Epoch 287/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -24826.0918 — ite: 3.7653  — ate: 0.2806 — pehe: 4.0388 \n",
      "3/3 [==============================] - 0s 223ms/step - loss: -22537.9009 - val_loss: -22902.3262 - val_y0: 3.3494 - val_y1: 3.5715 - val_ate_afte_scaled: 0.2221 - lr: 3.9062e-07\n",
      "Epoch 288/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -24081.5859 — ite: 3.8211  — ate: 0.8319 — pehe: 4.1552 \n",
      "3/3 [==============================] - 1s 246ms/step - loss: -22803.6689 - val_loss: -22905.8066 - val_y0: 3.1243 - val_y1: 3.5710 - val_ate_afte_scaled: 0.4467 - lr: 3.9062e-07\n",
      "Epoch 289/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -24568.4258\n",
      "Epoch 00289: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      " — ite: 3.7939  — ate: 0.4263 — pehe: 3.9942 \n",
      "3/3 [==============================] - 0s 229ms/step - loss: -22411.2573 - val_loss: -22905.5293 - val_y0: 3.1996 - val_y1: 3.6484 - val_ate_afte_scaled: 0.4488 - lr: 3.9062e-07\n",
      "Epoch 290/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -23363.8594 — ite: 3.7061  — ate: 0.7184 — pehe: 3.9790 \n",
      "3/3 [==============================] - 0s 221ms/step - loss: -22789.5688 - val_loss: -22912.6543 - val_y0: 3.1015 - val_y1: 3.5716 - val_ate_afte_scaled: 0.4700 - lr: 1.9531e-07\n",
      "Epoch 291/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -23880.6875 — ite: 3.7234  — ate: 0.4131 — pehe: 3.9871 \n",
      "3/3 [==============================] - 0s 231ms/step - loss: -22136.0786 - val_loss: -22889.0410 - val_y0: 3.1343 - val_y1: 3.6098 - val_ate_afte_scaled: 0.4755 - lr: 1.9531e-07\n",
      "Epoch 292/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -23852.8145 — ite: 3.8327  — ate: 0.3229 — pehe: 4.0911 \n",
      "3/3 [==============================] - 0s 227ms/step - loss: -22644.8604 - val_loss: -22915.4551 - val_y0: 3.1204 - val_y1: 3.6671 - val_ate_afte_scaled: 0.5466 - lr: 1.9531e-07\n",
      "Epoch 293/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -24128.4609 — ite: 3.7764  — ate: 0.5128 — pehe: 4.1830 \n",
      "3/3 [==============================] - 0s 218ms/step - loss: -22738.8394 - val_loss: -22958.1523 - val_y0: 3.2987 - val_y1: 3.6174 - val_ate_afte_scaled: 0.3187 - lr: 1.9531e-07\n",
      "Epoch 294/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -21865.6875\n",
      "Epoch 00294: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      " — ite: 3.7934  — ate: 0.8977 — pehe: 4.1246 \n",
      "3/3 [==============================] - 0s 236ms/step - loss: -22977.6958 - val_loss: -22935.5723 - val_y0: 3.0945 - val_y1: 3.5788 - val_ate_afte_scaled: 0.4843 - lr: 1.9531e-07\n",
      "Epoch 295/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -23353.7441 — ite: 3.8176  — ate: 0.7183 — pehe: 4.0087 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: -22668.2021 - val_loss: -22933.4707 - val_y0: 3.1984 - val_y1: 3.6410 - val_ate_afte_scaled: 0.4427 - lr: 9.7656e-08\n",
      "Epoch 296/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -22742.1497 — ite: 3.7962  — ate: 0.4632 — pehe: 4.0196 \n",
      "3/3 [==============================] - 1s 273ms/step - loss: -22739.5449 - val_loss: -22963.2285 - val_y0: 3.1007 - val_y1: 3.4896 - val_ate_afte_scaled: 0.3890 - lr: 9.7656e-08\n",
      "Epoch 297/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -24362.3105 — ite: 3.7435  — ate: 0.6397 — pehe: 3.9103 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: -22663.7573 - val_loss: -22915.5410 - val_y0: 3.0850 - val_y1: 3.5420 - val_ate_afte_scaled: 0.4570 - lr: 9.7656e-08\n",
      "Epoch 298/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22663.7598 — ite: 3.7651  — ate: 0.4309 — pehe: 4.0679 \n",
      "3/3 [==============================] - 0s 221ms/step - loss: -22596.7212 - val_loss: -22956.3125 - val_y0: 3.1392 - val_y1: 3.6376 - val_ate_afte_scaled: 0.4984 - lr: 9.7656e-08\n",
      "Epoch 299/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -24819.4609\n",
      "Epoch 00299: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      " — ite: 3.7635  — ate: 0.6931 — pehe: 3.8962 \n",
      "3/3 [==============================] - 0s 226ms/step - loss: -22044.2158 - val_loss: -22945.4531 - val_y0: 3.0398 - val_y1: 3.6046 - val_ate_afte_scaled: 0.5648 - lr: 9.7656e-08\n",
      "Epoch 300/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -21886.0625 — ite: 3.6606  — ate: 0.5215 — pehe: 3.8665 \n",
      "3/3 [==============================] - 0s 210ms/step - loss: -22936.9619 - val_loss: -22928.7480 - val_y0: 3.2008 - val_y1: 3.7142 - val_ate_afte_scaled: 0.5134 - lr: 4.8828e-08\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Colab command to allow us to run Colab in TF2\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "%reload_ext tensorboard \n",
    "\n",
    "model = CEVAE()\n",
    "### MAIN CODE ####\n",
    "val_split=0.2\n",
    "batch_size=64\n",
    "verbose=True\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    " \n",
    "callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='loss', patience=40, min_delta=0), \n",
    "        #40 is Shi's recommendation patience for this dataset, but you should tune for your data \n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0, cooldown=0, min_lr=0),\n",
    "        #This learning rate scheduling is quite agressive which seems good for this dataset\n",
    "        metrics_for_cevae(data,verbose),\n",
    "        tensorboard_callback\n",
    "    ]\n",
    "    \n",
    "#optimizer hyperparameters\n",
    "learning_rate = 5e-5\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate))\n",
    "\n",
    "model.fit(\n",
    "    [data['x'],data['t'],data['ys']],\n",
    "    callbacks=callbacks,\n",
    "    validation_split=val_split,\n",
    "    epochs=300,\n",
    "    batch_size=200,\n",
    "    verbose=verbose\n",
    "    )\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 66691), started 0:10:25 ago. (Use '!kill 66691' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a132f3201c948dae\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a132f3201c948dae\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
