{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.layers as tfkl\n",
    "tfd,tfpl = tfp.distributions,tfp.layers\n",
    "import tensorflow.keras.backend as tfkb\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from networks import fc_net, p_x_z, p_t_z, p_y_tz, q_t_x, q_y_tx, q_z_txy\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from evaluation import *\n",
    "#################################IHDP Data\n",
    "# data information \n",
    "t_dim = 1\n",
    "y_dim, default_y_scale = 1,tf.exp(0.)\n",
    "M = None        # batch size during training\n",
    "z_dim = 20          # latent z dimension\n",
    "lamba = 1e-4    # weight decay\n",
    "nh, h = 3, 200  # number and size of hidden layers\n",
    "binfeats = [i for i in np.arange(6,25,1)]\n",
    "numfeats = [i for i in range(6)]\n",
    "x_bin_dim = len(binfeats)\n",
    "x_num_dim = len(numfeats)\n",
    "################################################\n",
    "activation_global = 'elu'\n",
    "\n",
    "def fc_net(input_shape, layers, out_layers = [], activation = activation_global, lamba = 1e-4):\n",
    "    net = tfk.Sequential([tfkl.InputLayer([input_shape])])\n",
    "    for hidden in layers:\n",
    "        net.add(tfkl.Dense(\n",
    "            hidden, \n",
    "            activation = activation,\n",
    "            kernel_regularizer = tf.keras.regularizers.l2(lamba),\n",
    "            kernel_initializer='RandomNormal',\n",
    "            )\n",
    "        )\n",
    "    if len(out_layers) > 0:\n",
    "        [outdim, activation_out] = out_layers\n",
    "        net.add(tfkl.Dense(outdim, activation = activation_out))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 “ihdp_npci_1-100.train.npz” 已经存在；不获取。\n",
      "\n",
      "文件 “ihdp_npci_1-100.test.npz” 已经存在；不获取。\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-0.65613806, -1.0024741 , -0.360898  ,  0.16170253,  0.24605164,\n",
       "        -0.8577868 ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "         1.        ,  0.        ,  0.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ],\n",
       "       dtype=float32),\n",
       " [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24],\n",
       " [0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title First load the data! (Click Play)\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz \n",
    "\n",
    "def load_IHDP_data(training_data,testing_data,i):\n",
    "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "        train_data=np.load(trf); test_data=np.load(tef)\n",
    "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
    "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
    "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
    "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    "        ycf=np.concatenate((train_data['ycf'][:,i],  test_data['ycf'][:,i])).astype('float32')\n",
    "\n",
    "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "        data['y']=data['y'].reshape(-1,1)\n",
    "        data['ycf'] = ycf.reshape(-1,1)\n",
    "        \n",
    "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "\n",
    "    return data\n",
    "\n",
    "ind = 7\n",
    "rep = 1\n",
    "data = load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.test.npz',i = ind)\n",
    "for key in data:\n",
    "    if key != 'y_scaler':\n",
    "        data[key] = np.repeat(data[key],repeats = rep, axis = 0)\n",
    "data['x'][0,],binfeats,numfeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CEVAE(tf.keras.Model):\n",
    "#     def __init__(self):\n",
    "#         super(CEVAE, self).__init__()\n",
    "#         ########################################\n",
    "#         # networks\n",
    "#         self.activation = 'elu'\n",
    "#         # CEVAE Model (decoder)\n",
    "#         self.t_dim = t_bin_dim\n",
    "#         self.q_y_xt_shared_hqy = fc_net(x_bin_dim + x_num_dim, (nh - 1) * [h], [])\n",
    "#         self.q_y_xt0_mu = fc_net(h, [h], [y_dim, None])\n",
    "#         self.q_y_xt1_mu = fc_net(h, [h], [y_dim, None])\n",
    "\n",
    "#     def call(self, data, training=False):\n",
    "#         if training:\n",
    "#             x,t = data[0],data[1]\n",
    "#             hqy = self.q_y_xt_shared_hqy(x)\n",
    "#             qy_t0_mu = self.q_y_xt0_mu(hqy)\n",
    "#             qy_t1_mu = self.q_y_xt1_mu(hqy)\n",
    "#             # y_loc =  t * qy_t1_mu + (1-t) * qy_t0_mu\n",
    "#             # return tfd.Normal(\n",
    "#             #     loc =  y_loc, \n",
    "#             #     scale = tf.ones_like(y_loc),\n",
    "#             #     )\n",
    "#             y0 = tfd.Normal(\n",
    "#                 loc =  qy_t0_mu, \n",
    "#                 scale = tf.ones_like(qy_t0_mu),\n",
    "#                 )\n",
    "#             y1 = tfd.Normal(\n",
    "#                 loc =  qy_t1_mu, \n",
    "#                 scale = tf.ones_like(qy_t1_mu),\n",
    "#                 )\n",
    "#             return y0,y1\n",
    "#         else:\n",
    "#             x = data\n",
    "#             hqy = self.q_y_xt_shared_hqy(x)\n",
    "#             qy_t0_mu = self.q_y_xt0_mu(hqy)\n",
    "#             qy_t1_mu = self.q_y_xt1_mu(hqy)\n",
    "\n",
    "#             y0 = tfd.Normal(\n",
    "#                 loc =  qy_t0_mu, \n",
    "#                 scale = tf.ones_like(qy_t0_mu),\n",
    "#                 )\n",
    "#             y1 = tfd.Normal(\n",
    "#                 loc =  qy_t1_mu, \n",
    "#                 scale = tf.ones_like(qy_t1_mu),\n",
    "#                 )\n",
    "#             return y0,y1\n",
    "\n",
    "\n",
    "#     def cevae_loss(self, data, pred, training = False):\n",
    "#         # if training:\n",
    "#         #     _, t_train, y_train = data[0],data[1],data[2]\n",
    "#         #     y_pred = pred\n",
    "#         #     loss = y_pred.log_prob(y_train)\n",
    "#         #     loss = -tfkb.mean(loss)\n",
    "#         #     return lossxs\n",
    "#         # else:\n",
    "#         #     _, t_train, y_train = data[0],data[1],data[2]\n",
    "#         #     y0,y1 = pred\n",
    "#         #     loss = y0.log_prob(y_train)*(1-t_train) + y1.log_prob(y_train)* t_train\n",
    "#         #     loss = -tfkb.mean(loss)\n",
    "#         #     return loss\n",
    "#         _, t_train, y_train = data[0],data[1],data[2]\n",
    "#         y0,y1 = pred\n",
    "#         loss = y0.log_prob(y_train)*(1-t_train) + y1.log_prob(y_train)* t_train\n",
    "#         loss = -tfkb.mean(loss)\n",
    "#         return loss\n",
    "\n",
    "#     def train_step(self, data):\n",
    "#         data = data[0]\n",
    "#         x,t,_ = data\n",
    "        \n",
    "#         with tf.GradientTape() as tape:\n",
    "#             pred = self([x,t], training=True)  # Forward pass\n",
    "#             loss = self.cevae_loss(data = data, pred = pred, training = True)\n",
    "#         # Compute gradients\n",
    "#         trainable_vars = self.trainable_variables\n",
    "#         gradients = tape.gradient(loss, trainable_vars)\n",
    "#         # Update weights\n",
    "#         self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "#         metrics = {\n",
    "#             \"loss\": loss,\n",
    "#         }\n",
    "#         return metrics\n",
    "\n",
    "#     def test_step(self, data):\n",
    "#         # Unpack the data. Its structure depends on your model and\n",
    "#         # on what you pass to `fit()`.\n",
    "#         data = data[0]\n",
    "#         x,t,y = data\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             pred = self(x, training=False)  # Forward pass\n",
    "#             loss = self.cevae_loss(data = data, pred = pred, training = False)\n",
    "#             y0, y1 = pred[0].sample(),pred[1].sample()\n",
    "#         metrics = {\"loss\":loss,\"y0\": tfkb.mean(y0),\"y1\": tfkb.mean(y1)}\n",
    "#         return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class metrics_for_cevae(Callback):\n",
    "    def __init__(self,data, verbose=0):   \n",
    "        super(metrics_for_cevae, self).__init__()\n",
    "        self.data=data #feed the callback the full dataset\n",
    "        self.verbose=verbose\n",
    "\n",
    "        #needed for PEHEnn; Called in self.find_ynn\n",
    "        self.data['o_idx']=tf.range(self.data['t'].shape[0])\n",
    "        self.data['c_idx']=self.data['o_idx'][self.data['t'].squeeze()==0] #These are the indices of the control units\n",
    "        self.data['t_idx']=self.data['o_idx'][self.data['t'].squeeze()==1] #These are the indices of the treated units\n",
    "        # ['x', 't', 'y', 'mu_0', 'mu_1', 'y_scaler', 'ys', 'o_idx', 'c_idx', 't_idx']\n",
    "        self.y = tf.cast(data['y'],tf.float32)\n",
    "        self.t = tf.cast(data['t'],tf.float32)\n",
    "        self.y_cf = tf.cast(data['ycf'],tf.float32)\n",
    "        self.mu0 = tf.cast(data['mu_0'],tf.float32)\n",
    "        self.mu1 = tf.cast(data['mu_1'],tf.float32)\n",
    "        if self.mu0 is not None and self.mu1 is not None:\n",
    "            self.true_ite = self.mu1 - self.mu0\n",
    "\n",
    "    def rmse_ite(self, ypred1, ypred0):\n",
    "        idx1, idx0 = self.t, 1-self.t\n",
    "        ite1, ite0 = (self.y - ypred0) * idx1, (ypred1 - self.y)*idx0\n",
    "        pred_ite = ite1 + ite0\n",
    "        return tf.math.sqrt(tfkb.mean(tf.math.square(self.true_ite - pred_ite)))\n",
    "\n",
    "    def abs_ate(self, ypred1, ypred0):\n",
    "        return tf.math.abs(tfkb.mean(ypred1 - ypred0) - tfkb.mean(self.true_ite))\n",
    "\n",
    "    def pehe(self, ypred1, ypred0):\n",
    "        return tf.math.sqrt(tfkb.mean(tf.math.square((self.mu1 - self.mu0) - (ypred1 - ypred0))))\n",
    "\n",
    "    def y_errors(self, y0, y1):\n",
    "        ypred = (1 - self.t) * y0 + self.t * y1\n",
    "        ypred_cf = self.t * y0 + (1 - self.t) * y1\n",
    "        return self.y_errors_pcf(ypred, ypred_cf)\n",
    "\n",
    "    def y_errors_pcf(self, ypred, ypred_cf):\n",
    "        rmse_factual = tf.math.sqrt(tfkb.mean(tf.math.square(ypred - self.y)))\n",
    "        rmse_cfactual = tf.math.sqrt(tfkb.mean(tf.math.square(ypred_cf - self.y_cf)))\n",
    "        return rmse_factual, rmse_cfactual\n",
    "\n",
    "    def calc_stats(self, ypred1, ypred0):\n",
    "        ite = self.rmse_ite(ypred1, ypred0)\n",
    "        ate = self.abs_ate(ypred1, ypred0)\n",
    "        pehe = self.pehe(ypred1, ypred0)\n",
    "        return ite, ate, pehe\n",
    "\n",
    "    def get_concat_pred(self,pred):\n",
    "        ypred0, ypred1 = pred\n",
    "        ypred0 = ypred0.sample()\n",
    "        ypred1 = ypred1.sample()\n",
    "        try:\n",
    "            y_pred0,y_pred1 = self.data['y_scaler'].inverse_transform(ypred0),self.data['y_scaler'].inverse_transform(ypred1)\n",
    "        except:\n",
    "            y_pred0 = self.data['y_scaler'].inverse_transform(tf.expand_dims(ypred0,-1))\n",
    "            y_pred1 = self.data['y_scaler'].inverse_transform(tf.expand_dims(ypred1,-1))\n",
    "        y_pred0, y_pred1 = tf.squeeze(y_pred0),tf.squeeze(y_pred1)\n",
    "        return tf.cast(y_pred0,tf.float32), tf.cast(y_pred1,tf.float32)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        pred = self.model(self.data['x'])\n",
    "        y_infer = pred[0]\n",
    "        ypred0, ypred1 = self.get_concat_pred(y_infer)\n",
    "        ite, ate, pehe = self.calc_stats(ypred1, ypred0)\n",
    "        tf.summary.scalar(\"ate\", data=tfkb.mean(ypred1 - ypred0), step=epoch)\n",
    "        tf.summary.scalar(\"ite_error\", data=ite, step=epoch)\n",
    "        tf.summary.scalar(\"ate_error\", data=ate, step=epoch)\n",
    "        tf.summary.scalar(\"pehe_error\",data=pehe, step=epoch)\n",
    "        \n",
    "        out_str=f' — ite: {ite:.4f}  — ate: {ate:.4f} — pehe: {pehe:.4f} '\n",
    "        \n",
    "        if self.verbose > 0: print(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import *\n",
    "class CEVAE(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CEVAE, self).__init__()\n",
    "        ########################################\n",
    "        # networks\n",
    "        self.activation = 'elu'\n",
    "        # CEVAE Model \n",
    "        ## (encoder)\n",
    "        self.q_y_tx = q_y_tx(x_bin_dim, x_num_dim, y_dim, t_dim, nh, h)\n",
    "        self.q_t_x = q_t_x(x_bin_dim, x_num_dim, t_dim, nh, h)\n",
    "        self.q_z_txy = q_z_txy(x_bin_dim, x_num_dim, y_dim, t_dim, z_dim, nh,h)\n",
    "        ## (decoder)\n",
    "        self.p_x_z = p_x_z(x_bin_dim, x_num_dim, z_dim, nh, h)\n",
    "        self.p_t_z = p_t_z(t_dim, z_dim, nh, h)\n",
    "        self.p_y_tz = p_y_tz(y_dim, t_dim, z_dim, nh, h)\n",
    "        \n",
    "\n",
    "    def call(self, data, training=False):\n",
    "        if training:\n",
    "            x_train,t_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "            \n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            \n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.sample()\n",
    "            # decoder\n",
    "            ## p(x|z)\n",
    "            x_num,x_bin = self.p_x_z(z_infer_sample)\n",
    "            ## p(t|z)\n",
    "            t = self.p_t_z(z_infer_sample)\n",
    "            ## p(y|t,z)\n",
    "            y = self.p_y_tz(tf.concat([t_train,z_infer_sample],-1) )\n",
    "            \n",
    "            return y_infer,t_infer,z_infer,y,t,x_num,x_bin\n",
    "        else:\n",
    "            x_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "        \n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            \n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.sample()\n",
    "            return y_infer,t_infer,z_infer\n",
    "\n",
    "\n",
    "    def cevae_loss(self, data, pred, training = False):\n",
    "        x_train, t_train, y_train = data[0],data[1],data[2]\n",
    "        x_train_num, x_train_bin = x_train[:,:x_num_dim],x_train[:,x_num_dim:]\n",
    "        y_infer,t_infer,z_infer,y,t,x_num,x_bin = pred\n",
    "        y0,y1 = y_infer\n",
    "\n",
    "        # reconstruct loss\n",
    "        recon_x_num = tfkb.sum(x_num.log_prob(x_train_num), 1)\n",
    "        recon_x_bin = tfkb.sum(x_bin.log_prob(x_train_bin), 1)\n",
    "        recon_y = tfkb.sum(y.log_prob(y_train), 1)\n",
    "        recon_t = tfkb.sum(t.log_prob(t_train), 1)\n",
    "\n",
    "        # kl loss\n",
    "        z_infer_sample = z_infer.sample()\n",
    "        z = tfd.Normal(loc = [0] * 20, scale = [1]*20)\n",
    "        kl_z = tfkb.sum((z.log_prob(z_infer_sample) - z_infer.log_prob(z_infer_sample)), -1)\n",
    "        \n",
    "        # aux loss\n",
    "        aux_y = tfkb.sum(y0.log_prob(y_train)*(1-t_train) + y1.log_prob(y_train)* t_train, 1)\n",
    "        aux_t = tfkb.sum(t_infer.log_prob(t_train), 1)\n",
    "    \n",
    "\n",
    "        loss = -tfkb.mean(recon_x_bin + recon_x_num + recon_y + recon_t + aux_y + aux_t + kl_z)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, data):\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "            loss = self.cevae_loss(data = data, pred = pred, training = True)\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        metrics = {\n",
    "            \"loss\": loss,\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "        y_infer = pred[0]\n",
    "        loss = self.cevae_loss(data = data, pred = pred, training = False)\n",
    "        y0, y1 = y_infer[0].sample(),y_infer[1].sample()\n",
    "        metrics = {\"loss\":loss,\"y0\": tfkb.mean(y0),\"y1\": tfkb.mean(y1)}\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 33.1078 — ite: 4.6466  — ate: 3.6275 — pehe: 5.6244 \n",
      "3/3 [==============================] - 5s 676ms/step - loss: 32.9690 - val_loss: 32.5098 - val_y0: 0.0061 - val_y1: 0.0231 - lr: 5.0000e-05\n",
      "Epoch 2/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 32.9965 — ite: 4.6601  — ate: 3.2342 — pehe: 5.2751 \n",
      "3/3 [==============================] - 1s 304ms/step - loss: 32.1460 - val_loss: 33.3579 - val_y0: -0.0103 - val_y1: 0.1338 - lr: 5.0000e-05\n",
      "Epoch 3/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 31.1569 — ite: 4.4822  — ate: 2.5920 — pehe: 4.9529 \n",
      "3/3 [==============================] - 1s 265ms/step - loss: 31.5990 - val_loss: 32.2091 - val_y0: -0.1119 - val_y1: 0.2906 - lr: 5.0000e-05\n",
      "Epoch 4/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 30.9820 — ite: 4.3006  — ate: 2.2434 — pehe: 4.7100 \n",
      "3/3 [==============================] - 1s 260ms/step - loss: 30.8017 - val_loss: 31.6426 - val_y0: -0.2508 - val_y1: 0.3265 - lr: 5.0000e-05\n",
      "Epoch 5/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 31.1166 — ite: 4.2654  — ate: 1.8601 — pehe: 4.4369 \n",
      "3/3 [==============================] - 1s 261ms/step - loss: 30.4009 - val_loss: 31.3525 - val_y0: -0.4925 - val_y1: 0.3286 - lr: 5.0000e-05\n",
      "Epoch 6/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 29.6158 — ite: 4.1015  — ate: 1.3261 — pehe: 4.1288 \n",
      "3/3 [==============================] - 1s 282ms/step - loss: 29.7174 - val_loss: 30.5321 - val_y0: -0.3943 - val_y1: 0.5166 - lr: 5.0000e-05\n",
      "Epoch 7/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 29.8463 — ite: 4.0849  — ate: 1.0931 — pehe: 4.2928 \n",
      "3/3 [==============================] - 0s 231ms/step - loss: 29.4161 - val_loss: 30.1645 - val_y0: -0.4534 - val_y1: 0.5726 - lr: 5.0000e-05\n",
      "Epoch 8/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 29.3142 — ite: 4.0707  — ate: 1.1323 — pehe: 4.0725 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: 29.0642 - val_loss: 29.7947 - val_y0: -0.2456 - val_y1: 0.6989 - lr: 5.0000e-05\n",
      "Epoch 9/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 29.0415 — ite: 3.9831  — ate: 0.8557 — pehe: 4.0111 \n",
      "3/3 [==============================] - 1s 256ms/step - loss: 28.6277 - val_loss: 29.1754 - val_y0: -0.3523 - val_y1: 0.8401 - lr: 5.0000e-05\n",
      "Epoch 10/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.0381 — ite: 3.9267  — ate: 1.0418 — pehe: 4.1322 \n",
      "3/3 [==============================] - 1s 243ms/step - loss: 28.2817 - val_loss: 28.5530 - val_y0: -0.3987 - val_y1: 0.7874 - lr: 5.0000e-05\n",
      "Epoch 11/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.4967 — ite: 3.9811  — ate: 1.0124 — pehe: 4.2376 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: 27.9621 - val_loss: 28.0697 - val_y0: -0.1862 - val_y1: 0.8314 - lr: 5.0000e-05\n",
      "Epoch 12/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.1053 — ite: 3.8941  — ate: 0.6565 — pehe: 4.0742 \n",
      "3/3 [==============================] - 0s 224ms/step - loss: 27.7607 - val_loss: 28.4720 - val_y0: -0.3243 - val_y1: 0.8447 - lr: 5.0000e-05\n",
      "Epoch 13/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.7143 — ite: 3.8929  — ate: 0.4663 — pehe: 3.9535 \n",
      "3/3 [==============================] - 0s 218ms/step - loss: 27.3690 - val_loss: 28.0606 - val_y0: -0.3653 - val_y1: 0.9891 - lr: 5.0000e-05\n",
      "Epoch 14/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.5172 — ite: 3.8718  — ate: 0.5659 — pehe: 4.0959 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: 27.1331 - val_loss: 27.2830 - val_y0: -0.2800 - val_y1: 0.9104 - lr: 5.0000e-05\n",
      "Epoch 15/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.0936 — ite: 3.8751  — ate: 0.7051 — pehe: 4.0444 \n",
      "3/3 [==============================] - 0s 232ms/step - loss: 26.7325 - val_loss: 27.8637 - val_y0: -0.2142 - val_y1: 0.9678 - lr: 5.0000e-05\n",
      "Epoch 16/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.2794 — ite: 3.8356  — ate: 0.5999 — pehe: 3.8896 \n",
      "3/3 [==============================] - 0s 226ms/step - loss: 26.5582 - val_loss: 26.9658 - val_y0: -0.1418 - val_y1: 1.0041 - lr: 5.0000e-05\n",
      "Epoch 17/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 26.4969 — ite: 3.8334  — ate: 0.8747 — pehe: 3.9983 \n",
      "3/3 [==============================] - 0s 226ms/step - loss: 26.2569 - val_loss: 26.5279 - val_y0: -0.3380 - val_y1: 0.9612 - lr: 5.0000e-05\n",
      "Epoch 18/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.9519 — ite: 3.8878  — ate: 0.8062 — pehe: 3.9919 \n",
      "3/3 [==============================] - 0s 242ms/step - loss: 26.0538 - val_loss: 26.0845 - val_y0: -0.2696 - val_y1: 0.9960 - lr: 5.0000e-05\n",
      "Epoch 19/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 26.0416 — ite: 3.9058  — ate: 0.4013 — pehe: 3.9693 \n",
      "3/3 [==============================] - 1s 261ms/step - loss: 25.6594 - val_loss: 26.0730 - val_y0: -0.2525 - val_y1: 0.9623 - lr: 5.0000e-05\n",
      "Epoch 20/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 25.8801 — ite: 3.8628  — ate: 0.4644 — pehe: 3.8926 \n",
      "3/3 [==============================] - 1s 252ms/step - loss: 25.8435 - val_loss: 26.3592 - val_y0: -0.3920 - val_y1: 1.1469 - lr: 5.0000e-05\n",
      "Epoch 21/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.2885 — ite: 3.8606  — ate: 0.1675 — pehe: 3.8731 \n",
      "3/3 [==============================] - 0s 235ms/step - loss: 25.3642 - val_loss: 25.7997 - val_y0: -0.4088 - val_y1: 0.8863 - lr: 5.0000e-05\n",
      "Epoch 22/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.1415 — ite: 3.7652  — ate: 0.3281 — pehe: 3.9106 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: 25.1532 - val_loss: 25.5616 - val_y0: -0.3038 - val_y1: 1.0731 - lr: 5.0000e-05\n",
      "Epoch 23/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.8554 — ite: 3.8470  — ate: 0.1081 — pehe: 3.9238 \n",
      "3/3 [==============================] - 0s 204ms/step - loss: 24.8179 - val_loss: 25.6254 - val_y0: -0.5266 - val_y1: 1.0209 - lr: 5.0000e-05\n",
      "Epoch 24/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.8525 — ite: 3.8037  — ate: 0.2725 — pehe: 3.7606 \n",
      "3/3 [==============================] - 0s 196ms/step - loss: 24.7405 - val_loss: 25.6138 - val_y0: -0.1557 - val_y1: 1.0721 - lr: 5.0000e-05\n",
      "Epoch 25/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.4588 — ite: 3.8518  — ate: 0.1384 — pehe: 4.0878 \n",
      "3/3 [==============================] - 0s 182ms/step - loss: 24.5077 - val_loss: 25.2053 - val_y0: -0.2905 - val_y1: 1.2513 - lr: 5.0000e-05\n",
      "Epoch 26/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.6048 — ite: 3.8256  — ate: 0.2659 — pehe: 3.9368 \n",
      "3/3 [==============================] - 0s 192ms/step - loss: 24.3026 - val_loss: 24.6721 - val_y0: -0.3507 - val_y1: 1.0670 - lr: 5.0000e-05\n",
      "Epoch 27/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.3902 — ite: 3.7810  — ate: 0.0820 — pehe: 3.7082 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: 24.1852 - val_loss: 24.7277 - val_y0: -0.3688 - val_y1: 1.0401 - lr: 5.0000e-05\n",
      "Epoch 28/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.6897 — ite: 3.8577  — ate: 0.0413 — pehe: 3.8549 \n",
      "3/3 [==============================] - 0s 220ms/step - loss: 23.6766 - val_loss: 24.1631 - val_y0: -0.3623 - val_y1: 1.2023 - lr: 5.0000e-05\n",
      "Epoch 29/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.1523 — ite: 3.8270  — ate: 0.2797 — pehe: 3.7421 \n",
      "3/3 [==============================] - 0s 210ms/step - loss: 23.9136 - val_loss: 24.1366 - val_y0: -0.2495 - val_y1: 1.0367 - lr: 5.0000e-05\n",
      "Epoch 30/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.3918 — ite: 3.8273  — ate: 0.0729 — pehe: 3.9706 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: 23.4895 - val_loss: 23.9249 - val_y0: -0.3431 - val_y1: 1.2076 - lr: 5.0000e-05\n",
      "Epoch 31/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.5443 — ite: 3.7580  — ate: 0.1403 — pehe: 3.8745 \n",
      "3/3 [==============================] - 0s 219ms/step - loss: 23.4012 - val_loss: 23.7083 - val_y0: -0.2656 - val_y1: 1.2526 - lr: 5.0000e-05\n",
      "Epoch 32/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.0771 — ite: 3.7614  — ate: 0.0982 — pehe: 3.8847 \n",
      "3/3 [==============================] - 0s 219ms/step - loss: 23.2027 - val_loss: 23.5371 - val_y0: -0.2498 - val_y1: 1.1750 - lr: 5.0000e-05\n",
      "Epoch 33/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.3190 — ite: 3.8756  — ate: 0.0742 — pehe: 3.8546 \n",
      "3/3 [==============================] - 1s 259ms/step - loss: 22.8662 - val_loss: 23.4086 - val_y0: -0.3447 - val_y1: 1.0935 - lr: 5.0000e-05\n",
      "Epoch 34/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.0655 — ite: 3.7518  — ate: 0.0663 — pehe: 3.8140 \n",
      "3/3 [==============================] - 1s 300ms/step - loss: 22.8250 - val_loss: 23.5191 - val_y0: -0.3471 - val_y1: 1.2046 - lr: 5.0000e-05\n",
      "Epoch 35/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.7069 — ite: 3.7654  — ate: 0.0146 — pehe: 3.6318 \n",
      "3/3 [==============================] - 1s 251ms/step - loss: 22.6898 - val_loss: 22.5968 - val_y0: -0.2176 - val_y1: 1.0655 - lr: 5.0000e-05\n",
      "Epoch 36/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.9135 — ite: 3.8204  — ate: 0.0519 — pehe: 3.9729 \n",
      "3/3 [==============================] - 1s 294ms/step - loss: 22.7184 - val_loss: 23.3713 - val_y0: -0.1678 - val_y1: 1.2192 - lr: 5.0000e-05\n",
      "Epoch 37/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 22.3747 — ite: 3.8219  — ate: 0.1526 — pehe: 3.6818 \n",
      "3/3 [==============================] - 1s 255ms/step - loss: 22.5009 - val_loss: 22.5816 - val_y0: -0.3334 - val_y1: 1.2901 - lr: 5.0000e-05\n",
      "Epoch 38/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.0118 — ite: 3.8010  — ate: 0.1513 — pehe: 3.8724 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: 22.3136 - val_loss: 23.0510 - val_y0: -0.3791 - val_y1: 1.2906 - lr: 5.0000e-05\n",
      "Epoch 39/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.1460 — ite: 3.7923  — ate: 0.1748 — pehe: 3.8407 \n",
      "3/3 [==============================] - 0s 223ms/step - loss: 21.8837 - val_loss: 22.4190 - val_y0: -0.3421 - val_y1: 1.2669 - lr: 5.0000e-05\n",
      "Epoch 40/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.0428 — ite: 3.8388  — ate: 0.3388 — pehe: 3.9057 \n",
      "3/3 [==============================] - 0s 222ms/step - loss: 22.0898 - val_loss: 22.2061 - val_y0: -0.3146 - val_y1: 1.1313 - lr: 5.0000e-05\n",
      "Epoch 41/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.2739 — ite: 3.8090  — ate: 0.3221 — pehe: 3.8725 \n",
      "3/3 [==============================] - 0s 214ms/step - loss: 21.6360 - val_loss: 21.7971 - val_y0: -0.2325 - val_y1: 1.1802 - lr: 5.0000e-05\n",
      "Epoch 42/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.4184 — ite: 3.8902  — ate: 0.2251 — pehe: 3.8576 \n",
      "3/3 [==============================] - 0s 222ms/step - loss: 21.5851 - val_loss: 22.1519 - val_y0: -0.3519 - val_y1: 1.1302 - lr: 5.0000e-05\n",
      "Epoch 43/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.1468 — ite: 3.7720  — ate: 0.2405 — pehe: 3.8508 \n",
      "3/3 [==============================] - 1s 303ms/step - loss: 21.0217 - val_loss: 22.0308 - val_y0: -0.2564 - val_y1: 1.1984 - lr: 5.0000e-05\n",
      "Epoch 44/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.4715 — ite: 3.7198  — ate: 0.0125 — pehe: 3.7526 \n",
      "3/3 [==============================] - 0s 228ms/step - loss: 21.0167 - val_loss: 21.7555 - val_y0: -0.1015 - val_y1: 1.3214 - lr: 5.0000e-05\n",
      "Epoch 45/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.8995 — ite: 3.8129  — ate: 0.3086 — pehe: 3.9174 \n",
      "3/3 [==============================] - 0s 238ms/step - loss: 20.9754 - val_loss: 21.7395 - val_y0: -0.3407 - val_y1: 1.0804 - lr: 5.0000e-05\n",
      "Epoch 46/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.1555 — ite: 3.8182  — ate: 0.0731 — pehe: 3.7144 \n",
      "3/3 [==============================] - 1s 312ms/step - loss: 20.6042 - val_loss: 21.5397 - val_y0: -0.3350 - val_y1: 1.1564 - lr: 5.0000e-05\n",
      "Epoch 47/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.4278 — ite: 3.8159  — ate: 0.1737 — pehe: 3.8295 \n",
      "3/3 [==============================] - 0s 232ms/step - loss: 20.2945 - val_loss: 21.1775 - val_y0: -0.1381 - val_y1: 1.2397 - lr: 5.0000e-05\n",
      "Epoch 48/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.4958 — ite: 3.8364  — ate: 0.3291 — pehe: 3.8639 \n",
      "3/3 [==============================] - 0s 214ms/step - loss: 20.3522 - val_loss: 21.1964 - val_y0: -0.3134 - val_y1: 1.2279 - lr: 5.0000e-05\n",
      "Epoch 49/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.0276 — ite: 3.8627  — ate: 0.1676 — pehe: 3.7276 \n",
      "3/3 [==============================] - 0s 222ms/step - loss: 20.2424 - val_loss: 21.0945 - val_y0: -0.2696 - val_y1: 1.2052 - lr: 5.0000e-05\n",
      "Epoch 50/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.6949 — ite: 3.7870  — ate: 0.0422 — pehe: 3.8726 \n",
      "3/3 [==============================] - 0s 222ms/step - loss: 20.1098 - val_loss: 21.1525 - val_y0: -0.2938 - val_y1: 1.1692 - lr: 5.0000e-05\n",
      "Epoch 51/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.1133 — ite: 3.8280  — ate: 0.1283 — pehe: 3.6962 \n",
      "3/3 [==============================] - 0s 218ms/step - loss: 20.0445 - val_loss: 20.8007 - val_y0: -0.1900 - val_y1: 1.2146 - lr: 5.0000e-05\n",
      "Epoch 52/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.6374 — ite: 3.8306  — ate: 0.1450 — pehe: 3.8383 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: 19.8803 - val_loss: 20.7755 - val_y0: -0.3328 - val_y1: 1.1149 - lr: 5.0000e-05\n",
      "Epoch 53/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.3063 — ite: 3.7989  — ate: 0.0660 — pehe: 3.8142 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: 19.9182 - val_loss: 20.4387 - val_y0: -0.3207 - val_y1: 1.1963 - lr: 5.0000e-05\n",
      "Epoch 54/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.7383 — ite: 3.7902  — ate: 0.0158 — pehe: 3.7000 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: 19.7440 - val_loss: 20.4398 - val_y0: -0.2510 - val_y1: 1.3735 - lr: 5.0000e-05\n",
      "Epoch 55/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.7289 — ite: 3.7316  — ate: 0.0020 — pehe: 3.7896 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: 19.6124 - val_loss: 20.0772 - val_y0: -0.4107 - val_y1: 1.1101 - lr: 5.0000e-05\n",
      "Epoch 56/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.6758 — ite: 3.8357  — ate: 0.0030 — pehe: 3.8403 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: 19.0919 - val_loss: 19.9669 - val_y0: -0.1440 - val_y1: 1.0217 - lr: 5.0000e-05\n",
      "Epoch 57/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.1153 — ite: 3.9316  — ate: 0.0437 — pehe: 3.8320 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: 19.2243 - val_loss: 20.3082 - val_y0: -0.1940 - val_y1: 1.0568 - lr: 5.0000e-05\n",
      "Epoch 58/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.4890 — ite: 3.8057  — ate: 0.0809 — pehe: 3.6739 \n",
      "3/3 [==============================] - 0s 227ms/step - loss: 19.0242 - val_loss: 19.4505 - val_y0: -0.1975 - val_y1: 1.1623 - lr: 5.0000e-05\n",
      "Epoch 59/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.5803 — ite: 3.8680  — ate: 0.1176 — pehe: 3.7465 \n",
      "3/3 [==============================] - 0s 220ms/step - loss: 18.7250 - val_loss: 19.4689 - val_y0: -0.2742 - val_y1: 1.0976 - lr: 5.0000e-05\n",
      "Epoch 60/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.4747 — ite: 3.8562  — ate: 0.0134 — pehe: 3.8699 \n",
      "3/3 [==============================] - 0s 223ms/step - loss: 18.8497 - val_loss: 19.3287 - val_y0: -0.2248 - val_y1: 1.1044 - lr: 5.0000e-05\n",
      "Epoch 61/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.5076 — ite: 3.8915  — ate: 0.0596 — pehe: 3.8727 \n",
      "3/3 [==============================] - 0s 226ms/step - loss: 18.8103 - val_loss: 19.7227 - val_y0: -0.1426 - val_y1: 1.1091 - lr: 5.0000e-05\n",
      "Epoch 62/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.4108 — ite: 3.8025  — ate: 0.0351 — pehe: 3.8337 \n",
      "3/3 [==============================] - 0s 221ms/step - loss: 18.4208 - val_loss: 19.0089 - val_y0: -0.2458 - val_y1: 1.2729 - lr: 5.0000e-05\n",
      "Epoch 63/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.5407 — ite: 3.8444  — ate: 0.0037 — pehe: 3.7331 \n",
      "3/3 [==============================] - 0s 222ms/step - loss: 18.4925 - val_loss: 18.7424 - val_y0: -0.2878 - val_y1: 1.1512 - lr: 5.0000e-05\n",
      "Epoch 64/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.8905 — ite: 3.9643  — ate: 0.0311 — pehe: 4.0112 \n",
      "3/3 [==============================] - 0s 226ms/step - loss: 18.5812 - val_loss: 18.9327 - val_y0: -0.1951 - val_y1: 1.2398 - lr: 5.0000e-05\n",
      "Epoch 65/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.3468 — ite: 3.8559  — ate: 0.1914 — pehe: 3.8095 \n",
      "3/3 [==============================] - 0s 231ms/step - loss: 18.0740 - val_loss: 18.6675 - val_y0: -0.2327 - val_y1: 1.0868 - lr: 5.0000e-05\n",
      "Epoch 66/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.4655 — ite: 3.9640  — ate: 0.0622 — pehe: 3.8607 \n",
      "3/3 [==============================] - 0s 228ms/step - loss: 17.9295 - val_loss: 18.9861 - val_y0: -0.2042 - val_y1: 1.0824 - lr: 5.0000e-05\n",
      "Epoch 67/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17.7046 — ite: 3.8703  — ate: 0.0467 — pehe: 3.7800 \n",
      "3/3 [==============================] - 0s 224ms/step - loss: 18.0912 - val_loss: 18.7219 - val_y0: -0.1402 - val_y1: 1.1124 - lr: 5.0000e-05\n",
      "Epoch 68/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.6169 — ite: 3.8380  — ate: 0.0061 — pehe: 3.5585 \n",
      "3/3 [==============================] - 0s 219ms/step - loss: 17.5543 - val_loss: 17.9076 - val_y0: -0.2408 - val_y1: 1.1325 - lr: 5.0000e-05\n",
      "Epoch 69/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.0349 — ite: 3.9445  — ate: 0.0983 — pehe: 3.9892 \n",
      "3/3 [==============================] - 0s 226ms/step - loss: 17.3844 - val_loss: 17.8939 - val_y0: -0.1734 - val_y1: 1.1849 - lr: 5.0000e-05\n",
      "Epoch 70/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17.4998 — ite: 3.8406  — ate: 0.1468 — pehe: 3.8672 \n",
      "3/3 [==============================] - 0s 222ms/step - loss: 17.5060 - val_loss: 18.1780 - val_y0: -0.2305 - val_y1: 1.1862 - lr: 5.0000e-05\n",
      "Epoch 71/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17.8303 — ite: 3.7939  — ate: 0.2778 — pehe: 3.7723 \n",
      "3/3 [==============================] - 0s 220ms/step - loss: 17.0936 - val_loss: 17.6453 - val_y0: -0.1848 - val_y1: 1.2426 - lr: 5.0000e-05\n",
      "Epoch 72/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17.2973 — ite: 3.8291  — ate: 0.1349 — pehe: 3.7361 \n",
      "3/3 [==============================] - 0s 220ms/step - loss: 16.8809 - val_loss: 17.4194 - val_y0: -0.2260 - val_y1: 1.1730 - lr: 5.0000e-05\n",
      "Epoch 73/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17.4135 — ite: 3.8576  — ate: 0.1101 — pehe: 3.9683 \n",
      "3/3 [==============================] - 0s 224ms/step - loss: 16.7292 - val_loss: 17.5401 - val_y0: -0.2662 - val_y1: 1.1624 - lr: 5.0000e-05\n",
      "Epoch 74/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17.5672 — ite: 3.9858  — ate: 0.0789 — pehe: 3.9150 \n",
      "3/3 [==============================] - 0s 219ms/step - loss: 16.6290 - val_loss: 17.6205 - val_y0: -0.2646 - val_y1: 1.1255 - lr: 5.0000e-05\n",
      "Epoch 75/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16.8531 — ite: 3.8685  — ate: 0.3717 — pehe: 3.6110 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: 16.9907 - val_loss: 17.2741 - val_y0: -0.4259 - val_y1: 1.1459 - lr: 5.0000e-05\n",
      "Epoch 76/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16.6691 — ite: 3.8852  — ate: 0.0632 — pehe: 3.7657 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: 16.2107 - val_loss: 16.8215 - val_y0: -0.5032 - val_y1: 1.1512 - lr: 5.0000e-05\n",
      "Epoch 77/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17.2325 — ite: 3.8450  — ate: 0.1618 — pehe: 3.6921 \n",
      "3/3 [==============================] - 0s 226ms/step - loss: 16.3663 - val_loss: 16.3169 - val_y0: -0.2074 - val_y1: 1.0983 - lr: 5.0000e-05\n",
      "Epoch 78/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16.2065 — ite: 3.8212  — ate: 0.1106 — pehe: 3.7398 \n",
      "3/3 [==============================] - 0s 230ms/step - loss: 15.8921 - val_loss: 16.1732 - val_y0: -0.4039 - val_y1: 1.2502 - lr: 5.0000e-05\n",
      "Epoch 79/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15.8488 — ite: 3.9536  — ate: 0.0375 — pehe: 3.6997 \n",
      "3/3 [==============================] - 0s 229ms/step - loss: 15.7065 - val_loss: 16.4219 - val_y0: -0.2461 - val_y1: 1.0909 - lr: 5.0000e-05\n",
      "Epoch 80/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15.1796 — ite: 3.9583  — ate: 0.2255 — pehe: 3.8688 \n",
      "3/3 [==============================] - 0s 227ms/step - loss: 15.6535 - val_loss: 16.7382 - val_y0: -0.1611 - val_y1: 1.0817 - lr: 5.0000e-05\n",
      "Epoch 81/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15.6120 — ite: 3.8201  — ate: 0.0931 — pehe: 3.8617 \n",
      "3/3 [==============================] - 0s 222ms/step - loss: 15.3979 - val_loss: 15.3987 - val_y0: -0.1420 - val_y1: 1.2929 - lr: 5.0000e-05\n",
      "Epoch 82/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15.2603 — ite: 3.8108  — ate: 0.0498 — pehe: 3.8708 \n",
      "3/3 [==============================] - 0s 225ms/step - loss: 15.0584 - val_loss: 15.7160 - val_y0: -0.3096 - val_y1: 1.0819 - lr: 5.0000e-05\n",
      "Epoch 83/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14.6881 — ite: 3.9095  — ate: 0.2807 — pehe: 3.6388 \n",
      "3/3 [==============================] - 0s 234ms/step - loss: 15.0958 - val_loss: 14.8923 - val_y0: -0.3612 - val_y1: 1.2545 - lr: 5.0000e-05\n",
      "Epoch 84/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13.8706 — ite: 3.9280  — ate: 0.0721 — pehe: 3.8194 \n",
      "3/3 [==============================] - 0s 226ms/step - loss: 14.2606 - val_loss: 14.6350 - val_y0: -0.2511 - val_y1: 1.2145 - lr: 5.0000e-05\n",
      "Epoch 85/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13.8149 — ite: 3.8418  — ate: 0.2461 — pehe: 3.8247 \n",
      "3/3 [==============================] - 0s 220ms/step - loss: 13.9506 - val_loss: 14.8622 - val_y0: -0.2778 - val_y1: 1.0866 - lr: 5.0000e-05\n",
      "Epoch 86/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14.0227 — ite: 4.0228  — ate: 0.0957 — pehe: 3.8591 \n",
      "3/3 [==============================] - 0s 228ms/step - loss: 13.8706 - val_loss: 14.9844 - val_y0: -0.2315 - val_y1: 1.1239 - lr: 5.0000e-05\n",
      "Epoch 87/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13.7266 — ite: 3.9256  — ate: 0.1157 — pehe: 3.7901 \n",
      "3/3 [==============================] - 0s 222ms/step - loss: 13.7098 - val_loss: 14.3050 - val_y0: -0.2330 - val_y1: 1.0497 - lr: 5.0000e-05\n",
      "Epoch 88/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13.4462 — ite: 3.8996  — ate: 0.1929 — pehe: 3.7382 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: 13.6761 - val_loss: 14.3544 - val_y0: -0.2215 - val_y1: 1.1969 - lr: 5.0000e-05\n",
      "Epoch 89/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 12.2804 — ite: 3.9735  — ate: 0.0462 — pehe: 3.8859 \n",
      "3/3 [==============================] - 0s 244ms/step - loss: 13.3293 - val_loss: 12.9892 - val_y0: -0.4793 - val_y1: 1.0555 - lr: 5.0000e-05\n",
      "Epoch 90/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 12.3650 — ite: 3.8721  — ate: 0.0673 — pehe: 3.7766 \n",
      "3/3 [==============================] - 0s 229ms/step - loss: 13.0387 - val_loss: 13.8538 - val_y0: -0.1789 - val_y1: 1.1655 - lr: 5.0000e-05\n",
      "Epoch 91/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13.0108 — ite: 3.8675  — ate: 0.1679 — pehe: 3.8983 \n",
      "3/3 [==============================] - 0s 231ms/step - loss: 12.6727 - val_loss: 12.2776 - val_y0: -0.1698 - val_y1: 1.0338 - lr: 5.0000e-05\n",
      "Epoch 92/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13.1467 — ite: 3.8443  — ate: 0.0264 — pehe: 3.7862 \n",
      "3/3 [==============================] - 0s 229ms/step - loss: 12.1059 - val_loss: 12.0472 - val_y0: -0.2785 - val_y1: 1.1928 - lr: 5.0000e-05\n",
      "Epoch 93/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11.7657 — ite: 3.8720  — ate: 0.1175 — pehe: 3.7637 \n",
      "3/3 [==============================] - 0s 231ms/step - loss: 11.7010 - val_loss: 11.6958 - val_y0: -0.3105 - val_y1: 1.1924 - lr: 5.0000e-05\n",
      "Epoch 94/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10.5469 — ite: 3.9094  — ate: 0.2725 — pehe: 3.7927 \n",
      "3/3 [==============================] - 0s 224ms/step - loss: 11.2376 - val_loss: 11.6174 - val_y0: -0.2987 - val_y1: 1.0567 - lr: 5.0000e-05\n",
      "Epoch 95/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11.5740 — ite: 4.0493  — ate: 0.2390 — pehe: 3.8536 \n",
      "3/3 [==============================] - 0s 234ms/step - loss: 11.0185 - val_loss: 10.8851 - val_y0: -0.3704 - val_y1: 1.1750 - lr: 5.0000e-05\n",
      "Epoch 96/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11.6109 — ite: 4.1097  — ate: 0.2788 — pehe: 4.0397 \n",
      "3/3 [==============================] - 1s 238ms/step - loss: 9.9718 - val_loss: 10.8376 - val_y0: -0.2442 - val_y1: 1.2193 - lr: 5.0000e-05\n",
      "Epoch 97/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.9729 — ite: 3.9165  — ate: 0.1158 — pehe: 4.0798 \n",
      "3/3 [==============================] - 1s 244ms/step - loss: 10.2264 - val_loss: 9.6935 - val_y0: -0.2668 - val_y1: 1.1459 - lr: 5.0000e-05\n",
      "Epoch 98/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 9.2685 — ite: 3.9904  — ate: 0.0007 — pehe: 3.7448 \n",
      "3/3 [==============================] - 1s 243ms/step - loss: 9.5805 - val_loss: 10.0502 - val_y0: -0.2936 - val_y1: 1.2096 - lr: 5.0000e-05\n",
      "Epoch 99/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.7678 — ite: 3.9413  — ate: 0.0269 — pehe: 3.7028 \n",
      "3/3 [==============================] - 1s 244ms/step - loss: 8.5510 - val_loss: 8.4762 - val_y0: -0.3244 - val_y1: 0.9913 - lr: 5.0000e-05\n",
      "Epoch 100/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.9946 — ite: 3.8737  — ate: 0.2826 — pehe: 3.7658 \n",
      "3/3 [==============================] - 0s 228ms/step - loss: 8.1570 - val_loss: 8.5601 - val_y0: -0.3365 - val_y1: 1.0863 - lr: 5.0000e-05\n",
      "Epoch 101/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.0256 — ite: 4.0069  — ate: 0.1039 — pehe: 3.8704 \n",
      "3/3 [==============================] - 0s 236ms/step - loss: 8.0582 - val_loss: 7.4307 - val_y0: -0.2958 - val_y1: 0.9984 - lr: 5.0000e-05\n",
      "Epoch 102/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.9407 — ite: 3.9359  — ate: 0.1086 — pehe: 3.9586 \n",
      "3/3 [==============================] - 0s 217ms/step - loss: 6.9182 - val_loss: 6.8261 - val_y0: -0.3983 - val_y1: 1.2158 - lr: 5.0000e-05\n",
      "Epoch 103/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.0267 — ite: 3.8725  — ate: 0.0928 — pehe: 3.6495 \n",
      "3/3 [==============================] - 0s 221ms/step - loss: 6.5881 - val_loss: 7.1043 - val_y0: -0.1939 - val_y1: 1.2199 - lr: 5.0000e-05\n",
      "Epoch 104/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.5666 — ite: 4.0231  — ate: 0.1389 — pehe: 3.8983 \n",
      "3/3 [==============================] - 0s 226ms/step - loss: 5.8331 - val_loss: 5.4727 - val_y0: -0.2638 - val_y1: 1.1638 - lr: 5.0000e-05\n",
      "Epoch 105/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.1384 — ite: 3.9657  — ate: 0.1911 — pehe: 3.8366 \n",
      "3/3 [==============================] - 0s 233ms/step - loss: 4.8048 - val_loss: 5.0517 - val_y0: -0.3571 - val_y1: 0.9681 - lr: 5.0000e-05\n",
      "Epoch 106/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.2785 — ite: 4.0020  — ate: 0.0464 — pehe: 3.9185 \n",
      "3/3 [==============================] - 0s 227ms/step - loss: 3.8454 - val_loss: 3.5341 - val_y0: -0.1392 - val_y1: 1.1880 - lr: 5.0000e-05\n",
      "Epoch 107/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.8542 — ite: 3.8880  — ate: 0.2865 — pehe: 3.8238 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: 2.5855 - val_loss: 3.0497 - val_y0: -0.1503 - val_y1: 1.2049 - lr: 5.0000e-05\n",
      "Epoch 108/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 2.5369 — ite: 3.9553  — ate: 0.1774 — pehe: 3.7862 \n",
      "3/3 [==============================] - 0s 241ms/step - loss: 3.0539 - val_loss: 3.2901 - val_y0: -0.3532 - val_y1: 1.1259 - lr: 5.0000e-05\n",
      "Epoch 109/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.1418 — ite: 4.0153  — ate: 0.0150 — pehe: 3.8497 \n",
      "3/3 [==============================] - 0s 228ms/step - loss: 2.0177 - val_loss: 1.1797 - val_y0: -0.3330 - val_y1: 1.0600 - lr: 5.0000e-05\n",
      "Epoch 110/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9048 — ite: 4.0639  — ate: 0.0037 — pehe: 3.9148 \n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.2359 - val_loss: 0.0569 - val_y0: -0.3401 - val_y1: 1.2892 - lr: 5.0000e-05\n",
      "Epoch 111/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -0.0980 — ite: 4.0079  — ate: 0.0807 — pehe: 3.8693 \n",
      "3/3 [==============================] - 0s 224ms/step - loss: -0.6524 - val_loss: -1.1040 - val_y0: -0.3248 - val_y1: 1.2443 - lr: 5.0000e-05\n",
      "Epoch 112/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -2.8618 — ite: 4.0169  — ate: 0.4257 — pehe: 3.9543 \n",
      "3/3 [==============================] - 0s 236ms/step - loss: -2.2268 - val_loss: -1.5020 - val_y0: -0.3130 - val_y1: 1.2035 - lr: 5.0000e-05\n",
      "Epoch 113/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -2.6908 — ite: 4.0933  — ate: 0.3619 — pehe: 3.9018 \n",
      "3/3 [==============================] - 1s 246ms/step - loss: -3.7261 - val_loss: -3.2761 - val_y0: -0.2684 - val_y1: 1.3373 - lr: 5.0000e-05\n",
      "Epoch 114/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -0.4965 — ite: 4.0241  — ate: 0.3761 — pehe: 3.8448 \n",
      "3/3 [==============================] - 0s 240ms/step - loss: -4.9025 - val_loss: -4.6458 - val_y0: -0.3742 - val_y1: 1.2269 - lr: 5.0000e-05\n",
      "Epoch 115/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -3.7566 — ite: 4.0219  — ate: 0.4127 — pehe: 4.0220 \n",
      "3/3 [==============================] - 0s 237ms/step - loss: -6.2519 - val_loss: -7.0312 - val_y0: -0.3444 - val_y1: 1.2371 - lr: 5.0000e-05\n",
      "Epoch 116/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -6.9443 — ite: 4.1212  — ate: 0.2591 — pehe: 3.9669 \n",
      "3/3 [==============================] - 0s 228ms/step - loss: -7.5797 - val_loss: -7.9477 - val_y0: -0.2169 - val_y1: 1.1724 - lr: 5.0000e-05\n",
      "Epoch 117/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -11.8263 — ite: 4.0909  — ate: 0.2465 — pehe: 3.9016 \n",
      "3/3 [==============================] - 0s 236ms/step - loss: -8.8742 - val_loss: -10.1396 - val_y0: -0.4478 - val_y1: 1.2992 - lr: 5.0000e-05\n",
      "Epoch 118/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -9.7776 — ite: 4.0685  — ate: 0.1882 — pehe: 3.8660 \n",
      "3/3 [==============================] - 0s 230ms/step - loss: -10.9700 - val_loss: -12.5014 - val_y0: -0.2832 - val_y1: 1.1433 - lr: 5.0000e-05\n",
      "Epoch 119/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -12.0900 — ite: 4.0440  — ate: 0.0175 — pehe: 4.0113 \n",
      "3/3 [==============================] - 0s 220ms/step - loss: -13.2878 - val_loss: -13.4931 - val_y0: -0.3472 - val_y1: 1.0710 - lr: 5.0000e-05\n",
      "Epoch 120/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -17.8760 — ite: 3.9367  — ate: 0.2277 — pehe: 3.8716 \n",
      "3/3 [==============================] - 0s 224ms/step - loss: -14.2942 - val_loss: -14.8482 - val_y0: -0.4353 - val_y1: 1.1427 - lr: 5.0000e-05\n",
      "Epoch 121/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -17.6366 — ite: 4.1857  — ate: 0.1312 — pehe: 3.9082 \n",
      "3/3 [==============================] - 0s 222ms/step - loss: -18.3878 - val_loss: -19.2712 - val_y0: -0.2328 - val_y1: 1.1898 - lr: 5.0000e-05\n",
      "Epoch 122/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20.3985 — ite: 4.0419  — ate: 0.2579 — pehe: 4.0474 \n",
      "3/3 [==============================] - 0s 235ms/step - loss: -20.5502 - val_loss: -20.5018 - val_y0: -0.3924 - val_y1: 1.1479 - lr: 5.0000e-05\n",
      "Epoch 123/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -24.6633 — ite: 4.1547  — ate: 0.1069 — pehe: 3.9479 \n",
      "3/3 [==============================] - 0s 237ms/step - loss: -22.5011 - val_loss: -22.6081 - val_y0: -0.3090 - val_y1: 1.2577 - lr: 5.0000e-05\n",
      "Epoch 124/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -25.3445 — ite: 4.1822  — ate: 0.5706 — pehe: 4.1225 \n",
      "3/3 [==============================] - 1s 263ms/step - loss: -27.4165 - val_loss: -28.9883 - val_y0: -0.2050 - val_y1: 1.3169 - lr: 5.0000e-05\n",
      "Epoch 125/140\n",
      "3/3 [==============================] - ETA: 0s - loss: -27.9575 — ite: 4.3017  — ate: 0.3543 — pehe: 4.0757 \n",
      "3/3 [==============================] - 1s 300ms/step - loss: -28.9165 - val_loss: -31.3509 - val_y0: -0.4029 - val_y1: 1.2427 - lr: 5.0000e-05\n",
      "Epoch 126/140\n",
      "3/3 [==============================] - ETA: 0s - loss: -32.0482 — ite: 4.1758  — ate: 0.2681 — pehe: 4.0379 \n",
      "3/3 [==============================] - 1s 352ms/step - loss: -33.2531 - val_loss: -32.4680 - val_y0: -0.1318 - val_y1: 1.3480 - lr: 5.0000e-05\n",
      "Epoch 127/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -28.8873 — ite: 4.2152  — ate: 0.0984 — pehe: 3.9055 \n",
      "3/3 [==============================] - 1s 388ms/step - loss: -35.2251 - val_loss: -36.9432 - val_y0: -0.2266 - val_y1: 1.1554 - lr: 5.0000e-05\n",
      "Epoch 128/140\n",
      "3/3 [==============================] - ETA: 0s - loss: -38.2259 — ite: 4.2613  — ate: 0.3339 — pehe: 3.9214 \n",
      "3/3 [==============================] - 1s 426ms/step - loss: -38.9823 - val_loss: -40.0183 - val_y0: -0.2087 - val_y1: 1.2229 - lr: 5.0000e-05\n",
      "Epoch 129/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -44.9838 — ite: 4.4330  — ate: 0.1171 — pehe: 4.2167 \n",
      "3/3 [==============================] - 1s 294ms/step - loss: -43.8350 - val_loss: -47.8453 - val_y0: -0.2869 - val_y1: 1.1856 - lr: 5.0000e-05\n",
      "Epoch 130/140\n",
      "3/3 [==============================] - ETA: 0s - loss: -47.8536 — ite: 4.3350  — ate: 0.2931 — pehe: 4.1024 \n",
      "3/3 [==============================] - 0s 235ms/step - loss: -47.5879 - val_loss: -50.9310 - val_y0: -0.4312 - val_y1: 1.2175 - lr: 5.0000e-05\n",
      "Epoch 131/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -49.1186 — ite: 4.3935  — ate: 0.1056 — pehe: 4.2217 \n",
      "3/3 [==============================] - 0s 231ms/step - loss: -54.3547 - val_loss: -56.3696 - val_y0: -0.3183 - val_y1: 1.0923 - lr: 5.0000e-05\n",
      "Epoch 132/140\n",
      "3/3 [==============================] - ETA: 0s - loss: -57.5928 — ite: 4.4220  — ate: 0.6275 — pehe: 4.1339 \n",
      "3/3 [==============================] - 1s 307ms/step - loss: -58.4049 - val_loss: -61.1796 - val_y0: -0.3642 - val_y1: 1.3171 - lr: 5.0000e-05\n",
      "Epoch 133/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -56.4929 — ite: 4.5446  — ate: 0.3871 — pehe: 4.1047 \n",
      "3/3 [==============================] - 1s 306ms/step - loss: -63.5312 - val_loss: -63.8067 - val_y0: -0.3811 - val_y1: 1.3371 - lr: 5.0000e-05\n",
      "Epoch 134/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -60.6281 — ite: 4.5582  — ate: 0.3734 — pehe: 4.3028 \n",
      "3/3 [==============================] - 0s 235ms/step - loss: -66.1133 - val_loss: -72.5607 - val_y0: -0.2539 - val_y1: 1.1736 - lr: 5.0000e-05\n",
      "Epoch 135/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -76.8646 — ite: 4.6691  — ate: 0.6266 — pehe: 4.1819 \n",
      "3/3 [==============================] - 1s 261ms/step - loss: -72.4297 - val_loss: -80.9230 - val_y0: -0.3108 - val_y1: 1.2477 - lr: 5.0000e-05\n",
      "Epoch 136/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -88.6886 — ite: 4.8347  — ate: 0.5772 — pehe: 4.2525 \n",
      "3/3 [==============================] - 1s 263ms/step - loss: -83.6169 - val_loss: -85.6504 - val_y0: -0.3940 - val_y1: 1.1542 - lr: 5.0000e-05\n",
      "Epoch 137/140\n",
      "3/3 [==============================] - ETA: 0s - loss: -89.2676 — ite: 4.8599  — ate: 0.7911 — pehe: 4.4368 \n",
      "3/3 [==============================] - 1s 252ms/step - loss: -88.8936 - val_loss: -94.3786 - val_y0: -0.4705 - val_y1: 1.3563 - lr: 5.0000e-05\n",
      "Epoch 138/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -85.0162 — ite: 4.9414  — ate: 0.5925 — pehe: 4.5074 \n",
      "3/3 [==============================] - 1s 253ms/step - loss: -99.4889 - val_loss: -101.5145 - val_y0: -0.4794 - val_y1: 1.1556 - lr: 5.0000e-05\n",
      "Epoch 139/140\n",
      "3/3 [==============================] - ETA: 0s - loss: -105.6729 — ite: 5.0394  — ate: 0.3707 — pehe: 4.2476 \n",
      "3/3 [==============================] - 1s 277ms/step - loss: -104.0623 - val_loss: -108.4767 - val_y0: -0.2372 - val_y1: 1.0994 - lr: 5.0000e-05\n",
      "Epoch 140/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -123.7317 — ite: 5.2832  — ate: 0.7244 — pehe: 4.5219 \n",
      "3/3 [==============================] - 1s 267ms/step - loss: -112.7635 - val_loss: -118.0253 - val_y0: 0.0223 - val_y1: 0.9823 - lr: 5.0000e-05\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Colab command to allow us to run Colab in TF2\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "%reload_ext tensorboard \n",
    "\n",
    "model = CEVAE()\n",
    "### MAIN CODE ####\n",
    "val_split=0.2\n",
    "batch_size=64\n",
    "verbose=True\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    " \n",
    "callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='val_loss', patience=40, min_delta=0), \n",
    "        #40 is Shi's recommendation patience for this dataset, but you should tune for your data \n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0, cooldown=0, min_lr=0),\n",
    "        #This learning rate scheduling is quite agressive which seems good for this dataset\n",
    "        metrics_for_cevae(data,verbose),\n",
    "        tensorboard_callback\n",
    "    ]\n",
    "    \n",
    "#optimizer hyperparameters\n",
    "learning_rate = 5e-5\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate = learning_rate, \n",
    "        # momentum = momentum, \n",
    "        # nesterov=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "model.fit(\n",
    "    [data['x'],data['t'],data['ys']],\n",
    "    callbacks=callbacks,\n",
    "    validation_split=val_split,\n",
    "    epochs=140,\n",
    "    batch_size=200,\n",
    "    verbose=verbose\n",
    "    )\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 11635), started 1 day, 6:10:34 ago. (Use '!kill 11635' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b11ffe793fec6138\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b11ffe793fec6138\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
