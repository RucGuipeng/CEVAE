{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-08 22:08:22.291167: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.layers as tfkl\n",
    "tfd,tfpl = tfp.distributions,tfp.layers\n",
    "import tensorflow.keras.backend as tfkb\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from networks import fc_net, p_x_z, p_t_z, p_y_tz, q_t_x, q_y_tx, q_z_txy\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from evaluation import *\n",
    "#################################IHDP Data\n",
    "# data information \n",
    "t_dim = 1\n",
    "y_dim, default_y_scale = 1,tf.exp(0.)\n",
    "M = None        # batch size during training\n",
    "z_dim = 20          # latent z dimension\n",
    "lamba = 1e-4    # weight decay\n",
    "nh, h = 3, 200  # number and size of hidden layers\n",
    "binfeats = [i for i in np.arange(6,25,1)]\n",
    "numfeats = [i for i in range(6)]\n",
    "x_bin_dim = len(binfeats)\n",
    "x_num_dim = len(numfeats)\n",
    "################################################\n",
    "activation_global = 'elu'\n",
    "\n",
    "def fc_net(input_shape, layers, out_layers = [], activation = activation_global, lamba = 1e-4):\n",
    "    net = tfk.Sequential([tfkl.InputLayer([input_shape])])\n",
    "    for hidden in layers:\n",
    "        net.add(tfkl.Dense(\n",
    "            hidden, \n",
    "            activation = activation,\n",
    "            kernel_regularizer = tf.keras.regularizers.l2(lamba),\n",
    "            kernel_initializer='RandomNormal',\n",
    "            )\n",
    "        )\n",
    "    if len(out_layers) > 0:\n",
    "        [outdim, activation_out] = out_layers\n",
    "        net.add(tfkl.Dense(outdim, activation = activation_out))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 “ihdp_npci_1-100.train.npz” 已经存在；不获取。\n",
      "\n",
      "文件 “ihdp_npci_1-100.test.npz” 已经存在；不获取。\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-0.65613806, -1.0024741 , -0.360898  ,  0.16170253,  0.24605164,\n",
       "        -0.8577868 ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "         1.        ,  0.        ,  0.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ],\n",
       "       dtype=float32),\n",
       " [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24],\n",
       " [0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title First load the data! (Click Play)\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz \n",
    "\n",
    "def load_IHDP_data(training_data,testing_data,i):\n",
    "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "        train_data=np.load(trf); test_data=np.load(tef)\n",
    "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
    "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
    "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
    "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    "        ycf=np.concatenate((train_data['ycf'][:,i],  test_data['ycf'][:,i])).astype('float32')\n",
    "\n",
    "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "        data['y']=data['y'].reshape(-1,1)\n",
    "        data['ycf'] = ycf.reshape(-1,1)\n",
    "        \n",
    "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "\n",
    "    return data\n",
    "\n",
    "ind = 7\n",
    "rep = 1\n",
    "data = load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.test.npz',i = ind)\n",
    "for key in data:\n",
    "    if key != 'y_scaler':\n",
    "        data[key] = np.repeat(data[key],repeats = rep, axis = 0)\n",
    "data['x'][0,],binfeats,numfeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CEVAE(tf.keras.Model):\n",
    "#     def __init__(self):\n",
    "#         super(CEVAE, self).__init__()\n",
    "#         ########################################\n",
    "#         # networks\n",
    "#         self.activation = 'elu'\n",
    "#         # CEVAE Model (decoder)\n",
    "#         self.t_dim = t_bin_dim\n",
    "#         self.q_y_xt_shared_hqy = fc_net(x_bin_dim + x_num_dim, (nh - 1) * [h], [])\n",
    "#         self.q_y_xt0_mu = fc_net(h, [h], [y_dim, None])\n",
    "#         self.q_y_xt1_mu = fc_net(h, [h], [y_dim, None])\n",
    "\n",
    "#     def call(self, data, training=False):\n",
    "#         if training:\n",
    "#             x,t = data[0],data[1]\n",
    "#             hqy = self.q_y_xt_shared_hqy(x)\n",
    "#             qy_t0_mu = self.q_y_xt0_mu(hqy)\n",
    "#             qy_t1_mu = self.q_y_xt1_mu(hqy)\n",
    "#             # y_loc =  t * qy_t1_mu + (1-t) * qy_t0_mu\n",
    "#             # return tfd.Normal(\n",
    "#             #     loc =  y_loc, \n",
    "#             #     scale = tf.ones_like(y_loc),\n",
    "#             #     )\n",
    "#             y0 = tfd.Normal(\n",
    "#                 loc =  qy_t0_mu, \n",
    "#                 scale = tf.ones_like(qy_t0_mu),\n",
    "#                 )\n",
    "#             y1 = tfd.Normal(\n",
    "#                 loc =  qy_t1_mu, \n",
    "#                 scale = tf.ones_like(qy_t1_mu),\n",
    "#                 )\n",
    "#             return y0,y1\n",
    "#         else:\n",
    "#             x = data\n",
    "#             hqy = self.q_y_xt_shared_hqy(x)\n",
    "#             qy_t0_mu = self.q_y_xt0_mu(hqy)\n",
    "#             qy_t1_mu = self.q_y_xt1_mu(hqy)\n",
    "\n",
    "#             y0 = tfd.Normal(\n",
    "#                 loc =  qy_t0_mu, \n",
    "#                 scale = tf.ones_like(qy_t0_mu),\n",
    "#                 )\n",
    "#             y1 = tfd.Normal(\n",
    "#                 loc =  qy_t1_mu, \n",
    "#                 scale = tf.ones_like(qy_t1_mu),\n",
    "#                 )\n",
    "#             return y0,y1\n",
    "\n",
    "\n",
    "#     def cevae_loss(self, data, pred, training = False):\n",
    "#         # if training:\n",
    "#         #     _, t_train, y_train = data[0],data[1],data[2]\n",
    "#         #     y_pred = pred\n",
    "#         #     loss = y_pred.log_prob(y_train)\n",
    "#         #     loss = -tfkb.mean(loss)\n",
    "#         #     return lossxs\n",
    "#         # else:\n",
    "#         #     _, t_train, y_train = data[0],data[1],data[2]\n",
    "#         #     y0,y1 = pred\n",
    "#         #     loss = y0.log_prob(y_train)*(1-t_train) + y1.log_prob(y_train)* t_train\n",
    "#         #     loss = -tfkb.mean(loss)\n",
    "#         #     return loss\n",
    "#         _, t_train, y_train = data[0],data[1],data[2]\n",
    "#         y0,y1 = pred\n",
    "#         loss = y0.log_prob(y_train)*(1-t_train) + y1.log_prob(y_train)* t_train\n",
    "#         loss = -tfkb.mean(loss)\n",
    "#         return loss\n",
    "\n",
    "#     def train_step(self, data):\n",
    "#         data = data[0]\n",
    "#         x,t,_ = data\n",
    "        \n",
    "#         with tf.GradientTape() as tape:\n",
    "#             pred = self([x,t], training=True)  # Forward pass\n",
    "#             loss = self.cevae_loss(data = data, pred = pred, training = True)\n",
    "#         # Compute gradients\n",
    "#         trainable_vars = self.trainable_variables\n",
    "#         gradients = tape.gradient(loss, trainable_vars)\n",
    "#         # Update weights\n",
    "#         self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "#         metrics = {\n",
    "#             \"loss\": loss,\n",
    "#         }\n",
    "#         return metrics\n",
    "\n",
    "#     def test_step(self, data):\n",
    "#         # Unpack the data. Its structure depends on your model and\n",
    "#         # on what you pass to `fit()`.\n",
    "#         data = data[0]\n",
    "#         x,t,y = data\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             pred = self(x, training=False)  # Forward pass\n",
    "#             loss = self.cevae_loss(data = data, pred = pred, training = False)\n",
    "#             y0, y1 = pred[0].sample(),pred[1].sample()\n",
    "#         metrics = {\"loss\":loss,\"y0\": tfkb.mean(y0),\"y1\": tfkb.mean(y1)}\n",
    "#         return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class metrics_for_cevae(Callback):\n",
    "    def __init__(self,data, verbose=0):   \n",
    "        super(metrics_for_cevae, self).__init__()\n",
    "        self.data=data #feed the callback the full dataset\n",
    "        self.verbose=verbose\n",
    "\n",
    "        #needed for PEHEnn; Called in self.find_ynn\n",
    "        self.data['o_idx']=tf.range(self.data['t'].shape[0])\n",
    "        self.data['c_idx']=self.data['o_idx'][self.data['t'].squeeze()==0] #These are the indices of the control units\n",
    "        self.data['t_idx']=self.data['o_idx'][self.data['t'].squeeze()==1] #These are the indices of the treated units\n",
    "        # ['x', 't', 'y', 'mu_0', 'mu_1', 'y_scaler', 'ys', 'o_idx', 'c_idx', 't_idx']\n",
    "        self.y = tf.cast(data['y'],tf.float32)\n",
    "        self.t = tf.cast(data['t'],tf.float32)\n",
    "        self.y_cf = tf.cast(data['ycf'],tf.float32)\n",
    "        self.mu0 = tf.cast(data['mu_0'],tf.float32)\n",
    "        self.mu1 = tf.cast(data['mu_1'],tf.float32)\n",
    "        if self.mu0 is not None and self.mu1 is not None:\n",
    "            self.true_ite = self.mu1 - self.mu0\n",
    "\n",
    "    def rmse_ite(self, ypred1, ypred0):\n",
    "        idx1, idx0 = self.t, 1-self.t\n",
    "        ite1, ite0 = (self.y - ypred0) * idx1, (ypred1 - self.y)*idx0\n",
    "        pred_ite = ite1 + ite0\n",
    "        return tf.math.sqrt(tfkb.mean(tf.math.square(self.true_ite - pred_ite)))\n",
    "\n",
    "    def abs_ate(self, ypred1, ypred0):\n",
    "        return tf.math.abs(tfkb.mean(ypred1 - ypred0) - tfkb.mean(self.true_ite))\n",
    "\n",
    "    def pehe(self, ypred1, ypred0):\n",
    "        return tf.math.sqrt(tfkb.mean(tf.math.square((self.mu1 - self.mu0) - (ypred1 - ypred0))))\n",
    "\n",
    "    def y_errors(self, y0, y1):\n",
    "        ypred = (1 - self.t) * y0 + self.t * y1\n",
    "        ypred_cf = self.t * y0 + (1 - self.t) * y1\n",
    "        return self.y_errors_pcf(ypred, ypred_cf)\n",
    "\n",
    "    def y_errors_pcf(self, ypred, ypred_cf):\n",
    "        rmse_factual = tf.math.sqrt(tfkb.mean(tf.math.square(ypred - self.y)))\n",
    "        rmse_cfactual = tf.math.sqrt(tfkb.mean(tf.math.square(ypred_cf - self.y_cf)))\n",
    "        return rmse_factual, rmse_cfactual\n",
    "\n",
    "    def calc_stats(self, ypred1, ypred0):\n",
    "        ite = self.rmse_ite(ypred1, ypred0)\n",
    "        ate = self.abs_ate(ypred1, ypred0)\n",
    "        pehe = self.pehe(ypred1, ypred0)\n",
    "        return ite, ate, pehe\n",
    "\n",
    "    def get_concat_pred(self,pred):\n",
    "        ypred0, ypred1 = pred\n",
    "        ypred0 = ypred0.sample()\n",
    "        ypred1 = ypred1.sample()\n",
    "        try:\n",
    "            y_pred0,y_pred1 = self.data['y_scaler'].inverse_transform(ypred0),self.data['y_scaler'].inverse_transform(ypred1)\n",
    "        except:\n",
    "            y_pred0 = self.data['y_scaler'].inverse_transform(tf.expand_dims(ypred0,-1))\n",
    "            y_pred1 = self.data['y_scaler'].inverse_transform(tf.expand_dims(ypred1,-1))\n",
    "        y_pred0, y_pred1 = tf.squeeze(y_pred0),tf.squeeze(y_pred1)\n",
    "        return tf.cast(y_pred0,tf.float32), tf.cast(y_pred1,tf.float32)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        pred = self.model(self.data['x'])\n",
    "        y_infer = pred[0]\n",
    "        ypred0, ypred1 = self.get_concat_pred(y_infer)\n",
    "        ite, ate, pehe = self.calc_stats(ypred1, ypred0)\n",
    "        tf.summary.scalar(\"ate\", data=tfkb.mean(ypred1 - ypred0), step=epoch)\n",
    "        tf.summary.scalar(\"ite_error\", data=ite, step=epoch)\n",
    "        tf.summary.scalar(\"ate_error\", data=ate, step=epoch)\n",
    "        tf.summary.scalar(\"pehe_error\",data=pehe, step=epoch)\n",
    "        \n",
    "        out_str=f' — ite: {ite:.4f}  — ate: {ate:.4f} — pehe: {pehe:.4f} '\n",
    "        \n",
    "        if self.verbose > 0: print(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import *\n",
    "class CEVAE(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CEVAE, self).__init__()\n",
    "        ########################################\n",
    "        # networks\n",
    "        self.activation = 'elu'\n",
    "        # CEVAE Model \n",
    "        ## (encoder)\n",
    "        self.q_y_tx = q_y_tx(x_bin_dim, x_num_dim, y_dim, t_dim, nh, h)\n",
    "        self.q_t_x = q_t_x(x_bin_dim, x_num_dim, t_dim, nh, h)\n",
    "        self.q_z_txy = q_z_txy(x_bin_dim, x_num_dim, y_dim, t_dim, z_dim, nh,h)\n",
    "        ## (decoder)\n",
    "        self.p_x_z = p_x_z(x_bin_dim, x_num_dim, z_dim, nh, h)\n",
    "        self.p_t_z = p_t_z(t_dim, z_dim, nh, h)\n",
    "        self.p_y_tz = p_y_tz(y_dim, t_dim, z_dim, nh, h)\n",
    "        \n",
    "\n",
    "    def call(self, data, training=False):\n",
    "        if training:\n",
    "            x_train,t_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "            \n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            \n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.sample()\n",
    "            # decoder\n",
    "            ## p(x|z)\n",
    "            x_con,x_bin = self.p_x_z(z_infer_sample)\n",
    "            ## p(t|z)\n",
    "            t = self.p_t_z(z_infer_sample)\n",
    "            ## p(y|t,z)\n",
    "            y = self.p_y_tz(tf.concat([t_train,z_infer_sample],-1) )\n",
    "            \n",
    "            return y_infer,t_infer,z_infer,y,t,x_con,x_bin\n",
    "        else:\n",
    "            x_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "            \n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            \n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.sample()\n",
    "            return y_infer,t_infer,z_infer\n",
    "\n",
    "\n",
    "    def cevae_loss(self, data, pred, training = False):\n",
    "        x_train, t_train, y_train = data[0],data[1],data[2]\n",
    "        x_train_num, x_train_bin = x_train[:,:x_num_dim],x_train[:,x_num_dim:]\n",
    "        y_infer,t_infer,z_infer,y,t,x_con,x_bin = pred\n",
    "        y0,y1 = y_infer\n",
    "\n",
    "        # reconstruct loss\n",
    "        recon_x_num = x_bin.log_prob(x_train_num)\n",
    "        tf.print(tf.shape(recon_x_num))\n",
    "        recon_x_num = tfkb.sum(recon_x_num,1)\n",
    "        tf.print(tf.shape(recon_x_num))\n",
    "        recon_y = y.log_prob(y_train)\n",
    "        recon_t = t.log_prob(t_train)\n",
    "\n",
    "        # kl loss\n",
    "        z_infer_sample = z_infer.sample()\n",
    "        z = tfd.Normal(loc = [0] * 20, scale = [1]*20)\n",
    "        kl_z = tfkb.sum((z.log_prob(z_infer_sample) - z_infer.log_prob(z_infer_sample)), -1)\n",
    "        \n",
    "        # aux loss\n",
    "        aux_y = y0.log_prob(y_train)*(1-t_train) + y1.log_prob(y_train)* t_train\n",
    "        aux_t = t_infer.log_prob(t_train)\n",
    "    \n",
    "\n",
    "        loss = -tfkb.mean( recon_y + recon_t + aux_y + aux_t + kl_z)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, data):\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "            loss = self.cevae_loss(data = data, pred = pred, training = True)\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        metrics = {\n",
    "            \"loss\": loss,\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "        y_infer = pred[0]\n",
    "        loss = self.cevae_loss(data = data, pred = pred, training = False)\n",
    "        y0, y1 = y_infer[0].sample(),y_infer[1].sample()\n",
    "        metrics = {\"loss\":loss,\"y0\": tfkb.mean(y0),\"y1\": tfkb.mean(y1)}\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/140\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_18/kernel:0', 'dense_18/bias:0', 'dense_19/kernel:0', 'dense_19/bias:0', 'dense_20/kernel:0', 'dense_20/bias:0', 'dense_21/kernel:0', 'dense_21/bias:0', 'dense_22/kernel:0', 'dense_22/bias:0', 'cevae/p_x_z/dense_23/kernel:0', 'cevae/p_x_z/dense_23/bias:0', 'cevae/p_x_z/dense_24/kernel:0', 'cevae/p_x_z/dense_24/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_18/kernel:0', 'dense_18/bias:0', 'dense_19/kernel:0', 'dense_19/bias:0', 'dense_20/kernel:0', 'dense_20/bias:0', 'dense_21/kernel:0', 'dense_21/bias:0', 'dense_22/kernel:0', 'dense_22/bias:0', 'cevae/p_x_z/dense_23/kernel:0', 'cevae/p_x_z/dense_23/bias:0', 'cevae/p_x_z/dense_24/kernel:0', 'cevae/p_x_z/dense_24/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "[200 6]\n",
      "[200]\n",
      "1/3 [=========>....................] - ETA: 9s - loss: 7.9415[200 6]\n",
      "[200]\n",
      "[197 6]\n",
      "[197]\n",
      "[150 6]\n",
      "[150]\n",
      " — ite: 4.2017  — ate: 2.3762 — pehe: 4.7951 \n",
      "3/3 [==============================] - 6s 528ms/step - loss: 7.8841 - val_loss: 7.7096 - val_y0: -0.2225 - val_y1: 0.2732 - lr: 5.0000e-05\n",
      "Epoch 2/140\n",
      "[200 6]\n",
      "[200]\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.6370[200 6]\n",
      "[200]\n",
      "[197 6]\n",
      "[197]\n",
      "[150 6]\n",
      "[150]\n",
      " — ite: 4.1992  — ate: 2.0923 — pehe: 4.6289 \n"
     ]
    }
   ],
   "source": [
    "#Colab command to allow us to run Colab in TF2\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "%reload_ext tensorboard \n",
    "\n",
    "model = CEVAE()\n",
    "### MAIN CODE ####\n",
    "val_split=0.2\n",
    "batch_size=64\n",
    "verbose=True\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    " \n",
    "callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='val_loss', patience=40, min_delta=0), \n",
    "        #40 is Shi's recommendation patience for this dataset, but you should tune for your data \n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0, cooldown=0, min_lr=0),\n",
    "        #This learning rate scheduling is quite agressive which seems good for this dataset\n",
    "        metrics_for_cevae(data,verbose),\n",
    "        tensorboard_callback\n",
    "    ]\n",
    "    \n",
    "#optimizer hyperparameters\n",
    "learning_rate = 5e-5\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate = learning_rate, \n",
    "        # momentum = momentum, \n",
    "        # nesterov=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "model.fit(\n",
    "    [data['x'],data['t'],data['ys']],\n",
    "    callbacks=callbacks,\n",
    "    validation_split=val_split,\n",
    "    epochs=140,\n",
    "    batch_size=200,\n",
    "    verbose=verbose\n",
    "    )\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
