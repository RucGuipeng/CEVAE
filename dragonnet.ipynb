{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as tfkb\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.metrics import binary_accuracy\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras.layers import Layer\n",
    "class EpsilonLayer(Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(EpsilonLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.epsilon = self.add_weight(name='epsilon',\n",
    "                                       shape=[1, 1],\n",
    "                                       initializer='RandomNormal',\n",
    "                                       #  initializer='ones',\n",
    "                                       trainable=True)\n",
    "        super(EpsilonLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        #note there is only one epsilon were just duplicating it for conformability\n",
    "        return self.epsilon * tf.ones_like(inputs)[:, 0:1]\n",
    "\n",
    "def make_dragonnet(input_dim, reg_l2):\n",
    "\n",
    "    x = Input(shape=(input_dim,), name='input')\n",
    "    # representation\n",
    "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_1')(x)\n",
    "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_2')(phi)\n",
    "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_3')(phi)\n",
    "\n",
    "    # HYPOTHESIS\n",
    "    y0_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_1')(phi)\n",
    "    y1_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_1')(phi)\n",
    "\n",
    "    # second layer\n",
    "    y0_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_2')(y0_hidden)\n",
    "    y1_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_2')(y1_hidden)\n",
    "\n",
    "    # third\n",
    "    y0_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y0_predictions')(y0_hidden)\n",
    "    y1_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y1_predictions')(y1_hidden)\n",
    "\n",
    "    #propensity prediction\n",
    "    #Note that the activation is actually sigmoid, but we will squish it in the loss function for numerical stability reasons\n",
    "    t_predictions = Dense(units=1,activation=None,name='t_prediction')(phi)\n",
    "    #Although the epsilon layer takes an input, it really just houses a free parameter. \n",
    "    epsilons = EpsilonLayer()(t_predictions)\n",
    "    concat_pred = Concatenate(1)([y0_predictions, y1_predictions,t_predictions,epsilons,phi])\n",
    "    model = Model(inputs=x, outputs=concat_pred)\n",
    "    return model\n",
    "\n",
    "class Base_Loss(Loss):\n",
    "    #initialize instance attributes\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.name='standard_loss'\n",
    "\n",
    "    def split_pred(self,concat_pred):\n",
    "        #generic helper to make sure we dont make mistakes\n",
    "        preds={}\n",
    "        preds['y0_pred'] = concat_pred[:, 0]\n",
    "        preds['y1_pred'] = concat_pred[:, 1]\n",
    "        preds['t_pred'] = concat_pred[:, 2]\n",
    "        preds['phi'] = concat_pred[:, 3:]\n",
    "        return preds\n",
    "\n",
    "    #for logging purposes only\n",
    "    def treatment_acc(self,concat_true,concat_pred):\n",
    "        t_true = concat_true[:, 1]\n",
    "        p = self.split_pred(concat_pred)\n",
    "        #Since this isn't used as a loss, I've used tf.reduce_mean for interpretability\n",
    "        return tf.reduce_mean(binary_accuracy(t_true, tf.math.sigmoid(p['t_pred']), threshold=0.5))\n",
    "\n",
    "    def treatment_bce(self,concat_true,concat_pred):\n",
    "        t_true = concat_true[:, 1]\n",
    "        p = self.split_pred(concat_pred)\n",
    "        lossP = tf.reduce_sum(binary_crossentropy(t_true,p['t_pred'],from_logits=True))\n",
    "        return lossP\n",
    "    \n",
    "    def regression_loss(self,concat_true,concat_pred):\n",
    "        y_true = concat_true[:, 0]\n",
    "        t_true = concat_true[:, 1]\n",
    "        p = self.split_pred(concat_pred)\n",
    "        loss0 = tf.reduce_sum((1. - t_true) * tf.square(y_true - p['y0_pred']))\n",
    "        loss1 = tf.reduce_sum(t_true * tf.square(y_true - p['y1_pred']))\n",
    "        return loss0+loss1\n",
    "\n",
    "    def standard_loss(self,concat_true,concat_pred):\n",
    "        lossR = self.regression_loss(concat_true,concat_pred)\n",
    "        lossP = self.treatment_bce(concat_true,concat_pred)\n",
    "        return lossR + self.alpha * lossP\n",
    "\n",
    "    #compute loss\n",
    "    def call(self, concat_true, concat_pred):        \n",
    "        return self.standard_loss(concat_true,concat_pred)\n",
    "        \n",
    "class TarReg_Loss(Base_Loss):\n",
    "    #initialize instance attributes\n",
    "    def __init__(self, alpha=1,beta=1):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta=beta\n",
    "        self.name='tarreg_loss'\n",
    "\n",
    "    def split_pred(self,concat_pred):\n",
    "        #generic helper to make sure we dont make mistakes\n",
    "        preds={}\n",
    "        preds['y0_pred'] = concat_pred[:, 0]\n",
    "        preds['y1_pred'] = concat_pred[:, 1]\n",
    "        preds['t_pred'] = concat_pred[:, 2]\n",
    "        preds['epsilon'] = concat_pred[:, 3] #we're moving epsilon into slot three\n",
    "        preds['phi'] = concat_pred[:, 4:]\n",
    "        return preds\n",
    "\n",
    "    def calc_hstar(self,concat_true,concat_pred):\n",
    "        #step 2 above\n",
    "        p=self.split_pred(concat_pred)\n",
    "        y_true = concat_true[:, 0]\n",
    "        t_true = concat_true[:, 1]\n",
    "\n",
    "        t_pred = tf.math.sigmoid(concat_pred[:, 2])\n",
    "        t_pred = (t_pred + 0.001) / 1.002 # a little numerical stability trick implemented by Shi\n",
    "        y_pred = t_true * p['y1_pred'] + (1 - t_true) * p['y0_pred']\n",
    "\n",
    "        #calling it cc for \"clever covariate\" as in SuperLearner TMLE literature\n",
    "        cc = t_true / t_pred - (1 - t_true) / (1 - t_pred)\n",
    "        h_star = y_pred + p['epsilon'] * cc\n",
    "        return h_star\n",
    "\n",
    "    def call(self,concat_true,concat_pred):\n",
    "        y_true = concat_true[:, 0]\n",
    "\n",
    "        standard_loss=self.standard_loss(concat_true,concat_pred)\n",
    "        h_star=self.calc_hstar(concat_true,concat_pred)\n",
    "        #step 3 above\n",
    "        targeted_regularization = tf.reduce_sum(tf.square(y_true - h_star))\n",
    "\n",
    "        # final\n",
    "        loss = standard_loss + self.beta * targeted_regularization\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import TarReg_Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 “ihdp_npci_1-100.train.npz” 已经存在；不获取。\n",
      "\n",
      "文件 “ihdp_npci_1-100.test.npz” 已经存在；不获取。\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1344, 25)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title First load the data! (Click Play)\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz \n",
    "\n",
    "def load_IHDP_data(training_data,testing_data,i=7):\n",
    "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "        train_data=np.load(trf); test_data=np.load(tef)\n",
    "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
    "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
    "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
    "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    "\n",
    "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "        data['y']=data['y'].reshape(-1,1)\n",
    "        \n",
    "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "\n",
    "    return data\n",
    "\n",
    "data_train=load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.train.npz')\n",
    "data_valid=load_IHDP_data(training_data='./ihdp_npci_1-100.test.npz',testing_data='./ihdp_npci_1-100.test.npz')\n",
    "np.shape(data_train['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      " 6/21 [=======>......................] - ETA: 0s - loss: 140.8566 - tarreg_loss: 136.1558 - regression_loss: 62.6049 - treatment_acc: 0.4948 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0099s vs `on_train_batch_end` time: 0.0101s). Check your callbacks.\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 115.9769 - tarreg_loss: 111.2761 - regression_loss: 52.9216 - treatment_acc: 0.4852 — ate_err: 1.1887  — aipw_err: 0.6943 — tarreg_err: 1.1721 — cate_err: 2.0503 — cate_nn_err: 2.3487 \n",
      " — ate_err: 0.5727  — aipw_err: 0.6116 — tarreg_err: 0.5560 — cate_err: 2.3910 — cate_nn_err: 2.6645 \n",
      "21/21 [==============================] - 3s 70ms/step - loss: 110.3566 - tarreg_loss: 105.6558 - regression_loss: 50.4484 - treatment_acc: 0.4740 - val_loss: 65.4963 - val_tarreg_loss: 53.6645 - val_regression_loss: 26.4656 - val_treatment_acc: 0.3906 - lr: 1.0000e-05\n",
      "Epoch 2/300\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 59.4445 - tarreg_loss: 54.7423 - regression_loss: 27.0530 - treatment_acc: 0.4383 — ate_err: 0.2903  — aipw_err: 0.8539 — tarreg_err: 0.1564 — cate_err: 2.1003 — cate_nn_err: 1.9508 \n",
      " — ate_err: 0.3425  — aipw_err: 2.6843 — tarreg_err: 0.4764 — cate_err: 2.9520 — cate_nn_err: 2.3503 \n",
      "21/21 [==============================] - 1s 37ms/step - loss: 58.3348 - tarreg_loss: 53.6325 - regression_loss: 26.4748 - treatment_acc: 0.4375 - val_loss: 46.6446 - val_tarreg_loss: 36.9694 - val_regression_loss: 18.1157 - val_treatment_acc: 0.3598 - lr: 1.0000e-05\n",
      "Epoch 3/300\n",
      "21/21 [==============================] - ETA: 0s - loss: 47.0018 - tarreg_loss: 42.2982 - regression_loss: 20.7798 - treatment_acc: 0.4606 — ate_err: 0.4382  — aipw_err: 0.2335 — tarreg_err: 0.3676 — cate_err: 2.2706 — cate_nn_err: 1.8920 \n",
      " — ate_err: 0.1305  — aipw_err: 2.8372 — tarreg_err: 0.2011 — cate_err: 3.1398 — cate_nn_err: 2.2443 \n",
      "21/21 [==============================] - 1s 37ms/step - loss: 47.0018 - tarreg_loss: 42.2982 - regression_loss: 20.7798 - treatment_acc: 0.4606 - val_loss: 42.2194 - val_tarreg_loss: 33.1218 - val_regression_loss: 16.3079 - val_treatment_acc: 0.4313 - lr: 1.0000e-05\n",
      "Epoch 4/300\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 42.6573 - tarreg_loss: 37.9537 - regression_loss: 18.6337 - treatment_acc: 0.4967 — ate_err: 0.3154  — aipw_err: 0.0474 — tarreg_err: 0.2757 — cate_err: 2.2184 — cate_nn_err: 1.7957 \n",
      " — ate_err: 0.2954  — aipw_err: 0.1946 — tarreg_err: 0.3351 — cate_err: 3.1047 — cate_nn_err: 2.1290 \n",
      "21/21 [==============================] - 1s 33ms/step - loss: 42.0871 - tarreg_loss: 37.3836 - regression_loss: 18.3567 - treatment_acc: 0.4881 - val_loss: 37.8569 - val_tarreg_loss: 29.5136 - val_regression_loss: 14.4096 - val_treatment_acc: 0.4929 - lr: 1.0000e-05\n",
      "Epoch 5/300\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 38.7200 - tarreg_loss: 34.0164 - regression_loss: 16.6596 - treatment_acc: 0.5255 — ate_err: 0.1832  — aipw_err: 0.4219 — tarreg_err: 0.1538 — cate_err: 2.2031 — cate_nn_err: 1.7552 \n",
      " — ate_err: 0.4474  — aipw_err: 5.3614 — tarreg_err: 0.4769 — cate_err: 3.1112 — cate_nn_err: 2.1453 \n",
      "21/21 [==============================] - 1s 32ms/step - loss: 38.8656 - tarreg_loss: 34.1619 - regression_loss: 16.7315 - treatment_acc: 0.5186 - val_loss: 36.1271 - val_tarreg_loss: 28.0476 - val_regression_loss: 13.6481 - val_treatment_acc: 0.5033 - lr: 1.0000e-05\n",
      "Epoch 6/300\n",
      "21/21 [==============================] - ETA: 0s - loss: 36.6843 - tarreg_loss: 31.9804 - regression_loss: 15.6390 - treatment_acc: 0.5476 — ate_err: 0.2819  — aipw_err: 0.1705 — tarreg_err: 0.2641 — cate_err: 2.2256 — cate_nn_err: 1.7595 \n",
      " — ate_err: 0.3336  — aipw_err: 1.1989 — tarreg_err: 0.3513 — cate_err: 3.1075 — cate_nn_err: 2.1363 \n",
      "21/21 [==============================] - 1s 33ms/step - loss: 36.6843 - tarreg_loss: 31.9804 - regression_loss: 15.6390 - treatment_acc: 0.5476 - val_loss: 35.8979 - val_tarreg_loss: 27.7109 - val_regression_loss: 13.5160 - val_treatment_acc: 0.5241 - lr: 1.0000e-05\n",
      "Epoch 7/300\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 35.8819 - tarreg_loss: 31.1777 - regression_loss: 15.2391 - treatment_acc: 0.5799 — ate_err: 0.3964  — aipw_err: 0.2624 — tarreg_err: 0.3861 — cate_err: 2.2571 — cate_nn_err: 1.7298 \n",
      " — ate_err: 0.2025  — aipw_err: 0.9579 — tarreg_err: 0.2128 — cate_err: 3.0959 — cate_nn_err: 2.1521 \n",
      "21/21 [==============================] - 1s 32ms/step - loss: 35.1813 - tarreg_loss: 30.4771 - regression_loss: 14.8907 - treatment_acc: 0.5744 - val_loss: 36.9717 - val_tarreg_loss: 28.5541 - val_regression_loss: 13.9499 - val_treatment_acc: 0.5341 - lr: 1.0000e-05\n",
      "Epoch 8/300\n",
      "14/21 [===================>..........] - ETA: 0s - loss: 34.5748 - tarreg_loss: 29.8707 - regression_loss: 14.5942 - treatment_acc: 0.5926 — ate_err: 0.2769  — aipw_err: 0.1722 — tarreg_err: 0.2679 — cate_err: 2.2018 — cate_nn_err: 1.6968 \n",
      " — ate_err: 0.3416  — aipw_err: 8.2712 — tarreg_err: 0.3506 — cate_err: 3.0490 — cate_nn_err: 2.1112 \n",
      "21/21 [==============================] - 1s 31ms/step - loss: 34.3012 - tarreg_loss: 29.5971 - regression_loss: 14.4547 - treatment_acc: 0.5982 - val_loss: 35.5304 - val_tarreg_loss: 27.3015 - val_regression_loss: 13.3133 - val_treatment_acc: 0.5857 - lr: 1.0000e-05\n",
      "Epoch 9/300\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 33.2645 - tarreg_loss: 28.5604 - regression_loss: 13.9394 - treatment_acc: 0.6250 — ate_err: 0.1792  — aipw_err: 0.3683 — tarreg_err: 0.1688 — cate_err: 2.2127 — cate_nn_err: 1.6921 \n",
      " — ate_err: 0.4464  — aipw_err: 2.5224 — tarreg_err: 0.4568 — cate_err: 3.0858 — cate_nn_err: 2.1386 \n",
      "21/21 [==============================] - 1s 34ms/step - loss: 33.5089 - tarreg_loss: 28.8048 - regression_loss: 14.0604 - treatment_acc: 0.6250 - val_loss: 34.9454 - val_tarreg_loss: 26.8117 - val_regression_loss: 13.0640 - val_treatment_acc: 0.6373 - lr: 1.0000e-05\n",
      "Epoch 10/300\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 33.2375 - tarreg_loss: 28.5335 - regression_loss: 13.9187 - treatment_acc: 0.6406 — ate_err: 0.2737  — aipw_err: 0.2016 — tarreg_err: 0.2664 — cate_err: 2.2261 — cate_nn_err: 1.6895 \n",
      " — ate_err: 0.3323  — aipw_err: 0.4713 — tarreg_err: 0.3397 — cate_err: 3.0635 — cate_nn_err: 2.1248 \n",
      "21/21 [==============================] - 1s 33ms/step - loss: 33.1076 - tarreg_loss: 28.4036 - regression_loss: 13.8551 - treatment_acc: 0.6414 - val_loss: 35.4847 - val_tarreg_loss: 27.2186 - val_regression_loss: 13.2761 - val_treatment_acc: 0.6477 - lr: 1.0000e-05\n",
      "Epoch 11/300\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 32.6836 - tarreg_loss: 27.9797 - regression_loss: 13.6498 - treatment_acc: 0.6539 — ate_err: 0.2112  — aipw_err: 1.9556 — tarreg_err: 0.2053 — cate_err: 2.2090 — cate_nn_err: 1.6824 \n",
      " — ate_err: 0.4014  — aipw_err: 0.5748 — tarreg_err: 0.4073 — cate_err: 3.0549 — cate_nn_err: 2.1361 \n",
      "21/21 [==============================] - 1s 33ms/step - loss: 32.5755 - tarreg_loss: 27.8716 - regression_loss: 13.5965 - treatment_acc: 0.6570 - val_loss: 34.5924 - val_tarreg_loss: 26.4660 - val_regression_loss: 12.8959 - val_treatment_acc: 0.6686 - lr: 1.0000e-05\n",
      "Epoch 12/300\n",
      "21/21 [==============================] - ETA: 0s - loss: 32.2935 - tarreg_loss: 27.5900 - regression_loss: 13.4510 - treatment_acc: 0.6726 — ate_err: 0.1713  — aipw_err: 0.4903 — tarreg_err: 0.1653 — cate_err: 2.1869 — cate_nn_err: 1.6660 \n",
      " — ate_err: 0.4453  — aipw_err: 21.5904 — tarreg_err: 0.4513 — cate_err: 3.0343 — cate_nn_err: 2.0661 \n",
      "21/21 [==============================] - 1s 39ms/step - loss: 32.2935 - tarreg_loss: 27.5900 - regression_loss: 13.4510 - treatment_acc: 0.6726 - val_loss: 34.3002 - val_tarreg_loss: 26.1845 - val_regression_loss: 12.7544 - val_treatment_acc: 0.7098 - lr: 1.0000e-05\n",
      "Epoch 13/300\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 32.4540 - tarreg_loss: 27.7505 - regression_loss: 13.5350 - treatment_acc: 0.6840 — ate_err: 0.1837  — aipw_err: 0.1998 — tarreg_err: 0.1748 — cate_err: 2.1998 — cate_nn_err: 1.6580 \n",
      " — ate_err: 0.4313  — aipw_err: 13.9329 — tarreg_err: 0.4402 — cate_err: 3.0394 — cate_nn_err: 2.1171 \n",
      "21/21 [==============================] - 1s 32ms/step - loss: 31.8890 - tarreg_loss: 27.1855 - regression_loss: 13.2526 - treatment_acc: 0.6905 - val_loss: 34.3427 - val_tarreg_loss: 26.2422 - val_regression_loss: 12.7877 - val_treatment_acc: 0.7098 - lr: 1.0000e-05\n",
      "Epoch 14/300\n",
      "21/21 [==============================] - ETA: 0s - loss: 31.3467 - tarreg_loss: 26.6435 - regression_loss: 12.9818 - treatment_acc: 0.6972 — ate_err: 0.3818  — aipw_err: 0.2088 — tarreg_err: 0.3827 — cate_err: 2.2262 — cate_nn_err: 1.6615 \n",
      " — ate_err: 0.1890  — aipw_err: 2.0653 — tarreg_err: 0.1881 — cate_err: 3.0179 — cate_nn_err: 2.0373 \n",
      "21/21 [==============================] - 1s 31ms/step - loss: 31.3467 - tarreg_loss: 26.6435 - regression_loss: 12.9818 - treatment_acc: 0.6972 - val_loss: 35.8512 - val_tarreg_loss: 27.5060 - val_regression_loss: 13.4151 - val_treatment_acc: 0.7206 - lr: 1.0000e-05\n",
      "Epoch 15/300\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 30.9601 - tarreg_loss: 26.2570 - regression_loss: 12.7904 - treatment_acc: 0.7073 — ate_err: 0.4187  — aipw_err: 0.2743 — tarreg_err: 0.4231 — cate_err: 2.2353 — cate_nn_err: 1.6525 \n",
      " — ate_err: 0.1494  — aipw_err: 1.4039 — tarreg_err: 0.1450 — cate_err: 3.0130 — cate_nn_err: 2.0149 \n",
      "21/21 [==============================] - 1s 31ms/step - loss: 31.1515 - tarreg_loss: 26.4484 - regression_loss: 12.8868 - treatment_acc: 0.7083 - val_loss: 36.1018 - val_tarreg_loss: 27.7117 - val_regression_loss: 13.5103 - val_treatment_acc: 0.7415 - lr: 1.0000e-05\n",
      "Epoch 16/300\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 31.1933 - tarreg_loss: 26.4906 - regression_loss: 12.9073 - treatment_acc: 0.7250 — ate_err: 0.1930  — aipw_err: 0.2080 — tarreg_err: 0.1883 — cate_err: 2.1837 — cate_nn_err: 1.6145 \n",
      " — ate_err: 0.4058  — aipw_err: 233.4557 — tarreg_err: 0.4106 — cate_err: 3.0149 — cate_nn_err: 2.0376 \n",
      "21/21 [==============================] - 1s 30ms/step - loss: 31.0510 - tarreg_loss: 26.3483 - regression_loss: 12.8361 - treatment_acc: 0.7210 - val_loss: 33.4340 - val_tarreg_loss: 25.4074 - val_regression_loss: 12.3723 - val_treatment_acc: 0.7931 - lr: 1.0000e-05\n",
      "Epoch 17/300\n",
      "14/21 [===================>..........] - ETA: 0s - loss: 30.6611 - tarreg_loss: 25.9584 - regression_loss: 12.6422 - treatment_acc: 0.7277 — ate_err: 0.1704  — aipw_err: 0.1609 — tarreg_err: 0.1641 — cate_err: 2.1855 — cate_nn_err: 1.6095 \n",
      " — ate_err: 0.4305  — aipw_err: 3.3981 — tarreg_err: 0.4369 — cate_err: 3.0138 — cate_nn_err: 1.9834 \n",
      "21/21 [==============================] - 1s 28ms/step - loss: 30.6041 - tarreg_loss: 25.9015 - regression_loss: 12.6154 - treatment_acc: 0.7351 - val_loss: 33.3846 - val_tarreg_loss: 25.3810 - val_regression_loss: 12.3610 - val_treatment_acc: 0.8139 - lr: 1.0000e-05\n",
      "Epoch 18/300\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 28.7095 - tarreg_loss: 24.0072 - regression_loss: 11.6681 - treatment_acc: 0.7471 — ate_err: 0.1641  — aipw_err: 0.2233 — tarreg_err: 0.1622 — cate_err: 2.1742 — cate_nn_err: 1.6023 \n",
      " — ate_err: 0.4302  — aipw_err: 1.6156 — tarreg_err: 0.4321 — cate_err: 2.9957 — cate_nn_err: 1.9733 \n",
      "21/21 [==============================] - 1s 27ms/step - loss: 30.3706 - tarreg_loss: 25.6683 - regression_loss: 12.4991 - treatment_acc: 0.7455 - val_loss: 32.9365 - val_tarreg_loss: 24.9994 - val_regression_loss: 12.1687 - val_treatment_acc: 0.8139 - lr: 1.0000e-05\n",
      "Epoch 19/300\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 30.6964 - tarreg_loss: 25.9941 - regression_loss: 12.6635 - treatment_acc: 0.7531 — ate_err: 0.2765  — aipw_err: 0.0118 — tarreg_err: 0.2796 — cate_err: 2.1819 — cate_nn_err: 1.6050 \n",
      " — ate_err: 0.2994  — aipw_err: 61.7883 — tarreg_err: 0.2963 — cate_err: 2.9768 — cate_nn_err: 1.9440 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 30.1949 - tarreg_loss: 25.4927 - regression_loss: 12.4128 - treatment_acc: 0.7515 - val_loss: 33.4258 - val_tarreg_loss: 25.3894 - val_regression_loss: 12.3596 - val_treatment_acc: 0.8447 - lr: 1.0000e-05\n",
      "Epoch 20/300\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 30.1303 - tarreg_loss: 25.4284 - regression_loss: 12.3813 - treatment_acc: 0.7586 — ate_err: 0.1649  — aipw_err: 0.2001 — tarreg_err: 0.1616 — cate_err: 2.1774 — cate_nn_err: 1.5892 \n",
      " — ate_err: 0.4279  — aipw_err: 2.6276 — tarreg_err: 0.4312 — cate_err: 2.9971 — cate_nn_err: 1.9577 \n",
      "21/21 [==============================] - 1s 31ms/step - loss: 29.9891 - tarreg_loss: 25.2872 - regression_loss: 12.3111 - treatment_acc: 0.7634 - val_loss: 32.6706 - val_tarreg_loss: 24.7801 - val_regression_loss: 12.0615 - val_treatment_acc: 0.8651 - lr: 1.0000e-05\n",
      "Epoch 21/300\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 29.8384 - tarreg_loss: 25.1367 - regression_loss: 12.2331 - treatment_acc: 0.7641 — ate_err: 0.2267  — aipw_err: 0.1268 — tarreg_err: 0.2248 — cate_err: 2.1824 — cate_nn_err: 1.6072 \n",
      " — ate_err: 0.3441  — aipw_err: 1.9816 — tarreg_err: 0.3460 — cate_err: 2.9969 — cate_nn_err: 1.9383 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 29.6629 - tarreg_loss: 24.9611 - regression_loss: 12.1454 - treatment_acc: 0.7649 - val_loss: 33.2431 - val_tarreg_loss: 25.1995 - val_regression_loss: 12.2728 - val_treatment_acc: 0.8651 - lr: 1.0000e-05\n",
      "Epoch 22/300\n",
      "21/21 [==============================] - ETA: 0s - loss: 29.4519 - tarreg_loss: 24.7504 - regression_loss: 12.0434 - treatment_acc: 0.7708 — ate_err: 0.3529  — aipw_err: 0.3407 — tarreg_err: 0.3590 — cate_err: 2.2375 — cate_nn_err: 1.6013 \n",
      " — ate_err: 0.1908  — aipw_err: 1.5506 — tarreg_err: 0.1847 — cate_err: 3.0293 — cate_nn_err: 2.0380 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 29.4519 - tarreg_loss: 24.7504 - regression_loss: 12.0434 - treatment_acc: 0.7708 - val_loss: 34.0922 - val_tarreg_loss: 25.9688 - val_regression_loss: 12.6432 - val_treatment_acc: 0.8651 - lr: 1.0000e-05\n",
      "Epoch 23/300\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 29.2116 - tarreg_loss: 24.5102 - regression_loss: 11.9234 - treatment_acc: 0.7771 — ate_err: 0.0692  — aipw_err: 0.3060 — tarreg_err: 0.0646 — cate_err: 2.1570 — cate_nn_err: 1.5986 \n",
      " — ate_err: 0.5144  — aipw_err: 1.1749 — tarreg_err: 0.5190 — cate_err: 2.9990 — cate_nn_err: 1.9199 \n",
      "21/21 [==============================] - 1s 31ms/step - loss: 29.1803 - tarreg_loss: 24.4789 - regression_loss: 11.9081 - treatment_acc: 0.7783 - val_loss: 31.4935 - val_tarreg_loss: 23.6903 - val_regression_loss: 11.5165 - val_treatment_acc: 0.8651 - lr: 1.0000e-05\n",
      "Epoch 24/300\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 28.9547 - tarreg_loss: 24.2535 - regression_loss: 11.7967 - treatment_acc: 0.7943 — ate_err: 0.2771  — aipw_err: 0.2537 — tarreg_err: 0.2794 — cate_err: 2.2047 — cate_nn_err: 1.5932 \n",
      " — ate_err: 0.2669  — aipw_err: 0.2346 — tarreg_err: 0.2646 — cate_err: 3.0064 — cate_nn_err: 1.9611 \n",
      "21/21 [==============================] - 1s 28ms/step - loss: 29.1035 - tarreg_loss: 24.4023 - regression_loss: 11.8705 - treatment_acc: 0.7879 - val_loss: 33.0255 - val_tarreg_loss: 25.0247 - val_regression_loss: 12.1821 - val_treatment_acc: 0.8651 - lr: 1.0000e-05\n",
      "Epoch 25/300\n",
      "14/21 [===================>..........] - ETA: 0s - loss: 29.3909 - tarreg_loss: 24.6898 - regression_loss: 12.0149 - treatment_acc: 0.8002 — ate_err: 0.1994  — aipw_err: 0.1641 — tarreg_err: 0.1971 — cate_err: 2.1867 — cate_nn_err: 1.5743 \n",
      " — ate_err: 0.3582  — aipw_err: 2.2198 — tarreg_err: 0.3604 — cate_err: 3.0049 — cate_nn_err: 2.0008 \n",
      "21/21 [==============================] - 1s 28ms/step - loss: 28.8487 - tarreg_loss: 24.1476 - regression_loss: 11.7442 - treatment_acc: 0.7924 - val_loss: 32.4082 - val_tarreg_loss: 24.4738 - val_regression_loss: 11.9135 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 26/300\n",
      "21/21 [==============================] - ETA: 0s - loss: 28.6069 - tarreg_loss: 23.9060 - regression_loss: 11.6238 - treatment_acc: 0.7961 — ate_err: 0.1237  — aipw_err: 0.2395 — tarreg_err: 0.1202 — cate_err: 2.1693 — cate_nn_err: 1.5837 \n",
      " — ate_err: 0.4437  — aipw_err: 1.6917 — tarreg_err: 0.4473 — cate_err: 2.9961 — cate_nn_err: 1.8947 \n",
      "21/21 [==============================] - 1s 31ms/step - loss: 28.6069 - tarreg_loss: 23.9060 - regression_loss: 11.6238 - treatment_acc: 0.7961 - val_loss: 31.3863 - val_tarreg_loss: 23.6050 - val_regression_loss: 11.4786 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 27/300\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 28.7608 - tarreg_loss: 24.0601 - regression_loss: 11.7002 - treatment_acc: 0.7961 — ate_err: 0.1158  — aipw_err: 0.1236 — tarreg_err: 0.1107 — cate_err: 2.1663 — cate_nn_err: 1.5633 \n",
      " — ate_err: 0.4569  — aipw_err: 1.6466 — tarreg_err: 0.4620 — cate_err: 2.9908 — cate_nn_err: 1.9473 \n",
      "21/21 [==============================] - 1s 31ms/step - loss: 28.6571 - tarreg_loss: 23.9564 - regression_loss: 11.6486 - treatment_acc: 0.7991 - val_loss: 31.5209 - val_tarreg_loss: 23.7123 - val_regression_loss: 11.5340 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 28/300\n",
      "21/21 [==============================] - ETA: 0s - loss: 28.3923 - tarreg_loss: 23.6918 - regression_loss: 11.5188 - treatment_acc: 0.7984 — ate_err: 0.2476  — aipw_err: 0.2430 — tarreg_err: 0.2515 — cate_err: 2.1826 — cate_nn_err: 1.5758 \n",
      " — ate_err: 0.2891  — aipw_err: 1.4043 — tarreg_err: 0.2853 — cate_err: 2.9889 — cate_nn_err: 1.9416 \n",
      "21/21 [==============================] - 1s 30ms/step - loss: 28.3923 - tarreg_loss: 23.6918 - regression_loss: 11.5188 - treatment_acc: 0.7984 - val_loss: 31.9198 - val_tarreg_loss: 24.0694 - val_regression_loss: 11.7062 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 29/300\n",
      "21/21 [==============================] - ETA: 0s - loss: 28.3272 - tarreg_loss: 23.6268 - regression_loss: 11.4853 - treatment_acc: 0.7976 — ate_err: 0.2244  — aipw_err: 0.1655 — tarreg_err: 0.2234 — cate_err: 2.2017 — cate_nn_err: 1.5747 \n",
      " — ate_err: 0.3113  — aipw_err: 1.4365 — tarreg_err: 0.3123 — cate_err: 3.0183 — cate_nn_err: 1.9646 \n",
      "21/21 [==============================] - 1s 32ms/step - loss: 28.3272 - tarreg_loss: 23.6268 - regression_loss: 11.4853 - treatment_acc: 0.7976 - val_loss: 32.3627 - val_tarreg_loss: 24.4062 - val_regression_loss: 11.8819 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 30/300\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 28.0629 - tarreg_loss: 23.3626 - regression_loss: 11.3529 - treatment_acc: 0.7934 — ate_err: 0.2783  — aipw_err: 0.1744 — tarreg_err: 0.2809 — cate_err: 2.2052 — cate_nn_err: 1.5581 \n",
      " — ate_err: 0.2448  — aipw_err: 1.3631 — tarreg_err: 0.2421 — cate_err: 3.0073 — cate_nn_err: 1.9599 \n",
      "21/21 [==============================] - 1s 28ms/step - loss: 28.0904 - tarreg_loss: 23.3901 - regression_loss: 11.3680 - treatment_acc: 0.7991 - val_loss: 32.8021 - val_tarreg_loss: 24.8105 - val_regression_loss: 12.0785 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 31/300\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 28.1521 - tarreg_loss: 23.4522 - regression_loss: 11.4020 - treatment_acc: 0.8066 — ate_err: 0.1963  — aipw_err: 0.2331 — tarreg_err: 0.1972 — cate_err: 2.1769 — cate_nn_err: 1.5516 \n",
      " — ate_err: 0.3407  — aipw_err: 1.2673 — tarreg_err: 0.3398 — cate_err: 2.9919 — cate_nn_err: 1.9347 \n",
      "21/21 [==============================] - 1s 28ms/step - loss: 28.1953 - tarreg_loss: 23.4953 - regression_loss: 11.4193 - treatment_acc: 0.8028 - val_loss: 31.2381 - val_tarreg_loss: 23.4642 - val_regression_loss: 11.4103 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 32/300\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 28.2499 - tarreg_loss: 23.5501 - regression_loss: 11.4505 - treatment_acc: 0.8057 — ate_err: 0.1321  — aipw_err: 0.1611 — tarreg_err: 0.1282 — cate_err: 2.1696 — cate_nn_err: 1.5565 \n",
      " — ate_err: 0.4100  — aipw_err: 1.2864 — tarreg_err: 0.4139 — cate_err: 3.0003 — cate_nn_err: 1.9253 \n",
      "21/21 [==============================] - 1s 28ms/step - loss: 27.8596 - tarreg_loss: 23.1597 - regression_loss: 11.2560 - treatment_acc: 0.8073 - val_loss: 30.8903 - val_tarreg_loss: 23.1061 - val_regression_loss: 11.2354 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 33/300\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 27.5486 - tarreg_loss: 22.8490 - regression_loss: 11.1005 - treatment_acc: 0.8135 — ate_err: 0.1602  — aipw_err: 0.1699 — tarreg_err: 0.1576 — cate_err: 2.2035 — cate_nn_err: 1.5552 \n",
      " — ate_err: 0.3709  — aipw_err: 1.2471 — tarreg_err: 0.3735 — cate_err: 3.0397 — cate_nn_err: 1.9436 \n",
      "21/21 [==============================] - 1s 27ms/step - loss: 27.7571 - tarreg_loss: 23.0575 - regression_loss: 11.2051 - treatment_acc: 0.8080 - val_loss: 31.7441 - val_tarreg_loss: 23.8287 - val_regression_loss: 11.5974 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 34/300\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 28.0632 - tarreg_loss: 23.3638 - regression_loss: 11.3590 - treatment_acc: 0.8125 — ate_err: 0.2635  — aipw_err: 0.1756 — tarreg_err: 0.2653 — cate_err: 2.2176 — cate_nn_err: 1.5672 \n",
      " — ate_err: 0.2367  — aipw_err: 1.2058 — tarreg_err: 0.2350 — cate_err: 3.0416 — cate_nn_err: 1.9330 \n",
      "21/21 [==============================] - 1s 28ms/step - loss: 27.6674 - tarreg_loss: 22.9680 - regression_loss: 11.1614 - treatment_acc: 0.8080 - val_loss: 32.7509 - val_tarreg_loss: 24.6913 - val_regression_loss: 12.0235 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 35/300\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 26.9170 - tarreg_loss: 22.2177 - regression_loss: 10.7811 - treatment_acc: 0.7917 — ate_err: 0.1278  — aipw_err: 0.1764 — tarreg_err: 0.1253 — cate_err: 2.1747 — cate_nn_err: 1.5547 \n",
      " — ate_err: 0.4033  — aipw_err: 1.1838 — tarreg_err: 0.4057 — cate_err: 3.0073 — cate_nn_err: 1.9162 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 27.7765 - tarreg_loss: 23.0772 - regression_loss: 11.2136 - treatment_acc: 0.8080 - val_loss: 30.8822 - val_tarreg_loss: 23.0858 - val_regression_loss: 11.2265 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 36/300\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 27.9150 - tarreg_loss: 23.2160 - regression_loss: 11.2847 - treatment_acc: 0.8142 — ate_err: 0.1755  — aipw_err: 0.2019 — tarreg_err: 0.1736 — cate_err: 2.1962 — cate_nn_err: 1.5448 \n",
      " — ate_err: 0.3382  — aipw_err: 1.1290 — tarreg_err: 0.3401 — cate_err: 3.0272 — cate_nn_err: 1.9051 \n",
      "21/21 [==============================] - 1s 31ms/step - loss: 27.4819 - tarreg_loss: 22.7829 - regression_loss: 11.0683 - treatment_acc: 0.8095 - val_loss: 30.9765 - val_tarreg_loss: 23.1795 - val_regression_loss: 11.2746 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 37/300\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 27.1545 - tarreg_loss: 22.4556 - regression_loss: 10.9077 - treatment_acc: 0.8125 — ate_err: 0.1337  — aipw_err: 0.1024 — tarreg_err: 0.1281 — cate_err: 2.1858 — cate_nn_err: 1.5479 \n",
      " — ate_err: 0.3868  — aipw_err: 1.2031 — tarreg_err: 0.3924 — cate_err: 3.0232 — cate_nn_err: 1.8972 \n",
      "21/21 [==============================] - 1s 28ms/step - loss: 27.2402 - tarreg_loss: 22.5413 - regression_loss: 10.9502 - treatment_acc: 0.8103 - val_loss: 30.9911 - val_tarreg_loss: 23.1661 - val_regression_loss: 11.2718 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 38/300\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 27.3170 - tarreg_loss: 22.6182 - regression_loss: 10.9897 - treatment_acc: 0.8133 — ate_err: 0.1806  — aipw_err: 0.2162 — tarreg_err: 0.1817 — cate_err: 2.1805 — cate_nn_err: 1.5520 \n",
      " — ate_err: 0.3330  — aipw_err: 1.0661 — tarreg_err: 0.3319 — cate_err: 3.0019 — cate_nn_err: 1.8870 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 27.1759 - tarreg_loss: 22.4772 - regression_loss: 10.9189 - treatment_acc: 0.8110 - val_loss: 30.7259 - val_tarreg_loss: 22.9588 - val_regression_loss: 11.1626 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 39/300\n",
      "21/21 [==============================] - ETA: 0s - loss: 27.3082 - tarreg_loss: 22.6098 - regression_loss: 10.9850 - treatment_acc: 0.8110 — ate_err: 0.2854  — aipw_err: 0.1954 — tarreg_err: 0.2884 — cate_err: 2.2330 — cate_nn_err: 1.5524 \n",
      " — ate_err: 0.2032  — aipw_err: 1.0736 — tarreg_err: 0.2002 — cate_err: 3.0546 — cate_nn_err: 1.9122 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 27.3082 - tarreg_loss: 22.6098 - regression_loss: 10.9850 - treatment_acc: 0.8110 - val_loss: 32.7172 - val_tarreg_loss: 24.6440 - val_regression_loss: 12.0010 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 40/300\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 27.3555 - tarreg_loss: 22.6571 - regression_loss: 11.0084 - treatment_acc: 0.8184 — ate_err: 0.2182  — aipw_err: 0.1938 — tarreg_err: 0.2196 — cate_err: 2.1983 — cate_nn_err: 1.5403 \n",
      " — ate_err: 0.2820  — aipw_err: 1.0547 — tarreg_err: 0.2806 — cate_err: 3.0207 — cate_nn_err: 1.8889 \n",
      "21/21 [==============================] - 1s 28ms/step - loss: 27.2424 - tarreg_loss: 22.5441 - regression_loss: 10.9513 - treatment_acc: 0.8110 - val_loss: 31.3420 - val_tarreg_loss: 23.4744 - val_regression_loss: 11.4211 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 41/300\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 27.4991 - tarreg_loss: 22.8011 - regression_loss: 11.0809 - treatment_acc: 0.8171 — ate_err: 0.1863  — aipw_err: 0.2740 — tarreg_err: 0.1881 — cate_err: 2.1848 — cate_nn_err: 1.5494 \n",
      " — ate_err: 0.3204  — aipw_err: 0.9514 — tarreg_err: 0.3187 — cate_err: 3.0113 — cate_nn_err: 1.8938 \n",
      "21/21 [==============================] - 1s 27ms/step - loss: 27.1644 - tarreg_loss: 22.4665 - regression_loss: 10.9124 - treatment_acc: 0.8110 - val_loss: 30.3267 - val_tarreg_loss: 22.5932 - val_regression_loss: 10.9816 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 42/300\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 27.4256 - tarreg_loss: 22.7276 - regression_loss: 11.0480 - treatment_acc: 0.8174 — ate_err: 0.1719  — aipw_err: 0.1084 — tarreg_err: 0.1663 — cate_err: 2.1875 — cate_nn_err: 1.5386 \n",
      " — ate_err: 0.3399  — aipw_err: 1.1083 — tarreg_err: 0.3455 — cate_err: 3.0139 — cate_nn_err: 1.9031 \n",
      "21/21 [==============================] - 1s 28ms/step - loss: 27.0790 - tarreg_loss: 22.3811 - regression_loss: 10.8721 - treatment_acc: 0.8110 - val_loss: 31.1899 - val_tarreg_loss: 23.3339 - val_regression_loss: 11.3609 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 43/300\n",
      "21/21 [==============================] - ETA: 0s - loss: 27.2014 - tarreg_loss: 22.5038 - regression_loss: 10.9328 - treatment_acc: 0.8110 — ate_err: 0.2665  — aipw_err: 0.1653 — tarreg_err: 0.2661 — cate_err: 2.2227 — cate_nn_err: 1.5531 \n",
      " — ate_err: 0.2152  — aipw_err: 1.0328 — tarreg_err: 0.2156 — cate_err: 3.0435 — cate_nn_err: 1.8820 \n",
      "21/21 [==============================] - 1s 28ms/step - loss: 27.2014 - tarreg_loss: 22.5038 - regression_loss: 10.9328 - treatment_acc: 0.8110 - val_loss: 32.2640 - val_tarreg_loss: 24.2229 - val_regression_loss: 11.8001 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 44/300\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 27.0538 - tarreg_loss: 22.3563 - regression_loss: 10.8586 - treatment_acc: 0.8109 — ate_err: 0.2968  — aipw_err: 0.2123 — tarreg_err: 0.3001 — cate_err: 2.2338 — cate_nn_err: 1.5462 \n",
      " — ate_err: 0.1824  — aipw_err: 0.9727 — tarreg_err: 0.1791 — cate_err: 3.0450 — cate_nn_err: 1.8923 \n",
      "21/21 [==============================] - 1s 30ms/step - loss: 26.8871 - tarreg_loss: 22.1897 - regression_loss: 10.7757 - treatment_acc: 0.8110 - val_loss: 32.4972 - val_tarreg_loss: 24.4451 - val_regression_loss: 11.9043 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 45/300\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 27.9103 - tarreg_loss: 23.2132 - regression_loss: 11.2898 - treatment_acc: 0.8052 — ate_err: 0.1619  — aipw_err: 0.1733 — tarreg_err: 0.1598 — cate_err: 2.1925 — cate_nn_err: 1.5374 \n",
      " — ate_err: 0.3431  — aipw_err: 0.9960 — tarreg_err: 0.3452 — cate_err: 3.0242 — cate_nn_err: 1.8704 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 26.8353 - tarreg_loss: 22.1383 - regression_loss: 10.7534 - treatment_acc: 0.8110 - val_loss: 30.7304 - val_tarreg_loss: 22.9211 - val_regression_loss: 11.1521 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 46/300\n",
      "21/21 [==============================] - ETA: 0s - loss: 26.7240 - tarreg_loss: 22.0270 - regression_loss: 10.6982 - treatment_acc: 0.8110 — ate_err: 0.1594  — aipw_err: 0.1737 — tarreg_err: 0.1581 — cate_err: 2.1944 — cate_nn_err: 1.5374 \n",
      " — ate_err: 0.3443  — aipw_err: 0.9837 — tarreg_err: 0.3456 — cate_err: 3.0302 — cate_nn_err: 1.8712 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 26.7240 - tarreg_loss: 22.0270 - regression_loss: 10.6982 - treatment_acc: 0.8110 - val_loss: 30.6254 - val_tarreg_loss: 22.8476 - val_regression_loss: 11.1151 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 47/300\n",
      "21/21 [==============================] - ETA: 0s - loss: 26.6798 - tarreg_loss: 21.9832 - regression_loss: 10.6768 - treatment_acc: 0.8110 — ate_err: 0.1391  — aipw_err: 0.1715 — tarreg_err: 0.1366 — cate_err: 2.1890 — cate_nn_err: 1.5431 \n",
      " — ate_err: 0.3666  — aipw_err: 0.9731 — tarreg_err: 0.3691 — cate_err: 3.0333 — cate_nn_err: 1.8575 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 26.6798 - tarreg_loss: 21.9832 - regression_loss: 10.6768 - treatment_acc: 0.8110 - val_loss: 30.2567 - val_tarreg_loss: 22.5134 - val_regression_loss: 10.9495 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 48/300\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 26.7270 - tarreg_loss: 22.0305 - regression_loss: 10.6993 - treatment_acc: 0.8003 — ate_err: 0.2915  — aipw_err: 0.1874 — tarreg_err: 0.2925 — cate_err: 2.2542 — cate_nn_err: 1.5533 \n",
      " — ate_err: 0.1731  — aipw_err: 0.9474 — tarreg_err: 0.1722 — cate_err: 3.0835 — cate_nn_err: 1.8879 \n",
      "21/21 [==============================] - 1s 26ms/step - loss: 26.6015 - tarreg_loss: 21.9051 - regression_loss: 10.6381 - treatment_acc: 0.8110 - val_loss: 33.0127 - val_tarreg_loss: 24.8229 - val_regression_loss: 12.1009 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 49/300\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 26.9010 - tarreg_loss: 22.2047 - regression_loss: 10.7912 - treatment_acc: 0.8240 — ate_err: 0.1790  — aipw_err: 0.1906 — tarreg_err: 0.1793 — cate_err: 2.1940 — cate_nn_err: 1.5342 \n",
      " — ate_err: 0.3131  — aipw_err: 0.9277 — tarreg_err: 0.3128 — cate_err: 3.0230 — cate_nn_err: 1.8624 \n",
      "21/21 [==============================] - 1s 28ms/step - loss: 26.6376 - tarreg_loss: 21.9413 - regression_loss: 10.6567 - treatment_acc: 0.8110 - val_loss: 30.5880 - val_tarreg_loss: 22.8083 - val_regression_loss: 11.0961 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 50/300\n",
      "21/21 [==============================] - ETA: 0s - loss: 26.7670 - tarreg_loss: 22.0710 - regression_loss: 10.7179 - treatment_acc: 0.8110 — ate_err: 0.1500  — aipw_err: 0.1411 — tarreg_err: 0.1478 — cate_err: 2.1830 — cate_nn_err: 1.5253 \n",
      " — ate_err: 0.3538  — aipw_err: 0.9749 — tarreg_err: 0.3560 — cate_err: 3.0120 — cate_nn_err: 1.8505 \n",
      "21/21 [==============================] - 1s 30ms/step - loss: 26.7670 - tarreg_loss: 22.0710 - regression_loss: 10.7179 - treatment_acc: 0.8110 - val_loss: 30.3121 - val_tarreg_loss: 22.5479 - val_regression_loss: 10.9689 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 51/300\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 26.7851 - tarreg_loss: 22.0893 - regression_loss: 10.7303 - treatment_acc: 0.8188 — ate_err: 0.1555  — aipw_err: 0.2177 — tarreg_err: 0.1550 — cate_err: 2.2002 — cate_nn_err: 1.5388 \n",
      " — ate_err: 0.3393  — aipw_err: 0.8811 — tarreg_err: 0.3398 — cate_err: 3.0439 — cate_nn_err: 1.8582 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 26.6237 - tarreg_loss: 21.9280 - regression_loss: 10.6486 - treatment_acc: 0.8110 - val_loss: 30.3453 - val_tarreg_loss: 22.5511 - val_regression_loss: 10.9695 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 52/300\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 26.3806 - tarreg_loss: 21.6851 - regression_loss: 10.5294 - treatment_acc: 0.8000 — ate_err: 0.1261  — aipw_err: 0.0754 — tarreg_err: 0.1182 — cate_err: 2.2144 — cate_nn_err: 1.5223 \n",
      " — ate_err: 0.3809  — aipw_err: 1.0268 — tarreg_err: 0.3888 — cate_err: 3.0639 — cate_nn_err: 1.8509 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 26.3783 - tarreg_loss: 21.6828 - regression_loss: 10.5273 - treatment_acc: 0.8110 - val_loss: 31.1055 - val_tarreg_loss: 23.2223 - val_regression_loss: 11.3129 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 53/300\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 26.4622 - tarreg_loss: 21.7669 - regression_loss: 10.5729 - treatment_acc: 0.8133 — ate_err: 0.1673  — aipw_err: 0.1942 — tarreg_err: 0.1683 — cate_err: 2.2049 — cate_nn_err: 1.5401 \n",
      " — ate_err: 0.3205  — aipw_err: 0.8864 — tarreg_err: 0.3195 — cate_err: 3.0444 — cate_nn_err: 1.8552 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 26.3371 - tarreg_loss: 21.6418 - regression_loss: 10.5101 - treatment_acc: 0.8110 - val_loss: 30.5237 - val_tarreg_loss: 22.7373 - val_regression_loss: 11.0625 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 54/300\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 26.2851 - tarreg_loss: 21.5900 - regression_loss: 10.4843 - treatment_acc: 0.8115 — ate_err: 0.2331  — aipw_err: 0.2175 — tarreg_err: 0.2354 — cate_err: 2.1976 — cate_nn_err: 1.5417 \n",
      " — ate_err: 0.2470  — aipw_err: 0.8537 — tarreg_err: 0.2447 — cate_err: 3.0163 — cate_nn_err: 1.8398 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 26.4422 - tarreg_loss: 21.7472 - regression_loss: 10.5628 - treatment_acc: 0.8110 - val_loss: 30.5456 - val_tarreg_loss: 22.7553 - val_regression_loss: 11.0699 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 55/300\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 26.1933 - tarreg_loss: 21.4985 - regression_loss: 10.4367 - treatment_acc: 0.7948 — ate_err: 0.1373  — aipw_err: 0.1468 — tarreg_err: 0.1353 — cate_err: 2.1816 — cate_nn_err: 1.5185 \n",
      " — ate_err: 0.3670  — aipw_err: 0.9194 — tarreg_err: 0.3690 — cate_err: 3.0186 — cate_nn_err: 1.8133 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 26.3392 - tarreg_loss: 21.6444 - regression_loss: 10.5123 - treatment_acc: 0.8110 - val_loss: 30.0230 - val_tarreg_loss: 22.3202 - val_regression_loss: 10.8580 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 56/300\n",
      "21/21 [==============================] - ETA: 0s - loss: 26.2442 - tarreg_loss: 21.5496 - regression_loss: 10.4653 - treatment_acc: 0.8110 — ate_err: 0.2118  — aipw_err: 0.1638 — tarreg_err: 0.2109 — cate_err: 2.2201 — cate_nn_err: 1.5292 \n",
      " — ate_err: 0.2679  — aipw_err: 0.8957 — tarreg_err: 0.2687 — cate_err: 3.0545 — cate_nn_err: 1.8454 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 26.2442 - tarreg_loss: 21.5496 - regression_loss: 10.4653 - treatment_acc: 0.8110 - val_loss: 31.1982 - val_tarreg_loss: 23.2913 - val_regression_loss: 11.3438 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 57/300\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 26.5110 - tarreg_loss: 21.8166 - regression_loss: 10.5968 - treatment_acc: 0.8033 — ate_err: 0.1910  — aipw_err: 0.1941 — tarreg_err: 0.1930 — cate_err: 2.1974 — cate_nn_err: 1.5296 \n",
      " — ate_err: 0.2942  — aipw_err: 0.8514 — tarreg_err: 0.2922 — cate_err: 3.0271 — cate_nn_err: 1.8207 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 26.2980 - tarreg_loss: 21.6037 - regression_loss: 10.4913 - treatment_acc: 0.8110 - val_loss: 30.4830 - val_tarreg_loss: 22.6916 - val_regression_loss: 11.0410 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 58/300\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 25.7410 - tarreg_loss: 21.0470 - regression_loss: 10.2146 - treatment_acc: 0.8125 — ate_err: 0.2104  — aipw_err: 0.1809 — tarreg_err: 0.2105 — cate_err: 2.2099 — cate_nn_err: 1.5197 \n",
      " — ate_err: 0.2752  — aipw_err: 0.8613 — tarreg_err: 0.2752 — cate_err: 3.0371 — cate_nn_err: 1.8173 \n",
      "21/21 [==============================] - 1s 30ms/step - loss: 26.2820 - tarreg_loss: 21.5880 - regression_loss: 10.4853 - treatment_acc: 0.8110 - val_loss: 30.7297 - val_tarreg_loss: 22.9136 - val_regression_loss: 11.1549 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 59/300\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 25.9401 - tarreg_loss: 21.2462 - regression_loss: 10.3157 - treatment_acc: 0.8084 — ate_err: 0.1865  — aipw_err: 0.1577 — tarreg_err: 0.1840 — cate_err: 2.2037 — cate_nn_err: 1.5324 \n",
      " — ate_err: 0.2988  — aipw_err: 0.8764 — tarreg_err: 0.3012 — cate_err: 3.0409 — cate_nn_err: 1.8317 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 26.0499 - tarreg_loss: 21.3561 - regression_loss: 10.3709 - treatment_acc: 0.8110 - val_loss: 30.7670 - val_tarreg_loss: 22.9076 - val_regression_loss: 11.1556 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 60/300\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 26.2221 - tarreg_loss: 21.5285 - regression_loss: 10.4553 - treatment_acc: 0.8082 — ate_err: 0.0976  — aipw_err: 0.1545 — tarreg_err: 0.0965 — cate_err: 2.1770 — cate_nn_err: 1.5045 \n",
      " — ate_err: 0.4175  — aipw_err: 0.8742 — tarreg_err: 0.4186 — cate_err: 3.0135 — cate_nn_err: 1.7827 \n",
      "21/21 [==============================] - 1s 27ms/step - loss: 26.1070 - tarreg_loss: 21.4135 - regression_loss: 10.3986 - treatment_acc: 0.8110 - val_loss: 29.3000 - val_tarreg_loss: 21.7413 - val_regression_loss: 10.5705 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 61/300\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 26.1144 - tarreg_loss: 21.4212 - regression_loss: 10.4032 - treatment_acc: 0.8141 — ate_err: 0.0377  — aipw_err: 0.1843 — tarreg_err: 0.0356 — cate_err: 2.1800 — cate_nn_err: 1.5376 \n",
      " — ate_err: 0.4734  — aipw_err: 0.8333 — tarreg_err: 0.4755 — cate_err: 3.0432 — cate_nn_err: 1.8131 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 26.0509 - tarreg_loss: 21.3576 - regression_loss: 10.3712 - treatment_acc: 0.8110 - val_loss: 29.0855 - val_tarreg_loss: 21.4798 - val_regression_loss: 10.4397 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 62/300\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 26.5914 - tarreg_loss: 21.8983 - regression_loss: 10.6433 - treatment_acc: 0.8232 — ate_err: 0.3200  — aipw_err: 0.2202 — tarreg_err: 0.3248 — cate_err: 2.2336 — cate_nn_err: 1.5387 \n",
      " — ate_err: 0.1373  — aipw_err: 0.7881 — tarreg_err: 0.1324 — cate_err: 3.0473 — cate_nn_err: 1.8260 \n",
      "21/21 [==============================] - 1s 28ms/step - loss: 26.1583 - tarreg_loss: 21.4653 - regression_loss: 10.4249 - treatment_acc: 0.8110 - val_loss: 32.3683 - val_tarreg_loss: 24.3350 - val_regression_loss: 11.8575 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 63/300\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 26.0408 - tarreg_loss: 21.3480 - regression_loss: 10.3662 - treatment_acc: 0.8097 — ate_err: 0.2479  — aipw_err: 0.1805 — tarreg_err: 0.2498 — cate_err: 2.2231 — cate_nn_err: 1.5351 \n",
      " — ate_err: 0.2260  — aipw_err: 0.8269 — tarreg_err: 0.2241 — cate_err: 3.0476 — cate_nn_err: 1.8114 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 26.0355 - tarreg_loss: 21.3427 - regression_loss: 10.3637 - treatment_acc: 0.8110 - val_loss: 31.2731 - val_tarreg_loss: 23.3629 - val_regression_loss: 11.3795 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 64/300\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 25.7818 - tarreg_loss: 21.0893 - regression_loss: 10.2393 - treatment_acc: 0.8097 — ate_err: 0.2913  — aipw_err: 0.1994 — tarreg_err: 0.2946 — cate_err: 2.2029 — cate_nn_err: 1.5454 \n",
      " — ate_err: 0.1784  — aipw_err: 0.7980 — tarreg_err: 0.1751 — cate_err: 3.0111 — cate_nn_err: 1.7992 \n",
      "21/21 [==============================] - 1s 27ms/step - loss: 25.9452 - tarreg_loss: 21.2528 - regression_loss: 10.3214 - treatment_acc: 0.8110 - val_loss: 31.3515 - val_tarreg_loss: 23.4693 - val_regression_loss: 11.4304 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 65/300\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 25.9318 - tarreg_loss: 21.2396 - regression_loss: 10.3128 - treatment_acc: 0.8125 — ate_err: 0.2706  — aipw_err: 0.1804 — tarreg_err: 0.2714 — cate_err: 2.2248 — cate_nn_err: 1.5137 \n",
      " — ate_err: 0.2038  — aipw_err: 0.8143 — tarreg_err: 0.2030 — cate_err: 3.0401 — cate_nn_err: 1.8016 \n",
      "21/21 [==============================] - 1s 30ms/step - loss: 25.9140 - tarreg_loss: 21.2218 - regression_loss: 10.3040 - treatment_acc: 0.8110 - val_loss: 31.5100 - val_tarreg_loss: 23.5770 - val_regression_loss: 11.4895 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 66/300\n",
      "21/21 [==============================] - ETA: 0s - loss: 25.7664 - tarreg_loss: 21.0746 - regression_loss: 10.2326 - treatment_acc: 0.8110 — ate_err: 0.3163  — aipw_err: 0.2180 — tarreg_err: 0.3210 — cate_err: 2.2199 — cate_nn_err: 1.5204 \n",
      " — ate_err: 0.1487  — aipw_err: 0.7660 — tarreg_err: 0.1440 — cate_err: 3.0240 — cate_nn_err: 1.8051 \n",
      "21/21 [==============================] - 1s 30ms/step - loss: 25.7664 - tarreg_loss: 21.0746 - regression_loss: 10.2326 - treatment_acc: 0.8110 - val_loss: 31.8010 - val_tarreg_loss: 23.8506 - val_regression_loss: 11.6186 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 67/300\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 25.7870 - tarreg_loss: 21.0953 - regression_loss: 10.2419 - treatment_acc: 0.8086 — ate_err: 0.1570  — aipw_err: 0.1783 — tarreg_err: 0.1555 — cate_err: 2.2006 — cate_nn_err: 1.5254 \n",
      " — ate_err: 0.3239  — aipw_err: 0.8022 — tarreg_err: 0.3254 — cate_err: 3.0437 — cate_nn_err: 1.7975 \n",
      "21/21 [==============================] - 1s 30ms/step - loss: 25.9241 - tarreg_loss: 21.2324 - regression_loss: 10.3109 - treatment_acc: 0.8110 - val_loss: 29.9661 - val_tarreg_loss: 22.2251 - val_regression_loss: 10.8176 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 68/300\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 24.5218 - tarreg_loss: 19.8304 - regression_loss: 9.6107 - treatment_acc: 0.8125 — ate_err: 0.1948  — aipw_err: 0.1325 — tarreg_err: 0.1931 — cate_err: 2.2248 — cate_nn_err: 1.5136 \n",
      " — ate_err: 0.2820  — aipw_err: 0.8478 — tarreg_err: 0.2838 — cate_err: 3.0615 — cate_nn_err: 1.8124 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 25.7852 - tarreg_loss: 21.0937 - regression_loss: 10.2429 - treatment_acc: 0.8110 - val_loss: 31.1393 - val_tarreg_loss: 23.2547 - val_regression_loss: 11.3340 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 69/300\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 25.8243 - tarreg_loss: 21.1332 - regression_loss: 10.2639 - treatment_acc: 0.8164 — ate_err: 0.2125  — aipw_err: 0.2048 — tarreg_err: 0.2164 — cate_err: 2.1837 — cate_nn_err: 1.5135 \n",
      " — ate_err: 0.2765  — aipw_err: 0.7638 — tarreg_err: 0.2726 — cate_err: 2.9998 — cate_nn_err: 1.7810 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 25.8570 - tarreg_loss: 21.1659 - regression_loss: 10.2783 - treatment_acc: 0.8110 - val_loss: 29.6963 - val_tarreg_loss: 22.0690 - val_regression_loss: 10.7351 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 70/300\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 25.3288 - tarreg_loss: 20.6379 - regression_loss: 10.0172 - treatment_acc: 0.8154 — ate_err: 0.2149  — aipw_err: 0.1568 — tarreg_err: 0.2149 — cate_err: 2.2199 — cate_nn_err: 1.5351 \n",
      " — ate_err: 0.2571  — aipw_err: 0.8118 — tarreg_err: 0.2571 — cate_err: 3.0570 — cate_nn_err: 1.8026 \n",
      "21/21 [==============================] - 1s 30ms/step - loss: 25.6897 - tarreg_loss: 20.9989 - regression_loss: 10.1972 - treatment_acc: 0.8110 - val_loss: 31.0898 - val_tarreg_loss: 23.1891 - val_regression_loss: 11.3000 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 71/300\n",
      "21/21 [==============================] - ETA: 0s - loss: 25.7419 - tarreg_loss: 21.0513 - regression_loss: 10.2240 - treatment_acc: 0.8110 — ate_err: 0.1493  — aipw_err: 0.1893 — tarreg_err: 0.1499 — cate_err: 2.2151 — cate_nn_err: 1.5139 \n",
      " — ate_err: 0.3287  — aipw_err: 0.7691 — tarreg_err: 0.3281 — cate_err: 3.0645 — cate_nn_err: 1.8021 \n",
      "21/21 [==============================] - 1s 28ms/step - loss: 25.7419 - tarreg_loss: 21.0513 - regression_loss: 10.2240 - treatment_acc: 0.8110 - val_loss: 30.1065 - val_tarreg_loss: 22.3463 - val_regression_loss: 10.8786 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 72/300\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 25.3462 - tarreg_loss: 20.6558 - regression_loss: 10.0256 - treatment_acc: 0.8084 — ate_err: 0.1810  — aipw_err: 0.1869 — tarreg_err: 0.1808 — cate_err: 2.2343 — cate_nn_err: 1.5044 \n",
      " — ate_err: 0.2997  — aipw_err: 0.7666 — tarreg_err: 0.2999 — cate_err: 3.0780 — cate_nn_err: 1.8029 \n",
      "21/21 [==============================] - 1s 31ms/step - loss: 25.6807 - tarreg_loss: 20.9903 - regression_loss: 10.1934 - treatment_acc: 0.8110 - val_loss: 30.6104 - val_tarreg_loss: 22.7858 - val_regression_loss: 11.0996 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 73/300\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 25.8396 - tarreg_loss: 21.1494 - regression_loss: 10.2742 - treatment_acc: 0.8125 — ate_err: 0.0097  — aipw_err: 0.1654 — tarreg_err: 0.0071 — cate_err: 2.1746 — cate_nn_err: 1.5288 \n",
      " — ate_err: 0.4995  — aipw_err: 0.7831 — tarreg_err: 0.5021 — cate_err: 3.0376 — cate_nn_err: 1.7986 \n",
      "21/21 [==============================] - 1s 28ms/step - loss: 25.5144 - tarreg_loss: 20.8242 - regression_loss: 10.1112 - treatment_acc: 0.8110 - val_loss: 28.5253 - val_tarreg_loss: 21.0125 - val_regression_loss: 10.2123 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 74/300\n",
      "21/21 [==============================] - ETA: 0s - loss: 25.6917 - tarreg_loss: 21.0019 - regression_loss: 10.1999 - treatment_acc: 0.8110 — ate_err: 0.1318  — aipw_err: 0.1649 — tarreg_err: 0.1308 — cate_err: 2.2004 — cate_nn_err: 1.5185 \n",
      " — ate_err: 0.3542  — aipw_err: 0.7800 — tarreg_err: 0.3551 — cate_err: 3.0424 — cate_nn_err: 1.7967 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 25.6917 - tarreg_loss: 21.0019 - regression_loss: 10.1999 - treatment_acc: 0.8110 - val_loss: 29.7777 - val_tarreg_loss: 22.0573 - val_regression_loss: 10.7369 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 75/300\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 25.6184 - tarreg_loss: 20.9288 - regression_loss: 10.1641 - treatment_acc: 0.8102 — ate_err: 0.1535  — aipw_err: 0.1876 — tarreg_err: 0.1536 — cate_err: 2.2185 — cate_nn_err: 1.5065 \n",
      " — ate_err: 0.3322  — aipw_err: 0.7535 — tarreg_err: 0.3321 — cate_err: 3.0642 — cate_nn_err: 1.7728 \n",
      "21/21 [==============================] - 1s 30ms/step - loss: 25.5438 - tarreg_loss: 20.8542 - regression_loss: 10.1269 - treatment_acc: 0.8110 - val_loss: 29.8439 - val_tarreg_loss: 22.1411 - val_regression_loss: 10.7787 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 76/300\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 25.4457 - tarreg_loss: 20.7564 - regression_loss: 10.0785 - treatment_acc: 0.8125 — ate_err: 0.2324  — aipw_err: 0.1931 — tarreg_err: 0.2340 — cate_err: 2.2202 — cate_nn_err: 1.5147 \n",
      " — ate_err: 0.2399  — aipw_err: 0.7438 — tarreg_err: 0.2383 — cate_err: 3.0440 — cate_nn_err: 1.7683 \n",
      "21/21 [==============================] - 1s 37ms/step - loss: 25.6088 - tarreg_loss: 20.9195 - regression_loss: 10.1599 - treatment_acc: 0.8110 - val_loss: 30.6638 - val_tarreg_loss: 22.8371 - val_regression_loss: 11.1250 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 77/300\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 25.4676 - tarreg_loss: 20.7785 - regression_loss: 10.0895 - treatment_acc: 0.8141 — ate_err: 0.1562  — aipw_err: 0.1573 — tarreg_err: 0.1539 — cate_err: 2.2073 — cate_nn_err: 1.5090 \n",
      " — ate_err: 0.3320  — aipw_err: 0.7783 — tarreg_err: 0.3344 — cate_err: 3.0467 — cate_nn_err: 1.7643 \n",
      "21/21 [==============================] - 1s 31ms/step - loss: 25.5112 - tarreg_loss: 20.8221 - regression_loss: 10.1106 - treatment_acc: 0.8110 - val_loss: 29.6640 - val_tarreg_loss: 21.9984 - val_regression_loss: 10.7104 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 78/300\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 25.5767 - tarreg_loss: 20.8879 - regression_loss: 10.1476 - treatment_acc: 0.8164 — ate_err: 0.0929  — aipw_err: 0.1062 — tarreg_err: 0.0875 — cate_err: 2.1889 — cate_nn_err: 1.5078 \n",
      " — ate_err: 0.4075  — aipw_err: 0.8295 — tarreg_err: 0.4128 — cate_err: 3.0373 — cate_nn_err: 1.7765 \n",
      "21/21 [==============================] - 1s 30ms/step - loss: 25.4263 - tarreg_loss: 20.7375 - regression_loss: 10.0705 - treatment_acc: 0.8110 - val_loss: 29.3708 - val_tarreg_loss: 21.7408 - val_regression_loss: 10.5829 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 79/300\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 25.5767 - tarreg_loss: 20.8882 - regression_loss: 10.1455 - treatment_acc: 0.8125 — ate_err: 0.1928  — aipw_err: 0.2027 — tarreg_err: 0.1952 — cate_err: 2.2044 — cate_nn_err: 1.5046 \n",
      " — ate_err: 0.2867  — aipw_err: 0.7191 — tarreg_err: 0.2844 — cate_err: 3.0305 — cate_nn_err: 1.7787 \n",
      "21/21 [==============================] - 1s 30ms/step - loss: 25.5246 - tarreg_loss: 20.8361 - regression_loss: 10.1194 - treatment_acc: 0.8110 - val_loss: 29.8870 - val_tarreg_loss: 22.1931 - val_regression_loss: 10.8046 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 80/300\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 25.6843 - tarreg_loss: 20.9960 - regression_loss: 10.1983 - treatment_acc: 0.8109 — ate_err: 0.1801  — aipw_err: 0.1760 — tarreg_err: 0.1807 — cate_err: 2.1900 — cate_nn_err: 1.5109 \n",
      " — ate_err: 0.3001  — aipw_err: 0.7417 — tarreg_err: 0.2994 — cate_err: 3.0201 — cate_nn_err: 1.7732 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 25.5428 - tarreg_loss: 20.8545 - regression_loss: 10.1277 - treatment_acc: 0.8110 - val_loss: 29.8098 - val_tarreg_loss: 22.1330 - val_regression_loss: 10.7768 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 81/300\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 25.7139 - tarreg_loss: 21.0259 - regression_loss: 10.2145 - treatment_acc: 0.8078 — ate_err: 0.1409  — aipw_err: 0.1826 — tarreg_err: 0.1413 — cate_err: 2.1991 — cate_nn_err: 1.5023 \n",
      " — ate_err: 0.3510  — aipw_err: 0.7330 — tarreg_err: 0.3505 — cate_err: 3.0380 — cate_nn_err: 1.7815 \n",
      "21/21 [==============================] - 1s 30ms/step - loss: 25.4312 - tarreg_loss: 20.7432 - regression_loss: 10.0735 - treatment_acc: 0.8110 - val_loss: 29.2901 - val_tarreg_loss: 21.7236 - val_regression_loss: 10.5729 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 82/300\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 25.4881 - tarreg_loss: 20.8004 - regression_loss: 10.1054 - treatment_acc: 0.8208 — ate_err: 0.1472  — aipw_err: 0.1672 — tarreg_err: 0.1449 — cate_err: 2.2482 — cate_nn_err: 1.5104 \n",
      " — ate_err: 0.3174  — aipw_err: 0.7465 — tarreg_err: 0.3197 — cate_err: 3.1131 — cate_nn_err: 1.8115 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 25.3812 - tarreg_loss: 20.6935 - regression_loss: 10.0503 - treatment_acc: 0.8110 - val_loss: 30.7151 - val_tarreg_loss: 22.8141 - val_regression_loss: 11.1211 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 83/300\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 25.0557 - tarreg_loss: 20.3682 - regression_loss: 9.8883 - treatment_acc: 0.8167 — ate_err: 0.1190  — aipw_err: 0.1358 — tarreg_err: 0.1161 — cate_err: 2.1870 — cate_nn_err: 1.4946 \n",
      " — ate_err: 0.3797  — aipw_err: 0.7752 — tarreg_err: 0.3826 — cate_err: 3.0269 — cate_nn_err: 1.7684 \n",
      "21/21 [==============================] - 1s 28ms/step - loss: 25.3949 - tarreg_loss: 20.7075 - regression_loss: 10.0564 - treatment_acc: 0.8110 - val_loss: 29.4613 - val_tarreg_loss: 21.8874 - val_regression_loss: 10.6577 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 84/300\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 24.9695 - tarreg_loss: 20.2824 - regression_loss: 9.8445 - treatment_acc: 0.8109 — ate_err: 0.2345  — aipw_err: 0.2077 — tarreg_err: 0.2380 — cate_err: 2.2161 — cate_nn_err: 1.4928 \n",
      " — ate_err: 0.2320  — aipw_err: 0.6939 — tarreg_err: 0.2285 — cate_err: 3.0440 — cate_nn_err: 1.7727 \n",
      "21/21 [==============================] - 1s 30ms/step - loss: 25.3588 - tarreg_loss: 20.6716 - regression_loss: 10.0393 - treatment_acc: 0.8110 - val_loss: 30.3208 - val_tarreg_loss: 22.5319 - val_regression_loss: 10.9742 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 85/300\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 24.8713 - tarreg_loss: 20.1844 - regression_loss: 9.7972 - treatment_acc: 0.8109 — ate_err: 0.2977  — aipw_err: 0.2462 — tarreg_err: 0.3033 — cate_err: 2.2307 — cate_nn_err: 1.4944 \n",
      " — ate_err: 0.1634  — aipw_err: 0.6496 — tarreg_err: 0.1578 — cate_err: 3.0397 — cate_nn_err: 1.7742 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 25.2108 - tarreg_loss: 20.5238 - regression_loss: 9.9667 - treatment_acc: 0.8110 - val_loss: 30.8923 - val_tarreg_loss: 23.0711 - val_regression_loss: 11.2394 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 86/300\n",
      "21/21 [==============================] - ETA: 0s - loss: 25.2971 - tarreg_loss: 20.6105 - regression_loss: 10.0092 - treatment_acc: 0.8110 — ate_err: 0.2066  — aipw_err: 0.1826 — tarreg_err: 0.2072 — cate_err: 2.2180 — cate_nn_err: 1.5093 \n",
      " — ate_err: 0.2606  — aipw_err: 0.7146 — tarreg_err: 0.2600 — cate_err: 3.0537 — cate_nn_err: 1.7861 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 25.2971 - tarreg_loss: 20.6105 - regression_loss: 10.0092 - treatment_acc: 0.8110 - val_loss: 30.6288 - val_tarreg_loss: 22.8127 - val_regression_loss: 11.1196 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 87/300\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 25.0657 - tarreg_loss: 20.3793 - regression_loss: 9.8951 - treatment_acc: 0.8156 — ate_err: 0.1116  — aipw_err: 0.1737 — tarreg_err: 0.1086 — cate_err: 2.2375 — cate_nn_err: 1.4884 \n",
      " — ate_err: 0.3694  — aipw_err: 0.7214 — tarreg_err: 0.3723 — cate_err: 3.0998 — cate_nn_err: 1.8137 \n",
      "21/21 [==============================] - 1s 30ms/step - loss: 25.1986 - tarreg_loss: 20.5122 - regression_loss: 9.9607 - treatment_acc: 0.8110 - val_loss: 29.7551 - val_tarreg_loss: 22.0387 - val_regression_loss: 10.7352 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 88/300\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 25.2325 - tarreg_loss: 20.5463 - regression_loss: 9.9716 - treatment_acc: 0.8109 — ate_err: 0.3047  — aipw_err: 0.2251 — tarreg_err: 0.3100 — cate_err: 2.2190 — cate_nn_err: 1.4886 \n",
      " — ate_err: 0.1531  — aipw_err: 0.6610 — tarreg_err: 0.1478 — cate_err: 3.0244 — cate_nn_err: 1.7672 \n",
      "21/21 [==============================] - 1s 35ms/step - loss: 25.2844 - tarreg_loss: 20.5982 - regression_loss: 9.9981 - treatment_acc: 0.8110 - val_loss: 31.1497 - val_tarreg_loss: 23.3168 - val_regression_loss: 11.3638 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 89/300\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 25.2446 - tarreg_loss: 20.5588 - regression_loss: 9.9893 - treatment_acc: 0.8240  — ate_err: 0.1838  — aipw_err: 0.2144 — tarreg_err: 0.1859 — cate_err: 2.2182 — cate_nn_err: 1.4935 \n",
      " — ate_err: 0.2872  — aipw_err: 0.6697 — tarreg_err: 0.2851 — cate_err: 3.0509 — cate_nn_err: 1.7751 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 25.1830 - tarreg_loss: 20.4972 - regression_loss: 9.9519 - treatment_acc: 0.8110 - val_loss: 29.7716 - val_tarreg_loss: 22.0919 - val_regression_loss: 10.7595 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 90/300\n",
      "21/21 [==============================] - ETA: 0s - loss: 25.1725 - tarreg_loss: 20.4869 - regression_loss: 9.9502 - treatment_acc: 0.8110  — ate_err: 0.2012  — aipw_err: 0.1646 — tarreg_err: 0.2006 — cate_err: 2.2051 — cate_nn_err: 1.4828 \n",
      " — ate_err: 0.2765  — aipw_err: 0.7208 — tarreg_err: 0.2771 — cate_err: 3.0313 — cate_nn_err: 1.7621 \n",
      "21/21 [==============================] - 1s 28ms/step - loss: 25.1725 - tarreg_loss: 20.4869 - regression_loss: 9.9502 - treatment_acc: 0.8110 - val_loss: 30.0221 - val_tarreg_loss: 22.3051 - val_regression_loss: 10.8691 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 91/300\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 24.8377 - tarreg_loss: 20.1525 - regression_loss: 9.7815 - treatment_acc: 0.8024 — ate_err: 0.1556  — aipw_err: 0.1881 — tarreg_err: 0.1558 — cate_err: 2.2318 — cate_nn_err: 1.4994 \n",
      " — ate_err: 0.3128  — aipw_err: 0.6908 — tarreg_err: 0.3125 — cate_err: 3.0883 — cate_nn_err: 1.7937 \n",
      "21/21 [==============================] - 1s 27ms/step - loss: 25.0383 - tarreg_loss: 20.3531 - regression_loss: 9.8838 - treatment_acc: 0.8110 - val_loss: 30.0193 - val_tarreg_loss: 22.2698 - val_regression_loss: 10.8510 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 92/300\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 25.6030 - tarreg_loss: 20.9179 - regression_loss: 10.1664 - treatment_acc: 0.8083 — ate_err: 0.2452  — aipw_err: 0.1609 — tarreg_err: 0.2449 — cate_err: 2.2129 — cate_nn_err: 1.4896 \n",
      " — ate_err: 0.2180  — aipw_err: 0.7182 — tarreg_err: 0.2183 — cate_err: 3.0319 — cate_nn_err: 1.7625 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 25.2647 - tarreg_loss: 20.5797 - regression_loss: 9.9969 - treatment_acc: 0.8110 - val_loss: 30.8045 - val_tarreg_loss: 22.9861 - val_regression_loss: 11.2105 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 93/300\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 25.3580 - tarreg_loss: 20.6734 - regression_loss: 10.0448 - treatment_acc: 0.8096 — ate_err: 0.3065  — aipw_err: 0.1941 — tarreg_err: 0.3069 — cate_err: 2.2822 — cate_nn_err: 1.5075 \n",
      " — ate_err: 0.1267  — aipw_err: 0.6794 — tarreg_err: 0.1264 — cate_err: 3.1264 — cate_nn_err: 1.8060 \n",
      "21/21 [==============================] - 1s 28ms/step - loss: 25.0866 - tarreg_loss: 20.4019 - regression_loss: 9.9083 - treatment_acc: 0.8110 - val_loss: 33.2060 - val_tarreg_loss: 24.9816 - val_regression_loss: 12.2073 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 94/300\n",
      "21/21 [==============================] - ETA: 0s - loss: 25.1245 - tarreg_loss: 20.4400 - regression_loss: 9.9280 - treatment_acc: 0.8110 — ate_err: 0.1571  — aipw_err: 0.1594 — tarreg_err: 0.1559 — cate_err: 2.1844 — cate_nn_err: 1.4738 \n",
      " — ate_err: 0.3388  — aipw_err: 0.7173 — tarreg_err: 0.3400 — cate_err: 3.0152 — cate_nn_err: 1.7576 \n",
      "21/21 [==============================] - 1s 30ms/step - loss: 25.1245 - tarreg_loss: 20.4400 - regression_loss: 9.9280 - treatment_acc: 0.8110 - val_loss: 29.1142 - val_tarreg_loss: 21.5669 - val_regression_loss: 10.5020 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 95/300\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 24.9107 - tarreg_loss: 20.2265 - regression_loss: 9.8202 - treatment_acc: 0.8156  — ate_err: 0.0422  — aipw_err: 0.1130 — tarreg_err: 0.0365 — cate_err: 2.2062 — cate_nn_err: 1.5033 \n",
      " — ate_err: 0.4487  — aipw_err: 0.7622 — tarreg_err: 0.4544 — cate_err: 3.0718 — cate_nn_err: 1.7963 \n",
      "21/21 [==============================] - 1s 28ms/step - loss: 24.9926 - tarreg_loss: 20.3084 - regression_loss: 9.8600 - treatment_acc: 0.8110 - val_loss: 29.1403 - val_tarreg_loss: 21.5105 - val_regression_loss: 10.4739 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 96/300\n",
      "21/21 [==============================] - ETA: 0s - loss: 25.0655 - tarreg_loss: 20.3816 - regression_loss: 9.8959 - treatment_acc: 0.8110 — ate_err: 0.2069  — aipw_err: 0.1991 — tarreg_err: 0.2078 — cate_err: 2.2553 — cate_nn_err: 1.4849 \n",
      " — ate_err: 0.2456  — aipw_err: 0.6644 — tarreg_err: 0.2448 — cate_err: 3.1051 — cate_nn_err: 1.8153 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 25.0655 - tarreg_loss: 20.3816 - regression_loss: 9.8959 - treatment_acc: 0.8110 - val_loss: 31.0138 - val_tarreg_loss: 23.0991 - val_regression_loss: 11.2671 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 97/300\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 25.0834 - tarreg_loss: 20.3997 - regression_loss: 9.9097 - treatment_acc: 0.8158 — ate_err: 0.1593  — aipw_err: 0.1068 — tarreg_err: 0.1552 — cate_err: 2.1877 — cate_nn_err: 1.4765 \n",
      " — ate_err: 0.3294  — aipw_err: 0.7644 — tarreg_err: 0.3335 — cate_err: 3.0169 — cate_nn_err: 1.7928 \n",
      "21/21 [==============================] - 1s 31ms/step - loss: 24.9195 - tarreg_loss: 20.2358 - regression_loss: 9.8268 - treatment_acc: 0.8110 - val_loss: 29.7931 - val_tarreg_loss: 22.1130 - val_regression_loss: 10.7796 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 98/300\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 24.9527 - tarreg_loss: 20.2695 - regression_loss: 9.8444 - treatment_acc: 0.8154 — ate_err: 0.1310  — aipw_err: 0.1886 — tarreg_err: 0.1316 — cate_err: 2.1928 — cate_nn_err: 1.5054 \n",
      " — ate_err: 0.3436  — aipw_err: 0.6696 — tarreg_err: 0.3430 — cate_err: 3.0375 — cate_nn_err: 1.7533 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 24.9113 - tarreg_loss: 20.2280 - regression_loss: 9.8227 - treatment_acc: 0.8110 - val_loss: 28.9590 - val_tarreg_loss: 21.3457 - val_regression_loss: 10.3920 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 99/300\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 25.0507 - tarreg_loss: 20.3676 - regression_loss: 9.8928 - treatment_acc: 0.8086  — ate_err: 0.2123  — aipw_err: 0.1653 — tarreg_err: 0.2113 — cate_err: 2.2476 — cate_nn_err: 1.4817 \n",
      " — ate_err: 0.2519  — aipw_err: 0.6956 — tarreg_err: 0.2529 — cate_err: 3.0918 — cate_nn_err: 1.8166 \n",
      "21/21 [==============================] - 1s 30ms/step - loss: 24.8994 - tarreg_loss: 20.2163 - regression_loss: 9.8178 - treatment_acc: 0.8110 - val_loss: 30.8374 - val_tarreg_loss: 22.9847 - val_regression_loss: 11.2135 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 100/300\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 24.9035 - tarreg_loss: 20.2207 - regression_loss: 9.8202 - treatment_acc: 0.8102 — ate_err: 0.1074  — aipw_err: 0.1767 — tarreg_err: 0.1078 — cate_err: 2.1865 — cate_nn_err: 1.4932 \n",
      " — ate_err: 0.3813  — aipw_err: 0.6796 — tarreg_err: 0.3809 — cate_err: 3.0348 — cate_nn_err: 1.7365 \n",
      "21/21 [==============================] - 1s 31ms/step - loss: 24.9375 - tarreg_loss: 20.2547 - regression_loss: 9.8376 - treatment_acc: 0.8110 - val_loss: 28.7745 - val_tarreg_loss: 21.2253 - val_regression_loss: 10.3329 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 101/300\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 24.8838 - tarreg_loss: 20.2013 - regression_loss: 9.8119 - treatment_acc: 0.8174  — ate_err: 0.1845  — aipw_err: 0.1768 — tarreg_err: 0.1860 — cate_err: 2.2013 — cate_nn_err: 1.4905 \n",
      " — ate_err: 0.2873  — aipw_err: 0.6762 — tarreg_err: 0.2858 — cate_err: 3.0279 — cate_nn_err: 1.7421 \n",
      "21/21 [==============================] - 1s 30ms/step - loss: 24.9633 - tarreg_loss: 20.2808 - regression_loss: 9.8503 - treatment_acc: 0.8110 - val_loss: 29.7136 - val_tarreg_loss: 22.0828 - val_regression_loss: 10.7609 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 102/300\n",
      "21/21 [==============================] - ETA: 0s - loss: 24.8913 - tarreg_loss: 20.2090 - regression_loss: 9.8148 - treatment_acc: 0.8110 — ate_err: 0.2682  — aipw_err: 0.2321 — tarreg_err: 0.2717 — cate_err: 2.2356 — cate_nn_err: 1.4864 \n",
      " — ate_err: 0.1926  — aipw_err: 0.6145 — tarreg_err: 0.1892 — cate_err: 3.0622 — cate_nn_err: 1.7588 \n",
      "21/21 [==============================] - 1s 30ms/step - loss: 24.8913 - tarreg_loss: 20.2090 - regression_loss: 9.8148 - treatment_acc: 0.8110 - val_loss: 30.5564 - val_tarreg_loss: 22.7806 - val_regression_loss: 11.1066 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 103/300\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 25.2838 - tarreg_loss: 20.6018 - regression_loss: 10.0115 - treatment_acc: 0.8125 — ate_err: 0.2952  — aipw_err: 0.1862 — tarreg_err: 0.2958 — cate_err: 2.2276 — cate_nn_err: 1.4854 \n",
      " — ate_err: 0.1652  — aipw_err: 0.6637 — tarreg_err: 0.1646 — cate_err: 3.0343 — cate_nn_err: 1.7592 \n",
      "21/21 [==============================] - 1s 36ms/step - loss: 24.8035 - tarreg_loss: 20.1215 - regression_loss: 9.7715 - treatment_acc: 0.8110 - val_loss: 31.3322 - val_tarreg_loss: 23.4352 - val_regression_loss: 11.4382 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 104/300\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 24.7987 - tarreg_loss: 20.1170 - regression_loss: 9.7715 - treatment_acc: 0.8142 — ate_err: 0.2124  — aipw_err: 0.2062 — tarreg_err: 0.2132 — cate_err: 2.2353 — cate_nn_err: 1.5004 \n",
      " — ate_err: 0.2425  — aipw_err: 0.6362 — tarreg_err: 0.2417 — cate_err: 3.0830 — cate_nn_err: 1.7582 \n",
      "21/21 [==============================] - 1s 38ms/step - loss: 24.6942 - tarreg_loss: 20.0125 - regression_loss: 9.7180 - treatment_acc: 0.8110 - val_loss: 30.4971 - val_tarreg_loss: 22.6823 - val_regression_loss: 11.0622 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 105/300\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 24.8491 - tarreg_loss: 20.1676 - regression_loss: 9.7951 - treatment_acc: 0.8094 — ate_err: 0.2118  — aipw_err: 0.1832 — tarreg_err: 0.2133 — cate_err: 2.2122 — cate_nn_err: 1.5003 \n",
      " — ate_err: 0.2602  — aipw_err: 0.6595 — tarreg_err: 0.2587 — cate_err: 3.0354 — cate_nn_err: 1.7655 \n",
      "21/21 [==============================] - 1s 36ms/step - loss: 24.8659 - tarreg_loss: 20.1844 - regression_loss: 9.8037 - treatment_acc: 0.8110 - val_loss: 29.9545 - val_tarreg_loss: 22.3288 - val_regression_loss: 10.8853 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 106/300\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 24.9268 - tarreg_loss: 20.2458 - regression_loss: 9.8356 - treatment_acc: 0.8133 — ate_err: 0.2661  — aipw_err: 0.2163 — tarreg_err: 0.2688 — cate_err: 2.2455 — cate_nn_err: 1.4937 \n",
      " — ate_err: 0.1847  — aipw_err: 0.6217 — tarreg_err: 0.1821 — cate_err: 3.0783 — cate_nn_err: 1.7721 \n",
      "21/21 [==============================] - 1s 33ms/step - loss: 24.7136 - tarreg_loss: 20.0325 - regression_loss: 9.7284 - treatment_acc: 0.8110 - val_loss: 30.8360 - val_tarreg_loss: 22.9956 - val_regression_loss: 11.2167 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 107/300\n",
      "15/21 [====================>.........] - ETA: 0s - loss: 24.8983 - tarreg_loss: 20.2174 - regression_loss: 9.8226 - treatment_acc: 0.8198 — ate_err: 0.2870  — aipw_err: 0.2074 — tarreg_err: 0.2900 — cate_err: 2.2116 — cate_nn_err: 1.4995 \n",
      " — ate_err: 0.1637  — aipw_err: 0.6275 — tarreg_err: 0.1607 — cate_err: 3.0178 — cate_nn_err: 1.7439 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 24.7731 - tarreg_loss: 20.0922 - regression_loss: 9.7584 - treatment_acc: 0.8110 - val_loss: 30.7651 - val_tarreg_loss: 22.9851 - val_regression_loss: 11.2111 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 108/300\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 24.7284 - tarreg_loss: 20.0479 - regression_loss: 9.7352 - treatment_acc: 0.8109 — ate_err: 0.1864  — aipw_err: 0.1878 — tarreg_err: 0.1875 — cate_err: 2.2636 — cate_nn_err: 1.4922 \n",
      " — ate_err: 0.2684  — aipw_err: 0.6449 — tarreg_err: 0.2673 — cate_err: 3.1243 — cate_nn_err: 1.7898 \n",
      "21/21 [==============================] - 1s 30ms/step - loss: 24.9313 - tarreg_loss: 20.2507 - regression_loss: 9.8368 - treatment_acc: 0.8110 - val_loss: 30.6481 - val_tarreg_loss: 22.8380 - val_regression_loss: 11.1415 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 109/300\n",
      "21/21 [==============================] - ETA: 0s - loss: 24.7218 - tarreg_loss: 20.0414 - regression_loss: 9.7344 - treatment_acc: 0.8110\n",
      "Epoch 00109: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      " — ate_err: 0.1862  — aipw_err: 0.1928 — tarreg_err: 0.1879 — cate_err: 2.2153 — cate_nn_err: 1.4991 \n",
      " — ate_err: 0.2736  — aipw_err: 0.6367 — tarreg_err: 0.2719 — cate_err: 3.0548 — cate_nn_err: 1.7576 \n",
      "21/21 [==============================] - 1s 30ms/step - loss: 24.7218 - tarreg_loss: 20.0414 - regression_loss: 9.7344 - treatment_acc: 0.8110 - val_loss: 30.1422 - val_tarreg_loss: 22.4077 - val_regression_loss: 10.9264 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 110/300\n",
      "16/21 [=====================>........] - ETA: 0s - loss: 24.9961 - tarreg_loss: 20.3160 - regression_loss: 9.8731 - treatment_acc: 0.8174  — ate_err: 0.2083  — aipw_err: 0.2211 — tarreg_err: 0.2115 — cate_err: 2.2384 — cate_nn_err: 1.4936 \n",
      " — ate_err: 0.2556  — aipw_err: 0.6082 — tarreg_err: 0.2524 — cate_err: 3.0785 — cate_nn_err: 1.7726 \n",
      "21/21 [==============================] - 1s 32ms/step - loss: 24.5220 - tarreg_loss: 19.8419 - regression_loss: 9.6346 - treatment_acc: 0.8110 - val_loss: 29.9233 - val_tarreg_loss: 22.2241 - val_regression_loss: 10.8333 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n",
      "Epoch 111/300\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 24.7126 - tarreg_loss: 20.0327 - regression_loss: 9.7302 - treatment_acc: 0.8094 — ate_err: 0.1564  — aipw_err: 0.1713 — tarreg_err: 0.1551 — cate_err: 2.2213 — cate_nn_err: 1.4900 \n",
      " — ate_err: 0.3178  — aipw_err: 0.6619 — tarreg_err: 0.3190 — cate_err: 3.0682 — cate_nn_err: 1.7698 \n",
      "21/21 [==============================] - 1s 30ms/step - loss: 24.4708 - tarreg_loss: 19.7909 - regression_loss: 9.6098 - treatment_acc: 0.8110 - val_loss: 29.5006 - val_tarreg_loss: 21.8580 - val_regression_loss: 10.6545 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n",
      "Epoch 112/300\n",
      "14/21 [===================>..........] - ETA: 0s - loss: 24.5070 - tarreg_loss: 19.8271 - regression_loss: 9.6273 - treatment_acc: 0.8114 — ate_err: 0.1502  — aipw_err: 0.1691 — tarreg_err: 0.1498 — cate_err: 2.2083 — cate_nn_err: 1.4889 \n",
      " — ate_err: 0.3264  — aipw_err: 0.6621 — tarreg_err: 0.3268 — cate_err: 3.0474 — cate_nn_err: 1.7689 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 24.5885 - tarreg_loss: 19.9086 - regression_loss: 9.6680 - treatment_acc: 0.8110 - val_loss: 29.2633 - val_tarreg_loss: 21.6811 - val_regression_loss: 10.5657 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n",
      "Epoch 113/300\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 24.3396 - tarreg_loss: 19.6599 - regression_loss: 9.5451 - treatment_acc: 0.8125 — ate_err: 0.2306  — aipw_err: 0.1875 — tarreg_err: 0.2315 — cate_err: 2.2309 — cate_nn_err: 1.4885 \n",
      " — ate_err: 0.2291  — aipw_err: 0.6412 — tarreg_err: 0.2282 — cate_err: 3.0605 — cate_nn_err: 1.7714 \n",
      "21/21 [==============================] - 1s 29ms/step - loss: 24.4447 - tarreg_loss: 19.7650 - regression_loss: 9.5975 - treatment_acc: 0.8110 - val_loss: 30.4944 - val_tarreg_loss: 22.7132 - val_regression_loss: 11.0805 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x160bec4c0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "%load_ext tensorboard\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "val_split=0.2\n",
    "batch_size=64\n",
    "verbose=1\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0)\n",
    "\n",
    "sgd_callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='val_loss', patience=40, min_delta=0.),\n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0., cooldown=0, min_lr=0),\n",
    "        tensorboard_callback,\n",
    "        TarReg_Metrics(data_train,'train', verbose=verbose),\n",
    "        TarReg_Metrics(data_valid,'valid', verbose=verbose) \n",
    "   ]\n",
    "\n",
    "sgd_lr = 1e-5\n",
    "momentum = 0.9\n",
    "\n",
    "dragonnet_model=make_dragonnet(data_train['x'].shape[1],.01)\n",
    "tarreg_loss=TarReg_Loss(alpha=1)\n",
    "\n",
    "dragonnet_model.compile(optimizer=SGD(lr=sgd_lr, momentum=momentum, nesterov=True),\n",
    "                      loss=tarreg_loss,\n",
    "                 metrics=[tarreg_loss,tarreg_loss.regression_loss,tarreg_loss.treatment_acc])\n",
    "\n",
    "dragonnet_model.fit(x=data_train['x'],y=np.concatenate([data_train['ys'], data_train['t']], 1),\n",
    "                 callbacks=sgd_callbacks,\n",
    "                  validation_data=[data_valid['x'],np.concatenate([data_valid['ys'], data_valid['t']], 1)],\n",
    "                  epochs=300,\n",
    "                  batch_size=batch_size,\n",
    "                  verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 45269), started 2 days, 2:45:33 ago. (Use '!kill 45269' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-cdbbfe4a158d77de\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-cdbbfe4a158d77de\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
