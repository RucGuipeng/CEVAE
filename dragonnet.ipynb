{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.metrics import binary_accuracy\n",
    "from tensorflow.keras.losses import Loss\n",
    "def make_aipw(input_dim, reg_l2):\n",
    "\n",
    "    x = Input(shape=(input_dim,), name='input')\n",
    "    # representation\n",
    "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_1')(x)\n",
    "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_2')(phi)\n",
    "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_3')(phi)\n",
    "\n",
    "    # HYPOTHESIS\n",
    "    y0_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_1')(phi)\n",
    "    y1_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_1')(phi)\n",
    "\n",
    "    # second layer\n",
    "    y0_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_2')(y0_hidden)\n",
    "    y1_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_2')(y1_hidden)\n",
    "\n",
    "    # third\n",
    "    y0_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y0_predictions')(y0_hidden)\n",
    "    y1_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y1_predictions')(y1_hidden)\n",
    "\n",
    "    #propensity prediction\n",
    "    #Note that the activation is actually sigmoid, but we will squish it in the loss function for numerical stability reasons\n",
    "    t_prediction = Dense(units=1,activation=None,name='t_prediction')(phi)\n",
    "\n",
    "    concat_pred = Concatenate(1)([y0_predictions, y1_predictions,t_prediction,phi])\n",
    "    model = Model(inputs=x, outputs=concat_pred)\n",
    "    return model\n",
    "\n",
    "class Base_Loss(Loss):\n",
    "    #initialize instance attributes\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.name='standard_loss'\n",
    "\n",
    "    def split_pred(self,concat_pred):\n",
    "        #generic helper to make sure we dont make mistakes\n",
    "        preds={}\n",
    "        preds['y0_pred'] = concat_pred[:, 0]\n",
    "        preds['y1_pred'] = concat_pred[:, 1]\n",
    "        preds['t_pred'] = concat_pred[:, 2]\n",
    "        preds['phi'] = concat_pred[:, 3:]\n",
    "        return preds\n",
    "\n",
    "    #for logging purposes only\n",
    "    def treatment_acc(self,concat_true,concat_pred):\n",
    "        t_true = concat_true[:, 1]\n",
    "        p = self.split_pred(concat_pred)\n",
    "        #Since this isn't used as a loss, I've used tf.reduce_mean for interpretability\n",
    "        return tf.reduce_mean(binary_accuracy(t_true, tf.math.sigmoid(p['t_pred']), threshold=0.5))\n",
    "\n",
    "    def treatment_bce(self,concat_true,concat_pred):\n",
    "        t_true = concat_true[:, 1]\n",
    "        p = self.split_pred(concat_pred)\n",
    "        lossP = tf.reduce_sum(binary_crossentropy(t_true,p['t_pred'],from_logits=True))\n",
    "        return lossP\n",
    "    \n",
    "    def regression_loss(self,concat_true,concat_pred):\n",
    "        y_true = concat_true[:, 0]\n",
    "        t_true = concat_true[:, 1]\n",
    "        p = self.split_pred(concat_pred)\n",
    "        loss0 = tf.reduce_sum((1. - t_true) * tf.square(y_true - p['y0_pred']))\n",
    "        loss1 = tf.reduce_sum(t_true * tf.square(y_true - p['y1_pred']))\n",
    "        return loss0+loss1\n",
    "\n",
    "    def standard_loss(self,concat_true,concat_pred):\n",
    "        lossR = self.regression_loss(concat_true,concat_pred)\n",
    "        lossP = self.treatment_bce(concat_true,concat_pred)\n",
    "        return lossR + self.alpha * lossP\n",
    "\n",
    "    #compute loss\n",
    "    def call(self, concat_true, concat_pred):        \n",
    "        return self.standard_loss(concat_true,concat_pred)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "def pdist2sq(A, B):\n",
    "    #helper for PEHEnn\n",
    "    #calculates squared euclidean distance between rows of two matrices  \n",
    "    #https://gist.github.com/mbsariyildiz/34cdc26afb630e8cae079048eef91865\n",
    "    # squared norms of each row in A and B\n",
    "    na = tf.reduce_sum(tf.square(A), 1)\n",
    "    nb = tf.reduce_sum(tf.square(B), 1)    \n",
    "    # na as a row and nb as a column vectors\n",
    "    na = tf.reshape(na, [-1, 1])\n",
    "    nb = tf.reshape(nb, [1, -1])\n",
    "    # return pairwise euclidean difference matrix\n",
    "    D=tf.reduce_sum((tf.expand_dims(A, 1)-tf.expand_dims(B, 0))**2,2) \n",
    "    return D\n",
    "\n",
    "from evaluation import Full_Metrics, metrics_for_cevae\n",
    "\n",
    "#https://towardsdatascience.com/implementing-macro-f1-score-in-keras-what-not-to-do-e9f1aa04029d\n",
    "class AIPW_Metrics(Callback):\n",
    "    def __init__(self,data,name='train', verbose=0):   \n",
    "        super(AIPW_Metrics, self).__init__()\n",
    "        self.data=data #feed the callback the full dataset\n",
    "        self.verbose=verbose\n",
    "        self.name = name\n",
    "\n",
    "        #needed for PEHEnn; Called in self.find_ynn\n",
    "        self.data['o_idx']=tf.range(self.data['t'].shape[0])\n",
    "        self.data['c_idx']=self.data['o_idx'][self.data['t'].squeeze()==0] #These are the indices of the control units\n",
    "        self.data['t_idx']=self.data['o_idx'][self.data['t'].squeeze()==1] #These are the indices of the treated units\n",
    "    \n",
    "    def split_pred(self,concat_pred):\n",
    "        preds={}\n",
    "        preds['y0_pred'] = self.data['y_scaler'].inverse_transform(np.reshape(concat_pred[:, 0],[-1,1]))[:,0]\n",
    "        preds['y1_pred'] = self.data['y_scaler'].inverse_transform(np.reshape(concat_pred[:, 1],[-1,1]))[:,0]\n",
    "        preds['t_pred'] = concat_pred[:, 2]\n",
    "        preds['phi'] = concat_pred[:, 3:]\n",
    "        return preds\n",
    "\n",
    "    def find_ynn(self, Phi):\n",
    "        #helper for PEHEnn\n",
    "        PhiC, PhiT =tf.dynamic_partition(Phi,tf.cast(tf.squeeze(self.data['t']),tf.int32),2) #separate control and treated reps\n",
    "        dists=tf.sqrt(pdist2sq(PhiC,PhiT)) #calculate squared distance then sqrt to get euclidean\n",
    "        yT_nn_idx=tf.gather(self.data['c_idx'],tf.argmin(dists,axis=0),1) #get c_idxs of smallest distances for treated units\n",
    "        yC_nn_idx=tf.gather(self.data['t_idx'],tf.argmin(dists,axis=1),1) #get t_idxs of smallest distances for control units\n",
    "        yT_nn=tf.gather(self.data['y'],yT_nn_idx,1) #now use these to retrieve y values\n",
    "        yC_nn=tf.gather(self.data['y'],yC_nn_idx,1)\n",
    "        y_nn=tf.dynamic_stitch([self.data['t_idx'],self.data['c_idx']],[yT_nn,yC_nn]) #stitch em back up!\n",
    "        return y_nn\n",
    "\n",
    "    def PEHEnn(self,concat_pred):\n",
    "        p = self.split_pred(concat_pred)\n",
    "        y_nn = self.find_ynn(p['phi']) #now its 3 plus because \n",
    "        cate_nn_err=tf.reduce_mean( tf.square( (1-2*self.data['t']) * (y_nn-self.data['y']) - (p['y1_pred']-p['y0_pred']) ) )\n",
    "        return cate_nn_err\n",
    "\n",
    "    def ATE(self,concat_pred):\n",
    "        p = self.split_pred(concat_pred)\n",
    "        return p['y1_pred']-p['y0_pred']\n",
    "\n",
    "    def PEHE(self,concat_pred):\n",
    "        #simulation only\n",
    "        p = self.split_pred(concat_pred)\n",
    "        cate_err=tf.reduce_mean( tf.square( ( (self.data['mu_1']-self.data['mu_0']) - (p['y1_pred']-p['y0_pred']) ) ) )\n",
    "        return cate_err \n",
    "   \n",
    "    #THIS IS THE NEW PART\n",
    "    def AIPW(self,concat_pred):\n",
    "        p = self.split_pred(concat_pred)\n",
    "        t_pred=tf.math.sigmoid(p['t_pred'])\n",
    "        t_pred = (t_pred + 0.001) / 1.002 # a little numerical stability trick implemented by Shi\n",
    "        y_pred = p['y0_pred'] * (1 - self.data['t']) + p['y1_pred'] * self.data['t']\n",
    "        #cc stands for clever covariate which is I think what it's called in TMLE lit\n",
    "        cc = self.data['t'] * (1.0 / p['t_pred']) - (1.0 - self.data['t']) / (1.0 - p['t_pred'])\n",
    "        cate = cc * (self.data['y'] - y_pred) + p['y1_pred'] - p['y0_pred']\n",
    "        return cate\n",
    "    \n",
    "    def rmse(self,concat_pred):\n",
    "        p = self.split_pred(concat_pred)\n",
    "        y_pred = p['y0_pred'] * (1 - self.data['t']) + p['y1_pred'] * self.data['t']\n",
    "        return tf.math.square(self.data['y']-y_pred)\n",
    "    def regret(self,concat_pred):\n",
    "        p = self.split_pred(concat_pred)\n",
    "        t_pred=tf.math.sigmoid(p['t_pred'])\n",
    "        y_pred = p['y0_pred'] * (1 - t_pred) + p['y1_pred'] * t_pred\n",
    "        return tf.math.square(self.data['y']-y_pred)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        concat_pred=self.model.predict(self.data['x'])\n",
    "        #Calculate Empirical Metrics        \n",
    "        ate_pred=tf.reduce_mean(self.ATE(concat_pred)); tf.summary.scalar(f'{self.name}_ate', data=ate_pred, step=epoch)\n",
    "        pehe_nn=self.PEHEnn(concat_pred); tf.summary.scalar(f'{self.name}_cate_nn_err', data=tf.sqrt(pehe_nn), step=epoch)\n",
    "        aipw=tf.reduce_mean(self.AIPW(concat_pred)); tf.summary.scalar(f'{self.name}_aipw', data=aipw, step=epoch)\n",
    "        rmse = tf.reduce_mean(self.rmse(concat_pred)); tf.summary.scalar(f'{self.name}_rmse', data=aipw, step=epoch)\n",
    "        regret = tf.reduce_mean(self.regret(concat_pred)); tf.summary.scalar(f'{self.name}_regret', data=aipw, step=epoch)\n",
    "        #Simulation Metrics\n",
    "        ate_true=tf.reduce_mean(self.data['mu_1']-self.data['mu_0'])\n",
    "        ate_err=tf.abs(ate_true-ate_pred); tf.summary.scalar(f'{self.name}_ate_err', data=ate_err, step=epoch)\n",
    "        pehe =self.PEHE(concat_pred); tf.summary.scalar(f'{self.name}_cate_err', data=tf.sqrt(pehe), step=epoch)\n",
    "        aipw_err =self.PEHE(concat_pred); tf.summary.scalar(f'{self.name}_aipw_err', data=aipw_err, step=epoch)\n",
    "        out_str=f' — ate_err: {ate_err:.4f}  — aipw_err: {aipw_err:.4f} — cate_err: {tf.sqrt(pehe):.4f} — cate_nn_err: {tf.sqrt(pehe_nn):.4f} - rmse:{rmse:.4f} - regret:{regret:.4f} '\n",
    "        \n",
    "        if self.verbose > 0: print(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 “ihdp_npci_1-100.train.npz” 已经存在；不获取。\n",
      "\n",
      "文件 “ihdp_npci_1-100.test.npz” 已经存在；不获取。\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1344, 25)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title First load the data! (Click Play)\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz \n",
    "\n",
    "def load_IHDP_data(training_data,testing_data,i=7):\n",
    "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "        train_data=np.load(trf); test_data=np.load(tef)\n",
    "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
    "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
    "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
    "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    "\n",
    "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "        data['y']=data['y'].reshape(-1,1)\n",
    "        \n",
    "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "\n",
    "    return data\n",
    "\n",
    "data_train=load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.train.npz')\n",
    "data_valid=load_IHDP_data(training_data='./ihdp_npci_1-100.test.npz',testing_data='./ihdp_npci_1-100.test.npz')\n",
    "np.shape(data_train['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      " 1/21 [>.............................] - ETA: 22s - loss: 57.7918 - standard_loss: 53.0910 - regression_loss: 52.3950 - treatment_acc: 0.5156WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0082s vs `on_train_batch_end` time: 0.0083s). Check your callbacks.\n",
      "21/21 [==============================] - 3s 79ms/step - loss: 62.5602 - standard_loss: 57.8595 - regression_loss: 57.1628 - treatment_acc: 0.4821 - val_loss: 45.7890 - val_standard_loss: 36.1322 - val_regression_loss: 35.4342 - val_treatment_acc: 0.3807 - lr: 1.0000e-05\n",
      "Epoch 2/300\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 42.2002 - standard_loss: 37.4991 - regression_loss: 36.8031 - treatment_acc: 0.4814 - val_loss: 32.3338 - val_standard_loss: 24.4891 - val_regression_loss: 23.7900 - val_treatment_acc: 0.4010 - lr: 1.0000e-05\n",
      "Epoch 3/300\n",
      "21/21 [==============================] - 1s 35ms/step - loss: 32.2097 - standard_loss: 27.5076 - regression_loss: 26.8122 - treatment_acc: 0.4903 - val_loss: 26.8289 - val_standard_loss: 19.6331 - val_regression_loss: 18.9353 - val_treatment_acc: 0.4626 - lr: 1.0000e-05\n",
      "Epoch 4/300\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 27.9789 - standard_loss: 23.2757 - regression_loss: 22.5822 - treatment_acc: 0.5112 - val_loss: 24.4687 - val_standard_loss: 17.5676 - val_regression_loss: 16.8726 - val_treatment_acc: 0.5033 - lr: 1.0000e-05\n",
      "Epoch 5/300\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 25.9712 - standard_loss: 21.2676 - regression_loss: 20.5768 - treatment_acc: 0.5327 - val_loss: 23.3487 - val_standard_loss: 16.5858 - val_regression_loss: 15.8936 - val_treatment_acc: 0.5137 - lr: 1.0000e-05\n",
      "Epoch 6/300\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 24.5185 - standard_loss: 19.8149 - regression_loss: 19.1266 - treatment_acc: 0.5603 - val_loss: 22.6965 - val_standard_loss: 16.0042 - val_regression_loss: 15.3145 - val_treatment_acc: 0.5137 - lr: 1.0000e-05\n",
      "Epoch 7/300\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 23.4969 - standard_loss: 18.7932 - regression_loss: 18.1072 - treatment_acc: 0.5863 - val_loss: 22.1767 - val_standard_loss: 15.5344 - val_regression_loss: 14.8475 - val_treatment_acc: 0.5649 - lr: 1.0000e-05\n",
      "Epoch 8/300\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 22.5758 - standard_loss: 17.8722 - regression_loss: 17.1885 - treatment_acc: 0.6064 - val_loss: 21.4238 - val_standard_loss: 14.8963 - val_regression_loss: 14.2124 - val_treatment_acc: 0.6165 - lr: 1.0000e-05\n",
      "Epoch 9/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 21.8568 - standard_loss: 17.1533 - regression_loss: 16.4720 - treatment_acc: 0.6287 - val_loss: 21.2872 - val_standard_loss: 14.7571 - val_regression_loss: 14.0756 - val_treatment_acc: 0.6269 - lr: 1.0000e-05\n",
      "Epoch 10/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 21.2417 - standard_loss: 16.5382 - regression_loss: 15.8593 - treatment_acc: 0.6488 - val_loss: 20.8754 - val_standard_loss: 14.3958 - val_regression_loss: 13.7170 - val_treatment_acc: 0.6581 - lr: 1.0000e-05\n",
      "Epoch 11/300\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 20.7670 - standard_loss: 16.0635 - regression_loss: 15.3866 - treatment_acc: 0.6600 - val_loss: 20.7394 - val_standard_loss: 14.2718 - val_regression_loss: 13.5956 - val_treatment_acc: 0.6686 - lr: 1.0000e-05\n",
      "Epoch 12/300\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 20.4856 - standard_loss: 15.7824 - regression_loss: 15.1077 - treatment_acc: 0.6801 - val_loss: 20.7954 - val_standard_loss: 14.3024 - val_regression_loss: 13.6284 - val_treatment_acc: 0.7098 - lr: 1.0000e-05\n",
      "Epoch 13/300\n",
      "21/21 [==============================] - 1s 40ms/step - loss: 20.1471 - standard_loss: 15.4439 - regression_loss: 14.7712 - treatment_acc: 0.6927 - val_loss: 20.7778 - val_standard_loss: 14.2772 - val_regression_loss: 13.6056 - val_treatment_acc: 0.7206 - lr: 1.0000e-05\n",
      "Epoch 14/300\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 19.8758 - standard_loss: 15.1728 - regression_loss: 14.5021 - treatment_acc: 0.7039 - val_loss: 20.8930 - val_standard_loss: 14.3625 - val_regression_loss: 13.6931 - val_treatment_acc: 0.7519 - lr: 1.0000e-05\n",
      "Epoch 15/300\n",
      "21/21 [==============================] - 1s 35ms/step - loss: 19.7613 - standard_loss: 15.0585 - regression_loss: 14.3897 - treatment_acc: 0.7210 - val_loss: 20.5084 - val_standard_loss: 14.0320 - val_regression_loss: 13.3650 - val_treatment_acc: 0.7623 - lr: 1.0000e-05\n",
      "Epoch 16/300\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 19.5308 - standard_loss: 14.8283 - regression_loss: 14.1612 - treatment_acc: 0.7336 - val_loss: 20.0722 - val_standard_loss: 13.6628 - val_regression_loss: 12.9983 - val_treatment_acc: 0.7936 - lr: 1.0000e-05\n",
      "Epoch 17/300\n",
      "21/21 [==============================] - 1s 35ms/step - loss: 19.3182 - standard_loss: 14.6159 - regression_loss: 13.9507 - treatment_acc: 0.7426 - val_loss: 20.6202 - val_standard_loss: 14.1132 - val_regression_loss: 13.4504 - val_treatment_acc: 0.8139 - lr: 1.0000e-05\n",
      "Epoch 18/300\n",
      "21/21 [==============================] - 1s 40ms/step - loss: 19.1428 - standard_loss: 14.4407 - regression_loss: 13.7772 - treatment_acc: 0.7552 - val_loss: 20.1827 - val_standard_loss: 13.7470 - val_regression_loss: 13.0865 - val_treatment_acc: 0.8546 - lr: 1.0000e-05\n",
      "Epoch 19/300\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 19.0550 - standard_loss: 14.3531 - regression_loss: 13.6914 - treatment_acc: 0.7664 - val_loss: 20.3767 - val_standard_loss: 13.8978 - val_regression_loss: 13.2391 - val_treatment_acc: 0.8651 - lr: 1.0000e-05\n",
      "Epoch 20/300\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 18.9555 - standard_loss: 14.2539 - regression_loss: 13.5938 - treatment_acc: 0.7671 - val_loss: 20.2253 - val_standard_loss: 13.7729 - val_regression_loss: 13.1163 - val_treatment_acc: 0.8651 - lr: 1.0000e-05\n",
      "Epoch 21/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 18.8372 - standard_loss: 14.1359 - regression_loss: 13.4776 - treatment_acc: 0.7723 - val_loss: 19.9615 - val_standard_loss: 13.5439 - val_regression_loss: 12.8894 - val_treatment_acc: 0.8651 - lr: 1.0000e-05\n",
      "Epoch 22/300\n",
      "21/21 [==============================] - 1s 36ms/step - loss: 18.6794 - standard_loss: 13.9784 - regression_loss: 13.3217 - treatment_acc: 0.7857 - val_loss: 20.3812 - val_standard_loss: 13.8935 - val_regression_loss: 13.2405 - val_treatment_acc: 0.8651 - lr: 1.0000e-05\n",
      "Epoch 23/300\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 18.5914 - standard_loss: 13.8907 - regression_loss: 13.2355 - treatment_acc: 0.7894 - val_loss: 19.6221 - val_standard_loss: 13.2556 - val_regression_loss: 12.6051 - val_treatment_acc: 0.8755 - lr: 1.0000e-05\n",
      "Epoch 24/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 18.5193 - standard_loss: 13.8189 - regression_loss: 13.1654 - treatment_acc: 0.7946 - val_loss: 20.0148 - val_standard_loss: 13.5825 - val_regression_loss: 12.9334 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 25/300\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 18.4036 - standard_loss: 13.7035 - regression_loss: 13.0515 - treatment_acc: 0.7991 - val_loss: 19.9848 - val_standard_loss: 13.5527 - val_regression_loss: 12.9055 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 26/300\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 18.3086 - standard_loss: 13.6087 - regression_loss: 12.9582 - treatment_acc: 0.8013 - val_loss: 19.6761 - val_standard_loss: 13.2937 - val_regression_loss: 12.6484 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 27/300\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 18.2726 - standard_loss: 13.5730 - regression_loss: 12.9240 - treatment_acc: 0.7999 - val_loss: 19.7640 - val_standard_loss: 13.3602 - val_regression_loss: 12.7166 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 28/300\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 18.1747 - standard_loss: 13.4755 - regression_loss: 12.8280 - treatment_acc: 0.7991 - val_loss: 19.5809 - val_standard_loss: 13.2094 - val_regression_loss: 12.5676 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 29/300\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 18.1141 - standard_loss: 13.4151 - regression_loss: 12.7690 - treatment_acc: 0.8036 - val_loss: 19.6299 - val_standard_loss: 13.2426 - val_regression_loss: 12.6024 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 30/300\n",
      "21/21 [==============================] - 1s 38ms/step - loss: 18.0194 - standard_loss: 13.3207 - regression_loss: 12.6760 - treatment_acc: 0.8065 - val_loss: 19.9446 - val_standard_loss: 13.5209 - val_regression_loss: 12.8823 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 31/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 18.0550 - standard_loss: 13.3566 - regression_loss: 12.7133 - treatment_acc: 0.8080 - val_loss: 19.5530 - val_standard_loss: 13.1852 - val_regression_loss: 12.5483 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 32/300\n",
      "21/21 [==============================] - 1s 36ms/step - loss: 17.8546 - standard_loss: 13.1565 - regression_loss: 12.5146 - treatment_acc: 0.8080 - val_loss: 19.1432 - val_standard_loss: 12.8230 - val_regression_loss: 12.1880 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 33/300\n",
      "21/21 [==============================] - 1s 37ms/step - loss: 17.8035 - standard_loss: 13.1057 - regression_loss: 12.4653 - treatment_acc: 0.8080 - val_loss: 19.5424 - val_standard_loss: 13.1567 - val_regression_loss: 12.5231 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 34/300\n",
      "21/21 [==============================] - 1s 37ms/step - loss: 17.7582 - standard_loss: 13.0607 - regression_loss: 12.4217 - treatment_acc: 0.8095 - val_loss: 19.3318 - val_standard_loss: 12.9823 - val_regression_loss: 12.3503 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 35/300\n",
      "21/21 [==============================] - 1s 44ms/step - loss: 17.6922 - standard_loss: 12.9950 - regression_loss: 12.3572 - treatment_acc: 0.8103 - val_loss: 19.3247 - val_standard_loss: 12.9636 - val_regression_loss: 12.3332 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 36/300\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 17.6188 - standard_loss: 12.9218 - regression_loss: 12.2854 - treatment_acc: 0.8110 - val_loss: 19.1607 - val_standard_loss: 12.8325 - val_regression_loss: 12.2038 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 37/300\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 17.5187 - standard_loss: 12.8220 - regression_loss: 12.1869 - treatment_acc: 0.8110 - val_loss: 19.0769 - val_standard_loss: 12.7549 - val_regression_loss: 12.1277 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 38/300\n",
      "21/21 [==============================] - 1s 41ms/step - loss: 17.4694 - standard_loss: 12.7729 - regression_loss: 12.1392 - treatment_acc: 0.8110 - val_loss: 18.9812 - val_standard_loss: 12.6745 - val_regression_loss: 12.0489 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 39/300\n",
      "21/21 [==============================] - 1s 41ms/step - loss: 17.4866 - standard_loss: 12.7906 - regression_loss: 12.1582 - treatment_acc: 0.8110 - val_loss: 19.1157 - val_standard_loss: 12.7836 - val_regression_loss: 12.1594 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 40/300\n",
      "21/21 [==============================] - 1s 40ms/step - loss: 17.3955 - standard_loss: 12.6997 - regression_loss: 12.0685 - treatment_acc: 0.8110 - val_loss: 19.5068 - val_standard_loss: 13.1211 - val_regression_loss: 12.4983 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 41/300\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 17.3380 - standard_loss: 12.6425 - regression_loss: 12.0125 - treatment_acc: 0.8110 - val_loss: 18.6577 - val_standard_loss: 12.3934 - val_regression_loss: 11.7723 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 42/300\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 17.3225 - standard_loss: 12.6272 - regression_loss: 11.9986 - treatment_acc: 0.8110 - val_loss: 18.7262 - val_standard_loss: 12.4522 - val_regression_loss: 11.8327 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 43/300\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 17.2888 - standard_loss: 12.5938 - regression_loss: 11.9664 - treatment_acc: 0.8110 - val_loss: 19.0409 - val_standard_loss: 12.7118 - val_regression_loss: 12.0935 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 44/300\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 17.1852 - standard_loss: 12.4905 - regression_loss: 11.8643 - treatment_acc: 0.8110 - val_loss: 19.3658 - val_standard_loss: 12.9876 - val_regression_loss: 12.3706 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 45/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 17.1361 - standard_loss: 12.4416 - regression_loss: 11.8166 - treatment_acc: 0.8110 - val_loss: 18.5668 - val_standard_loss: 12.2986 - val_regression_loss: 11.6833 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 46/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 17.0645 - standard_loss: 12.3704 - regression_loss: 11.7466 - treatment_acc: 0.8110 - val_loss: 18.6086 - val_standard_loss: 12.3422 - val_regression_loss: 11.7282 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 47/300\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 17.0361 - standard_loss: 12.3422 - regression_loss: 11.7196 - treatment_acc: 0.8110 - val_loss: 18.6822 - val_standard_loss: 12.3996 - val_regression_loss: 11.7871 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 48/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 16.9728 - standard_loss: 12.2793 - regression_loss: 11.6579 - treatment_acc: 0.8110 - val_loss: 19.0353 - val_standard_loss: 12.6946 - val_regression_loss: 12.0833 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 49/300\n",
      "21/21 [==============================] - 1s 35ms/step - loss: 16.9637 - standard_loss: 12.2703 - regression_loss: 11.6500 - treatment_acc: 0.8110 - val_loss: 18.9000 - val_standard_loss: 12.5834 - val_regression_loss: 11.9735 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 50/300\n",
      "21/21 [==============================] - 1s 38ms/step - loss: 16.9677 - standard_loss: 12.2747 - regression_loss: 11.6555 - treatment_acc: 0.8110 - val_loss: 18.9618 - val_standard_loss: 12.6237 - val_regression_loss: 12.0151 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 51/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 16.8986 - standard_loss: 12.2059 - regression_loss: 11.5879 - treatment_acc: 0.8110 - val_loss: 18.4672 - val_standard_loss: 12.2070 - val_regression_loss: 11.5999 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 52/300\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 16.8041 - standard_loss: 12.1117 - regression_loss: 11.4948 - treatment_acc: 0.8110 - val_loss: 18.4624 - val_standard_loss: 12.1917 - val_regression_loss: 11.5859 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 53/300\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 16.7556 - standard_loss: 12.0634 - regression_loss: 11.4477 - treatment_acc: 0.8110 - val_loss: 18.4249 - val_standard_loss: 12.1656 - val_regression_loss: 11.5611 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 54/300\n",
      "21/21 [==============================] - 1s 35ms/step - loss: 16.7985 - standard_loss: 12.1065 - regression_loss: 11.4919 - treatment_acc: 0.8110 - val_loss: 18.2566 - val_standard_loss: 12.0204 - val_regression_loss: 11.4172 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 55/300\n",
      "21/21 [==============================] - 1s 36ms/step - loss: 16.7257 - standard_loss: 12.0340 - regression_loss: 11.4206 - treatment_acc: 0.8110 - val_loss: 18.2279 - val_standard_loss: 11.9869 - val_regression_loss: 11.3850 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 56/300\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 16.6585 - standard_loss: 11.9671 - regression_loss: 11.3547 - treatment_acc: 0.8110 - val_loss: 18.4651 - val_standard_loss: 12.1980 - val_regression_loss: 11.5974 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 57/300\n",
      "21/21 [==============================] - 1s 37ms/step - loss: 16.6564 - standard_loss: 11.9653 - regression_loss: 11.3541 - treatment_acc: 0.8110 - val_loss: 18.2779 - val_standard_loss: 12.0261 - val_regression_loss: 11.4268 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 58/300\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 16.5972 - standard_loss: 11.9063 - regression_loss: 11.2961 - treatment_acc: 0.8110 - val_loss: 18.5346 - val_standard_loss: 12.2516 - val_regression_loss: 11.6535 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 59/300\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 16.5428 - standard_loss: 11.8523 - regression_loss: 11.2431 - treatment_acc: 0.8110 - val_loss: 18.4253 - val_standard_loss: 12.1513 - val_regression_loss: 11.5545 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 60/300\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 16.5719 - standard_loss: 11.8816 - regression_loss: 11.2736 - treatment_acc: 0.8110 - val_loss: 18.4365 - val_standard_loss: 12.1630 - val_regression_loss: 11.5674 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 61/300\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 16.5126 - standard_loss: 11.8227 - regression_loss: 11.2157 - treatment_acc: 0.8110 - val_loss: 17.9922 - val_standard_loss: 11.7791 - val_regression_loss: 11.1849 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 62/300\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 16.4847 - standard_loss: 11.7950 - regression_loss: 11.1890 - treatment_acc: 0.8110 - val_loss: 18.7393 - val_standard_loss: 12.4220 - val_regression_loss: 11.8289 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 63/300\n",
      "21/21 [==============================] - 1s 40ms/step - loss: 16.4786 - standard_loss: 11.7891 - regression_loss: 11.1842 - treatment_acc: 0.8110 - val_loss: 18.8200 - val_standard_loss: 12.4859 - val_regression_loss: 11.8940 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 64/300\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 16.4333 - standard_loss: 11.7441 - regression_loss: 11.1402 - treatment_acc: 0.8110 - val_loss: 18.8416 - val_standard_loss: 12.5096 - val_regression_loss: 11.9188 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 65/300\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 16.4574 - standard_loss: 11.7686 - regression_loss: 11.1656 - treatment_acc: 0.8110 - val_loss: 19.1332 - val_standard_loss: 12.7547 - val_regression_loss: 12.1651 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 66/300\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 16.4252 - standard_loss: 11.7366 - regression_loss: 11.1347 - treatment_acc: 0.8110 - val_loss: 18.3302 - val_standard_loss: 12.0696 - val_regression_loss: 11.4813 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 67/300\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 16.3429 - standard_loss: 11.6546 - regression_loss: 11.0536 - treatment_acc: 0.8110 - val_loss: 18.0067 - val_standard_loss: 11.7800 - val_regression_loss: 11.1929 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 68/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 16.2908 - standard_loss: 11.6028 - regression_loss: 11.0029 - treatment_acc: 0.8110 - val_loss: 18.5860 - val_standard_loss: 12.2742 - val_regression_loss: 11.6882 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 69/300\n",
      "21/21 [==============================] - 1s 36ms/step - loss: 16.2950 - standard_loss: 11.6073 - regression_loss: 11.0082 - treatment_acc: 0.8110 - val_loss: 18.2046 - val_standard_loss: 11.9492 - val_regression_loss: 11.3644 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 70/300\n",
      "21/21 [==============================] - 1s 42ms/step - loss: 16.2512 - standard_loss: 11.5638 - regression_loss: 10.9658 - treatment_acc: 0.8110 - val_loss: 18.2505 - val_standard_loss: 11.9900 - val_regression_loss: 11.4064 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 71/300\n",
      "21/21 [==============================] - 1s 36ms/step - loss: 16.2123 - standard_loss: 11.5252 - regression_loss: 10.9282 - treatment_acc: 0.8110 - val_loss: 17.8610 - val_standard_loss: 11.6443 - val_regression_loss: 11.0618 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 72/300\n",
      "21/21 [==============================] - 1s 35ms/step - loss: 16.2106 - standard_loss: 11.5238 - regression_loss: 10.9277 - treatment_acc: 0.8110 - val_loss: 18.0887 - val_standard_loss: 11.8467 - val_regression_loss: 11.2654 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 73/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 16.1709 - standard_loss: 11.4843 - regression_loss: 10.8892 - treatment_acc: 0.8110 - val_loss: 17.8222 - val_standard_loss: 11.6088 - val_regression_loss: 11.0286 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 74/300\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 16.2096 - standard_loss: 11.5233 - regression_loss: 10.9292 - treatment_acc: 0.8110 - val_loss: 18.0613 - val_standard_loss: 11.8104 - val_regression_loss: 11.2313 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 75/300\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 16.1349 - standard_loss: 11.4489 - regression_loss: 10.8557 - treatment_acc: 0.8110 - val_loss: 18.0868 - val_standard_loss: 11.8423 - val_regression_loss: 11.2644 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 76/300\n",
      "21/21 [==============================] - 1s 35ms/step - loss: 16.1271 - standard_loss: 11.4414 - regression_loss: 10.8492 - treatment_acc: 0.8110 - val_loss: 17.7722 - val_standard_loss: 11.5677 - val_regression_loss: 10.9908 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 77/300\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 16.0943 - standard_loss: 11.4089 - regression_loss: 10.8175 - treatment_acc: 0.8110 - val_loss: 17.8385 - val_standard_loss: 11.6206 - val_regression_loss: 11.0448 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 78/300\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 16.0662 - standard_loss: 11.3810 - regression_loss: 10.7906 - treatment_acc: 0.8110 - val_loss: 18.3277 - val_standard_loss: 12.0443 - val_regression_loss: 11.4697 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 79/300\n",
      "21/21 [==============================] - 1s 41ms/step - loss: 16.0719 - standard_loss: 11.3871 - regression_loss: 10.7976 - treatment_acc: 0.8110 - val_loss: 17.8695 - val_standard_loss: 11.6476 - val_regression_loss: 11.0741 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 80/300\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 16.0548 - standard_loss: 11.3703 - regression_loss: 10.7817 - treatment_acc: 0.8110 - val_loss: 17.9795 - val_standard_loss: 11.7453 - val_regression_loss: 11.1728 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 81/300\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 16.0496 - standard_loss: 11.3654 - regression_loss: 10.7776 - treatment_acc: 0.8110 - val_loss: 17.7282 - val_standard_loss: 11.5292 - val_regression_loss: 10.9578 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 82/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 16.0244 - standard_loss: 11.3405 - regression_loss: 10.7536 - treatment_acc: 0.8110 - val_loss: 17.9025 - val_standard_loss: 11.6657 - val_regression_loss: 11.0953 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 83/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 15.9981 - standard_loss: 11.3144 - regression_loss: 10.7284 - treatment_acc: 0.8110 - val_loss: 17.8525 - val_standard_loss: 11.6285 - val_regression_loss: 11.0593 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 84/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 15.9782 - standard_loss: 11.2948 - regression_loss: 10.7098 - treatment_acc: 0.8110 - val_loss: 17.7648 - val_standard_loss: 11.5521 - val_regression_loss: 10.9840 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 85/300\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 15.9217 - standard_loss: 11.2386 - regression_loss: 10.6544 - treatment_acc: 0.8110 - val_loss: 18.2582 - val_standard_loss: 11.9782 - val_regression_loss: 11.4109 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 86/300\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 15.9537 - standard_loss: 11.2709 - regression_loss: 10.6876 - treatment_acc: 0.8110 - val_loss: 17.8385 - val_standard_loss: 11.6182 - val_regression_loss: 11.0520 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 87/300\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 15.9142 - standard_loss: 11.2318 - regression_loss: 10.6493 - treatment_acc: 0.8110 - val_loss: 17.4340 - val_standard_loss: 11.2659 - val_regression_loss: 10.7007 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 88/300\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 15.9617 - standard_loss: 11.2794 - regression_loss: 10.6978 - treatment_acc: 0.8110 - val_loss: 18.1700 - val_standard_loss: 11.8986 - val_regression_loss: 11.3345 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 89/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 15.8771 - standard_loss: 11.1952 - regression_loss: 10.6144 - treatment_acc: 0.8110 - val_loss: 18.0632 - val_standard_loss: 11.8129 - val_regression_loss: 11.2497 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 90/300\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 15.8789 - standard_loss: 11.1973 - regression_loss: 10.6173 - treatment_acc: 0.8110 - val_loss: 18.2146 - val_standard_loss: 11.9368 - val_regression_loss: 11.3746 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 91/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 15.8577 - standard_loss: 11.1765 - regression_loss: 10.5973 - treatment_acc: 0.8110 - val_loss: 17.3078 - val_standard_loss: 11.1560 - val_regression_loss: 10.5948 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 92/300\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 15.9261 - standard_loss: 11.2452 - regression_loss: 10.6668 - treatment_acc: 0.8110 - val_loss: 18.0565 - val_standard_loss: 11.7938 - val_regression_loss: 11.2336 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 93/300\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 15.8600 - standard_loss: 11.1794 - regression_loss: 10.6018 - treatment_acc: 0.8110 - val_loss: 18.8067 - val_standard_loss: 12.4412 - val_regression_loss: 11.8818 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 94/300\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 15.8658 - standard_loss: 11.1854 - regression_loss: 10.6086 - treatment_acc: 0.8110 - val_loss: 17.6887 - val_standard_loss: 11.4847 - val_regression_loss: 10.9264 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 95/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 15.8076 - standard_loss: 11.1276 - regression_loss: 10.5516 - treatment_acc: 0.8110 - val_loss: 17.4572 - val_standard_loss: 11.2681 - val_regression_loss: 10.7108 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 96/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 15.8265 - standard_loss: 11.1469 - regression_loss: 10.5716 - treatment_acc: 0.8110 - val_loss: 17.8764 - val_standard_loss: 11.6396 - val_regression_loss: 11.0831 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 97/300\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 15.7610 - standard_loss: 11.0815 - regression_loss: 10.5070 - treatment_acc: 0.8110 - val_loss: 18.5932 - val_standard_loss: 12.2474 - val_regression_loss: 11.6920 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 98/300\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 15.7873 - standard_loss: 11.1083 - regression_loss: 10.5346 - treatment_acc: 0.8110 - val_loss: 17.5929 - val_standard_loss: 11.3937 - val_regression_loss: 10.8392 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 99/300\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 15.7487 - standard_loss: 11.0699 - regression_loss: 10.4970 - treatment_acc: 0.8110 - val_loss: 18.6214 - val_standard_loss: 12.2757 - val_regression_loss: 11.7220 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 100/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 15.7736 - standard_loss: 11.0952 - regression_loss: 10.5230 - treatment_acc: 0.8110 - val_loss: 17.6592 - val_standard_loss: 11.4477 - val_regression_loss: 10.8950 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 101/300\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 15.7379 - standard_loss: 11.0598 - regression_loss: 10.4884 - treatment_acc: 0.8110 - val_loss: 17.8435 - val_standard_loss: 11.6097 - val_regression_loss: 11.0579 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 102/300\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 15.7278 - standard_loss: 11.0500 - regression_loss: 10.4793 - treatment_acc: 0.8110 - val_loss: 17.7043 - val_standard_loss: 11.5047 - val_regression_loss: 10.9537 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 103/300\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 15.6523 - standard_loss: 10.9748 - regression_loss: 10.4048 - treatment_acc: 0.8110 - val_loss: 18.5031 - val_standard_loss: 12.1803 - val_regression_loss: 11.6304 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 104/300\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 15.7082 - standard_loss: 11.0310 - regression_loss: 10.4619 - treatment_acc: 0.8110 - val_loss: 17.7147 - val_standard_loss: 11.5026 - val_regression_loss: 10.9535 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 105/300\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 15.7327 - standard_loss: 11.0558 - regression_loss: 10.4873 - treatment_acc: 0.8110 - val_loss: 17.8832 - val_standard_loss: 11.6509 - val_regression_loss: 11.1027 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 106/300\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 15.6534 - standard_loss: 10.9769 - regression_loss: 10.4091 - treatment_acc: 0.8110 - val_loss: 17.8684 - val_standard_loss: 11.6438 - val_regression_loss: 11.0965 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 107/300\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 15.7235 - standard_loss: 11.0472 - regression_loss: 10.4800 - treatment_acc: 0.8110 - val_loss: 18.0722 - val_standard_loss: 11.8008 - val_regression_loss: 11.2543 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 108/300\n",
      "17/21 [=======================>......] - ETA: 0s - loss: 15.7562 - standard_loss: 11.0803 - regression_loss: 10.5154 - treatment_acc: 0.8143\n",
      "Epoch 00108: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 15.6901 - standard_loss: 11.0142 - regression_loss: 10.4479 - treatment_acc: 0.8110 - val_loss: 17.9743 - val_standard_loss: 11.7322 - val_regression_loss: 11.1867 - val_treatment_acc: 0.8551 - lr: 1.0000e-05\n",
      "Epoch 109/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 15.5821 - standard_loss: 10.9065 - regression_loss: 10.3407 - treatment_acc: 0.8110 - val_loss: 17.5830 - val_standard_loss: 11.3914 - val_regression_loss: 10.8464 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n",
      "Epoch 110/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 15.5869 - standard_loss: 10.9114 - regression_loss: 10.3460 - treatment_acc: 0.8110 - val_loss: 17.8697 - val_standard_loss: 11.6299 - val_regression_loss: 11.0853 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n",
      "Epoch 111/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 15.5702 - standard_loss: 10.8949 - regression_loss: 10.3299 - treatment_acc: 0.8110 - val_loss: 17.9378 - val_standard_loss: 11.6910 - val_regression_loss: 11.1468 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n",
      "Epoch 112/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 15.5939 - standard_loss: 10.9187 - regression_loss: 10.3540 - treatment_acc: 0.8110 - val_loss: 17.9356 - val_standard_loss: 11.6885 - val_regression_loss: 11.1447 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n",
      "Epoch 113/300\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 15.5580 - standard_loss: 10.8830 - regression_loss: 10.3187 - treatment_acc: 0.8110 - val_loss: 17.8872 - val_standard_loss: 11.6501 - val_regression_loss: 11.1068 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n",
      "Epoch 114/300\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 15.5450 - standard_loss: 10.8702 - regression_loss: 10.3062 - treatment_acc: 0.8110 - val_loss: 17.7557 - val_standard_loss: 11.5339 - val_regression_loss: 10.9910 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n",
      "Epoch 115/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 15.5438 - standard_loss: 10.8691 - regression_loss: 10.3055 - treatment_acc: 0.8110 - val_loss: 17.7488 - val_standard_loss: 11.5300 - val_regression_loss: 10.9875 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n",
      "Epoch 116/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 15.5336 - standard_loss: 10.8590 - regression_loss: 10.2958 - treatment_acc: 0.8110 - val_loss: 17.5453 - val_standard_loss: 11.3535 - val_regression_loss: 10.8115 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n",
      "Epoch 117/300\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 15.5290 - standard_loss: 10.8546 - regression_loss: 10.2916 - treatment_acc: 0.8110 - val_loss: 18.0410 - val_standard_loss: 11.7830 - val_regression_loss: 11.2414 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n",
      "Epoch 118/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 15.5236 - standard_loss: 10.8494 - regression_loss: 10.2868 - treatment_acc: 0.8110 - val_loss: 17.4793 - val_standard_loss: 11.2999 - val_regression_loss: 10.7587 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n",
      "Epoch 119/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 15.5870 - standard_loss: 10.9129 - regression_loss: 10.3506 - treatment_acc: 0.8110 - val_loss: 17.5327 - val_standard_loss: 11.3437 - val_regression_loss: 10.8029 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n",
      "Epoch 120/300\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 15.5148 - standard_loss: 10.8409 - regression_loss: 10.2790 - treatment_acc: 0.8110 - val_loss: 18.0589 - val_standard_loss: 11.7915 - val_regression_loss: 11.2512 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n",
      "Epoch 121/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 15.5307 - standard_loss: 10.8570 - regression_loss: 10.2954 - treatment_acc: 0.8110 - val_loss: 17.7844 - val_standard_loss: 11.5546 - val_regression_loss: 11.0147 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n",
      "Epoch 122/300\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 15.5003 - standard_loss: 10.8267 - regression_loss: 10.2654 - treatment_acc: 0.8110 - val_loss: 17.9457 - val_standard_loss: 11.6985 - val_regression_loss: 11.1590 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n",
      "Epoch 123/300\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 15.5218 - standard_loss: 10.8484 - regression_loss: 10.2875 - treatment_acc: 0.8110 - val_loss: 17.6533 - val_standard_loss: 11.4443 - val_regression_loss: 10.9052 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n",
      "Epoch 124/300\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 15.4870 - standard_loss: 10.8137 - regression_loss: 10.2531 - treatment_acc: 0.8110 - val_loss: 17.8604 - val_standard_loss: 11.6234 - val_regression_loss: 11.0847 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n",
      "Epoch 125/300\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 15.4935 - standard_loss: 10.8204 - regression_loss: 10.2601 - treatment_acc: 0.8110 - val_loss: 17.4967 - val_standard_loss: 11.3155 - val_regression_loss: 10.7771 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n",
      "Epoch 126/300\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 15.4901 - standard_loss: 10.8171 - regression_loss: 10.2572 - treatment_acc: 0.8110 - val_loss: 17.9565 - val_standard_loss: 11.7034 - val_regression_loss: 11.1654 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n",
      "Epoch 127/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 15.4958 - standard_loss: 10.8229 - regression_loss: 10.2633 - treatment_acc: 0.8110 - val_loss: 17.7622 - val_standard_loss: 11.5300 - val_regression_loss: 10.9925 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n",
      "Epoch 128/300\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 15.4881 - standard_loss: 10.8154 - regression_loss: 10.2561 - treatment_acc: 0.8110 - val_loss: 17.6841 - val_standard_loss: 11.4808 - val_regression_loss: 10.9437 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n",
      "Epoch 129/300\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 15.4645 - standard_loss: 10.7920 - regression_loss: 10.2331 - treatment_acc: 0.8110 - val_loss: 17.6320 - val_standard_loss: 11.4287 - val_regression_loss: 10.8919 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n",
      "Epoch 130/300\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 15.4488 - standard_loss: 10.7764 - regression_loss: 10.2178 - treatment_acc: 0.8110 - val_loss: 17.7417 - val_standard_loss: 11.5151 - val_regression_loss: 10.9788 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n",
      "Epoch 131/300\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 15.4396 - standard_loss: 10.7674 - regression_loss: 10.2091 - treatment_acc: 0.8110 - val_loss: 17.7936 - val_standard_loss: 11.5678 - val_regression_loss: 11.0318 - val_treatment_acc: 0.8551 - lr: 5.0000e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x154ad54c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "%load_ext tensorboard\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "val_split=0.2\n",
    "batch_size=64\n",
    "verbose=1\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "sgd_callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='val_loss', patience=40, min_delta=0.),\n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0., cooldown=0, min_lr=0),\n",
    "        tensorboard_callback,\n",
    "        Full_Metrics(data_train,'train',verbose=verbose),\n",
    "        # AIPW_Metrics(data_valid,'valid',verbose=verbose),\n",
    "        Full_Metrics(data_valid,'valid',verbose=verbose),\n",
    "        ]\n",
    "      \n",
    "\n",
    "sgd_lr = 1e-5\n",
    "momentum = 0.9\n",
    "\n",
    "aipw_model=make_aipw(data_train['x'].shape[1],.01)\n",
    "aipw_loss=Base_Loss(alpha=1.0)\n",
    "\n",
    "aipw_model.compile(optimizer=SGD(lr=sgd_lr, momentum=momentum, nesterov=True),\n",
    "                    loss=aipw_loss,\n",
    "                    metrics=[aipw_loss,aipw_loss.regression_loss,aipw_loss.treatment_acc]\n",
    "                   )\n",
    "\n",
    "aipw_model.fit(\n",
    "        x=data_train['x'],\n",
    "        y=np.concatenate([data_train['ys'], data_train['t']], 1),\n",
    "        callbacks=sgd_callbacks,\n",
    "        validation_data=[data_valid['x'],np.concatenate([data_valid['ys'], data_valid['t']], 1)],\n",
    "        epochs=300,\n",
    "        batch_size=batch_size,\n",
    "        verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 45269), started 2 days, 0:53:37 ago. (Use '!kill 45269' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b4d8776f8e80140f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b4d8776f8e80140f\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
