{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 “ihdp_npci_1-100.train.npz” 已经存在；不获取。\n",
      "\n",
      "文件 “ihdp_npci_1-100.test.npz” 已经存在；不获取。\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([5.26691518]), array([2.59847927]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.layers as tfkl\n",
    "tfd,tfpl = tfp.distributions,tfp.layers\n",
    "import tensorflow.keras.backend as tfkb\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from evaluation import *\n",
    "from cevae_networks import *\n",
    "################################################\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='')\n",
    "parser.add_argument('--scale_penalize',    type = float, default = 0.001,  help = '')\n",
    "parser.add_argument('--learning_rate',     type = float, default = 0.001,  help = '')\n",
    "parser.add_argument('--default_y_scale',   type = float, default = 1.,  help = '')\n",
    "parser.add_argument('--t_dim',     type = int, default = 1,  help = '')\n",
    "parser.add_argument('--y_dim',     type = int, default = 1,  help = '')\n",
    "parser.add_argument('--x_dim',     type = int, default = 25, help = '')\n",
    "parser.add_argument('--z_dim',     type = int, default = 20, help = '')\n",
    "parser.add_argument('--x_num_dim', type = int, default = 6,  help = '')\n",
    "parser.add_argument('--x_bin_dim', type = int, default = 19, help = '')\n",
    "parser.add_argument('--nh', type = int, default = 3, help = 'number of hidden layers')\n",
    "parser.add_argument('--h',  type = int, default = 200, help = 'number of hidden units')\n",
    "args = parser.parse_args([])\n",
    "################################################\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz \n",
    "\n",
    "def load_IHDP_data(training_data,testing_data,i):\n",
    "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "        train_data=np.load(trf); test_data=np.load(tef)\n",
    "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
    "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
    "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
    "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    "        ycf=np.concatenate((train_data['ycf'][:,i],  test_data['ycf'][:,i])).astype('float32')\n",
    "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "        data['y']=data['y'].reshape(-1,1)\n",
    "        data['ycf'] = ycf.reshape(-1,1)\n",
    "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "    return data\n",
    "\n",
    "ind = 7\n",
    "rep = 1\n",
    "data = load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.test.npz',i = ind)\n",
    "for key in data:\n",
    "    if key != 'y_scaler':\n",
    "        data[key] = np.repeat(data[key],repeats = rep, axis = 0)\n",
    "data['y_scaler'].mean_, data['y_scaler'].scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEVAE(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CEVAE, self).__init__()\n",
    "        ########################################\n",
    "        # networks\n",
    "        self.activation = 'elu'\n",
    "        # CEVAE Model \n",
    "        ## (encoder)\n",
    "        self.q_y_tx = q_y_tx(args.x_bin_dim, args.x_num_dim, args.y_dim, args.t_dim, args.nh, args.h)\n",
    "        self.q_t_x = q_t_x(args.x_bin_dim, args.x_num_dim, args.t_dim, args.nh, args.h)\n",
    "        self.q_z_txy = q_z_txy(args.x_bin_dim, args.x_num_dim, args.y_dim, args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        ## (decoder)\n",
    "        self.p_x_z = p_x_z(args.x_bin_dim, args.x_num_dim, args.z_dim, args.nh, args.h)\n",
    "        self.p_t_z = p_t_z(args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        self.p_y_tz = p_y_tz(args.y_dim, args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        \n",
    "\n",
    "    def call(self, data, training=False):\n",
    "        if training:\n",
    "            x_train,t_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "            \n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            \n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.sample()\n",
    "            # decoder\n",
    "            ## p(x|z)\n",
    "            x_num,x_bin = self.p_x_z(z_infer_sample)\n",
    "            ## p(t|z)\n",
    "            t = self.p_t_z(z_infer_sample)\n",
    "            ## p(y|t,z)\n",
    "            y = self.p_y_tz(tf.concat([t_train,z_infer_sample],-1) )\n",
    "            \n",
    "            return y_infer,t_infer,z_infer,y,t,x_num,x_bin\n",
    "        else:\n",
    "            x_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "        \n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            \n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.sample()\n",
    "            return y_infer,t_infer,z_infer\n",
    "\n",
    "\n",
    "    def cevae_loss(self, data, pred, training = False):\n",
    "        x_train, t_train, y_train = data[0],data[1],data[2]\n",
    "        x_train_num, x_train_bin = x_train[:,:args.x_num_dim],x_train[:,args.x_num_dim:]\n",
    "        y_infer,t_infer,z_infer,y,t,x_num,x_bin = pred\n",
    "        y0,y1 = y_infer\n",
    "        # reconstruct loss\n",
    "        recon_x_num = tfkb.sum(x_num.log_prob(x_train_num), 1)\n",
    "        recon_x_bin = tfkb.sum(x_bin.log_prob(x_train_bin), 1)\n",
    "        recon_y = tfkb.sum(y.log_prob(y_train), 1)\n",
    "        recon_t = tfkb.sum(t.log_prob(t_train), 1)\n",
    "        # kl loss\n",
    "        z_infer_sample = z_infer.sample()\n",
    "        z = tfd.Normal(loc = [0] * 20, scale = [1]*20)\n",
    "        kl_z = tfkb.sum((z.log_prob(z_infer_sample) - z_infer.log_prob(z_infer_sample)), -1)\n",
    "        # aux loss\n",
    "        aux_y = tfkb.sum(y0.log_prob(y_train)*(1-t_train) + y1.log_prob(y_train)* t_train, 1)\n",
    "        aux_t = tfkb.sum(t_infer.log_prob(t_train), 1)\n",
    "        loss = -tfkb.mean(recon_x_bin + recon_x_num + recon_y + recon_t + aux_y + aux_t + kl_z)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, data):\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "            loss = self.cevae_loss(data = data, pred = pred, training = True)\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        metrics = {\"loss\": loss}\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "        y_infer = pred[0]\n",
    "        loss = self.cevae_loss(data = data, pred = pred, training = False)\n",
    "        y0, y1 = y_infer[0].sample(),y_infer[1].sample()\n",
    "        ate = tfkb.mean(y1) - tfkb.mean(y0)\n",
    "        metrics = {\"loss\":loss,\"y0\": tfkb.mean(y0),\"y1\": tfkb.mean(y1),'ate_afte_scaled': ate}\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 18:34:15.746788: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/3 [=========>....................] - ETA: 6s - loss: 29.3279 — ite: 4.6463  — ate: 3.2332 — pehe: 5.2061 \n",
      "3/3 [==============================] - 4s 466ms/step - loss: 29.1609 - val_loss: 29.7561 - val_y0: -0.2196 - val_y1: -0.0310 - val_ate_afte_scaled: 0.1886 - lr: 5.0000e-05\n",
      "Epoch 2/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 29.5134 — ite: 4.6620  — ate: 3.1212 — pehe: 5.0579 \n",
      "3/3 [==============================] - 0s 173ms/step - loss: 28.9946 - val_loss: 30.0591 - val_y0: -0.1252 - val_y1: 0.0826 - val_ate_afte_scaled: 0.2078 - lr: 5.0000e-05\n",
      "Epoch 3/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.4494WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0103s vs `on_train_batch_end` time: 0.0105s). Check your callbacks.\n",
      " — ite: 4.5017  — ate: 2.7417 — pehe: 4.8852 \n",
      "3/3 [==============================] - 0s 193ms/step - loss: 28.8223 - val_loss: 29.7550 - val_y0: -0.1367 - val_y1: 0.2288 - val_ate_afte_scaled: 0.3655 - lr: 5.0000e-05\n",
      "Epoch 4/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.7436 — ite: 4.3169  — ate: 2.6098 — pehe: 4.7370 \n",
      "3/3 [==============================] - 0s 198ms/step - loss: 28.7121 - val_loss: 29.3713 - val_y0: -0.2077 - val_y1: 0.2498 - val_ate_afte_scaled: 0.4575 - lr: 5.0000e-05\n",
      "Epoch 5/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.8157 — ite: 4.3429  — ate: 2.3903 — pehe: 4.5551 \n",
      "3/3 [==============================] - 0s 188ms/step - loss: 28.4621 - val_loss: 29.4346 - val_y0: -0.4069 - val_y1: 0.2319 - val_ate_afte_scaled: 0.6388 - lr: 5.0000e-05\n",
      "Epoch 6/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.0683 — ite: 4.1508  — ate: 1.9665 — pehe: 4.2159 \n",
      "3/3 [==============================] - 0s 196ms/step - loss: 28.2244 - val_loss: 28.8508 - val_y0: -0.2894 - val_y1: 0.3969 - val_ate_afte_scaled: 0.6863 - lr: 5.0000e-05\n",
      "Epoch 7/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.2490 — ite: 4.1613  — ate: 1.7892 — pehe: 4.4010 \n",
      "3/3 [==============================] - 0s 182ms/step - loss: 28.1384 - val_loss: 28.7691 - val_y0: -0.3456 - val_y1: 0.4343 - val_ate_afte_scaled: 0.7800 - lr: 5.0000e-05\n",
      "Epoch 8/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.3744 — ite: 4.1495  — ate: 1.8192 — pehe: 4.2004 \n",
      "3/3 [==============================] - 0s 198ms/step - loss: 27.9448 - val_loss: 28.7729 - val_y0: -0.1529 - val_y1: 0.5489 - val_ate_afte_scaled: 0.7018 - lr: 5.0000e-05\n",
      "Epoch 9/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.2994 — ite: 4.0631  — ate: 1.5100 — pehe: 4.0951 \n",
      "3/3 [==============================] - 0s 187ms/step - loss: 27.8619 - val_loss: 28.4317 - val_y0: -0.2848 - val_y1: 0.6773 - val_ate_afte_scaled: 0.9621 - lr: 5.0000e-05\n",
      "Epoch 10/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.3241 — ite: 3.9772  — ate: 1.6412 — pehe: 4.1983 \n",
      "3/3 [==============================] - 0s 180ms/step - loss: 27.6884 - val_loss: 28.2999 - val_y0: -0.3558 - val_y1: 0.6210 - val_ate_afte_scaled: 0.9768 - lr: 5.0000e-05\n",
      "Epoch 11/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.8897 — ite: 4.0382  — ate: 1.5255 — pehe: 4.3010 \n",
      "3/3 [==============================] - 0s 172ms/step - loss: 27.4996 - val_loss: 27.8981 - val_y0: -0.1700 - val_y1: 0.6714 - val_ate_afte_scaled: 0.8414 - lr: 5.0000e-05\n",
      "Epoch 12/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.5816 — ite: 3.9185  — ate: 1.0550 — pehe: 4.0845 \n",
      "3/3 [==============================] - 0s 174ms/step - loss: 27.5440 - val_loss: 28.1449 - val_y0: -0.3377 - val_y1: 0.6990 - val_ate_afte_scaled: 1.0368 - lr: 5.0000e-05\n",
      "Epoch 13/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.2914 — ite: 3.8885  — ate: 0.7396 — pehe: 3.9326 \n",
      "3/3 [==============================] - 0s 176ms/step - loss: 27.1787 - val_loss: 28.0163 - val_y0: -0.4028 - val_y1: 0.8673 - val_ate_afte_scaled: 1.2701 - lr: 5.0000e-05\n",
      "Epoch 14/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.3401 — ite: 3.8513  — ate: 0.7404 — pehe: 4.0666 \n",
      "3/3 [==============================] - 0s 174ms/step - loss: 26.9773 - val_loss: 27.8066 - val_y0: -0.3342 - val_y1: 0.8096 - val_ate_afte_scaled: 1.1438 - lr: 5.0000e-05\n",
      "Epoch 15/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.2470 — ite: 3.8565  — ate: 0.7905 — pehe: 4.0224 \n",
      "3/3 [==============================] - 0s 167ms/step - loss: 26.8704 - val_loss: 27.4647 - val_y0: -0.2806 - val_y1: 0.8888 - val_ate_afte_scaled: 1.1694 - lr: 5.0000e-05\n",
      "Epoch 16/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.1977 — ite: 3.8146  — ate: 0.5982 — pehe: 3.8574 \n",
      "3/3 [==============================] - 0s 168ms/step - loss: 26.6833 - val_loss: 27.5587 - val_y0: -0.2148 - val_y1: 0.9520 - val_ate_afte_scaled: 1.1668 - lr: 5.0000e-05\n",
      "Epoch 17/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 26.7321 — ite: 3.7880  — ate: 0.7980 — pehe: 3.9095 \n",
      "3/3 [==============================] - 0s 177ms/step - loss: 26.6735 - val_loss: 27.4081 - val_y0: -0.4151 - val_y1: 0.9342 - val_ate_afte_scaled: 1.3492 - lr: 5.0000e-05\n",
      "Epoch 18/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 26.8100 — ite: 3.8495  — ate: 0.6833 — pehe: 3.9514 \n",
      "3/3 [==============================] - 0s 177ms/step - loss: 26.5470 - val_loss: 26.9542 - val_y0: -0.3431 - val_y1: 0.9908 - val_ate_afte_scaled: 1.3340 - lr: 5.0000e-05\n",
      "Epoch 19/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 26.7373 — ite: 3.8843  — ate: 0.2536 — pehe: 3.9345 \n",
      "3/3 [==============================] - 0s 180ms/step - loss: 26.4841 - val_loss: 27.0726 - val_y0: -0.3199 - val_y1: 0.9733 - val_ate_afte_scaled: 1.2932 - lr: 5.0000e-05\n",
      "Epoch 20/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 26.7425 — ite: 3.8316  — ate: 0.3205 — pehe: 3.8706 \n",
      "3/3 [==============================] - 0s 176ms/step - loss: 26.5091 - val_loss: 26.9142 - val_y0: -0.4527 - val_y1: 1.1639 - val_ate_afte_scaled: 1.6166 - lr: 5.0000e-05\n",
      "Epoch 21/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.8448 — ite: 3.8262  — ate: 0.0229 — pehe: 3.8776 \n",
      "3/3 [==============================] - 0s 184ms/step - loss: 26.1990 - val_loss: 26.5135 - val_y0: -0.4701 - val_y1: 0.9035 - val_ate_afte_scaled: 1.3736 - lr: 5.0000e-05\n",
      "Epoch 22/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.8005 — ite: 3.7304  — ate: 0.1884 — pehe: 3.9200 \n",
      "3/3 [==============================] - 0s 173ms/step - loss: 26.0233 - val_loss: 26.3645 - val_y0: -0.3657 - val_y1: 1.0883 - val_ate_afte_scaled: 1.4540 - lr: 5.0000e-05\n",
      "Epoch 23/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 26.1091 — ite: 3.8173  — ate: 0.0022 — pehe: 3.9054 \n",
      "3/3 [==============================] - 0s 174ms/step - loss: 25.9184 - val_loss: 26.4996 - val_y0: -0.5830 - val_y1: 1.0286 - val_ate_afte_scaled: 1.6116 - lr: 5.0000e-05\n",
      "Epoch 24/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.7806 — ite: 3.7720  — ate: 0.2265 — pehe: 3.7288 \n",
      "3/3 [==============================] - 0s 183ms/step - loss: 25.7222 - val_loss: 26.5246 - val_y0: -0.2006 - val_y1: 1.0685 - val_ate_afte_scaled: 1.2691 - lr: 5.0000e-05\n",
      "Epoch 25/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.6839 — ite: 3.8420  — ate: 0.1310 — pehe: 4.1036 \n",
      "3/3 [==============================] - 0s 177ms/step - loss: 25.6475 - val_loss: 26.3499 - val_y0: -0.3298 - val_y1: 1.2386 - val_ate_afte_scaled: 1.5683 - lr: 5.0000e-05\n",
      "Epoch 26/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.8440 — ite: 3.8048  — ate: 0.2941 — pehe: 3.9341 \n",
      "3/3 [==============================] - 0s 168ms/step - loss: 25.3662 - val_loss: 25.6884 - val_y0: -0.3863 - val_y1: 1.0442 - val_ate_afte_scaled: 1.4305 - lr: 5.0000e-05\n",
      "Epoch 27/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.5540 — ite: 3.7545  — ate: 0.1405 — pehe: 3.7096 \n",
      "3/3 [==============================] - 0s 175ms/step - loss: 25.5388 - val_loss: 25.9956 - val_y0: -0.4021 - val_y1: 1.0077 - val_ate_afte_scaled: 1.4099 - lr: 5.0000e-05\n",
      "Epoch 28/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.1894 — ite: 3.8553  — ate: 0.1378 — pehe: 3.8917 \n",
      "3/3 [==============================] - 0s 176ms/step - loss: 25.1049 - val_loss: 25.5999 - val_y0: -0.3895 - val_y1: 1.1615 - val_ate_afte_scaled: 1.5510 - lr: 5.0000e-05\n",
      "Epoch 29/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.1218 — ite: 3.7993  — ate: 0.1200 — pehe: 3.7266 \n",
      "3/3 [==============================] - 0s 175ms/step - loss: 25.0341 - val_loss: 25.5312 - val_y0: -0.2663 - val_y1: 0.9821 - val_ate_afte_scaled: 1.2484 - lr: 5.0000e-05\n",
      "Epoch 30/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.0691 — ite: 3.7909  — ate: 0.1462 — pehe: 3.9397 \n",
      "3/3 [==============================] - 0s 175ms/step - loss: 25.0593 - val_loss: 25.5128 - val_y0: -0.3505 - val_y1: 1.1398 - val_ate_afte_scaled: 1.4903 - lr: 5.0000e-05\n",
      "Epoch 31/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.7665 — ite: 3.7208  — ate: 0.1217 — pehe: 3.8344 \n",
      "3/3 [==============================] - 0s 167ms/step - loss: 24.8415 - val_loss: 25.4908 - val_y0: -0.2644 - val_y1: 1.1774 - val_ate_afte_scaled: 1.4418 - lr: 5.0000e-05\n",
      "Epoch 32/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.7443 — ite: 3.7757  — ate: 0.1925 — pehe: 3.9274 \n",
      "3/3 [==============================] - 0s 186ms/step - loss: 24.6635 - val_loss: 25.3987 - val_y0: -0.2449 - val_y1: 1.0929 - val_ate_afte_scaled: 1.3378 - lr: 5.0000e-05\n",
      "Epoch 33/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.9263 — ite: 3.8744  — ate: 0.3672 — pehe: 3.8955 \n",
      "3/3 [==============================] - 0s 181ms/step - loss: 24.6071 - val_loss: 25.3310 - val_y0: -0.3449 - val_y1: 1.0056 - val_ate_afte_scaled: 1.3505 - lr: 5.0000e-05\n",
      "Epoch 34/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.9805 — ite: 3.7632  — ate: 0.2131 — pehe: 3.8287 \n",
      "3/3 [==============================] - 0s 177ms/step - loss: 24.4795 - val_loss: 25.2828 - val_y0: -0.3579 - val_y1: 1.1117 - val_ate_afte_scaled: 1.4696 - lr: 5.0000e-05\n",
      "Epoch 35/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.5599 — ite: 3.7829  — ate: 0.3052 — pehe: 3.6455 \n",
      "3/3 [==============================] - 0s 188ms/step - loss: 24.3469 - val_loss: 25.0156 - val_y0: -0.2301 - val_y1: 0.9673 - val_ate_afte_scaled: 1.1974 - lr: 5.0000e-05\n",
      "Epoch 36/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.3059 — ite: 3.8190  — ate: 0.2629 — pehe: 4.0064 \n",
      "3/3 [==============================] - 0s 178ms/step - loss: 24.3428 - val_loss: 24.8631 - val_y0: -0.1770 - val_y1: 1.1161 - val_ate_afte_scaled: 1.2931 - lr: 5.0000e-05\n",
      "Epoch 37/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.6815 — ite: 3.8017  — ate: 0.1924 — pehe: 3.6398 \n",
      "3/3 [==============================] - 0s 177ms/step - loss: 24.3447 - val_loss: 24.9927 - val_y0: -0.3378 - val_y1: 1.1811 - val_ate_afte_scaled: 1.5188 - lr: 5.0000e-05\n",
      "Epoch 38/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.1129 — ite: 3.8255  — ate: 0.2187 — pehe: 3.9274 \n"
     ]
    }
   ],
   "source": [
    "#Colab command to allow us to run Colab in TF2\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "%reload_ext tensorboard \n",
    "\n",
    "model = CEVAE()\n",
    "### MAIN CODE ####\n",
    "val_split=0.2\n",
    "batch_size=64\n",
    "verbose=True\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    " \n",
    "callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='loss', patience=40, min_delta=0), \n",
    "        #40 is Shi's recommendation patience for this dataset, but you should tune for your data \n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0, cooldown=0, min_lr=0),\n",
    "        #This learning rate scheduling is quite agressive which seems good for this dataset\n",
    "        metrics_for_cevae(data,verbose),\n",
    "        tensorboard_callback\n",
    "    ]\n",
    "    \n",
    "#optimizer hyperparameters\n",
    "learning_rate = 5e-5\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate))\n",
    "\n",
    "model.fit(\n",
    "    [data['x'],data['t'],data['ys']],\n",
    "    callbacks=callbacks,\n",
    "    validation_split=val_split,\n",
    "    epochs=300,\n",
    "    batch_size=200,\n",
    "    verbose=verbose\n",
    "    )\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 11635), started 1 day, 6:38:38 ago. (Use '!kill 11635' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-13bb416d17cd203f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-13bb416d17cd203f\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
