{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 “ihdp_npci_1-100.train.npz” 已经存在；不获取。\n",
      "\n",
      "文件 “ihdp_npci_1-100.test.npz” 已经存在；不获取。\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([5.26691518]), array([2.59847927]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.layers as tfkl\n",
    "tfd,tfpl = tfp.distributions,tfp.layers\n",
    "import tensorflow.keras.backend as tfkb\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from evaluation import *\n",
    "from cevae_networks import *\n",
    "################################################\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='')\n",
    "parser.add_argument('--scale_penalize',    type = float, default = 0.001,  help = '')\n",
    "parser.add_argument('--learning_rate',     type = float, default = 0.001,  help = '')\n",
    "parser.add_argument('--default_y_scale',   type = float, default = 1.,  help = '')\n",
    "parser.add_argument('--t_dim',     type = int, default = 1,  help = '')\n",
    "parser.add_argument('--y_dim',     type = int, default = 1,  help = '')\n",
    "parser.add_argument('--x_dim',     type = int, default = 25, help = '')\n",
    "parser.add_argument('--z_dim',     type = int, default = 20, help = '')\n",
    "parser.add_argument('--x_num_dim', type = int, default = 6,  help = '')\n",
    "parser.add_argument('--x_bin_dim', type = int, default = 19, help = '')\n",
    "parser.add_argument('--nh', type = int, default = 3, help = 'number of hidden layers')\n",
    "parser.add_argument('--h',  type = int, default = 200, help = 'number of hidden units')\n",
    "args = parser.parse_args([])\n",
    "################################################\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz \n",
    "\n",
    "def load_IHDP_data(training_data,testing_data,i):\n",
    "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "        train_data=np.load(trf); test_data=np.load(tef)\n",
    "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
    "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
    "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
    "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    "        ycf=np.concatenate((train_data['ycf'][:,i],  test_data['ycf'][:,i])).astype('float32')\n",
    "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "        data['y']=data['y'].reshape(-1,1)\n",
    "        data['ycf'] = ycf.reshape(-1,1)\n",
    "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "    return data\n",
    "\n",
    "ind = 7\n",
    "rep = 1\n",
    "data = load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.test.npz',i = ind)\n",
    "for key in data:\n",
    "    if key != 'y_scaler':\n",
    "        data[key] = np.repeat(data[key],repeats = rep, axis = 0)\n",
    "data['y_scaler'].mean_, data['y_scaler'].scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEVAE(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CEVAE, self).__init__()\n",
    "        ########################################\n",
    "        # networks\n",
    "        self.activation = 'elu'\n",
    "        # CEVAE Model \n",
    "        ## (encoder)\n",
    "        self.q_y_tx = q_y_tx(args.x_bin_dim, args.x_num_dim, args.y_dim, args.t_dim, args.nh, args.h)\n",
    "        self.q_t_x = q_t_x(args.x_bin_dim, args.x_num_dim, args.t_dim, args.nh, args.h)\n",
    "        self.q_z_txy = q_z_txy(args.x_bin_dim, args.x_num_dim, args.y_dim, args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        ## (decoder)\n",
    "        self.p_x_z = p_x_z(args.x_bin_dim, args.x_num_dim, args.z_dim, args.nh, args.h)\n",
    "        self.p_t_z = p_t_z(args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        self.p_y_tz = p_y_tz(args.y_dim, args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        \n",
    "\n",
    "    def call(self, data, training=False):\n",
    "        if training:\n",
    "            x_train,t_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "            \n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            \n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.sample()\n",
    "            # decoder\n",
    "            ## p(x|z)\n",
    "            x_num,x_bin = self.p_x_z(z_infer_sample)\n",
    "            ## p(t|z)\n",
    "            t = self.p_t_z(z_infer_sample)\n",
    "            ## p(y|t,z)\n",
    "            y = self.p_y_tz(tf.concat([t_train,z_infer_sample],-1) )\n",
    "            \n",
    "            return y_infer,t_infer,z_infer,y,t,x_num,x_bin\n",
    "        else:\n",
    "            x_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "        \n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            \n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.sample()\n",
    "            return y_infer,t_infer,z_infer\n",
    "\n",
    "\n",
    "    def cevae_loss(self, data, pred, training = False):\n",
    "        x_train, t_train, y_train = data[0],data[1],data[2]\n",
    "        x_train_num, x_train_bin = x_train[:,:args.x_num_dim],x_train[:,args.x_num_dim:]\n",
    "        y_infer,t_infer,z_infer,y,t,x_num,x_bin = pred\n",
    "        y0,y1 = y_infer\n",
    "        # reconstruct loss\n",
    "        recon_x_num = tfkb.sum(x_num.log_prob(x_train_num), 1)\n",
    "        recon_x_bin = tfkb.sum(x_bin.log_prob(x_train_bin), 1)\n",
    "        recon_y = tfkb.sum(y.log_prob(y_train), 1)\n",
    "        recon_t = tfkb.sum(t.log_prob(t_train), 1)\n",
    "        # kl loss\n",
    "        z_infer_sample = z_infer.sample()\n",
    "        z = tfd.Normal(loc = [0] * 20, scale = [1]*20)\n",
    "        kl_z = tfkb.sum((z.log_prob(z_infer_sample) - z_infer.log_prob(z_infer_sample)), -1)\n",
    "        # aux loss\n",
    "        aux_y = tfkb.sum(y0.log_prob(y_train)*(1-t_train) + y1.log_prob(y_train)* t_train, 1)\n",
    "        aux_t = tfkb.sum(t_infer.log_prob(t_train), 1)\n",
    "        loss = -tfkb.mean(recon_x_bin + recon_x_num + recon_y + recon_t + aux_y + aux_t + kl_z)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, data):\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "            loss = self.cevae_loss(data = data, pred = pred, training = True)\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        metrics = {\"loss\": loss}\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "        y_infer = pred[0]\n",
    "        loss = self.cevae_loss(data = data, pred = pred, training = False)\n",
    "        y0, y1 = y_infer[0].sample(),y_infer[1].sample()\n",
    "        ate = tfkb.mean(y1) - tfkb.mean(y0)\n",
    "        metrics = {\"loss\":loss,\"y0\": tfkb.mean(y0),\"y1\": tfkb.mean(y1),'ate_afte_scaled': ate}\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-26 19:48:55.448308: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/3 [=========>....................] - ETA: 5s - loss: 29.5398 — ite: 4.6212  — ate: 3.4892 — pehe: 5.3648 \n",
      "3/3 [==============================] - 4s 399ms/step - loss: 29.2927 - val_loss: 29.7514 - val_y0: -0.1025 - val_y1: -0.0137 - val_ate_afte_scaled: 0.0888 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 29.5418 — ite: 4.5852  — ate: 3.1689 — pehe: 5.0711 \n",
      "3/3 [==============================] - 0s 217ms/step - loss: 28.9438 - val_loss: 29.6203 - val_y0: -0.0434 - val_y1: 0.1439 - val_ate_afte_scaled: 0.1873 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.3323 — ite: 4.3601  — ate: 2.5991 — pehe: 4.7529 \n",
      "3/3 [==============================] - 0s 185ms/step - loss: 28.6314 - val_loss: 29.3769 - val_y0: -0.0822 - val_y1: 0.3352 - val_ate_afte_scaled: 0.4174 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.4637 — ite: 4.1293  — ate: 2.2939 — pehe: 4.5334 \n",
      "3/3 [==============================] - 0s 175ms/step - loss: 28.3480 - val_loss: 28.8027 - val_y0: -0.1725 - val_y1: 0.4026 - val_ate_afte_scaled: 0.5751 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.2986 — ite: 4.1035  — ate: 1.9219 — pehe: 4.2731 \n",
      "3/3 [==============================] - 0s 165ms/step - loss: 27.9320 - val_loss: 28.7528 - val_y0: -0.3814 - val_y1: 0.4327 - val_ate_afte_scaled: 0.8141 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.4781 — ite: 3.8851  — ate: 1.3655 — pehe: 3.9092 \n",
      "3/3 [==============================] - 0s 169ms/step - loss: 27.5984 - val_loss: 28.2453 - val_y0: -0.2656 - val_y1: 0.6462 - val_ate_afte_scaled: 0.9118 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.6122 — ite: 3.9094  — ate: 1.0823 — pehe: 4.1253 \n",
      "3/3 [==============================] - 0s 168ms/step - loss: 27.3413 - val_loss: 27.8081 - val_y0: -0.3146 - val_y1: 0.7307 - val_ate_afte_scaled: 1.0453 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.4226 — ite: 3.8455  — ate: 1.0289 — pehe: 3.8507 \n",
      "3/3 [==============================] - 0s 162ms/step - loss: 26.9921 - val_loss: 27.6324 - val_y0: -0.1086 - val_y1: 0.8900 - val_ate_afte_scaled: 0.9985 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.2831 — ite: 3.7917  — ate: 0.6586 — pehe: 3.8304 \n",
      "3/3 [==============================] - 0s 174ms/step - loss: 26.7530 - val_loss: 27.1280 - val_y0: -0.2237 - val_y1: 1.0577 - val_ate_afte_scaled: 1.2814 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 26.1947 — ite: 3.7334  — ate: 0.7440 — pehe: 3.9385 \n",
      "3/3 [==============================] - 0s 163ms/step - loss: 26.4683 - val_loss: 26.9516 - val_y0: -0.2799 - val_y1: 1.0332 - val_ate_afte_scaled: 1.3130 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 26.5996 — ite: 3.7723  — ate: 0.5883 — pehe: 4.0080 \n",
      "3/3 [==============================] - 0s 171ms/step - loss: 26.1500 - val_loss: 26.5230 - val_y0: -0.0862 - val_y1: 1.1061 - val_ate_afte_scaled: 1.1923 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 26.0167 — ite: 3.7168  — ate: 0.1061 — pehe: 3.8798 \n",
      "3/3 [==============================] - 0s 170ms/step - loss: 25.9468 - val_loss: 26.4796 - val_y0: -0.2478 - val_y1: 1.1434 - val_ate_afte_scaled: 1.3912 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.7653 — ite: 3.7896  — ate: 0.1984 — pehe: 3.8297 \n",
      "3/3 [==============================] - 0s 205ms/step - loss: 25.6527 - val_loss: 26.2403 - val_y0: -0.3133 - val_y1: 1.3062 - val_ate_afte_scaled: 1.6196 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.6640 — ite: 3.7523  — ate: 0.1577 — pehe: 3.9330 \n",
      "3/3 [==============================] - 0s 158ms/step - loss: 25.1644 - val_loss: 25.7476 - val_y0: -0.2477 - val_y1: 1.2296 - val_ate_afte_scaled: 1.4773 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.3214 — ite: 3.7555  — ate: 0.0295 — pehe: 3.8640 \n",
      "3/3 [==============================] - 0s 154ms/step - loss: 24.9641 - val_loss: 25.6152 - val_y0: -0.1917 - val_y1: 1.2806 - val_ate_afte_scaled: 1.4723 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.4651 — ite: 3.7522  — ate: 0.1099 — pehe: 3.7595 \n",
      "3/3 [==============================] - 0s 156ms/step - loss: 24.6538 - val_loss: 25.0827 - val_y0: -0.1248 - val_y1: 1.3016 - val_ate_afte_scaled: 1.4263 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.7118 — ite: 3.7339  — ate: 0.2148 — pehe: 3.7916 \n",
      "3/3 [==============================] - 0s 157ms/step - loss: 24.4724 - val_loss: 24.4374 - val_y0: -0.3188 - val_y1: 1.2418 - val_ate_afte_scaled: 1.5606 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.2327 — ite: 3.8213  — ate: 0.2300 — pehe: 3.7974 \n",
      "3/3 [==============================] - 0s 155ms/step - loss: 24.0792 - val_loss: 24.2594 - val_y0: -0.2410 - val_y1: 1.2542 - val_ate_afte_scaled: 1.4953 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.0263 — ite: 3.8777  — ate: 0.0863 — pehe: 3.8919 \n",
      "3/3 [==============================] - 0s 152ms/step - loss: 23.6399 - val_loss: 24.1663 - val_y0: -0.2152 - val_y1: 1.1956 - val_ate_afte_scaled: 1.4108 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.8915 — ite: 3.8128  — ate: 0.0672 — pehe: 3.7687 \n",
      "3/3 [==============================] - 0s 158ms/step - loss: 23.6445 - val_loss: 24.1728 - val_y0: -0.3466 - val_y1: 1.3544 - val_ate_afte_scaled: 1.7010 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.3377 — ite: 3.8507  — ate: 0.1521 — pehe: 3.7720 \n",
      "3/3 [==============================] - 0s 165ms/step - loss: 23.2429 - val_loss: 23.4027 - val_y0: -0.3580 - val_y1: 1.0701 - val_ate_afte_scaled: 1.4281 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.8255 — ite: 3.7420  — ate: 0.0743 — pehe: 3.8590 \n",
      "3/3 [==============================] - 0s 157ms/step - loss: 22.8494 - val_loss: 22.8139 - val_y0: -0.2531 - val_y1: 1.2322 - val_ate_afte_scaled: 1.4853 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.6835 — ite: 3.8435  — ate: 0.0759 — pehe: 3.8468 \n",
      "3/3 [==============================] - 0s 180ms/step - loss: 22.4438 - val_loss: 22.8608 - val_y0: -0.4729 - val_y1: 1.1563 - val_ate_afte_scaled: 1.6291 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.0564 — ite: 3.7947  — ate: 0.1399 — pehe: 3.6289 \n",
      "3/3 [==============================] - 0s 188ms/step - loss: 22.2233 - val_loss: 22.6425 - val_y0: -0.1028 - val_y1: 1.1874 - val_ate_afte_scaled: 1.2902 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.0485 — ite: 3.8613  — ate: 0.0087 — pehe: 4.0221 \n",
      "3/3 [==============================] - 0s 155ms/step - loss: 22.0708 - val_loss: 22.3139 - val_y0: -0.2510 - val_y1: 1.3524 - val_ate_afte_scaled: 1.6034 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.5796 — ite: 3.8118  — ate: 0.1492 — pehe: 3.8382 \n",
      "3/3 [==============================] - 0s 155ms/step - loss: 21.5063 - val_loss: 21.3559 - val_y0: -0.3212 - val_y1: 1.1530 - val_ate_afte_scaled: 1.4743 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.9294 — ite: 3.7860  — ate: 0.0060 — pehe: 3.6104 \n",
      "3/3 [==============================] - 0s 171ms/step - loss: 21.2380 - val_loss: 21.8069 - val_y0: -0.3458 - val_y1: 1.1088 - val_ate_afte_scaled: 1.4546 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.8730 — ite: 3.8693  — ate: 0.0274 — pehe: 3.7679 \n",
      "3/3 [==============================] - 0s 154ms/step - loss: 20.8115 - val_loss: 21.0378 - val_y0: -0.3335 - val_y1: 1.2485 - val_ate_afte_scaled: 1.5820 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.5667 — ite: 3.8488  — ate: 0.2058 — pehe: 3.6819 \n",
      "3/3 [==============================] - 0s 164ms/step - loss: 20.6525 - val_loss: 20.9955 - val_y0: -0.2128 - val_y1: 1.0569 - val_ate_afte_scaled: 1.2697 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.2674 — ite: 3.8286  — ate: 0.0902 — pehe: 3.8527 \n",
      "3/3 [==============================] - 0s 168ms/step - loss: 20.3896 - val_loss: 20.8135 - val_y0: -0.2980 - val_y1: 1.2022 - val_ate_afte_scaled: 1.5001 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.2518 — ite: 3.7572  — ate: 0.0764 — pehe: 3.7963 \n",
      "3/3 [==============================] - 0s 158ms/step - loss: 20.1364 - val_loss: 20.2733 - val_y0: -0.2253 - val_y1: 1.2221 - val_ate_afte_scaled: 1.4474 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.9415 — ite: 3.7817  — ate: 0.1750 — pehe: 3.8301 \n",
      "3/3 [==============================] - 0s 162ms/step - loss: 19.8003 - val_loss: 20.0100 - val_y0: -0.2167 - val_y1: 1.1158 - val_ate_afte_scaled: 1.3325 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.1986 — ite: 3.8954  — ate: 0.4066 — pehe: 3.8101 \n",
      "3/3 [==============================] - 0s 162ms/step - loss: 19.4302 - val_loss: 20.2262 - val_y0: -0.3155 - val_y1: 1.0076 - val_ate_afte_scaled: 1.3231 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.6623 — ite: 3.7951  — ate: 0.3085 — pehe: 3.7792 \n",
      "3/3 [==============================] - 0s 173ms/step - loss: 19.2132 - val_loss: 20.1128 - val_y0: -0.3235 - val_y1: 1.0970 - val_ate_afte_scaled: 1.4206 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.3959 — ite: 3.8203  — ate: 0.4274 — pehe: 3.6532 \n",
      "3/3 [==============================] - 0s 186ms/step - loss: 18.9476 - val_loss: 19.1800 - val_y0: -0.1950 - val_y1: 0.9428 - val_ate_afte_scaled: 1.1378 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.4579 — ite: 3.8491  — ate: 0.3738 — pehe: 3.9561 \n",
      "3/3 [==============================] - 0s 159ms/step - loss: 18.9797 - val_loss: 19.7363 - val_y0: -0.1464 - val_y1: 1.0909 - val_ate_afte_scaled: 1.2373 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.1279 — ite: 3.8574  — ate: 0.3059 — pehe: 3.6404 \n",
      "3/3 [==============================] - 0s 154ms/step - loss: 18.6938 - val_loss: 18.7360 - val_y0: -0.3055 - val_y1: 1.1563 - val_ate_afte_scaled: 1.4618 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.2829 — ite: 3.8533  — ate: 0.3323 — pehe: 3.8970 \n",
      "3/3 [==============================] - 0s 162ms/step - loss: 18.3717 - val_loss: 19.0966 - val_y0: -0.3471 - val_y1: 1.1519 - val_ate_afte_scaled: 1.4990 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.1174 — ite: 3.8244  — ate: 0.3033 — pehe: 3.7952 \n",
      "3/3 [==============================] - 0s 175ms/step - loss: 17.8985 - val_loss: 18.3610 - val_y0: -0.3094 - val_y1: 1.1313 - val_ate_afte_scaled: 1.4408 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17.8827 — ite: 3.8608  — ate: 0.0959 — pehe: 3.8517 \n",
      "3/3 [==============================] - 0s 163ms/step - loss: 17.9924 - val_loss: 18.2061 - val_y0: -0.2889 - val_y1: 1.0056 - val_ate_afte_scaled: 1.2945 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17.2932 — ite: 3.8210  — ate: 0.0371 — pehe: 3.7924 \n",
      "3/3 [==============================] - 0s 161ms/step - loss: 17.3495 - val_loss: 17.2340 - val_y0: -0.2255 - val_y1: 1.0650 - val_ate_afte_scaled: 1.2905 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17.1224 — ite: 3.9278  — ate: 0.0854 — pehe: 3.8229 \n",
      "3/3 [==============================] - 0s 164ms/step - loss: 17.0580 - val_loss: 17.7409 - val_y0: -0.3530 - val_y1: 1.0258 - val_ate_afte_scaled: 1.3789 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16.3674 — ite: 3.8074  — ate: 0.0564 — pehe: 3.8018 \n",
      "3/3 [==============================] - 0s 162ms/step - loss: 16.7404 - val_loss: 17.4048 - val_y0: -0.2571 - val_y1: 1.0996 - val_ate_afte_scaled: 1.3567 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16.7718 — ite: 3.7536  — ate: 0.2811 — pehe: 3.7272 \n",
      "3/3 [==============================] - 0s 167ms/step - loss: 16.0405 - val_loss: 17.3344 - val_y0: -0.1005 - val_y1: 1.2251 - val_ate_afte_scaled: 1.3256 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15.9480 — ite: 3.8232  — ate: 0.0201 — pehe: 3.8387 \n",
      "3/3 [==============================] - 0s 197ms/step - loss: 15.8757 - val_loss: 16.4947 - val_y0: -0.3473 - val_y1: 0.9780 - val_ate_afte_scaled: 1.3253 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16.4329 — ite: 3.8684  — ate: 0.3736 — pehe: 3.7379 \n",
      "3/3 [==============================] - 0s 168ms/step - loss: 14.7650 - val_loss: 15.7861 - val_y0: -0.3409 - val_y1: 1.0502 - val_ate_afte_scaled: 1.3911 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14.8282 — ite: 3.8448  — ate: 0.1136 — pehe: 3.8145 \n",
      "3/3 [==============================] - 0s 163ms/step - loss: 14.2342 - val_loss: 15.1685 - val_y0: -0.1517 - val_y1: 1.1314 - val_ate_afte_scaled: 1.2831 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13.6356 — ite: 3.8780  — ate: 0.0236 — pehe: 3.8505 \n",
      "3/3 [==============================] - 0s 169ms/step - loss: 13.9283 - val_loss: 15.0685 - val_y0: -0.3233 - val_y1: 1.1168 - val_ate_afte_scaled: 1.4400 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 12.9587 — ite: 3.8910  — ate: 0.1491 — pehe: 3.7210 \n",
      "3/3 [==============================] - 0s 168ms/step - loss: 13.5804 - val_loss: 14.3170 - val_y0: -0.2740 - val_y1: 1.0957 - val_ate_afte_scaled: 1.3696 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13.6456 — ite: 3.8134  — ate: 0.3190 — pehe: 3.8401 \n",
      "3/3 [==============================] - 0s 165ms/step - loss: 12.0077 - val_loss: 13.3483 - val_y0: -0.2845 - val_y1: 1.0562 - val_ate_afte_scaled: 1.3407 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 12.1049 — ite: 3.8567  — ate: 0.4669 — pehe: 3.7143 \n",
      "3/3 [==============================] - 0s 165ms/step - loss: 11.5755 - val_loss: 12.1998 - val_y0: -0.1885 - val_y1: 1.1024 - val_ate_afte_scaled: 1.2909 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.6699 — ite: 3.8660  — ate: 0.1282 — pehe: 3.8376 \n",
      "3/3 [==============================] - 0s 165ms/step - loss: 10.7746 - val_loss: 10.5837 - val_y0: -0.3447 - val_y1: 1.0143 - val_ate_afte_scaled: 1.3590 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10.5693 — ite: 3.8508  — ate: 0.1935 — pehe: 3.8202 \n",
      "3/3 [==============================] - 0s 176ms/step - loss: 9.7432 - val_loss: 10.0290 - val_y0: -0.3249 - val_y1: 1.1080 - val_ate_afte_scaled: 1.4329 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.4701 — ite: 3.8387  — ate: 0.2332 — pehe: 3.7155 \n",
      "3/3 [==============================] - 0s 165ms/step - loss: 8.6884 - val_loss: 7.8809 - val_y0: -0.2463 - val_y1: 1.2975 - val_ate_afte_scaled: 1.5438 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.2834 — ite: 3.7566  — ate: 0.2152 — pehe: 3.7772 \n",
      "3/3 [==============================] - 0s 176ms/step - loss: 6.8881 - val_loss: 6.3010 - val_y0: -0.4029 - val_y1: 1.0506 - val_ate_afte_scaled: 1.4535 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.1515 — ite: 3.9061  — ate: 0.1400 — pehe: 3.9139 \n",
      "3/3 [==============================] - 0s 185ms/step - loss: 4.6811 - val_loss: 5.4338 - val_y0: -0.1411 - val_y1: 0.9843 - val_ate_afte_scaled: 1.1254 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0301 — ite: 3.9695  — ate: 0.0602 — pehe: 3.8261 \n",
      "3/3 [==============================] - 0s 164ms/step - loss: 3.1321 - val_loss: 3.4805 - val_y0: -0.1975 - val_y1: 1.0279 - val_ate_afte_scaled: 1.2254 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.3795 — ite: 3.8757  — ate: 0.1251 — pehe: 3.7117 \n",
      "3/3 [==============================] - 0s 165ms/step - loss: 1.2849 - val_loss: -0.5122 - val_y0: -0.2176 - val_y1: 1.1395 - val_ate_afte_scaled: 1.3571 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.6343 — ite: 3.9237  — ate: 0.2302 — pehe: 3.7487 \n",
      "3/3 [==============================] - 0s 166ms/step - loss: -1.3704 - val_loss: -2.4667 - val_y0: -0.2784 - val_y1: 1.0643 - val_ate_afte_scaled: 1.3427 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.9195 — ite: 3.9060  — ate: 0.1818 — pehe: 3.8737 \n",
      "3/3 [==============================] - 0s 166ms/step - loss: -3.6141 - val_loss: -4.2303 - val_y0: -0.1931 - val_y1: 1.0750 - val_ate_afte_scaled: 1.2681 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -8.3793 — ite: 4.0274  — ate: 0.2791 — pehe: 3.9835 \n",
      "3/3 [==============================] - 0s 171ms/step - loss: -7.5635 - val_loss: -7.2008 - val_y0: -0.0956 - val_y1: 1.0853 - val_ate_afte_scaled: 1.1809 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -8.2930 — ite: 3.9414  — ate: 0.1396 — pehe: 3.9420 \n",
      "3/3 [==============================] - 0s 163ms/step - loss: -9.9553 - val_loss: -11.7773 - val_y0: -0.2361 - val_y1: 1.2556 - val_ate_afte_scaled: 1.4916 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -11.3396 — ite: 3.9785  — ate: 0.0645 — pehe: 3.8548 \n",
      "3/3 [==============================] - 0s 165ms/step - loss: -14.6317 - val_loss: -17.2726 - val_y0: -0.3236 - val_y1: 1.1519 - val_ate_afte_scaled: 1.4755 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -13.8024 — ite: 4.1098  — ate: 0.0641 — pehe: 4.1173 \n",
      "3/3 [==============================] - 0s 173ms/step - loss: -18.6168 - val_loss: -20.6940 - val_y0: -0.2295 - val_y1: 1.2308 - val_ate_afte_scaled: 1.4603 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -17.8300 — ite: 3.9988  — ate: 0.2126 — pehe: 3.9281 \n",
      "3/3 [==============================] - 0s 177ms/step - loss: -23.8256 - val_loss: -26.3026 - val_y0: -0.2418 - val_y1: 1.0820 - val_ate_afte_scaled: 1.3238 - lr: 1.0000e-04\n",
      "Epoch 66/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -16.8399 — ite: 4.1384  — ate: 0.2263 — pehe: 3.9756 \n",
      "3/3 [==============================] - 0s 193ms/step - loss: -29.9711 - val_loss: -31.8802 - val_y0: -0.1812 - val_y1: 1.0548 - val_ate_afte_scaled: 1.2359 - lr: 1.0000e-04\n",
      "Epoch 67/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -34.1338 — ite: 4.0866  — ate: 0.3114 — pehe: 3.9280 \n",
      "3/3 [==============================] - 0s 177ms/step - loss: -35.0599 - val_loss: -38.6395 - val_y0: -0.0939 - val_y1: 1.0713 - val_ate_afte_scaled: 1.1652 - lr: 1.0000e-04\n",
      "Epoch 68/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -32.6346 — ite: 4.0314  — ate: 0.2823 — pehe: 3.7207 \n",
      "3/3 [==============================] - 0s 174ms/step - loss: -44.9825 - val_loss: -48.8052 - val_y0: -0.1874 - val_y1: 1.0980 - val_ate_afte_scaled: 1.2854 - lr: 1.0000e-04\n",
      "Epoch 69/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -40.9018 — ite: 4.2139  — ate: 0.3208 — pehe: 4.0907 \n",
      "3/3 [==============================] - 0s 169ms/step - loss: -51.9408 - val_loss: -55.0767 - val_y0: -0.1227 - val_y1: 1.1725 - val_ate_afte_scaled: 1.2952 - lr: 1.0000e-04\n",
      "Epoch 70/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -59.9286 — ite: 4.1921  — ate: 0.1182 — pehe: 4.0715 \n",
      "3/3 [==============================] - 0s 171ms/step - loss: -59.1255 - val_loss: -63.2301 - val_y0: -0.2141 - val_y1: 1.2164 - val_ate_afte_scaled: 1.4305 - lr: 1.0000e-04\n",
      "Epoch 71/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -57.6331 — ite: 4.1870  — ate: 0.3953 — pehe: 4.0029 \n",
      "3/3 [==============================] - 0s 163ms/step - loss: -75.4516 - val_loss: -78.7644 - val_y0: -0.1897 - val_y1: 1.3075 - val_ate_afte_scaled: 1.4973 - lr: 1.0000e-04\n",
      "Epoch 72/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -69.7248 — ite: 4.4409  — ate: 0.2130 — pehe: 4.0272 \n",
      "3/3 [==============================] - 0s 172ms/step - loss: -89.5228 - val_loss: -91.8995 - val_y0: -0.2871 - val_y1: 1.2686 - val_ate_afte_scaled: 1.5558 - lr: 1.0000e-04\n",
      "Epoch 73/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -79.3937 — ite: 4.5108  — ate: 0.4453 — pehe: 4.2987 \n",
      "3/3 [==============================] - 0s 164ms/step - loss: -104.9518 - val_loss: -105.8650 - val_y0: -0.3476 - val_y1: 1.2320 - val_ate_afte_scaled: 1.5797 - lr: 1.0000e-04\n",
      "Epoch 74/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -97.7245 — ite: 4.8327  — ate: 0.2517 — pehe: 4.3809 \n",
      "3/3 [==============================] - 0s 166ms/step - loss: -117.4627 - val_loss: -122.6636 - val_y0: -0.2559 - val_y1: 1.0895 - val_ate_afte_scaled: 1.3454 - lr: 1.0000e-04\n",
      "Epoch 75/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -126.6366 — ite: 4.9672  — ate: 1.0567 — pehe: 4.2243 \n",
      "3/3 [==============================] - 0s 172ms/step - loss: -134.5339 - val_loss: -144.3984 - val_y0: -0.3019 - val_y1: 1.0257 - val_ate_afte_scaled: 1.3276 - lr: 1.0000e-04\n",
      "Epoch 76/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -137.8863 — ite: 5.0512  — ate: 0.8111 — pehe: 4.4335 \n",
      "3/3 [==============================] - 0s 204ms/step - loss: -152.4361 - val_loss: -169.1495 - val_y0: -0.3736 - val_y1: 0.9662 - val_ate_afte_scaled: 1.3397 - lr: 1.0000e-04\n",
      "Epoch 77/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -157.1907 — ite: 5.3260  — ate: 0.6432 — pehe: 4.3038 \n",
      "3/3 [==============================] - 0s 182ms/step - loss: -180.0806 - val_loss: -193.2475 - val_y0: -0.2169 - val_y1: 0.8028 - val_ate_afte_scaled: 1.0197 - lr: 1.0000e-04\n",
      "Epoch 78/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -179.3659 — ite: 5.5815  — ate: 0.9343 — pehe: 4.3935 \n",
      "3/3 [==============================] - 0s 169ms/step - loss: -203.9689 - val_loss: -221.0889 - val_y0: -0.4619 - val_y1: 0.8964 - val_ate_afte_scaled: 1.3583 - lr: 1.0000e-04\n",
      "Epoch 79/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -231.6487 — ite: 6.0639  — ate: 0.8937 — pehe: 4.5803 \n",
      "3/3 [==============================] - 0s 169ms/step - loss: -235.6001 - val_loss: -256.4980 - val_y0: -0.2378 - val_y1: 0.7618 - val_ate_afte_scaled: 0.9996 - lr: 1.0000e-04\n",
      "Epoch 80/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -270.0056 — ite: 6.2643  — ate: 0.6476 — pehe: 4.6758 \n",
      "3/3 [==============================] - 0s 171ms/step - loss: -271.9444 - val_loss: -292.9973 - val_y0: -0.1340 - val_y1: 0.7968 - val_ate_afte_scaled: 0.9308 - lr: 1.0000e-04\n",
      "Epoch 81/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -276.6171 — ite: 6.7319  — ate: 0.8720 — pehe: 4.7969 \n",
      "3/3 [==============================] - 0s 179ms/step - loss: -323.5313 - val_loss: -339.6561 - val_y0: -0.1714 - val_y1: 0.9884 - val_ate_afte_scaled: 1.1598 - lr: 1.0000e-04\n",
      "Epoch 82/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -341.2003 — ite: 7.2217  — ate: 0.7874 — pehe: 4.7971 \n",
      "3/3 [==============================] - 0s 174ms/step - loss: -359.6687 - val_loss: -391.9302 - val_y0: -0.3304 - val_y1: 0.8001 - val_ate_afte_scaled: 1.1305 - lr: 1.0000e-04\n",
      "Epoch 83/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -448.5148 — ite: 7.7656  — ate: 0.5472 — pehe: 4.5688 \n",
      "3/3 [==============================] - 0s 169ms/step - loss: -400.6348 - val_loss: -452.4912 - val_y0: -0.2412 - val_y1: 1.0755 - val_ate_afte_scaled: 1.3167 - lr: 1.0000e-04\n",
      "Epoch 84/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -468.8638 — ite: 8.2406  — ate: 0.7735 — pehe: 4.9198 \n",
      "3/3 [==============================] - 0s 175ms/step - loss: -496.3783 - val_loss: -527.3975 - val_y0: -0.1616 - val_y1: 0.9974 - val_ate_afte_scaled: 1.1590 - lr: 1.0000e-04\n",
      "Epoch 85/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -577.4535 — ite: 8.3744  — ate: 1.1044 — pehe: 4.8929 \n",
      "3/3 [==============================] - 0s 171ms/step - loss: -549.9413 - val_loss: -603.6181 - val_y0: -0.0797 - val_y1: 0.9734 - val_ate_afte_scaled: 1.0531 - lr: 1.0000e-04\n",
      "Epoch 86/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -631.8485 — ite: 8.7384  — ate: 1.0457 — pehe: 4.8915 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: -650.5882 - val_loss: -698.5413 - val_y0: 0.0857 - val_y1: 1.0933 - val_ate_afte_scaled: 1.0076 - lr: 1.0000e-04\n",
      "Epoch 87/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -687.3194 — ite: 9.2067  — ate: 1.2697 — pehe: 4.7947 \n",
      "3/3 [==============================] - 0s 178ms/step - loss: -741.9795 - val_loss: -808.6823 - val_y0: 0.1267 - val_y1: 0.9821 - val_ate_afte_scaled: 0.8553 - lr: 1.0000e-04\n",
      "Epoch 88/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -785.4055 — ite: 9.6770  — ate: 1.3089 — pehe: 4.8695 \n",
      "3/3 [==============================] - 0s 170ms/step - loss: -816.2436 - val_loss: -901.6511 - val_y0: 0.1036 - val_y1: 0.9588 - val_ate_afte_scaled: 0.8551 - lr: 1.0000e-04\n",
      "Epoch 89/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1002.0270 — ite: 9.9974  — ate: 1.6888 — pehe: 5.3006 \n",
      "3/3 [==============================] - 0s 182ms/step - loss: -952.6227 - val_loss: -1021.0490 - val_y0: 0.0371 - val_y1: 0.9118 - val_ate_afte_scaled: 0.8747 - lr: 1.0000e-04\n",
      "Epoch 90/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1101.8419 — ite: 10.3986  — ate: 1.8706 — pehe: 5.1924 \n",
      "3/3 [==============================] - 1s 249ms/step - loss: -1039.7984 - val_loss: -1140.6442 - val_y0: 0.5180 - val_y1: 1.1692 - val_ate_afte_scaled: 0.6512 - lr: 1.0000e-04\n",
      "Epoch 91/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1144.9733 — ite: 10.8375  — ate: 2.0976 — pehe: 5.4118 \n",
      "3/3 [==============================] - 1s 274ms/step - loss: -1204.1566 - val_loss: -1302.0432 - val_y0: 0.5604 - val_y1: 1.0194 - val_ate_afte_scaled: 0.4590 - lr: 1.0000e-04\n",
      "Epoch 92/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1223.6426 — ite: 11.4337  — ate: 1.9629 — pehe: 5.2381 \n",
      "3/3 [==============================] - 0s 192ms/step - loss: -1337.4677 - val_loss: -1444.6982 - val_y0: 0.5741 - val_y1: 1.2981 - val_ate_afte_scaled: 0.7240 - lr: 1.0000e-04\n",
      "Epoch 93/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1403.2114 — ite: 11.9890  — ate: 1.9610 — pehe: 5.2400 \n",
      "3/3 [==============================] - 0s 196ms/step - loss: -1557.1548 - val_loss: -1630.1930 - val_y0: 0.5702 - val_y1: 1.3641 - val_ate_afte_scaled: 0.7940 - lr: 1.0000e-04\n",
      "Epoch 94/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1676.0541 — ite: 12.5902  — ate: 2.1011 — pehe: 5.2727 \n",
      "3/3 [==============================] - 0s 193ms/step - loss: -1680.1939 - val_loss: -1823.4570 - val_y0: 0.7078 - val_y1: 1.3639 - val_ate_afte_scaled: 0.6561 - lr: 1.0000e-04\n",
      "Epoch 95/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1752.7534 — ite: 13.3544  — ate: 2.0770 — pehe: 5.3909 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: -1871.7959 - val_loss: -2047.8423 - val_y0: 0.7999 - val_y1: 1.6436 - val_ate_afte_scaled: 0.8437 - lr: 1.0000e-04\n",
      "Epoch 96/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1771.4066 — ite: 14.0168  — ate: 2.1271 — pehe: 5.6574 \n",
      "3/3 [==============================] - 0s 192ms/step - loss: -2181.2643 - val_loss: -2276.4021 - val_y0: 1.0536 - val_y1: 1.8093 - val_ate_afte_scaled: 0.7557 - lr: 1.0000e-04\n",
      "Epoch 97/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -2544.3701 — ite: 14.3366  — ate: 1.9496 — pehe: 5.6155 \n",
      "3/3 [==============================] - 0s 192ms/step - loss: -2334.4178 - val_loss: -2550.0586 - val_y0: 0.8690 - val_y1: 1.5784 - val_ate_afte_scaled: 0.7094 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -2637.3293 — ite: 15.2744  — ate: 2.0551 — pehe: 5.4192 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: -2602.3868 - val_loss: -2838.6130 - val_y0: 1.0668 - val_y1: 1.7805 - val_ate_afte_scaled: 0.7137 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -2719.3672 — ite: 15.9890  — ate: 2.0303 — pehe: 5.3903 \n",
      "3/3 [==============================] - 0s 188ms/step - loss: -3021.8715 - val_loss: -3191.1741 - val_y0: 1.1135 - val_y1: 1.6357 - val_ate_afte_scaled: 0.5221 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -2806.7678 — ite: 16.5748  — ate: 1.6157 — pehe: 5.1664 \n",
      "3/3 [==============================] - 0s 191ms/step - loss: -3416.4619 - val_loss: -3542.8621 - val_y0: 1.0035 - val_y1: 1.6914 - val_ate_afte_scaled: 0.6880 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -3240.0461 — ite: 17.5981  — ate: 1.8393 — pehe: 5.3876 \n",
      "3/3 [==============================] - 0s 214ms/step - loss: -3588.3311 - val_loss: -3917.2039 - val_y0: 1.0978 - val_y1: 1.7182 - val_ate_afte_scaled: 0.6204 - lr: 1.0000e-04\n",
      "Epoch 102/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -4049.0815 — ite: 17.6921  — ate: 1.4717 — pehe: 5.1965 \n",
      "3/3 [==============================] - 1s 260ms/step - loss: -4076.5762 - val_loss: -4401.4092 - val_y0: 0.9223 - val_y1: 1.9209 - val_ate_afte_scaled: 0.9986 - lr: 1.0000e-04\n",
      "Epoch 103/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -4345.3608 — ite: 18.5298  — ate: 1.2786 — pehe: 4.7023 \n",
      "3/3 [==============================] - 0s 241ms/step - loss: -4527.7915 - val_loss: -4849.6279 - val_y0: 1.3338 - val_y1: 2.2115 - val_ate_afte_scaled: 0.8777 - lr: 1.0000e-04\n",
      "Epoch 104/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -4705.9258 — ite: 19.0492  — ate: 0.9012 — pehe: 4.7765 \n",
      "3/3 [==============================] - 1s 278ms/step - loss: -4952.2268 - val_loss: -5383.5742 - val_y0: 1.2359 - val_y1: 2.2555 - val_ate_afte_scaled: 1.0197 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -5079.8228 — ite: 19.6377  — ate: 0.5405 — pehe: 4.5632 \n",
      "3/3 [==============================] - 1s 239ms/step - loss: -5653.0480 - val_loss: -5964.4141 - val_y0: 0.9282 - val_y1: 1.9661 - val_ate_afte_scaled: 1.0379 - lr: 1.0000e-04\n",
      "Epoch 106/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -6213.2319 — ite: 20.3203  — ate: 0.7315 — pehe: 4.6282 \n",
      "3/3 [==============================] - 1s 306ms/step - loss: -6115.2145 - val_loss: -6542.7544 - val_y0: 1.3374 - val_y1: 2.3969 - val_ate_afte_scaled: 1.0595 - lr: 1.0000e-04\n",
      "Epoch 107/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -6722.6914 — ite: 20.6636  — ate: 0.8611 — pehe: 4.4101 \n",
      "3/3 [==============================] - 0s 238ms/step - loss: -6973.6660 - val_loss: -7311.9160 - val_y0: 1.2977 - val_y1: 2.4288 - val_ate_afte_scaled: 1.1311 - lr: 1.0000e-04\n",
      "Epoch 108/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -8200.7363 — ite: 21.0357  — ate: 0.7530 — pehe: 4.3624 \n",
      "3/3 [==============================] - 0s 185ms/step - loss: -7251.5930 - val_loss: -8006.8315 - val_y0: 1.0998 - val_y1: 2.3553 - val_ate_afte_scaled: 1.2555 - lr: 1.0000e-04\n",
      "Epoch 109/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -8563.7598 — ite: 22.2248  — ate: 0.4690 — pehe: 4.4719 \n",
      "3/3 [==============================] - 0s 180ms/step - loss: -8164.0461 - val_loss: -8913.1533 - val_y0: 1.3589 - val_y1: 2.5650 - val_ate_afte_scaled: 1.2060 - lr: 1.0000e-04\n",
      "Epoch 110/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -8170.5112 — ite: 22.9912  — ate: 0.3124 — pehe: 4.6188 \n",
      "3/3 [==============================] - 0s 190ms/step - loss: -9124.4801 - val_loss: -9733.5762 - val_y0: 1.1981 - val_y1: 2.7071 - val_ate_afte_scaled: 1.5090 - lr: 1.0000e-04\n",
      "Epoch 111/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -10029.4434 — ite: 24.0075  — ate: 0.3777 — pehe: 4.4914 \n",
      "3/3 [==============================] - 0s 177ms/step - loss: -10186.3235 - val_loss: -10737.0693 - val_y0: 1.2501 - val_y1: 2.7082 - val_ate_afte_scaled: 1.4581 - lr: 1.0000e-04\n",
      "Epoch 112/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -11285.7354 — ite: 24.8061  — ate: 0.1244 — pehe: 4.3451 \n",
      "3/3 [==============================] - 0s 182ms/step - loss: -11221.3745 - val_loss: -11757.9180 - val_y0: 1.4394 - val_y1: 2.8433 - val_ate_afte_scaled: 1.4038 - lr: 1.0000e-04\n",
      "Epoch 113/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -11902.4375 — ite: 25.0577  — ate: 0.1092 — pehe: 4.3571 \n",
      "3/3 [==============================] - 0s 180ms/step - loss: -12332.4919 - val_loss: -12920.0898 - val_y0: 1.4334 - val_y1: 2.9462 - val_ate_afte_scaled: 1.5128 - lr: 1.0000e-04\n",
      "Epoch 114/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -11338.9727 — ite: 25.8891  — ate: 0.0966 — pehe: 4.2181 \n",
      "3/3 [==============================] - 0s 185ms/step - loss: -13601.7139 - val_loss: -14199.4463 - val_y0: 1.5354 - val_y1: 3.0343 - val_ate_afte_scaled: 1.4990 - lr: 1.0000e-04\n",
      "Epoch 115/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -13415.6875 — ite: 26.6538  — ate: 0.3197 — pehe: 4.2669 \n",
      "3/3 [==============================] - 0s 177ms/step - loss: -14773.0999 - val_loss: -15603.3467 - val_y0: 1.4608 - val_y1: 3.0131 - val_ate_afte_scaled: 1.5524 - lr: 1.0000e-04\n",
      "Epoch 116/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -15184.6133 — ite: 27.5237  — ate: 0.0323 — pehe: 4.4570 \n",
      "3/3 [==============================] - 0s 185ms/step - loss: -16057.8977 - val_loss: -16961.5254 - val_y0: 1.4105 - val_y1: 2.8932 - val_ate_afte_scaled: 1.4826 - lr: 1.0000e-04\n",
      "Epoch 117/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -18409.3867 — ite: 28.9522  — ate: 0.7446 — pehe: 4.5893 \n",
      "3/3 [==============================] - 0s 192ms/step - loss: -17217.6362 - val_loss: -18594.3770 - val_y0: 1.5222 - val_y1: 3.4653 - val_ate_afte_scaled: 1.9431 - lr: 1.0000e-04\n",
      "Epoch 118/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -18290.7773 — ite: 28.7400  — ate: 0.7141 — pehe: 4.4930 \n",
      "3/3 [==============================] - 0s 180ms/step - loss: 6595906886442.0195 - val_loss: -19748.0137 - val_y0: 1.2637 - val_y1: 2.8951 - val_ate_afte_scaled: 1.6314 - lr: 1.0000e-04\n",
      "Epoch 119/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -19545.5078 — ite: 26.9820  — ate: 0.5830 — pehe: 4.6589 \n",
      "3/3 [==============================] - 0s 179ms/step - loss: -20150.8604 - val_loss: -20079.6152 - val_y0: 0.0861 - val_y1: 1.7239 - val_ate_afte_scaled: 1.6378 - lr: 1.0000e-04\n",
      "Epoch 120/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -21291.0879 — ite: 25.9778  — ate: 0.7782 — pehe: 4.4800 \n",
      "3/3 [==============================] - 0s 169ms/step - loss: -19676.7480 - val_loss: -19995.5508 - val_y0: -0.8013 - val_y1: 0.9910 - val_ate_afte_scaled: 1.7923 - lr: 1.0000e-04\n",
      "Epoch 121/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -19808.3008 — ite: 25.3136  — ate: 0.7024 — pehe: 4.6292 \n",
      "3/3 [==============================] - 0s 170ms/step - loss: -20312.0435 - val_loss: -20078.9941 - val_y0: -1.1664 - val_y1: 0.4788 - val_ate_afte_scaled: 1.6452 - lr: 1.0000e-04\n",
      "Epoch 122/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20210.0742 — ite: 24.7780  — ate: 0.7887 — pehe: 4.5240 \n",
      "3/3 [==============================] - 0s 172ms/step - loss: -20039.6992 - val_loss: -19370.4707 - val_y0: -1.7259 - val_y1: 0.0223 - val_ate_afte_scaled: 1.7482 - lr: 1.0000e-04\n",
      "Epoch 123/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20710.4375 — ite: 24.5971  — ate: 0.6293 — pehe: 4.5317 \n",
      "3/3 [==============================] - 0s 171ms/step - loss: -16843.7668 - val_loss: -20077.2207 - val_y0: -1.9122 - val_y1: -0.1398 - val_ate_afte_scaled: 1.7724 - lr: 1.0000e-04\n",
      "Epoch 124/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20107.4570 — ite: 24.3506  — ate: 1.0878 — pehe: 4.7722 \n",
      "3/3 [==============================] - 0s 173ms/step - loss: -20538.6558 - val_loss: -20208.5566 - val_y0: -2.0187 - val_y1: -0.2938 - val_ate_afte_scaled: 1.7249 - lr: 1.0000e-04\n",
      "Epoch 125/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20888.2539 — ite: 24.2894  — ate: 0.8777 — pehe: 4.7247 \n",
      "3/3 [==============================] - 0s 177ms/step - loss: -20405.3989 - val_loss: -20580.5293 - val_y0: -2.3892 - val_y1: -0.5384 - val_ate_afte_scaled: 1.8507 - lr: 1.0000e-04\n",
      "Epoch 126/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20487.1895 — ite: 24.2305  — ate: 0.8543 — pehe: 4.5955 \n",
      "3/3 [==============================] - 0s 180ms/step - loss: -20749.8569 - val_loss: -20584.8359 - val_y0: -2.2433 - val_y1: -0.5348 - val_ate_afte_scaled: 1.7085 - lr: 1.0000e-04\n",
      "Epoch 127/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -18964.4727 — ite: 23.9819  — ate: 0.8366 — pehe: 4.5307 \n",
      "3/3 [==============================] - 0s 183ms/step - loss: -20547.9771 - val_loss: -20912.0957 - val_y0: -2.4634 - val_y1: -0.7955 - val_ate_afte_scaled: 1.6679 - lr: 1.0000e-04\n",
      "Epoch 128/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22168.1797 — ite: 23.9670  — ate: 1.1477 — pehe: 4.5931 \n",
      "3/3 [==============================] - 0s 178ms/step - loss: -20946.8223 - val_loss: -20990.8027 - val_y0: -2.5300 - val_y1: -0.7853 - val_ate_afte_scaled: 1.7447 - lr: 1.0000e-04\n",
      "Epoch 129/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -21595.6055\n",
      "Epoch 00129: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      " — ite: 24.0981  — ate: 0.8932 — pehe: 4.7549 \n",
      "3/3 [==============================] - 0s 180ms/step - loss: -20926.0776 - val_loss: -21041.4316 - val_y0: -2.6381 - val_y1: -0.8657 - val_ate_afte_scaled: 1.7724 - lr: 1.0000e-04\n",
      "Epoch 130/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -21164.4355 — ite: 23.9103  — ate: 1.0453 — pehe: 4.6760 \n",
      "3/3 [==============================] - 0s 191ms/step - loss: -20902.1699 - val_loss: -21299.0488 - val_y0: -2.7702 - val_y1: -0.8303 - val_ate_afte_scaled: 1.9399 - lr: 5.0000e-05\n",
      "Epoch 131/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20635.2109 — ite: 23.9512  — ate: 0.8098 — pehe: 4.6769 \n",
      "3/3 [==============================] - 0s 177ms/step - loss: -21554.5986 - val_loss: -21317.0527 - val_y0: -2.6377 - val_y1: -0.9512 - val_ate_afte_scaled: 1.6865 - lr: 5.0000e-05\n",
      "Epoch 132/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -21533.5801 — ite: 23.7995  — ate: 1.1674 — pehe: 4.7139 \n",
      "3/3 [==============================] - 0s 187ms/step - loss: -21533.1313 - val_loss: -21542.6504 - val_y0: -2.6791 - val_y1: -0.7805 - val_ate_afte_scaled: 1.8986 - lr: 5.0000e-05\n",
      "Epoch 133/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20305.4004 — ite: 23.9877  — ate: 0.9810 — pehe: 4.6082 \n",
      "3/3 [==============================] - 0s 183ms/step - loss: -21286.6133 - val_loss: -21552.0449 - val_y0: -2.7010 - val_y1: -0.7414 - val_ate_afte_scaled: 1.9596 - lr: 5.0000e-05\n",
      "Epoch 134/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20483.6777 — ite: 24.0282  — ate: 1.0557 — pehe: 4.7384 \n",
      "3/3 [==============================] - 0s 179ms/step - loss: -21181.2456 - val_loss: -21838.1777 - val_y0: -2.5956 - val_y1: -0.8886 - val_ate_afte_scaled: 1.7070 - lr: 5.0000e-05\n",
      "Epoch 135/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22968.7559 — ite: 23.9552  — ate: 1.0113 — pehe: 4.5933 \n",
      "3/3 [==============================] - 0s 174ms/step - loss: -21376.5811 - val_loss: -21999.3242 - val_y0: -2.6140 - val_y1: -0.8868 - val_ate_afte_scaled: 1.7271 - lr: 5.0000e-05\n",
      "Epoch 136/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -23424.6309\n",
      "Epoch 00136: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      " — ite: 23.9649  — ate: 0.8675 — pehe: 4.6144 \n",
      "3/3 [==============================] - 0s 179ms/step - loss: -21991.6128 - val_loss: -22193.1113 - val_y0: -2.6755 - val_y1: -0.9916 - val_ate_afte_scaled: 1.6840 - lr: 5.0000e-05\n",
      "Epoch 137/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20456.7852 — ite: 23.8620  — ate: 0.9020 — pehe: 4.6639 \n",
      "3/3 [==============================] - 0s 177ms/step - loss: -21866.9800 - val_loss: -22136.3633 - val_y0: -2.7072 - val_y1: -0.8100 - val_ate_afte_scaled: 1.8972 - lr: 2.5000e-05\n",
      "Epoch 138/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20967.9941 — ite: 23.8821  — ate: 0.8823 — pehe: 4.6954 \n",
      "3/3 [==============================] - 0s 183ms/step - loss: -22501.7944 - val_loss: -22119.6680 - val_y0: -2.7578 - val_y1: -0.9823 - val_ate_afte_scaled: 1.7756 - lr: 2.5000e-05\n",
      "Epoch 139/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20149.1230 — ite: 23.8988  — ate: 1.3112 — pehe: 4.5294 \n",
      "3/3 [==============================] - 0s 173ms/step - loss: -21863.0464 - val_loss: -22400.8242 - val_y0: -2.6778 - val_y1: -0.9498 - val_ate_afte_scaled: 1.7279 - lr: 2.5000e-05\n",
      "Epoch 140/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -23910.6953 — ite: 24.0414  — ate: 0.7763 — pehe: 4.5462 \n",
      "3/3 [==============================] - 0s 184ms/step - loss: -22138.8667 - val_loss: -22333.5723 - val_y0: -2.5079 - val_y1: -0.9406 - val_ate_afte_scaled: 1.5673 - lr: 2.5000e-05\n",
      "Epoch 141/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -24793.4648 — ite: 24.0385  — ate: 0.5987 — pehe: 4.7682 \n",
      "3/3 [==============================] - 0s 185ms/step - loss: -22317.3638 - val_loss: -22508.7480 - val_y0: -2.6696 - val_y1: -0.8919 - val_ate_afte_scaled: 1.7777 - lr: 2.5000e-05\n",
      "Epoch 142/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -21691.0742 — ite: 23.9370  — ate: 0.8651 — pehe: 4.6159 \n",
      "3/3 [==============================] - 0s 180ms/step - loss: -21902.6914 - val_loss: -22573.0703 - val_y0: -2.6974 - val_y1: -0.9610 - val_ate_afte_scaled: 1.7364 - lr: 2.5000e-05\n",
      "Epoch 143/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -21811.8203\n",
      "Epoch 00143: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      " — ite: 23.8424  — ate: 0.9806 — pehe: 4.5007 \n",
      "3/3 [==============================] - 0s 186ms/step - loss: -21875.9878 - val_loss: -22703.4941 - val_y0: -2.6212 - val_y1: -0.9037 - val_ate_afte_scaled: 1.7175 - lr: 2.5000e-05\n",
      "Epoch 144/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -23047.0820 — ite: 23.9863  — ate: 0.9554 — pehe: 4.6985 \n",
      "3/3 [==============================] - 0s 189ms/step - loss: -22828.9229 - val_loss: -22482.0723 - val_y0: -2.7582 - val_y1: -0.9429 - val_ate_afte_scaled: 1.8153 - lr: 1.2500e-05\n",
      "Epoch 145/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -21591.6855 — ite: 23.8362  — ate: 1.0130 — pehe: 4.7533 \n",
      "3/3 [==============================] - 0s 181ms/step - loss: -22572.0737 - val_loss: -22611.4668 - val_y0: -2.6928 - val_y1: -0.8763 - val_ate_afte_scaled: 1.8166 - lr: 1.2500e-05\n",
      "Epoch 146/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -23475.4902 — ite: 23.9743  — ate: 0.8711 — pehe: 4.7978 \n",
      "3/3 [==============================] - 0s 190ms/step - loss: -22703.8828 - val_loss: -22737.0566 - val_y0: -2.6850 - val_y1: -0.9221 - val_ate_afte_scaled: 1.7629 - lr: 1.2500e-05\n",
      "Epoch 147/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20573.9297 — ite: 24.0068  — ate: 0.9285 — pehe: 4.5752 \n",
      "3/3 [==============================] - 0s 190ms/step - loss: -23525.4839 - val_loss: -22585.5410 - val_y0: -2.5730 - val_y1: -1.0253 - val_ate_afte_scaled: 1.5477 - lr: 1.2500e-05\n",
      "Epoch 148/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -21512.2949 — ite: 24.0180  — ate: 1.2616 — pehe: 4.7911 \n",
      "3/3 [==============================] - 0s 183ms/step - loss: -22979.3301 - val_loss: -22623.2559 - val_y0: -2.6373 - val_y1: -0.9404 - val_ate_afte_scaled: 1.6970 - lr: 1.2500e-05\n",
      "Epoch 149/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -23138.8945 — ite: 24.0188  — ate: 0.8144 — pehe: 4.6264 \n",
      "3/3 [==============================] - 0s 186ms/step - loss: -22902.7080 - val_loss: -22807.0273 - val_y0: -2.7365 - val_y1: -0.9054 - val_ate_afte_scaled: 1.8311 - lr: 1.2500e-05\n",
      "Epoch 150/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -23267.2832 — ite: 24.0473  — ate: 1.2678 — pehe: 4.7989 \n",
      "3/3 [==============================] - 0s 192ms/step - loss: -23036.0732 - val_loss: -22845.0859 - val_y0: -2.6652 - val_y1: -0.8523 - val_ate_afte_scaled: 1.8128 - lr: 1.2500e-05\n",
      "Epoch 151/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22700.9922 — ite: 24.0314  — ate: 0.8221 — pehe: 4.5662 \n",
      "3/3 [==============================] - 0s 194ms/step - loss: -22528.4868 - val_loss: -22922.9766 - val_y0: -2.7152 - val_y1: -0.9047 - val_ate_afte_scaled: 1.8105 - lr: 1.2500e-05\n",
      "Epoch 152/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22577.3301\n",
      "Epoch 00152: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      " — ite: 23.9535  — ate: 0.9671 — pehe: 4.6064 \n",
      "3/3 [==============================] - 0s 182ms/step - loss: -22367.7021 - val_loss: -22924.1562 - val_y0: -2.6907 - val_y1: -0.7384 - val_ate_afte_scaled: 1.9522 - lr: 1.2500e-05\n",
      "Epoch 153/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22866.8223 — ite: 23.8659  — ate: 0.6410 — pehe: 4.6889 \n",
      "3/3 [==============================] - 0s 189ms/step - loss: -22583.0381 - val_loss: -22772.2773 - val_y0: -2.7714 - val_y1: -1.0007 - val_ate_afte_scaled: 1.7707 - lr: 6.2500e-06\n",
      "Epoch 154/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22315.7695 — ite: 23.9651  — ate: 0.9035 — pehe: 4.6423 \n",
      "3/3 [==============================] - 0s 195ms/step - loss: -23449.0674 - val_loss: -22778.3594 - val_y0: -2.7603 - val_y1: -0.9226 - val_ate_afte_scaled: 1.8377 - lr: 6.2500e-06\n",
      "Epoch 155/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22121.3574 — ite: 24.0099  — ate: 0.8764 — pehe: 4.6708 \n",
      "3/3 [==============================] - 0s 192ms/step - loss: -22745.5254 - val_loss: -22969.5938 - val_y0: -2.4750 - val_y1: -0.9051 - val_ate_afte_scaled: 1.5699 - lr: 6.2500e-06\n",
      "Epoch 156/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -21624.5957 — ite: 24.0443  — ate: 0.9937 — pehe: 4.6169 \n",
      "3/3 [==============================] - 0s 197ms/step - loss: -23020.1499 - val_loss: -22783.1465 - val_y0: -2.6366 - val_y1: -1.0887 - val_ate_afte_scaled: 1.5479 - lr: 6.2500e-06\n",
      "Epoch 157/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -21970.5898\n",
      "Epoch 00157: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      " — ite: 23.8716  — ate: 0.9428 — pehe: 4.6912 \n",
      "3/3 [==============================] - 0s 186ms/step - loss: -22862.5850 - val_loss: -22979.3398 - val_y0: -2.6733 - val_y1: -0.8484 - val_ate_afte_scaled: 1.8249 - lr: 6.2500e-06\n",
      "Epoch 158/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20610.5234 — ite: 23.9987  — ate: 0.8854 — pehe: 4.8515 \n",
      "3/3 [==============================] - 0s 195ms/step - loss: -22571.1567 - val_loss: -23108.0254 - val_y0: -2.7087 - val_y1: -1.0049 - val_ate_afte_scaled: 1.7038 - lr: 3.1250e-06\n",
      "Epoch 159/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -21769.6953 — ite: 23.9008  — ate: 0.7554 — pehe: 4.5277 \n",
      "3/3 [==============================] - 0s 191ms/step - loss: -23035.2812 - val_loss: -23025.5938 - val_y0: -2.6624 - val_y1: -0.8381 - val_ate_afte_scaled: 1.8243 - lr: 3.1250e-06\n",
      "Epoch 160/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22462.7129 — ite: 24.0016  — ate: 0.8760 — pehe: 4.5709 \n",
      "3/3 [==============================] - 0s 182ms/step - loss: -23719.3369 - val_loss: -22952.0449 - val_y0: -2.7473 - val_y1: -0.8302 - val_ate_afte_scaled: 1.9171 - lr: 3.1250e-06\n",
      "Epoch 161/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -24582.0684 — ite: 23.9692  — ate: 1.1260 — pehe: 4.4876 \n",
      "3/3 [==============================] - 0s 171ms/step - loss: -22176.3208 - val_loss: -23047.5176 - val_y0: -2.7143 - val_y1: -1.0097 - val_ate_afte_scaled: 1.7047 - lr: 3.1250e-06\n",
      "Epoch 162/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -23696.5254\n",
      "Epoch 00162: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      " — ite: 23.9760  — ate: 0.8744 — pehe: 4.7749 \n",
      "3/3 [==============================] - 0s 185ms/step - loss: -22739.0776 - val_loss: -22910.0391 - val_y0: -2.5932 - val_y1: -0.9978 - val_ate_afte_scaled: 1.5954 - lr: 3.1250e-06\n",
      "Epoch 163/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20927.8555 — ite: 23.8129  — ate: 1.0440 — pehe: 4.5433 \n",
      "3/3 [==============================] - 0s 187ms/step - loss: -23016.9326 - val_loss: -23276.0195 - val_y0: -2.7252 - val_y1: -0.9206 - val_ate_afte_scaled: 1.8046 - lr: 1.5625e-06\n",
      "Epoch 164/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22730.9141 — ite: 23.9457  — ate: 0.7563 — pehe: 4.4897 \n",
      "3/3 [==============================] - 0s 184ms/step - loss: -22756.0049 - val_loss: -22926.8809 - val_y0: -2.5947 - val_y1: -1.0686 - val_ate_afte_scaled: 1.5261 - lr: 1.5625e-06\n",
      "Epoch 165/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22138.2305 — ite: 23.9504  — ate: 1.0942 — pehe: 4.7360 \n",
      "3/3 [==============================] - 0s 187ms/step - loss: -23104.0225 - val_loss: -23100.2676 - val_y0: -2.7667 - val_y1: -0.8694 - val_ate_afte_scaled: 1.8972 - lr: 1.5625e-06\n",
      "Epoch 166/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -24358.2207 — ite: 24.0499  — ate: 1.0561 — pehe: 4.7554 \n",
      "3/3 [==============================] - 0s 189ms/step - loss: -23164.9478 - val_loss: -23164.5059 - val_y0: -2.7450 - val_y1: -0.8047 - val_ate_afte_scaled: 1.9403 - lr: 1.5625e-06\n",
      "Epoch 167/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -23271.7793\n",
      "Epoch 00167: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      " — ite: 23.8760  — ate: 0.9990 — pehe: 4.8336 \n",
      "3/3 [==============================] - 0s 181ms/step - loss: -22787.8574 - val_loss: -23000.0645 - val_y0: -2.8889 - val_y1: -0.8541 - val_ate_afte_scaled: 2.0348 - lr: 1.5625e-06\n",
      "Epoch 168/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -23280.7559 — ite: 23.9533  — ate: 0.8956 — pehe: 4.3581 \n",
      "3/3 [==============================] - 0s 181ms/step - loss: -22951.5889 - val_loss: -23044.5078 - val_y0: -2.6470 - val_y1: -0.7857 - val_ate_afte_scaled: 1.8612 - lr: 7.8125e-07\n",
      "Epoch 169/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -23579.6152 — ite: 23.8731  — ate: 0.9512 — pehe: 4.6116 \n",
      "3/3 [==============================] - 0s 192ms/step - loss: -22704.9653 - val_loss: -22954.4824 - val_y0: -2.6128 - val_y1: -1.0769 - val_ate_afte_scaled: 1.5358 - lr: 7.8125e-07\n",
      "Epoch 170/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22272.0547 — ite: 23.9194  — ate: 0.7270 — pehe: 4.8816 \n",
      "3/3 [==============================] - 0s 185ms/step - loss: -23052.6587 - val_loss: -23012.7598 - val_y0: -2.5056 - val_y1: -0.8670 - val_ate_afte_scaled: 1.6386 - lr: 7.8125e-07\n",
      "Epoch 171/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22500.4609 — ite: 23.9060  — ate: 0.9156 — pehe: 4.7080 \n",
      "3/3 [==============================] - 0s 187ms/step - loss: -23238.1284 - val_loss: -22935.1484 - val_y0: -2.7024 - val_y1: -0.8356 - val_ate_afte_scaled: 1.8668 - lr: 7.8125e-07\n",
      "Epoch 172/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -24667.1094\n",
      "Epoch 00172: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      " — ite: 23.9661  — ate: 0.5588 — pehe: 4.4544 \n",
      "3/3 [==============================] - 0s 196ms/step - loss: -22492.6133 - val_loss: -23019.2656 - val_y0: -2.8019 - val_y1: -0.7803 - val_ate_afte_scaled: 2.0216 - lr: 7.8125e-07\n",
      "Epoch 173/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -19980.7422 — ite: 23.9235  — ate: 0.9995 — pehe: 4.9655 \n",
      "3/3 [==============================] - 0s 187ms/step - loss: -23411.4077 - val_loss: -22971.9746 - val_y0: -2.7481 - val_y1: -1.0577 - val_ate_afte_scaled: 1.6904 - lr: 3.9062e-07\n",
      "Epoch 174/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -24751.4434 — ite: 23.8929  — ate: 0.8264 — pehe: 4.6094 \n",
      "3/3 [==============================] - 0s 192ms/step - loss: -23028.3267 - val_loss: -23224.9355 - val_y0: -2.6215 - val_y1: -0.8122 - val_ate_afte_scaled: 1.8093 - lr: 3.9062e-07\n",
      "Epoch 175/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22599.1152 — ite: 23.9237  — ate: 0.9709 — pehe: 4.7872 \n",
      "3/3 [==============================] - 0s 194ms/step - loss: -22669.4102 - val_loss: -22965.7949 - val_y0: -2.6427 - val_y1: -0.7441 - val_ate_afte_scaled: 1.8987 - lr: 3.9062e-07\n",
      "Epoch 176/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -25790.8301 — ite: 24.0599  — ate: 0.8100 — pehe: 4.6839 \n",
      "3/3 [==============================] - 0s 192ms/step - loss: -22347.1338 - val_loss: -23034.0527 - val_y0: -2.6784 - val_y1: -0.9200 - val_ate_afte_scaled: 1.7584 - lr: 3.9062e-07\n",
      "Epoch 177/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -23448.6230\n",
      "Epoch 00177: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      " — ite: 23.9943  — ate: 1.0323 — pehe: 4.6530 \n",
      "3/3 [==============================] - 0s 193ms/step - loss: -22728.5493 - val_loss: -22986.4746 - val_y0: -2.7223 - val_y1: -0.8521 - val_ate_afte_scaled: 1.8702 - lr: 3.9062e-07\n",
      "Epoch 178/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -23894.1406 — ite: 24.0133  — ate: 0.8971 — pehe: 4.7338 \n",
      "3/3 [==============================] - 0s 193ms/step - loss: -22671.6758 - val_loss: -23156.0527 - val_y0: -2.9183 - val_y1: -0.8311 - val_ate_afte_scaled: 2.0872 - lr: 1.9531e-07\n",
      "Epoch 179/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -25670.6172 — ite: 23.9413  — ate: 0.7466 — pehe: 4.6856 \n",
      "3/3 [==============================] - 0s 193ms/step - loss: -22397.9155 - val_loss: -22923.7148 - val_y0: -2.6677 - val_y1: -0.8043 - val_ate_afte_scaled: 1.8634 - lr: 1.9531e-07\n",
      "Epoch 180/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -21729.0703 — ite: 24.0605  — ate: 0.9652 — pehe: 4.6203 \n",
      "3/3 [==============================] - 0s 197ms/step - loss: -22945.2666 - val_loss: -23062.5938 - val_y0: -2.5965 - val_y1: -0.8867 - val_ate_afte_scaled: 1.7098 - lr: 1.9531e-07\n",
      "Epoch 181/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -24239.7305 — ite: 24.0789  — ate: 0.7417 — pehe: 4.7078 \n",
      "3/3 [==============================] - 0s 198ms/step - loss: -23538.9072 - val_loss: -23010.8770 - val_y0: -2.7936 - val_y1: -0.8241 - val_ate_afte_scaled: 1.9695 - lr: 1.9531e-07\n",
      "Epoch 182/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -23004.5781\n",
      "Epoch 00182: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      " — ite: 23.9963  — ate: 0.9655 — pehe: 4.6701 \n",
      "3/3 [==============================] - 0s 192ms/step - loss: -22790.7715 - val_loss: -22929.4473 - val_y0: -2.7018 - val_y1: -0.9184 - val_ate_afte_scaled: 1.7834 - lr: 1.9531e-07\n",
      "Epoch 183/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20555.1875 — ite: 24.0942  — ate: 0.9370 — pehe: 4.7016 \n",
      "3/3 [==============================] - 0s 201ms/step - loss: -23397.1992 - val_loss: -22913.2070 - val_y0: -2.4470 - val_y1: -0.9006 - val_ate_afte_scaled: 1.5464 - lr: 9.7656e-08\n",
      "Epoch 184/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22770.6191 — ite: 23.9668  — ate: 0.9148 — pehe: 4.6588 \n",
      "3/3 [==============================] - 0s 197ms/step - loss: -23151.0093 - val_loss: -23107.2969 - val_y0: -2.5915 - val_y1: -0.9006 - val_ate_afte_scaled: 1.6910 - lr: 9.7656e-08\n",
      "Epoch 185/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -24532.4551 — ite: 24.0139  — ate: 0.9368 — pehe: 4.8634 \n",
      "3/3 [==============================] - 0s 195ms/step - loss: -22255.9956 - val_loss: -23063.6953 - val_y0: -2.7464 - val_y1: -0.9918 - val_ate_afte_scaled: 1.7546 - lr: 9.7656e-08\n",
      "Epoch 186/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -23218.5898 — ite: 23.9135  — ate: 0.7614 — pehe: 4.5557 \n",
      "3/3 [==============================] - 0s 192ms/step - loss: -22386.1953 - val_loss: -22983.9824 - val_y0: -2.7041 - val_y1: -1.0137 - val_ate_afte_scaled: 1.6904 - lr: 9.7656e-08\n",
      "Epoch 187/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -23015.5742\n",
      "Epoch 00187: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      " — ite: 23.9674  — ate: 0.9296 — pehe: 4.6078 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: -22785.1387 - val_loss: -22919.6699 - val_y0: -2.6301 - val_y1: -0.9866 - val_ate_afte_scaled: 1.6435 - lr: 9.7656e-08\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Colab command to allow us to run Colab in TF2\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "%reload_ext tensorboard \n",
    "\n",
    "model = CEVAE()\n",
    "### MAIN CODE ####\n",
    "val_split=0.2\n",
    "batch_size=64\n",
    "verbose=True\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    " \n",
    "callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='loss', patience=40, min_delta=0), \n",
    "        #40 is Shi's recommendation patience for this dataset, but you should tune for your data \n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0, cooldown=0, min_lr=0),\n",
    "        #This learning rate scheduling is quite agressive which seems good for this dataset\n",
    "        metrics_for_cevae(data,verbose),\n",
    "        tensorboard_callback\n",
    "    ]\n",
    "    \n",
    "#optimizer hyperparameters\n",
    "learning_rate = 1e-4\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate))\n",
    "\n",
    "model.fit(\n",
    "    [data['x'],data['t'],data['ys']],\n",
    "    callbacks=callbacks,\n",
    "    validation_split=val_split,\n",
    "    epochs=300,\n",
    "    batch_size=200,\n",
    "    verbose=verbose\n",
    "    )\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 45269), started 1 day, 22:35:35 ago. (Use '!kill 45269' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b0022dff3a1320b6\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b0022dff3a1320b6\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
