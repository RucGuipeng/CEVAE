{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.layers as tfkl\n",
    "tfd,tfpl = tfp.distributions,tfp.layers\n",
    "import tensorflow.keras.backend as tfkb\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from evaluation import *\n",
    "from networks import *\n",
    "################################################\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='')\n",
    "parser.add_argument('--scale_penalize',    type = float, default = 0.001,  help = '')\n",
    "parser.add_argument('--learning_rate',     type = float, default = 0.001,  help = '')\n",
    "parser.add_argument('--default_y_scale',   type = float, default = 1.,  help = '')\n",
    "parser.add_argument('--t_dim',     type = int, default = 1,  help = '')\n",
    "parser.add_argument('--y_dim',     type = int, default = 1,  help = '')\n",
    "parser.add_argument('--x_dim',     type = int, default = 25, help = '')\n",
    "parser.add_argument('--z_dim',     type = int, default = 20, help = '')\n",
    "parser.add_argument('--x_num_dim', type = int, default = 6,  help = '')\n",
    "parser.add_argument('--x_bin_dim', type = int, default = 19, help = '')\n",
    "parser.add_argument('--nh', type = int, default = 3, help = 'number of hidden layers')\n",
    "parser.add_argument('--h',  type = int, default = 200, help = 'number of hidden units')\n",
    "args = parser.parse_args([])\n",
    "################################################\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz \n",
    "\n",
    "def load_IHDP_data(training_data,testing_data,i):\n",
    "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "        train_data=np.load(trf); test_data=np.load(tef)\n",
    "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
    "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
    "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
    "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    "        ycf=np.concatenate((train_data['ycf'][:,i],  test_data['ycf'][:,i])).astype('float32')\n",
    "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "        data['y']=data['y'].reshape(-1,1)\n",
    "        data['ycf'] = ycf.reshape(-1,1)\n",
    "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "    return data\n",
    "\n",
    "ind = 7\n",
    "rep = 1\n",
    "data = load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.test.npz',i = ind)\n",
    "for key in data:\n",
    "    if key != 'y_scaler':\n",
    "        data[key] = np.repeat(data[key],repeats = rep, axis = 0)\n",
    "data['y_scaler'].mean_, data['y_scaler'].scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEVAE(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CEVAE, self).__init__()\n",
    "        ########################################\n",
    "        # networks\n",
    "        self.activation = 'elu'\n",
    "        # CEVAE Model \n",
    "        ## (encoder)\n",
    "        self.q_y_tx = q_y_tx(args.x_bin_dim, args.x_num_dim, args.y_dim, args.t_dim, args.nh, args.h)\n",
    "        self.q_t_x = q_t_x(args.x_bin_dim, args.x_num_dim, args.t_dim, args.nh, args.h)\n",
    "        self.q_z_txy = q_z_txy(args.x_bin_dim, args.x_num_dim, args.y_dim, args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        ## (decoder)\n",
    "        self.p_x_z = p_x_z(args.x_bin_dim, args.x_num_dim, args.z_dim, args.nh, args.h)\n",
    "        self.p_t_z = p_t_z(args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        self.p_y_tz = p_y_tz(args.y_dim, args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        \n",
    "\n",
    "    def call(self, data, training=False):\n",
    "        if training:\n",
    "            x_train,t_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "            \n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            \n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.sample()\n",
    "            # decoder\n",
    "            ## p(x|z)\n",
    "            x_num,x_bin = self.p_x_z(z_infer_sample)\n",
    "            ## p(t|z)\n",
    "            t = self.p_t_z(z_infer_sample)\n",
    "            ## p(y|t,z)\n",
    "            y = self.p_y_tz(tf.concat([t_train,z_infer_sample],-1) )\n",
    "            \n",
    "            return y_infer,t_infer,z_infer,y,t,x_num,x_bin\n",
    "        else:\n",
    "            x_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "        \n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            \n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.sample()\n",
    "            return y_infer,t_infer,z_infer\n",
    "\n",
    "\n",
    "    def cevae_loss(self, data, pred, training = False):\n",
    "        x_train, t_train, y_train = data[0],data[1],data[2]\n",
    "        x_train_num, x_train_bin = x_train[:,:args.x_num_dim],x_train[:,args.x_num_dim:]\n",
    "        y_infer,t_infer,z_infer,y,t,x_num,x_bin = pred\n",
    "        y0,y1 = y_infer\n",
    "        # reconstruct loss\n",
    "        recon_x_num = tfkb.sum(x_num.log_prob(x_train_num), 1)\n",
    "        recon_x_bin = tfkb.sum(x_bin.log_prob(x_train_bin), 1)\n",
    "        recon_y = tfkb.sum(y.log_prob(y_train), 1)\n",
    "        recon_t = tfkb.sum(t.log_prob(t_train), 1)\n",
    "        # kl loss\n",
    "        z_infer_sample = z_infer.sample()\n",
    "        z = tfd.Normal(loc = [0] * 20, scale = [1]*20)\n",
    "        kl_z = tfkb.sum((z.log_prob(z_infer_sample) - z_infer.log_prob(z_infer_sample)), -1)\n",
    "        # aux loss\n",
    "        aux_y = tfkb.sum(y0.log_prob(y_train)*(1-t_train) + y1.log_prob(y_train)* t_train, 1)\n",
    "        aux_t = tfkb.sum(t_infer.log_prob(t_train), 1)\n",
    "        loss = -tfkb.mean(recon_x_bin + recon_x_num + recon_y + recon_t + aux_y + aux_t + kl_z)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, data):\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "            loss = self.cevae_loss(data = data, pred = pred, training = True)\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        metrics = {\"loss\": loss}\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "        y_infer = pred[0]\n",
    "        loss = self.cevae_loss(data = data, pred = pred, training = False)\n",
    "        y0, y1 = y_infer[0].sample(),y_infer[1].sample()\n",
    "        ate = tfkb.mean(y1) - tfkb.mean(y0)\n",
    "        metrics = {\"loss\":loss,\"y0\": tfkb.mean(y0),\"y1\": tfkb.mean(y1),'ate_afte_scaled': ate}\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-08 22:46:47.769632: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/140\n",
      "1/3 [=========>....................] - ETA: 7s - loss: 32.8377 — ite: 5.0452  — ate: 4.1946 — pehe: 6.0581 \n",
      "3/3 [==============================] - 5s 449ms/step - loss: 32.7635 - val_loss: 32.8661 - val_y0: -0.1367 - val_y1: -0.2334 - val_ate_afte_scaled: -0.0967 - lr: 5.0000e-05\n",
      "Epoch 2/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 33.4569 — ite: 5.0098  — ate: 3.9074 — pehe: 5.7186 \n",
      "3/3 [==============================] - 0s 181ms/step - loss: 32.2155 - val_loss: 32.9165 - val_y0: -0.0753 - val_y1: -0.0902 - val_ate_afte_scaled: -0.0149 - lr: 5.0000e-05\n",
      "Epoch 3/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 31.1110WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0121s vs `on_train_batch_end` time: 0.0131s). Check your callbacks.\n",
      " — ite: 4.7613  — ate: 3.3621 — pehe: 5.3596 \n",
      "3/3 [==============================] - 1s 276ms/step - loss: 31.5527 - val_loss: 32.1207 - val_y0: -0.1164 - val_y1: 0.0855 - val_ate_afte_scaled: 0.2019 - lr: 5.0000e-05\n",
      "Epoch 4/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 31.2822 — ite: 4.5173  — ate: 3.0947 — pehe: 5.0603 \n",
      "3/3 [==============================] - 0s 182ms/step - loss: 31.0603 - val_loss: 31.6308 - val_y0: -0.2052 - val_y1: 0.1363 - val_ate_afte_scaled: 0.3415 - lr: 5.0000e-05\n",
      "Epoch 5/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 30.8265 — ite: 4.4925  — ate: 2.7937 — pehe: 4.8317 \n",
      "3/3 [==============================] - 0s 182ms/step - loss: 30.6730 - val_loss: 31.4025 - val_y0: -0.4054 - val_y1: 0.1443 - val_ate_afte_scaled: 0.5497 - lr: 5.0000e-05\n",
      "Epoch 6/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 30.2149 — ite: 4.2809  — ate: 2.3140 — pehe: 4.4649 \n",
      "3/3 [==============================] - 0s 192ms/step - loss: 30.0969 - val_loss: 30.3467 - val_y0: -0.2819 - val_y1: 0.3323 - val_ate_afte_scaled: 0.6143 - lr: 5.0000e-05\n",
      "Epoch 7/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 29.9451 — ite: 4.2534  — ate: 2.1011 — pehe: 4.5827 \n",
      "3/3 [==============================] - 0s 187ms/step - loss: 29.7832 - val_loss: 30.0908 - val_y0: -0.3298 - val_y1: 0.3877 - val_ate_afte_scaled: 0.7175 - lr: 5.0000e-05\n",
      "Epoch 8/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 29.7334 — ite: 4.2134  — ate: 2.1243 — pehe: 4.3521 \n",
      "3/3 [==============================] - 0s 188ms/step - loss: 29.2656 - val_loss: 29.8265 - val_y0: -0.1246 - val_y1: 0.5139 - val_ate_afte_scaled: 0.6385 - lr: 5.0000e-05\n",
      "Epoch 9/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 29.3679 — ite: 4.1387  — ate: 1.8223 — pehe: 4.2781 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: 28.7520 - val_loss: 29.3341 - val_y0: -0.2388 - val_y1: 0.6540 - val_ate_afte_scaled: 0.8928 - lr: 5.0000e-05\n",
      "Epoch 10/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.4541 — ite: 4.0454  — ate: 1.9750 — pehe: 4.4020 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: 28.4081 - val_loss: 28.9775 - val_y0: -0.2919 - val_y1: 0.6042 - val_ate_afte_scaled: 0.8961 - lr: 5.0000e-05\n",
      "Epoch 11/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.5226 — ite: 4.0893  — ate: 1.8876 — pehe: 4.4500 \n",
      "3/3 [==============================] - 0s 186ms/step - loss: 28.1235 - val_loss: 28.4069 - val_y0: -0.0893 - val_y1: 0.6575 - val_ate_afte_scaled: 0.7469 - lr: 5.0000e-05\n",
      "Epoch 12/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.1245 — ite: 3.9677  — ate: 1.4408 — pehe: 4.2599 \n",
      "3/3 [==============================] - 0s 184ms/step - loss: 27.8702 - val_loss: 28.2149 - val_y0: -0.2431 - val_y1: 0.6872 - val_ate_afte_scaled: 0.9303 - lr: 5.0000e-05\n",
      "Epoch 13/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.4253 — ite: 3.9610  — ate: 1.1405 — pehe: 4.0952 \n",
      "3/3 [==============================] - 0s 187ms/step - loss: 27.4158 - val_loss: 28.2060 - val_y0: -0.2988 - val_y1: 0.8564 - val_ate_afte_scaled: 1.1552 - lr: 5.0000e-05\n",
      "Epoch 14/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.4928 — ite: 3.9143  — ate: 1.1485 — pehe: 4.2094 \n",
      "3/3 [==============================] - 0s 186ms/step - loss: 27.0085 - val_loss: 27.6527 - val_y0: -0.2250 - val_y1: 0.7987 - val_ate_afte_scaled: 1.0237 - lr: 5.0000e-05\n",
      "Epoch 15/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.5441 — ite: 3.9038  — ate: 1.2010 — pehe: 4.1354 \n",
      "3/3 [==============================] - 0s 179ms/step - loss: 27.0358 - val_loss: 27.5069 - val_y0: -0.1714 - val_y1: 0.8750 - val_ate_afte_scaled: 1.0464 - lr: 5.0000e-05\n",
      "Epoch 16/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.0963 — ite: 3.8666  — ate: 1.0102 — pehe: 3.9934 \n",
      "3/3 [==============================] - 0s 180ms/step - loss: 26.4369 - val_loss: 26.9060 - val_y0: -0.1088 - val_y1: 0.9325 - val_ate_afte_scaled: 1.0413 - lr: 5.0000e-05\n",
      "Epoch 17/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 26.6314 — ite: 3.8743  — ate: 1.2149 — pehe: 4.0763 \n",
      "3/3 [==============================] - 0s 181ms/step - loss: 26.3927 - val_loss: 27.0030 - val_y0: -0.3085 - val_y1: 0.9116 - val_ate_afte_scaled: 1.2201 - lr: 5.0000e-05\n",
      "Epoch 18/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 26.5039 — ite: 3.9126  — ate: 1.0982 — pehe: 4.0659 \n",
      "3/3 [==============================] - 0s 198ms/step - loss: 26.1785 - val_loss: 26.0906 - val_y0: -0.2376 - val_y1: 0.9663 - val_ate_afte_scaled: 1.2039 - lr: 5.0000e-05\n",
      "Epoch 19/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 26.1098 — ite: 3.9401  — ate: 0.6581 — pehe: 4.0390 \n",
      "3/3 [==============================] - 0s 206ms/step - loss: 25.8276 - val_loss: 25.9083 - val_y0: -0.2177 - val_y1: 0.9481 - val_ate_afte_scaled: 1.1658 - lr: 5.0000e-05\n",
      "Epoch 20/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.8153 — ite: 3.9083  — ate: 0.7241 — pehe: 3.9866 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: 25.7343 - val_loss: 26.1380 - val_y0: -0.3485 - val_y1: 1.1398 - val_ate_afte_scaled: 1.4883 - lr: 5.0000e-05\n",
      "Epoch 21/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.2705 — ite: 3.8694  — ate: 0.4161 — pehe: 3.8937 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: 25.2222 - val_loss: 25.4610 - val_y0: -0.3667 - val_y1: 0.8812 - val_ate_afte_scaled: 1.2479 - lr: 5.0000e-05\n",
      "Epoch 22/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.5039 — ite: 3.7794  — ate: 0.5556 — pehe: 3.9447 \n",
      "3/3 [==============================] - 0s 193ms/step - loss: 24.9247 - val_loss: 25.7912 - val_y0: -0.2684 - val_y1: 1.0686 - val_ate_afte_scaled: 1.3370 - lr: 5.0000e-05\n",
      "Epoch 23/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.1673 — ite: 3.8673  — ate: 0.3490 — pehe: 3.9508 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: 24.6328 - val_loss: 25.4303 - val_y0: -0.4852 - val_y1: 1.0158 - val_ate_afte_scaled: 1.5011 - lr: 5.0000e-05\n",
      "Epoch 24/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.7524 — ite: 3.8189  — ate: 0.5488 — pehe: 3.7804 \n",
      "3/3 [==============================] - 0s 188ms/step - loss: 24.7764 - val_loss: 25.4230 - val_y0: -0.1050 - val_y1: 1.0618 - val_ate_afte_scaled: 1.1668 - lr: 5.0000e-05\n",
      "Epoch 25/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.4060 — ite: 3.8905  — ate: 0.4047 — pehe: 4.1558 \n",
      "3/3 [==============================] - 0s 193ms/step - loss: 24.3810 - val_loss: 24.7225 - val_y0: -0.2417 - val_y1: 1.2421 - val_ate_afte_scaled: 1.4838 - lr: 5.0000e-05\n",
      "Epoch 26/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.4036 — ite: 3.8399  — ate: 0.5037 — pehe: 3.9687 \n",
      "3/3 [==============================] - 0s 191ms/step - loss: 24.2152 - val_loss: 24.5989 - val_y0: -0.3080 - val_y1: 1.0617 - val_ate_afte_scaled: 1.3697 - lr: 5.0000e-05\n",
      "Epoch 27/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.1825 — ite: 3.8111  — ate: 0.2911 — pehe: 3.7611 \n",
      "3/3 [==============================] - 0s 188ms/step - loss: 24.0328 - val_loss: 24.2460 - val_y0: -0.3302 - val_y1: 1.0409 - val_ate_afte_scaled: 1.3711 - lr: 5.0000e-05\n",
      "Epoch 28/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.7463 — ite: 3.8860  — ate: 0.2366 — pehe: 3.9035 \n",
      "3/3 [==============================] - 0s 182ms/step - loss: 23.6880 - val_loss: 23.9898 - val_y0: -0.3220 - val_y1: 1.2094 - val_ate_afte_scaled: 1.5314 - lr: 5.0000e-05\n",
      "Epoch 29/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.4118 — ite: 3.8371  — ate: 0.0622 — pehe: 3.7372 \n",
      "3/3 [==============================] - 0s 189ms/step - loss: 23.5434 - val_loss: 23.7816 - val_y0: -0.1985 - val_y1: 1.0452 - val_ate_afte_scaled: 1.2436 - lr: 5.0000e-05\n",
      "Epoch 30/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.5151 — ite: 3.8302  — ate: 0.1569 — pehe: 3.9731 \n",
      "3/3 [==============================] - 0s 231ms/step - loss: 23.3568 - val_loss: 24.1383 - val_y0: -0.2898 - val_y1: 1.2128 - val_ate_afte_scaled: 1.5027 - lr: 5.0000e-05\n",
      "Epoch 31/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.3963 — ite: 3.7615  — ate: 0.0986 — pehe: 3.8825 \n",
      "3/3 [==============================] - 0s 192ms/step - loss: 23.0799 - val_loss: 23.8817 - val_y0: -0.2099 - val_y1: 1.2564 - val_ate_afte_scaled: 1.4663 - lr: 5.0000e-05\n",
      "Epoch 32/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.9952 — ite: 3.8039  — ate: 0.1469 — pehe: 3.9381 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: 22.7535 - val_loss: 23.4725 - val_y0: -0.1940 - val_y1: 1.1758 - val_ate_afte_scaled: 1.3698 - lr: 5.0000e-05\n",
      "Epoch 33/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.8939 — ite: 3.9134  — ate: 0.3070 — pehe: 3.8925 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: 22.8123 - val_loss: 23.3881 - val_y0: -0.2945 - val_y1: 1.0926 - val_ate_afte_scaled: 1.3871 - lr: 5.0000e-05\n",
      "Epoch 34/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.2883 — ite: 3.7952  — ate: 0.1534 — pehe: 3.8938 \n",
      "3/3 [==============================] - 0s 201ms/step - loss: 22.5413 - val_loss: 22.9396 - val_y0: -0.3056 - val_y1: 1.1996 - val_ate_afte_scaled: 1.5052 - lr: 5.0000e-05\n",
      "Epoch 35/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.9505 — ite: 3.8141  — ate: 0.2394 — pehe: 3.6820 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: 22.2552 - val_loss: 22.7921 - val_y0: -0.1779 - val_y1: 1.0566 - val_ate_afte_scaled: 1.2344 - lr: 5.0000e-05\n",
      "Epoch 36/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.8100 — ite: 3.8277  — ate: 0.1825 — pehe: 4.0162 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: 22.0726 - val_loss: 22.4520 - val_y0: -0.1293 - val_y1: 1.2054 - val_ate_afte_scaled: 1.3347 - lr: 5.0000e-05\n",
      "Epoch 37/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.6971 — ite: 3.8261  — ate: 0.0787 — pehe: 3.6734 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: 22.0909 - val_loss: 22.3372 - val_y0: -0.3011 - val_y1: 1.2713 - val_ate_afte_scaled: 1.5724 - lr: 5.0000e-05\n",
      "Epoch 38/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.7118 — ite: 3.8411  — ate: 0.0848 — pehe: 3.9476 \n",
      "3/3 [==============================] - 0s 206ms/step - loss: 21.8988 - val_loss: 21.9470 - val_y0: -0.3478 - val_y1: 1.2687 - val_ate_afte_scaled: 1.6166 - lr: 5.0000e-05\n",
      "Epoch 39/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.1631 — ite: 3.8152  — ate: 0.0498 — pehe: 3.8652 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: 21.5883 - val_loss: 22.1524 - val_y0: -0.3135 - val_y1: 1.2467 - val_ate_afte_scaled: 1.5602 - lr: 5.0000e-05\n",
      "Epoch 40/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.4817 — ite: 3.8646  — ate: 0.1446 — pehe: 3.9198 \n",
      "3/3 [==============================] - 0s 182ms/step - loss: 21.3695 - val_loss: 21.6974 - val_y0: -0.2901 - val_y1: 1.1184 - val_ate_afte_scaled: 1.4085 - lr: 5.0000e-05\n",
      "Epoch 41/140\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.1713 — ite: 3.8012  — ate: 0.1717 — pehe: 3.8595 \n",
      "3/3 [==============================] - 0s 183ms/step - loss: 21.2112 - val_loss: 21.4148 - val_y0: -0.2149 - val_y1: 1.1772 - val_ate_afte_scaled: 1.3921 - lr: 5.0000e-05\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Colab command to allow us to run Colab in TF2\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "%reload_ext tensorboard \n",
    "\n",
    "model = CEVAE()\n",
    "### MAIN CODE ####\n",
    "val_split=0.2\n",
    "batch_size=64\n",
    "verbose=True\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    " \n",
    "callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='loss', patience=40, min_delta=0), \n",
    "        #40 is Shi's recommendation patience for this dataset, but you should tune for your data \n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0, cooldown=0, min_lr=0),\n",
    "        #This learning rate scheduling is quite agressive which seems good for this dataset\n",
    "        metrics_for_cevae(data,verbose),\n",
    "        tensorboard_callback\n",
    "    ]\n",
    "    \n",
    "#optimizer hyperparameters\n",
    "learning_rate = 5e-5\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate))\n",
    "\n",
    "model.fit(\n",
    "    [data['x'],data['t'],data['ys']],\n",
    "    callbacks=callbacks,\n",
    "    validation_split=val_split,\n",
    "    epochs=300,\n",
    "    batch_size=200,\n",
    "    verbose=verbose\n",
    "    )\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 11635), started 1 day, 6:38:38 ago. (Use '!kill 11635' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-13bb416d17cd203f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-13bb416d17cd203f\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
