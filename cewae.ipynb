{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 “ihdp_npci_1-100.train.npz” 已经存在；不获取。\n",
      "\n",
      "文件 “ihdp_npci_1-100.test.npz” 已经存在；不获取。\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([5.26691518]), array([2.59847927]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.layers as tfkl\n",
    "tfd,tfpl = tfp.distributions,tfp.layers\n",
    "import tensorflow.keras.backend as tfkb\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from evaluation import *\n",
    "from cevae_networks import *\n",
    "################################################\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='')\n",
    "parser.add_argument('--scale_penalize',    type = float, default = 0.001,  help = '')\n",
    "parser.add_argument('--learning_rate',     type = float, default = 0.001,  help = '')\n",
    "parser.add_argument('--default_y_scale',   type = float, default = 1.,  help = '')\n",
    "parser.add_argument('--t_dim',     type = int, default = 1,  help = '')\n",
    "parser.add_argument('--y_dim',     type = int, default = 1,  help = '')\n",
    "parser.add_argument('--x_dim',     type = int, default = 25, help = '')\n",
    "parser.add_argument('--z_dim',     type = int, default = 20, help = '')\n",
    "parser.add_argument('--x_num_dim', type = int, default = 6,  help = '')\n",
    "parser.add_argument('--x_bin_dim', type = int, default = 19, help = '')\n",
    "parser.add_argument('--val_split', type = float, default = 0.2, help = '')\n",
    "parser.add_argument('--batch_size', type = int, default = 200, help = '')\n",
    "parser.add_argument('--nh', type = int, default = 3, help = 'number of hidden layers')\n",
    "parser.add_argument('--h',  type = int, default = 200, help = 'number of hidden units')\n",
    "args = parser.parse_args([])\n",
    "################################################\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz \n",
    "\n",
    "def load_IHDP_data(training_data,testing_data,i):\n",
    "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "        train_data=np.load(trf); test_data=np.load(tef)\n",
    "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
    "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
    "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
    "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    "        ycf=np.concatenate((train_data['ycf'][:,i],  test_data['ycf'][:,i])).astype('float32')\n",
    "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "        data['y']=data['y'].reshape(-1,1)\n",
    "        data['ycf'] = ycf.reshape(-1,1)\n",
    "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "    return data\n",
    "\n",
    "ind = 7\n",
    "rep = 1\n",
    "data = load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.test.npz',i = ind)\n",
    "for key in data:\n",
    "    if key != 'y_scaler':\n",
    "        data[key] = np.repeat(data[key],repeats = rep, axis = 0)\n",
    "data['y_scaler'].mean_, data['y_scaler'].scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonLayer(tfkl.Layer):\n",
    "    def __init__(self):\n",
    "        super(EpsilonLayer, self).__init__()\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.epsilon = self.add_weight(name='epsilon',\n",
    "                                       shape=[1, 1],\n",
    "                                       initializer='RandomNormal',\n",
    "                                       #  initializer='ones',\n",
    "                                       trainable=True)\n",
    "        super(EpsilonLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        #note there is only one epsilon were just duplicating it for conformability\n",
    "        return self.epsilon * tf.ones_like(inputs)[:, 0:1]\n",
    "\n",
    "class CEWAE(tf.keras.Model):\n",
    "    def __init__(self, kernel = \"IMQ\"):\n",
    "        super(CEWAE, self).__init__()\n",
    "        ########################################\n",
    "        # networks\n",
    "        self.activation = 'elu'\n",
    "        self.kernel = kernel\n",
    "        # CEVAE Model \n",
    "        ## (encoder)\n",
    "        self.q_y_tx = q_y_tx(args.x_bin_dim, args.x_num_dim, args.y_dim, args.t_dim, args.nh, args.h)\n",
    "        self.q_t_x = q_t_x(args.x_bin_dim, args.x_num_dim, args.t_dim, args.nh, args.h)\n",
    "        self.q_z_txy = q_z_txy(args.x_bin_dim, args.x_num_dim, args.y_dim, args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        ## (decoder)\n",
    "        self.p_x_z = p_x_z(args.x_bin_dim, args.x_num_dim, args.z_dim, args.nh, args.h)\n",
    "        self.p_t_z = p_t_z(args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        self.p_y_tz = p_y_tz(args.y_dim, args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        self.epsilon_layer = EpsilonLayer()\n",
    "        self.beta = 1\n",
    "        self.lmbda = 1\n",
    "\n",
    "    def call(self, data, training=False):\n",
    "        if training:\n",
    "            x_train,t_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "            \n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            \n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.sample()\n",
    "            # decoder\n",
    "            ## p(x|z)\n",
    "            x_num,x_bin = self.p_x_z(z_infer_sample)\n",
    "            ## p(t|z)\n",
    "            t = self.p_t_z(z_infer_sample)\n",
    "            ## p(y|t,z)\n",
    "            t0z = tf.concat([tf.zeros_like(t_train),z_infer_sample],-1)\n",
    "            t1z = tf.concat([tf.ones_like(t_train),z_infer_sample],-1)\n",
    "            y0 = self.p_y_tz(t0z)\n",
    "            y1 = self.p_y_tz(t1z)\n",
    "            y = [y0,y1]\n",
    "            epsilon = self.epsilon_layer(t_infer_sample)\n",
    "            \n",
    "            return y_infer,t_infer,z_infer,y,t,x_num,x_bin,epsilon\n",
    "        else:\n",
    "            x_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.loc\n",
    "\n",
    "            t1z = tf.concat([tf.ones_like(t_infer_sample),z_infer_sample],-1)\n",
    "            t0z = tf.concat([tf.zeros_like(t_infer_sample),z_infer_sample],-1)\n",
    "            y0 = self.p_y_tz(t0z)\n",
    "            y1 = self.p_y_tz(t1z)\n",
    "            y = [y0,y1]\n",
    "            return y,t_infer,z_infer\n",
    "\n",
    "    def mmd_penalty(self, sample_qz, sample_pz, batch_size = args.batch_size):\n",
    "        opts = {'kernel': self.kernel, 'verbose':True, \"zdim\":20, \"pz\":\"normal\"} \n",
    "        sigma2_p = 1 ** 2\n",
    "        kernel = opts['kernel']\n",
    "        n = batch_size\n",
    "        n = tf.shape(sample_qz)[0]\n",
    "        n = tf.cast(n, tf.int32)\n",
    "        nf = tf.cast(n, tf.float32)\n",
    "        half_size = (n * n - n) / 2\n",
    "\n",
    "        norms_pz = tf.reduce_sum(tf.square(sample_pz), axis=1, keepdims=True)\n",
    "        dotprods_pz = tf.matmul(sample_pz, sample_pz, transpose_b=True)\n",
    "        distances_pz = norms_pz + tf.transpose(norms_pz) - 2. * dotprods_pz\n",
    "\n",
    "        norms_qz = tf.reduce_sum(tf.square(sample_qz), axis=1, keepdims=True)\n",
    "        dotprods_qz = tf.matmul(sample_qz, sample_qz, transpose_b=True)\n",
    "        distances_qz = norms_qz + tf.transpose(norms_qz) - 2. * dotprods_qz\n",
    "\n",
    "        dotprods = tf.matmul(sample_qz, sample_pz, transpose_b=True)\n",
    "        distances = norms_qz + tf.transpose(norms_pz) - 2. * dotprods\n",
    "\n",
    "        if kernel == 'RBF':\n",
    "            # Median heuristic for the sigma^2 of Gaussian kernel\n",
    "            sigma2_k = tf.nn.top_k(\n",
    "                tf.reshape(distances, [-1]), half_size).values[half_size - 1]\n",
    "            sigma2_k += tf.nn.top_k(\n",
    "                tf.reshape(distances_qz, [-1]), half_size).values[half_size - 1]\n",
    "            # Maximal heuristic for the sigma^2 of Gaussian kernel\n",
    "            # sigma2_k = tf.nn.top_k(tf.reshape(distances_qz, [-1]), 1).values[0]\n",
    "            # sigma2_k += tf.nn.top_k(tf.reshape(distances, [-1]), 1).values[0]\n",
    "            # sigma2_k = opts['latent_space_dim'] * sigma2_p\n",
    "            if opts['verbose']:\n",
    "                sigma2_k = tf.Print(sigma2_k, [sigma2_k], 'Kernel width:')\n",
    "            res1 = tf.exp( - distances_qz / 2. / sigma2_k)\n",
    "            res1 += tf.exp( - distances_pz / 2. / sigma2_k)\n",
    "            res1 = tf.multiply(res1, 1. - tf.eye(n))\n",
    "            res1 = tf.reduce_sum(res1) / (nf * nf - nf)\n",
    "            res2 = tf.exp( - distances / 2. / sigma2_k)\n",
    "            res2 = tf.reduce_sum(res2) * 2. / (nf * nf)\n",
    "            stat = res1 - res2\n",
    "        elif kernel == 'IMQ':\n",
    "            # k(x, y) = C / (C + ||x - y||^2)\n",
    "            # C = tf.nn.top_k(tf.reshape(distances, [-1]), half_size).values[half_size - 1]\n",
    "            # C += tf.nn.top_k(tf.reshape(distances_qz, [-1]), half_size).values[half_size - 1]\n",
    "            if opts['pz'] == 'normal':\n",
    "                Cbase = 2. * opts['zdim'] * sigma2_p\n",
    "            elif opts['pz'] == 'sphere':\n",
    "                Cbase = 2.\n",
    "            elif opts['pz'] == 'uniform':\n",
    "                # E ||x - y||^2 = E[sum (xi - yi)^2]\n",
    "                #               = zdim E[(xi - yi)^2]\n",
    "                #               = const * zdim\n",
    "                Cbase = opts['zdim']\n",
    "            stat = 0.\n",
    "            for scale in [.1, .2, .5, 1., 2., 5., 10.]:\n",
    "                C = Cbase * scale\n",
    "                res1 = C / (C + distances_qz)\n",
    "                res1 += C / (C + distances_pz)\n",
    "                res1 = tf.multiply(res1, 1. - tf.eye(n))\n",
    "                res1 = tf.reduce_sum(res1) / (nf * nf - nf)\n",
    "                res2 = C / (C + distances)\n",
    "                res2 = tf.reduce_sum(res2) * 2. / (nf * nf)\n",
    "                stat += res1 - res2\n",
    "        return stat\n",
    "\n",
    "    def cewae_loss(self, data, pred, training = False):\n",
    "        x_train, t_train, y_train = data[0],data[1],data[2]\n",
    "        x_train_num, x_train_bin = x_train[:,:args.x_num_dim],x_train[:,args.x_num_dim:]\n",
    "        y_infer,t_infer,z_infer,y,t,x_num,x_bin,epsilon = pred\n",
    "        # y0_infer,y1_infer = y_infer\n",
    "        y0,y1 = y\n",
    "        # reconstruct loss\n",
    "        rec_x_num = tfkb.mean(tf.math.square(x_train_num - x_num.sample()))\n",
    "        rec_x_bin = tf.reduce_sum(\n",
    "            tfk.losses.binary_crossentropy(\n",
    "                x_train_bin,\n",
    "                tf.cast(x_bin.sample(),tf.float32),\n",
    "                from_logits=False))\n",
    "        rec_t_bin = tf.reduce_sum(\n",
    "            tfk.losses.binary_crossentropy(\n",
    "                t_train,\n",
    "                tf.cast(t_infer.sample(),tf.float32),\n",
    "                from_logits=False))\n",
    "        rec_y0 = tf.math.square(y0.sample() - y_train)\n",
    "        rec_y1 = tf.math.square(y1.sample() - y_train)\n",
    "        rec_y = tfkb.mean(t_train * rec_y1 + (1-t_train)* rec_y0)\n",
    "        # regularization\n",
    "        # mmd penalty\n",
    "        pz = tfd.Normal(loc = tf.zeros_like(z_infer.sample()), scale = tf.ones_like(z_infer.sample()))\n",
    "        reg_mmd = self.mmd_penalty(z_infer.sample(), pz.sample())\n",
    "\n",
    "        # target regularization\n",
    "        # y_pred = y0.loc * (1-t_train) + y1.loc * t_train\n",
    "        # t_pred = tf.math.sigmoid(t.logits)\n",
    "        # cc = t_train/t_pred - (1-t_train) / (1-t_pred)\n",
    "        # t_reg = tf.math.square(y_pred + epsilon * cc - y_train)\n",
    "        # loss += tfkb.mean(t_reg) * self.beta\n",
    "\n",
    "        loss = rec_x_num + rec_x_bin + rec_t_bin + rec_y + reg_mmd * self.lmbda\n",
    "        loss = rec_x_num + rec_x_bin + rec_t_bin + rec_y + reg_mmd\n",
    "        # loss = rec_x_num + rec_x_bin + rec_t_bin + rec_y \n",
    "        return loss\n",
    "\n",
    "    def train_step(self, data):\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "            loss = self.cewae_loss(data = data, pred = pred, training = True)\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        metrics = {\"loss\": loss}\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "        y_infer = pred[0]\n",
    "        loss = self.cewae_loss(data = data, pred = pred, training = False)\n",
    "        y0, y1 = y_infer[0].sample(),y_infer[1].sample()\n",
    "        ate = tfkb.mean(y1) - tfkb.mean(y0)\n",
    "        metrics = {\"loss\":loss,\"y0\": tfkb.mean(y0),\"y1\": tfkb.mean(y1),'ate_afte_scaled': ate}\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_413/kernel:0', 'dense_413/bias:0', 'dense_414/kernel:0', 'dense_414/bias:0', 'dense_415/kernel:0', 'dense_415/bias:0', 'dense_416/kernel:0', 'dense_416/bias:0', 'dense_427/kernel:0', 'dense_427/bias:0', 'dense_428/kernel:0', 'dense_428/bias:0', 'dense_432/kernel:0', 'dense_432/bias:0', 'dense_433/kernel:0', 'dense_433/bias:0', 'dense_434/kernel:0', 'dense_434/bias:0', 'dense_435/kernel:0', 'dense_435/bias:0', 'cewae_11/epsilon_layer_11/epsilon:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_413/kernel:0', 'dense_413/bias:0', 'dense_414/kernel:0', 'dense_414/bias:0', 'dense_415/kernel:0', 'dense_415/bias:0', 'dense_416/kernel:0', 'dense_416/bias:0', 'dense_427/kernel:0', 'dense_427/bias:0', 'dense_428/kernel:0', 'dense_428/bias:0', 'dense_432/kernel:0', 'dense_432/bias:0', 'dense_433/kernel:0', 'dense_433/bias:0', 'dense_434/kernel:0', 'dense_434/bias:0', 'dense_435/kernel:0', 'dense_435/bias:0', 'cewae_11/epsilon_layer_11/epsilon:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "3/3 [==============================] - ETA: 0s - loss: 3040.7091 — ite: 4.5658  — ate: 3.3992 — pehe: 5.3374 \n",
      "3/3 [==============================] - 6s 801ms/step - loss: 3045.7072 - val_loss: 2491.5452 - val_y0: 0.1187 - val_y1: 0.5130 - val_ate_afte_scaled: 0.3943 - lr: 5.0000e-04\n",
      "Epoch 2/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3110.4604 — ite: 4.2795  — ate: 2.7142 — pehe: 4.6962 \n",
      "3/3 [==============================] - 0s 240ms/step - loss: 3090.5104 - val_loss: 2275.4897 - val_y0: -0.4140 - val_y1: 0.7692 - val_ate_afte_scaled: 1.1832 - lr: 5.0000e-04\n",
      "Epoch 3/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3080.0161 — ite: 3.9959  — ate: 1.4527 — pehe: 4.3466 \n",
      "3/3 [==============================] - 0s 221ms/step - loss: 3023.8801 - val_loss: 2249.6348 - val_y0: -0.6985 - val_y1: 0.9941 - val_ate_afte_scaled: 1.6926 - lr: 5.0000e-04\n",
      "Epoch 4/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3144.3484 — ite: 3.9034  — ate: 0.1626 — pehe: 4.0553 \n",
      "3/3 [==============================] - 0s 223ms/step - loss: 3081.7391 - val_loss: 2371.1221 - val_y0: -0.7356 - val_y1: 0.8514 - val_ate_afte_scaled: 1.5870 - lr: 5.0000e-04\n",
      "Epoch 5/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2880.3469 — ite: 3.9172  — ate: 0.1136 — pehe: 4.1122 \n",
      "3/3 [==============================] - 0s 219ms/step - loss: 3112.1946 - val_loss: 2503.3374 - val_y0: -0.6921 - val_y1: 0.7460 - val_ate_afte_scaled: 1.4381 - lr: 5.0000e-04\n",
      "Epoch 6/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3027.3086 — ite: 3.9681  — ate: 0.9889 — pehe: 4.0432 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: 3036.9905 - val_loss: 2330.4346 - val_y0: -0.2928 - val_y1: 0.6919 - val_ate_afte_scaled: 0.9847 - lr: 5.0000e-04\n",
      "Epoch 7/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3016.2722 — ite: 3.8747  — ate: 1.5684 — pehe: 3.9890 \n",
      "3/3 [==============================] - 0s 206ms/step - loss: 3025.8433 - val_loss: 2297.5396 - val_y0: -0.1016 - val_y1: 0.6195 - val_ate_afte_scaled: 0.7210 - lr: 5.0000e-04\n",
      "Epoch 8/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3120.5583\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      " — ite: 4.0059  — ate: 1.6994 — pehe: 4.3147 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: 3052.8644 - val_loss: 2297.7996 - val_y0: 0.0256 - val_y1: 0.1100 - val_ate_afte_scaled: 0.0844 - lr: 5.0000e-04\n",
      "Epoch 9/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3124.5806 — ite: 3.9033  — ate: 1.4784 — pehe: 4.1300 \n",
      "3/3 [==============================] - 0s 198ms/step - loss: 2983.0154 - val_loss: 2292.2400 - val_y0: -0.0472 - val_y1: -0.0195 - val_ate_afte_scaled: 0.0277 - lr: 2.5000e-04\n",
      "Epoch 10/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3020.2178 — ite: 3.9425  — ate: 1.2670 — pehe: 4.1275 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: 2998.3760 - val_loss: 2377.6685 - val_y0: -0.1860 - val_y1: -0.1900 - val_ate_afte_scaled: -0.0040 - lr: 2.5000e-04\n",
      "Epoch 11/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3038.5417 — ite: 3.9266  — ate: 1.1569 — pehe: 4.1428 \n",
      "3/3 [==============================] - 0s 197ms/step - loss: 3040.5564 - val_loss: 2466.6033 - val_y0: -0.1197 - val_y1: -0.1955 - val_ate_afte_scaled: -0.0758 - lr: 2.5000e-04\n",
      "Epoch 12/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3094.2893 — ite: 3.7907  — ate: 0.7675 — pehe: 3.9527 \n",
      "3/3 [==============================] - 0s 198ms/step - loss: 3062.1934 - val_loss: 2254.7637 - val_y0: -0.2066 - val_y1: -0.2106 - val_ate_afte_scaled: -0.0039 - lr: 2.5000e-04\n",
      "Epoch 13/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3338.6748\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      " — ite: 3.7952  — ate: 0.7151 — pehe: 4.0033 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: 3151.0672 - val_loss: 2211.5132 - val_y0: -0.3011 - val_y1: -0.0354 - val_ate_afte_scaled: 0.2657 - lr: 2.5000e-04\n",
      "Epoch 14/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3073.5134 — ite: 3.7955  — ate: 0.4898 — pehe: 4.0226 \n",
      "3/3 [==============================] - 0s 197ms/step - loss: 3005.1703 - val_loss: 2383.9270 - val_y0: -0.3502 - val_y1: -0.1667 - val_ate_afte_scaled: 0.1835 - lr: 1.2500e-04\n",
      "Epoch 15/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3060.0466 — ite: 3.7865  — ate: 0.2919 — pehe: 3.8604 \n",
      "3/3 [==============================] - 0s 192ms/step - loss: 3006.9847 - val_loss: 2264.0757 - val_y0: -0.3841 - val_y1: -0.1676 - val_ate_afte_scaled: 0.2164 - lr: 1.2500e-04\n",
      "Epoch 16/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2987.3562 — ite: 3.8053  — ate: 0.0405 — pehe: 3.8284 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: 3038.9078 - val_loss: 2243.3770 - val_y0: -0.2654 - val_y1: -0.2585 - val_ate_afte_scaled: 0.0069 - lr: 1.2500e-04\n",
      "Epoch 17/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3063.6382 — ite: 3.8832  — ate: 0.2180 — pehe: 3.9634 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: 2989.3033 - val_loss: 2316.3989 - val_y0: -0.3000 - val_y1: -0.2828 - val_ate_afte_scaled: 0.0173 - lr: 1.2500e-04\n",
      "Epoch 18/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2852.4651\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      " — ite: 3.7621  — ate: 0.5030 — pehe: 3.7224 \n",
      "3/3 [==============================] - 0s 195ms/step - loss: 2960.7950 - val_loss: 2377.7979 - val_y0: -0.3697 - val_y1: -0.0663 - val_ate_afte_scaled: 0.3034 - lr: 1.2500e-04\n",
      "Epoch 19/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3211.8286 — ite: 3.8281  — ate: 0.4026 — pehe: 3.7620 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: 3094.5190 - val_loss: 2354.3032 - val_y0: -0.4052 - val_y1: -0.2276 - val_ate_afte_scaled: 0.1776 - lr: 6.2500e-05\n",
      "Epoch 20/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3125.2852 — ite: 3.9152  — ate: 0.2370 — pehe: 3.8181 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: 3082.1869 - val_loss: 2239.7795 - val_y0: -0.4163 - val_y1: -0.3932 - val_ate_afte_scaled: 0.0231 - lr: 6.2500e-05\n",
      "Epoch 21/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2962.8743 — ite: 3.9175  — ate: 0.6308 — pehe: 3.9557 \n",
      "3/3 [==============================] - 0s 189ms/step - loss: 3110.3126 - val_loss: 2485.6873 - val_y0: -0.4036 - val_y1: -0.3183 - val_ate_afte_scaled: 0.0854 - lr: 6.2500e-05\n",
      "Epoch 22/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3085.0369 — ite: 3.8316  — ate: 0.5758 — pehe: 3.9780 \n",
      "3/3 [==============================] - 0s 197ms/step - loss: 2988.4630 - val_loss: 2308.1533 - val_y0: -0.4795 - val_y1: -0.3170 - val_ate_afte_scaled: 0.1625 - lr: 6.2500e-05\n",
      "Epoch 23/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3336.9229\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      " — ite: 3.8313  — ate: 0.7524 — pehe: 3.9078 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: 3154.4857 - val_loss: 2418.4263 - val_y0: -0.2786 - val_y1: -0.1821 - val_ate_afte_scaled: 0.0965 - lr: 6.2500e-05\n",
      "Epoch 24/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3145.0925 — ite: 3.8986  — ate: 0.3891 — pehe: 3.8941 \n",
      "3/3 [==============================] - 0s 197ms/step - loss: 3011.3721 - val_loss: 2314.0229 - val_y0: -0.4499 - val_y1: -0.2234 - val_ate_afte_scaled: 0.2265 - lr: 3.1250e-05\n",
      "Epoch 25/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3331.7002 — ite: 3.8838  — ate: 0.3776 — pehe: 3.7970 \n",
      "3/3 [==============================] - 0s 206ms/step - loss: 2994.0440 - val_loss: 2435.7915 - val_y0: -0.4418 - val_y1: -0.3039 - val_ate_afte_scaled: 0.1379 - lr: 3.1250e-05\n",
      "Epoch 26/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3167.9004 — ite: 3.8013  — ate: 0.4650 — pehe: 3.6469 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: 3050.3755 - val_loss: 2457.3936 - val_y0: -0.3916 - val_y1: -0.1753 - val_ate_afte_scaled: 0.2163 - lr: 3.1250e-05\n",
      "Epoch 27/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3115.1965 — ite: 3.8740  — ate: 0.3761 — pehe: 3.7919 \n",
      "3/3 [==============================] - 0s 205ms/step - loss: 3133.0960 - val_loss: 2348.4062 - val_y0: -0.4088 - val_y1: -0.4754 - val_ate_afte_scaled: -0.0666 - lr: 3.1250e-05\n",
      "Epoch 28/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3085.1211 — ite: 3.8213  — ate: 0.6504 — pehe: 3.7550 \n",
      "3/3 [==============================] - 0s 204ms/step - loss: 2945.0430 - val_loss: 2126.7161 - val_y0: -0.4828 - val_y1: -0.3825 - val_ate_afte_scaled: 0.1003 - lr: 3.1250e-05\n",
      "Epoch 29/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3031.1848 — ite: 3.9471  — ate: 0.6472 — pehe: 3.9399 \n",
      "3/3 [==============================] - 0s 205ms/step - loss: 3018.9639 - val_loss: 2280.1855 - val_y0: -0.4077 - val_y1: -0.2799 - val_ate_afte_scaled: 0.1278 - lr: 3.1250e-05\n",
      "Epoch 30/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3092.9285\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      " — ite: 3.8602  — ate: 0.4874 — pehe: 3.8077 \n",
      "3/3 [==============================] - 0s 195ms/step - loss: 3025.7485 - val_loss: 2457.9978 - val_y0: -0.4375 - val_y1: -0.1783 - val_ate_afte_scaled: 0.2592 - lr: 3.1250e-05\n",
      "Epoch 31/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2956.9111 — ite: 3.8828  — ate: 0.7273 — pehe: 3.8997 \n",
      "3/3 [==============================] - 0s 191ms/step - loss: 3096.2829 - val_loss: 2399.8423 - val_y0: -0.4294 - val_y1: -0.1974 - val_ate_afte_scaled: 0.2320 - lr: 1.5625e-05\n",
      "Epoch 32/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2940.6758 — ite: 3.7802  — ate: 0.4937 — pehe: 3.8007 \n",
      "3/3 [==============================] - 0s 198ms/step - loss: 3098.1948 - val_loss: 2352.0796 - val_y0: -0.5330 - val_y1: -0.4230 - val_ate_afte_scaled: 0.1100 - lr: 1.5625e-05\n",
      "Epoch 33/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3138.9841 — ite: 3.7640  — ate: 0.6001 — pehe: 3.8031 \n",
      "3/3 [==============================] - 0s 197ms/step - loss: 2923.2112 - val_loss: 2207.1338 - val_y0: -0.4561 - val_y1: -0.3276 - val_ate_afte_scaled: 0.1285 - lr: 1.5625e-05\n",
      "Epoch 34/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3214.3970 — ite: 3.8583  — ate: 0.5525 — pehe: 3.7471 \n",
      "3/3 [==============================] - 0s 198ms/step - loss: 3028.4155 - val_loss: 2414.9165 - val_y0: -0.5143 - val_y1: -0.1767 - val_ate_afte_scaled: 0.3376 - lr: 1.5625e-05\n",
      "Epoch 35/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2880.8879\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      " — ite: 3.8284  — ate: 0.3515 — pehe: 3.8710 \n",
      "3/3 [==============================] - 0s 201ms/step - loss: 3048.1973 - val_loss: 2110.3064 - val_y0: -0.3837 - val_y1: -0.2854 - val_ate_afte_scaled: 0.0983 - lr: 1.5625e-05\n",
      "Epoch 36/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3043.3809 — ite: 3.8169  — ate: 0.3199 — pehe: 3.7577 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: 3061.9304 - val_loss: 2528.9585 - val_y0: -0.4795 - val_y1: -0.2219 - val_ate_afte_scaled: 0.2576 - lr: 7.8125e-06\n",
      "Epoch 37/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3012.3142 — ite: 3.7870  — ate: 0.5167 — pehe: 3.8176 \n",
      "3/3 [==============================] - 0s 204ms/step - loss: 3042.5509 - val_loss: 2438.4116 - val_y0: -0.4317 - val_y1: -0.2797 - val_ate_afte_scaled: 0.1519 - lr: 7.8125e-06\n",
      "Epoch 38/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3064.8511 — ite: 3.8385  — ate: 0.6166 — pehe: 3.8301 \n",
      "3/3 [==============================] - 0s 194ms/step - loss: 3060.4351 - val_loss: 2199.3196 - val_y0: -0.5299 - val_y1: -0.4391 - val_ate_afte_scaled: 0.0908 - lr: 7.8125e-06\n",
      "Epoch 39/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3147.2979 — ite: 3.8053  — ate: 0.6079 — pehe: 3.7769 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: 3053.2687 - val_loss: 2391.9404 - val_y0: -0.4782 - val_y1: -0.3951 - val_ate_afte_scaled: 0.0831 - lr: 7.8125e-06\n",
      "Epoch 40/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3070.0159\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      " — ite: 3.9001  — ate: 0.4490 — pehe: 3.9002 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: 3096.3951 - val_loss: 2333.7690 - val_y0: -0.5192 - val_y1: -0.3079 - val_ate_afte_scaled: 0.2113 - lr: 7.8125e-06\n",
      "Epoch 41/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2893.8569 — ite: 3.9396  — ate: 0.6271 — pehe: 3.8125 \n",
      "3/3 [==============================] - 0s 207ms/step - loss: 3009.5349 - val_loss: 2318.3887 - val_y0: -0.3845 - val_y1: -0.3893 - val_ate_afte_scaled: -0.0048 - lr: 3.9063e-06\n",
      "Epoch 42/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2823.9214 — ite: 3.8966  — ate: 0.4293 — pehe: 3.7419 \n",
      "3/3 [==============================] - 0s 196ms/step - loss: 3003.7354 - val_loss: 2299.2031 - val_y0: -0.4631 - val_y1: -0.3646 - val_ate_afte_scaled: 0.0985 - lr: 3.9063e-06\n",
      "Epoch 43/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3022.2429 — ite: 3.8383  — ate: 0.6806 — pehe: 3.9940 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: 3051.3976 - val_loss: 2332.8967 - val_y0: -0.4970 - val_y1: -0.4821 - val_ate_afte_scaled: 0.0149 - lr: 3.9063e-06\n",
      "Epoch 44/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2961.5640 — ite: 3.7726  — ate: 0.4883 — pehe: 3.7350 \n",
      "3/3 [==============================] - 0s 195ms/step - loss: 3069.3827 - val_loss: 2362.5596 - val_y0: -0.4983 - val_y1: -0.3661 - val_ate_afte_scaled: 0.1322 - lr: 3.9063e-06\n",
      "Epoch 45/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3194.6111\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      " — ite: 3.9029  — ate: 0.6879 — pehe: 3.8367 \n",
      "3/3 [==============================] - 0s 194ms/step - loss: 3063.3736 - val_loss: 2285.0791 - val_y0: -0.3571 - val_y1: -0.2499 - val_ate_afte_scaled: 0.1073 - lr: 3.9063e-06\n",
      "Epoch 46/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3137.1323 — ite: 3.6768  — ate: 0.5050 — pehe: 3.7545 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: 3074.8958 - val_loss: 2201.6221 - val_y0: -0.4861 - val_y1: -0.2277 - val_ate_afte_scaled: 0.2584 - lr: 1.9531e-06\n",
      "Epoch 47/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2940.8484 — ite: 3.7892  — ate: 0.5178 — pehe: 3.7789 \n",
      "3/3 [==============================] - 0s 197ms/step - loss: 2986.0010 - val_loss: 2394.7983 - val_y0: -0.4035 - val_y1: -0.3636 - val_ate_afte_scaled: 0.0398 - lr: 1.9531e-06\n",
      "Epoch 48/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2811.1975 — ite: 3.8531  — ate: 0.7371 — pehe: 3.8556 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: 3001.3785 - val_loss: 2328.1924 - val_y0: -0.5192 - val_y1: -0.3952 - val_ate_afte_scaled: 0.1240 - lr: 1.9531e-06\n",
      "Epoch 49/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3276.4453 — ite: 3.8108  — ate: 0.4451 — pehe: 3.8218 \n",
      "3/3 [==============================] - 0s 201ms/step - loss: 3163.1033 - val_loss: 2537.6523 - val_y0: -0.4372 - val_y1: -0.2985 - val_ate_afte_scaled: 0.1387 - lr: 1.9531e-06\n",
      "Epoch 50/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3028.1396\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      " — ite: 3.8265  — ate: 0.6312 — pehe: 3.6756 \n",
      "3/3 [==============================] - 0s 201ms/step - loss: 2997.4915 - val_loss: 2248.5884 - val_y0: -0.5301 - val_y1: -0.4312 - val_ate_afte_scaled: 0.0990 - lr: 1.9531e-06\n",
      "Epoch 51/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3245.9922 — ite: 3.8652  — ate: 0.4355 — pehe: 3.8516 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: 3106.8329 - val_loss: 2151.0610 - val_y0: -0.5565 - val_y1: -0.3191 - val_ate_afte_scaled: 0.2374 - lr: 9.7656e-07\n",
      "Epoch 52/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3020.2332 — ite: 3.8124  — ate: 0.6749 — pehe: 3.8197 \n",
      "3/3 [==============================] - 0s 190ms/step - loss: 2948.8684 - val_loss: 2394.0879 - val_y0: -0.4051 - val_y1: -0.4228 - val_ate_afte_scaled: -0.0177 - lr: 9.7656e-07\n",
      "Epoch 53/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3286.4275 — ite: 3.8603  — ate: 0.4894 — pehe: 3.8567 \n",
      "3/3 [==============================] - 0s 205ms/step - loss: 3027.4375 - val_loss: 2218.9805 - val_y0: -0.4709 - val_y1: -0.3956 - val_ate_afte_scaled: 0.0753 - lr: 9.7656e-07\n",
      "Epoch 54/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3182.5725 — ite: 3.7845  — ate: 0.5572 — pehe: 3.8336 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: 3210.6788 - val_loss: 2281.7002 - val_y0: -0.4769 - val_y1: -0.2892 - val_ate_afte_scaled: 0.1877 - lr: 9.7656e-07\n",
      "Epoch 55/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3063.8018\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      " — ite: 3.8258  — ate: 0.7330 — pehe: 3.7802 \n",
      "3/3 [==============================] - 0s 195ms/step - loss: 3062.3784 - val_loss: 2347.9771 - val_y0: -0.4414 - val_y1: -0.2638 - val_ate_afte_scaled: 0.1775 - lr: 9.7656e-07\n",
      "Epoch 56/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3127.3796 — ite: 3.7840  — ate: 0.5173 — pehe: 3.9257 \n",
      "3/3 [==============================] - 0s 201ms/step - loss: 3125.2566 - val_loss: 2269.6653 - val_y0: -0.4376 - val_y1: -0.3818 - val_ate_afte_scaled: 0.0558 - lr: 4.8828e-07\n",
      "Epoch 57/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3017.7681 — ite: 3.8000  — ate: 0.5972 — pehe: 3.9182 \n",
      "3/3 [==============================] - 0s 197ms/step - loss: 2967.3570 - val_loss: 2404.0693 - val_y0: -0.5419 - val_y1: -0.2713 - val_ate_afte_scaled: 0.2706 - lr: 4.8828e-07\n",
      "Epoch 58/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3138.0359 — ite: 3.8068  — ate: 0.5731 — pehe: 3.8206 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: 2961.9720 - val_loss: 2327.2256 - val_y0: -0.5600 - val_y1: -0.2675 - val_ate_afte_scaled: 0.2925 - lr: 4.8828e-07\n",
      "Epoch 59/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3019.3762 — ite: 3.8562  — ate: 0.5854 — pehe: 3.7285 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: 3236.7802 - val_loss: 2206.6123 - val_y0: -0.3516 - val_y1: -0.2981 - val_ate_afte_scaled: 0.0536 - lr: 4.8828e-07\n",
      "Epoch 60/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3046.0691\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      " — ite: 3.7774  — ate: 0.5280 — pehe: 3.8197 \n",
      "3/3 [==============================] - 0s 201ms/step - loss: 3074.3783 - val_loss: 2526.2378 - val_y0: -0.4625 - val_y1: -0.3302 - val_ate_afte_scaled: 0.1322 - lr: 4.8828e-07\n",
      "Epoch 61/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3254.7847 — ite: 3.8419  — ate: 0.8506 — pehe: 3.7528 \n",
      "3/3 [==============================] - 0s 195ms/step - loss: 3110.1635 - val_loss: 2259.2317 - val_y0: -0.4846 - val_y1: -0.3482 - val_ate_afte_scaled: 0.1363 - lr: 2.4414e-07\n",
      "Epoch 62/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3130.9194 — ite: 3.7169  — ate: 0.3887 — pehe: 3.7260 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: 3074.4531 - val_loss: 2386.3057 - val_y0: -0.5079 - val_y1: -0.4332 - val_ate_afte_scaled: 0.0747 - lr: 2.4414e-07\n",
      "Epoch 63/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2958.5786 — ite: 3.7893  — ate: 0.7187 — pehe: 3.7138 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: 3029.1282 - val_loss: 2350.7959 - val_y0: -0.3981 - val_y1: -0.4347 - val_ate_afte_scaled: -0.0367 - lr: 2.4414e-07\n",
      "Epoch 64/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3065.5671 — ite: 3.8179  — ate: 0.5704 — pehe: 3.8123 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: 2957.1498 - val_loss: 2352.2451 - val_y0: -0.3154 - val_y1: -0.3250 - val_ate_afte_scaled: -0.0096 - lr: 2.4414e-07\n",
      "Epoch 65/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3157.0562\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      " — ite: 3.8359  — ate: 0.5220 — pehe: 3.7308 \n",
      "3/3 [==============================] - 0s 199ms/step - loss: 3113.2623 - val_loss: 2171.3945 - val_y0: -0.4270 - val_y1: -0.3195 - val_ate_afte_scaled: 0.1075 - lr: 2.4414e-07\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Colab command to allow us to run Colab in TF2\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "%reload_ext tensorboard \n",
    "\n",
    "model = CEWAE()\n",
    "### MAIN CODE ####\n",
    "verbose=True\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    " \n",
    "callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='loss', patience=40, min_delta=0), \n",
    "        #40 is Shi's recommendation patience for this dataset, but you should tune for your data \n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0, cooldown=0, min_lr=0),\n",
    "        #This learning rate scheduling is quite agressive which seems good for this dataset\n",
    "        metrics_for_cevae(data,verbose),\n",
    "        tensorboard_callback\n",
    "    ]\n",
    "    \n",
    "#optimizer hyperparameters\n",
    "learning_rate = 5e-4\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate))\n",
    "\n",
    "model.fit(\n",
    "    [data['x'],data['t'],data['ys']],\n",
    "    callbacks=callbacks,\n",
    "    validation_split=args.val_split,\n",
    "    epochs=300,\n",
    "    batch_size=args.batch_size,\n",
    "    verbose=verbose\n",
    "    )\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 45269), started 2:48:46 ago. (Use '!kill 45269' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3327203e50001169\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3327203e50001169\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
