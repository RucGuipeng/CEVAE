{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.layers as tfkl\n",
    "tfd,tfpl = tfp.distributions,tfp.layers\n",
    "import tensorflow.keras.backend as tfkb\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from evaluation import *\n",
    "from cevae_networks import *\n",
    "################################################\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='')\n",
    "parser.add_argument('--scale_penalize',    type = float, default = 0.001,  help = '')\n",
    "parser.add_argument('--learning_rate',     type = float, default = 0.001,  help = '')\n",
    "parser.add_argument('--default_y_scale',   type = float, default = 1.,  help = '')\n",
    "parser.add_argument('--t_dim',     type = int, default = 1,  help = '')\n",
    "parser.add_argument('--y_dim',     type = int, default = 1,  help = '')\n",
    "parser.add_argument('--x_dim',     type = int, default = 25, help = '')\n",
    "parser.add_argument('--z_dim',     type = int, default = 20, help = '')\n",
    "parser.add_argument('--x_num_dim', type = int, default = 6,  help = '')\n",
    "parser.add_argument('--x_bin_dim', type = int, default = 19, help = '')\n",
    "parser.add_argument('--val_split', type = float, default = 0.2, help = '')\n",
    "parser.add_argument('--batch_size', type = int, default = 256, help = '')\n",
    "parser.add_argument('--nh', type = int, default = 3, help = 'number of hidden layers')\n",
    "parser.add_argument('--h',  type = int, default = 200, help = 'number of hidden units')\n",
    "args = parser.parse_args([])\n",
    "################################################\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz \n",
    "\n",
    "def load_IHDP_data(training_data,testing_data,i):\n",
    "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "        train_data=np.load(trf); test_data=np.load(tef)\n",
    "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
    "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
    "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
    "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    "        ycf=np.concatenate((train_data['ycf'][:,i],  test_data['ycf'][:,i])).astype('float32')\n",
    "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "        data['y']=data['y'].reshape(-1,1)\n",
    "        data['ycf'] = ycf.reshape(-1,1)\n",
    "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "    return data\n",
    "\n",
    "ind = 7\n",
    "# rep = 5\n",
    "# rep = 1\n",
    "# data = load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.test.npz',i = ind)\n",
    "# for key in data:\n",
    "#     if key != 'y_scaler':\n",
    "#         data[key] = np.repeat(data[key],repeats = rep, axis = 0)\n",
    "# np.shape(data['x'])\n",
    "data_train = load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.train.npz',i = ind)\n",
    "data_valid = load_IHDP_data(training_data='./ihdp_npci_1-100.test.npz',testing_data='./ihdp_npci_1-100.test.npz',i = ind)\n",
    "np.shape(data_train['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonLayer(tfkl.Layer):\n",
    "    def __init__(self):\n",
    "        super(EpsilonLayer, self).__init__()\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.epsilon = self.add_weight(name='epsilon',\n",
    "                                       shape=[1, 1],\n",
    "                                       initializer='RandomNormal',\n",
    "                                       #  initializer='ones',\n",
    "                                       trainable=True)\n",
    "        super(EpsilonLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        #note there is only one epsilon were just duplicating it for conformability\n",
    "        return self.epsilon * tf.ones_like(inputs)[:, 0:1]\n",
    "\n",
    "class CEWAE(tf.keras.Model):\n",
    "    def __init__(self, kernel = \"IMQ\"):\n",
    "        super(CEWAE, self).__init__()\n",
    "        ########################################\n",
    "        # networks\n",
    "        self.activation = 'elu'\n",
    "        self.kernel = kernel\n",
    "        # CEVAE Model \n",
    "        ## (encoder)\n",
    "        self.q_y_tx = q_y_tx(args.x_bin_dim, args.x_num_dim, args.y_dim, args.t_dim, args.nh, args.h)\n",
    "        self.q_t_x = q_t_x(args.x_bin_dim, args.x_num_dim, args.t_dim, args.nh, args.h)\n",
    "        self.q_z_txy = q_z_txy(args.x_bin_dim, args.x_num_dim, args.y_dim, args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        ## (decoder)\n",
    "        self.p_x_z = p_x_z(args.x_bin_dim, args.x_num_dim, args.z_dim, args.nh, args.h)\n",
    "        self.p_t_z = p_t_z(args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        self.p_y_tz = p_y_tz(args.y_dim, args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        self.epsilon_layer = EpsilonLayer()\n",
    "        self.beta = 1\n",
    "        self.lmbda = 1\n",
    "\n",
    "    def call(self, data, training=False):\n",
    "        if training:\n",
    "            x_train,t_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "            \n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            \n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.sample()\n",
    "            # decoder\n",
    "            ## p(x|z)\n",
    "            x_num,x_bin = self.p_x_z(z_infer_sample)\n",
    "            ## p(t|z)\n",
    "            t = self.p_t_z(z_infer_sample)\n",
    "            ## p(y|t,z)\n",
    "            t0z = tf.concat([tf.zeros_like(t_train),z_infer_sample],-1)\n",
    "            t1z = tf.concat([tf.ones_like(t_train),z_infer_sample],-1)\n",
    "            y0 = self.p_y_tz(t0z)\n",
    "            y1 = self.p_y_tz(t1z)\n",
    "            y = [y0,y1]\n",
    "            epsilon = self.epsilon_layer(t_infer_sample)\n",
    "            \n",
    "            return y_infer,t_infer,z_infer,y,t,x_num,x_bin,epsilon\n",
    "        else:\n",
    "            x_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.loc\n",
    "\n",
    "            t1z = tf.concat([tf.ones_like(t_infer_sample),z_infer_sample],-1)\n",
    "            t0z = tf.concat([tf.zeros_like(t_infer_sample),z_infer_sample],-1)\n",
    "            y0 = self.p_y_tz(t0z)\n",
    "            y1 = self.p_y_tz(t1z)\n",
    "            y = [y0,y1]\n",
    "            return y,t_infer,z_infer\n",
    "\n",
    "    def mmd_penalty(self, sample_qz, sample_pz, batch_size = args.batch_size):\n",
    "        opts = {'kernel': self.kernel, 'verbose':True, \"zdim\":20, \"pz\":\"normal\"} \n",
    "        sigma2_p = 1 ** 2\n",
    "        kernel = opts['kernel']\n",
    "        n = tf.shape(sample_qz)[0]\n",
    "        n = tf.cast(n, tf.int32)\n",
    "        nf = tf.cast(n, tf.float32)\n",
    "        half_size = (n * n - n) / 2\n",
    "\n",
    "        norms_pz = tf.reduce_sum(tf.square(sample_pz), axis=1, keepdims=True)\n",
    "        dotprods_pz = tf.matmul(sample_pz, sample_pz, transpose_b=True)\n",
    "        distances_pz = norms_pz + tf.transpose(norms_pz) - 2. * dotprods_pz\n",
    "\n",
    "        norms_qz = tf.reduce_sum(tf.square(sample_qz), axis=1, keepdims=True)\n",
    "        dotprods_qz = tf.matmul(sample_qz, sample_qz, transpose_b=True)\n",
    "        distances_qz = norms_qz + tf.transpose(norms_qz) - 2. * dotprods_qz\n",
    "\n",
    "        dotprods = tf.matmul(sample_qz, sample_pz, transpose_b=True)\n",
    "        distances = norms_qz + tf.transpose(norms_pz) - 2. * dotprods\n",
    "\n",
    "        if kernel == 'RBF':\n",
    "            # Median heuristic for the sigma^2 of Gaussian kernel\n",
    "            sigma2_k = tf.compat.v1.nn.top_k(\n",
    "                tf.reshape(distances, [-1]), half_size).values[half_size - 1]\n",
    "            sigma2_k += tf.compat.v1.top_k(\n",
    "                tf.reshape(distances_qz, [-1]), half_size).values[half_size - 1]\n",
    "            # Maximal heuristic for the sigma^2 of Gaussian kernel\n",
    "            # sigma2_k = tf.nn.top_k(tf.reshape(distances_qz, [-1]), 1).values[0]\n",
    "            # sigma2_k += tf.nn.top_k(tf.reshape(distances, [-1]), 1).values[0]\n",
    "            # sigma2_k = opts['latent_space_dim'] * sigma2_p\n",
    "            if opts['verbose']:\n",
    "                sigma2_k = tf.Print(sigma2_k, [sigma2_k], 'Kernel width:')\n",
    "            res1 = tf.exp( - distances_qz / 2. / sigma2_k)\n",
    "            res1 += tf.exp( - distances_pz / 2. / sigma2_k)\n",
    "            res1 = tf.multiply(res1, 1. - tf.eye(n))\n",
    "            res1 = tf.reduce_sum(res1) / (nf * nf - nf)\n",
    "            res2 = tf.exp( - distances / 2. / sigma2_k)\n",
    "            res2 = tf.reduce_sum(res2) * 2. / (nf * nf)\n",
    "            stat = res1 - res2\n",
    "        elif kernel == 'IMQ':\n",
    "            # k(x, y) = C / (C + ||x - y||^2)\n",
    "            # C = tf.nn.top_k(tf.reshape(distances, [-1]), half_size).values[half_size - 1]\n",
    "            # C += tf.nn.top_k(tf.reshape(distances_qz, [-1]), half_size).values[half_size - 1]\n",
    "            if opts['pz'] == 'normal':\n",
    "                Cbase = 2. * opts['zdim'] * sigma2_p\n",
    "            elif opts['pz'] == 'sphere':\n",
    "                Cbase = 2.\n",
    "            elif opts['pz'] == 'uniform':\n",
    "                # E ||x - y||^2 = E[sum (xi - yi)^2]\n",
    "                #               = zdim E[(xi - yi)^2]\n",
    "                #               = const * zdim\n",
    "                Cbase = opts['zdim']\n",
    "            stat = 0.\n",
    "            for scale in [.1, .2, .5, 1., 2., 5., 10.]:\n",
    "                C = Cbase * scale\n",
    "                res1 = C / (C + distances_qz)\n",
    "                res1 += C / (C + distances_pz)\n",
    "                res1 = tf.multiply(res1, 1. - tf.eye(n))\n",
    "                res1 = tf.reduce_sum(res1) / (nf * nf - nf)\n",
    "                res2 = C / (C + distances)\n",
    "                res2 = tf.reduce_sum(res2) * 2. / (nf * nf)\n",
    "                stat += res1 - res2\n",
    "        return stat\n",
    "\n",
    "    def cewae_loss(self, data, pred, training = False):\n",
    "        x_train, t_train, y_train = data[0],data[1],data[2]\n",
    "        x_train_num, x_train_bin = x_train[:,:args.x_num_dim],x_train[:,args.x_num_dim:]\n",
    "        y_infer,t_infer,z_infer,y,t,x_num,x_bin,epsilon = pred\n",
    "        # y0_infer,y1_infer = y_infer\n",
    "        y0,y1 = y\n",
    "        # reconstruct loss\n",
    "        rec_x_num = tfkb.mean(tf.math.square(x_train_num - x_num.sample()))\n",
    "        rec_x_bin = tf.reduce_sum(\n",
    "            tfk.losses.binary_crossentropy(\n",
    "                x_train_bin,\n",
    "                tf.cast(x_bin.sample(),tf.float32),\n",
    "                from_logits=False))\n",
    "        rec_t_bin = tf.reduce_sum(\n",
    "            tfk.losses.binary_crossentropy(\n",
    "                t_train,\n",
    "                tf.cast(t_infer.sample(),tf.float32),\n",
    "                from_logits=False))\n",
    "        rec_y0 = tf.math.square(y0.sample() - y_train)\n",
    "        rec_y1 = tf.math.square(y1.sample() - y_train)\n",
    "        rec_y = tfkb.mean(t_train * rec_y1 + (1-t_train)* rec_y0)\n",
    "        # regularization\n",
    "        # mmd penalty\n",
    "        pz = tfd.Normal(loc = tf.zeros_like(z_infer.sample()), scale = tf.ones_like(z_infer.sample()))\n",
    "        reg_mmd = self.mmd_penalty(z_infer.sample(), pz.sample())\n",
    "\n",
    "        loss = rec_x_num + rec_x_bin + rec_t_bin + rec_y + reg_mmd * self.lmbda\n",
    "        loss = rec_x_num + rec_x_bin + rec_t_bin + rec_y + reg_mmd\n",
    "        # loss = rec_x_num + rec_x_bin + rec_t_bin + rec_y \n",
    "        return loss\n",
    "\n",
    "    def train_step(self, data):\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "            loss = self.cewae_loss(data = data, pred = pred, training = True)\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        metrics = {\"loss\": loss}\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "        y_infer = pred[0]\n",
    "        loss = self.cewae_loss(data = data, pred = pred, training = False)\n",
    "        y0, y1 = y_infer[0].sample(),y_infer[1].sample()\n",
    "        ate = tfkb.mean(y1) - tfkb.mean(y0)\n",
    "        metrics = {\"loss\":loss,\"y0\": tfkb.mean(y0),\"y1\": tfkb.mean(y1),'ate_afte_scaled': ate}\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Colab command to allow us to run Colab in TF2\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "%reload_ext tensorboard \n",
    "\n",
    "model = CEWAE(kernel = 'IMQ')\n",
    "### MAIN CODE ####\n",
    "verbose=True\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    " \n",
    "callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='var_loss', patience=40, min_delta=0), \n",
    "        #40 is Shi's recommendation patience for this dataset, but you should tune for your data \n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0, cooldown=0, min_lr=0),\n",
    "        #This learning rate scheduling is quite agressive which seems good for this dataset\n",
    "        metrics_for_cevae(data_train,'train',verbose),\n",
    "        metrics_for_cevae(data_valid,'valid',verbose),\n",
    "        tensorboard_callback\n",
    "    ]\n",
    "    \n",
    "#optimizer hyperparameters\n",
    "learning_rate = 5e-4\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate))\n",
    "\n",
    "model.fit(\n",
    "    [data_train['x'],data_train['t'],data_train['ys']],\n",
    "    callbacks=callbacks,\n",
    "    validation_data=[[data_valid['x'],data_valid['t'],data_valid['ys']]],\n",
    "    epochs=300,\n",
    "    batch_size=args.batch_size,\n",
    "    verbose=verbose\n",
    "    )\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
