{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-07 20:33:57.434901: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19, 6, 1, 1, 20)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.layers as tfkl\n",
    "tfd,tfpl = tfp.distributions,tfp.layers\n",
    "import tensorflow.keras.backend as tfkb\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from networks import fc_net, p_x_z, p_t_z, p_y_tz, q_t_x, q_y_tx, q_z_txy\n",
    "from evaluation import Evaluator, pdist2sq, Full_Metrics\n",
    "#################################IHDP Data\n",
    "# data information \n",
    "t_bin_dim = 1\n",
    "y_dim, default_y_scale = 1,tf.exp(0.)\n",
    "M = None        # batch size during training\n",
    "z_dim = 20          # latent z dimension\n",
    "lamba = 1e-4    # weight decay\n",
    "nh, h = 3, 200  # number and size of hidden layers\n",
    "binfeats = [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
    "numfeats = [i for i in range(25) if i not in binfeats]\n",
    "x_bin_dim = len(binfeats)\n",
    "x_num_dim = len(numfeats)\n",
    "x_bin_dim, x_num_dim, t_bin_dim, y_dim, z_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 “ihdp_npci_1-100.train.npz” 已经存在；不获取。\n",
      "\n",
      "文件 “ihdp_npci_1-100.test.npz” 已经存在；不获取。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz \n",
    "\n",
    "def load_IHDP_data(training_data,testing_data,i=7):\n",
    "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "        train_data=np.load(trf); test_data=np.load(tef)\n",
    "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
    "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
    "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
    "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    "        ycf=np.concatenate((train_data['ycf'][:,i],  test_data['ycf'][:,i])).astype('float32')\n",
    "\n",
    "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "        data['y']=data['y'].reshape(-1,1)\n",
    "        data['ycf'] = ycf.reshape(-1,1)\n",
    "        \n",
    "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "\n",
    "    return data\n",
    "\n",
    "data=load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.test.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'activation_global' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p0/kb961x5d6wd79gfp2h6t_8500000gn/T/ipykernel_21971/3291939890.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'reload_ext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tensorboard'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mfc_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_global\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtfkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInputLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'activation_global' is not defined"
     ]
    }
   ],
   "source": [
    "#Colab command to allow us to run Colab in TF2\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "%reload_ext tensorboard \n",
    "\n",
    "activation_global \n",
    "\n",
    "def fc_net(input_shape, layers, out_layers = [], activation = activation_global, lamba = 1e-4):\n",
    "    net = tfk.Sequential([tfkl.InputLayer([input_shape])])\n",
    "    for hidden in layers:\n",
    "        net.add(tfkl.Dense(\n",
    "            hidden, \n",
    "            activation = activation,\n",
    "            kernel_regularizer = tf.keras.regularizers.l2(lamba) \n",
    "            )\n",
    "        )\n",
    "    if len(out_layers) > 0:\n",
    "        [outdim, activation_out] = out_layers\n",
    "        net.add(tfkl.Dense(outdim, activation = activation_out))\n",
    "    return net\n",
    "\n",
    "class q_y_tx(tf.keras.Model):\n",
    "    def __init__(self, x_bin_dim, x_num_dim, y_dim, t_dim, nh, h):\n",
    "        super(q_y_tx, self).__init__()\n",
    "        self.t_dim = t_dim\n",
    "        self.q_y_xt_shared_hqy = fc_net(x_bin_dim + x_num_dim, (nh - 1) * [h], [])\n",
    "        self.q_y_xt0_mu = fc_net(h, [h], [y_dim, None])\n",
    "        self.q_y_xt1_mu = fc_net(h, [h], [y_dim, None])\n",
    "\n",
    "    def call(self, tx_input, training=False, serving=False):\n",
    "        t = tx_input[...,:self.t_dim] \n",
    "        x = tx_input[...,self.t_dim:]\n",
    "        hqy = self.q_y_xt_shared_hqy(x)\n",
    "        \n",
    "        qy_t0_mu = self.q_y_xt0_mu(hqy)\n",
    "        qy_t1_mu = self.q_y_xt1_mu(hqy)\n",
    "        \n",
    "        y_loc =  t * qy_t1_mu + (1-t) * qy_t0_mu\n",
    "        return tfd.Normal(\n",
    "            loc =  y_loc, \n",
    "            scale = tf.ones_like(y_loc),\n",
    "            ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEVAE(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CEVAE, self).__init__()\n",
    "        ########################################\n",
    "        # networks\n",
    "        self.activation = 'elu'\n",
    "        # CEVAE Model (decoder)\n",
    "        self.q_y_tx = q_y_tx(x_bin_dim, x_num_dim, y_dim, t_bin_dim, 3, 100)\n",
    "\n",
    "    def call(self, data, training=False):\n",
    "        if training:\n",
    "            x_train,t_train,y_train = data\n",
    "            ## q(y|x,t)\n",
    "            y_infer = self.q_y_tx( tf.concat([t_train, x_train],-1) )\n",
    "            return y_infer\n",
    "        else:\n",
    "            # when training need x,y,t\n",
    "            x_train = data\n",
    "            ## q(t|x)\n",
    "            t = tf.ones([tf.shape(x_train)[0],t_bin_dim])\n",
    "            t0 = tf.cast(tf.zeros_like(t), tf.float32)\n",
    "            t1 = tf.cast(tf.ones_like(t), tf.float32)\n",
    "            ## q(y|x,t)\n",
    "            t0x = tf.concat([t0, x_train],-1)\n",
    "            t1x = tf.concat([t1, x_train],-1)\n",
    "            y0 = self.q_y_tx(t0x).sample()\n",
    "            y1 = self.q_y_tx(t1x).sample()\n",
    "            return 0,y0,y1\n",
    "\n",
    "    def cevae_loss(self, data, pred):\n",
    "        # read labels\n",
    "        _, t_train, y_train = data[0],data[1],data[2]\n",
    "        # get preds\n",
    "        y_infer = pred\n",
    "        l7 = tfkb.sum(y_infer.log_prob(y_train),-1)\n",
    "        # layer_loss\n",
    "        l8 = tfkb.sum(self.losses)\n",
    "        \n",
    "        return l7,l8\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        # 这里data[0]因为会自动在外面拼接一层\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self(data, training=True)  # Forward pass\n",
    "            # loss = self.cevae_loss(data,pred)\n",
    "            l7,l8 = self.cevae_loss(data,pred)\n",
    "            loss = -tfkb.mean(l7 - l8)\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        metrics = {\n",
    "            \"loss\": loss,\n",
    "            # \"loss_t_aux\":tfkb.mean(l6),\n",
    "            \"loss_y_aux\":tfkb.mean(l7),\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self(x, training=False)  # Forward pass\n",
    "            t_infer,y0,y1 = pred\n",
    "        metrics = {\"y0\": tfkb.mean(y0),\"y1\": tfkb.mean(y1)}\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CEVAE()\n",
    "### MAIN CODE ####\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from evaluation import *\n",
    " \n",
    "val_split=0.2\n",
    "batch_size=64\n",
    "verbose=True\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    "yt = np.concatenate([data['ys'], data['t']], 1)\n",
    " \n",
    "sgd_callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='val_loss', patience=40, min_delta=0), \n",
    "        #40 is Shi's recommendation patience for this dataset, but you should tune for your data \n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0, cooldown=0, min_lr=0),\n",
    "        #This learning rate scheduling is quite agressive which seems good for this dataset\n",
    "        # Full_Metrics(data,verbose),\n",
    "        metrics_for_cevae(data,verbose),\n",
    "        tensorboard_callback\n",
    "    ]\n",
    "    \n",
    "#optimizer hyperparameters\n",
    "sgd_lr = 1e-5\n",
    "momentum = 0.9\n",
    "model.compile(\n",
    "    optimizer=SGD(\n",
    "        learning_rate=sgd_lr, \n",
    "        momentum=momentum, \n",
    "        nesterov=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "model.fit(\n",
    "    [data['x'],data['t'],data['y']],\n",
    "    callbacks=sgd_callbacks,\n",
    "    validation_split=val_split,\n",
    "    epochs=30,\n",
    "    batch_size=batch_size,\n",
    "    verbose=verbose\n",
    "    )\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
