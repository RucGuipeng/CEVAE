{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 “ihdp_npci_1-100.train.npz” 已经存在；不获取。\n",
      "\n",
      "文件 “ihdp_npci_1-100.test.npz” 已经存在；不获取。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title First load the data! (Click Play)\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz \n",
    "\n",
    "def load_IHDP_data(training_data,testing_data,i=7):\n",
    "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "        train_data=np.load(trf); test_data=np.load(tef)\n",
    "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
    "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
    "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
    "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    "\n",
    "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "        data['y']=data['y'].reshape(-1,1)\n",
    "        \n",
    "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "\n",
    "    return data\n",
    "\n",
    "data=load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.test.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-07 20:56:24.843542: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "#Colab command to allow us to run Colab in TF2\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "%reload_ext tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "def pdist2sq(A, B):\n",
    "    #helper for PEHEnn\n",
    "    #calculates squared euclidean distance between rows of two matrices  \n",
    "    #https://gist.github.com/mbsariyildiz/34cdc26afb630e8cae079048eef91865\n",
    "    # squared norms of each row in A and B\n",
    "    na = tf.reduce_sum(tf.square(A), 1)\n",
    "    nb = tf.reduce_sum(tf.square(B), 1)    \n",
    "    # na as a row and nb as a column vectors\n",
    "    na = tf.reshape(na, [-1, 1])\n",
    "    nb = tf.reshape(nb, [1, -1])\n",
    "    # return pairwise euclidean difference matrix\n",
    "    D=tf.reduce_sum((tf.expand_dims(A, 1)-tf.expand_dims(B, 0))**2,2) \n",
    "    return D\n",
    "\n",
    "\n",
    "#https://towardsdatascience.com/implementing-macro-f1-score-in-keras-what-not-to-do-e9f1aa04029d\n",
    "class Full_Metrics(Callback):\n",
    "    def __init__(self,data, verbose=0):   \n",
    "        super(Full_Metrics, self).__init__()\n",
    "        self.data=data #feed the callback the full dataset\n",
    "        self.verbose=verbose\n",
    "\n",
    "        #needed for PEHEnn; Called in self.find_ynn\n",
    "        self.data['o_idx']=tf.range(self.data['t'].shape[0])\n",
    "        self.data['c_idx']=self.data['o_idx'][self.data['t'].squeeze()==0] #These are the indices of the control units\n",
    "        self.data['t_idx']=self.data['o_idx'][self.data['t'].squeeze()==1] #These are the indices of the treated units\n",
    "    \n",
    "    def split_pred(self,concat_pred):\n",
    "        #this helps us keep ptrack of things so we don't make mistakes\n",
    "        preds={}\n",
    "        preds['y0_pred'] = self.data['y_scaler'].inverse_transform(np.reshape(concat_pred[:, 0],[-1,1]))[:,0]\n",
    "        preds['y1_pred'] = self.data['y_scaler'].inverse_transform(np.reshape(concat_pred[:, 1],[-1,1]))[:,0]\n",
    "        preds['phi'] = concat_pred[:, 2:]\n",
    "        return preds\n",
    "\n",
    "    def find_ynn(self, Phi):\n",
    "        #helper for PEHEnn\n",
    "        PhiC, PhiT =tf.dynamic_partition(Phi,tf.cast(tf.squeeze(self.data['t']),tf.int32),2) #separate control and treated reps\n",
    "        dists=tf.sqrt(pdist2sq(PhiC,PhiT)) #calculate squared distance then sqrt to get euclidean\n",
    "        yT_nn_idx=tf.gather(self.data['c_idx'],tf.argmin(dists,axis=0),1) #get c_idxs of smallest distances for treated units\n",
    "        yC_nn_idx=tf.gather(self.data['t_idx'],tf.argmin(dists,axis=1),1) #get t_idxs of smallest distances for control units\n",
    "        yT_nn=tf.gather(self.data['y'],yT_nn_idx,1) #now use these to retrieve y values\n",
    "        yC_nn=tf.gather(self.data['y'],yC_nn_idx,1)\n",
    "        y_nn=tf.dynamic_stitch([self.data['t_idx'],self.data['c_idx']],[yT_nn,yC_nn]) #stitch em back up!\n",
    "        return y_nn\n",
    "\n",
    "    def PEHEnn(self,concat_pred):\n",
    "        p = self.split_pred(concat_pred)\n",
    "        y_nn = self.find_ynn(p['phi']) #now its 3 plus because \n",
    "        cate_nn_err=tf.reduce_mean( tf.square( (1-2*self.data['t']) * (y_nn-self.data['y']) - (p['y1_pred']-p['y0_pred']) ) )\n",
    "        return cate_nn_err\n",
    "\n",
    "    def ATE(self,concat_pred):\n",
    "        p = self.split_pred(concat_pred)\n",
    "        return p['y1_pred']-p['y0_pred']\n",
    "\n",
    "    def PEHE(self,concat_pred):\n",
    "        #simulation only\n",
    "        p = self.split_pred(concat_pred)\n",
    "        cate_err=tf.reduce_mean( tf.square( ( (self.data['mu_1']-self.data['mu_0']) - (p['y1_pred']-p['y0_pred']) ) ) )\n",
    "        return cate_err \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        concat_pred=self.model.predict(self.data['x'])\n",
    "        #Calculate Empirical Metrics        \n",
    "        ate_pred=tf.reduce_mean(self.ATE(concat_pred)); tf.summary.scalar('ate', data=ate_pred, step=epoch)\n",
    "        pehe_nn=self.PEHEnn(concat_pred); tf.summary.scalar('cate_nn_err', data=tf.sqrt(pehe_nn), step=epoch)\n",
    "        \n",
    "        #Simulation Metrics\n",
    "        ate_true=tf.reduce_mean(self.data['mu_1']-self.data['mu_0'])\n",
    "        ate_err=tf.abs(ate_true-ate_pred); tf.summary.scalar('ate_err', data=ate_err, step=epoch)\n",
    "        pehe =self.PEHE(concat_pred); tf.summary.scalar('cate_err', data=tf.sqrt(pehe), step=epoch)\n",
    "        out_str=f' — ate_err: {ate_err:.4f}  — cate_err: {tf.sqrt(pehe):.4f} — cate_nn_err: {tf.sqrt(pehe_nn):.4f} '\n",
    "        \n",
    "        if self.verbose > 0: print(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.stats import sem\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.layers as tfkl\n",
    "tfd,tfpl = tfp.distributions,tfp.layers\n",
    "import tensorflow.keras.backend as tfkb\n",
    "\n",
    "class tarnet(tf.keras.Model):\n",
    "    def __init__(self, input_dim, reg_l2):\n",
    "        super(tarnet, self).__init__()\n",
    "        ########################################\n",
    "        # networks\n",
    "        self.activation = 'elu'\n",
    "        self.share_bottom = tfk.Sequential(\n",
    "            [\n",
    "                tfkl.InputLayer([input_dim]),\n",
    "                tfkl.Dense(200,activation = self.activation, kernel_initializer='RandomNormal'),\n",
    "                tfkl.Dense(200,activation = self.activation, kernel_initializer='RandomNormal'),\n",
    "                tfkl.Dense(200,activation = self.activation, kernel_initializer='RandomNormal'),\n",
    "            ])\n",
    "        self.tower_t0 = tfk.Sequential(\n",
    "            [\n",
    "                tfkl.InputLayer([200]),\n",
    "                tfkl.Dense(100,activation = self.activation, kernel_regularizer=regularizers.l2(reg_l2)),\n",
    "                tfkl.Dense(100,activation = self.activation, kernel_regularizer=regularizers.l2(reg_l2)),\n",
    "                tfkl.Dense(1,activation = None, kernel_regularizer=regularizers.l2(reg_l2)),\n",
    "            ])\n",
    "\n",
    "        self.tower_t1 = tfk.Sequential(\n",
    "            [\n",
    "                tfkl.InputLayer([200]),\n",
    "                tfkl.Dense(100,activation = self.activation, kernel_regularizer=regularizers.l2(reg_l2)),\n",
    "                tfkl.Dense(100,activation = self.activation, kernel_regularizer=regularizers.l2(reg_l2)),\n",
    "                tfkl.Dense(1,activation = None, kernel_regularizer=regularizers.l2(reg_l2)),\n",
    "            ])\n",
    "    def call(self, data, training=False, serving=False):\n",
    "        # Dataset_inp\n",
    "        hidden_bottom = self.share_bottom(data)\n",
    "        y_t0 = self.tower_t0(hidden_bottom)\n",
    "        y_t1 = self.tower_t1(hidden_bottom)\n",
    "    \n",
    "        output = tf.concat([y_t0,y_t1,hidden_bottom],-1)\n",
    "        return output\n",
    "\n",
    "#make model\n",
    "tarnet_model=tarnet(data['x'].shape[1],.01)\n",
    "# fake_inputs = tfk.Input(25,dtype = tf.float32)\n",
    "# tarnet_model(fake_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/10 [==>...........................] - ETA: 6s - loss: 74.8962 - regression_loss: 70.1760WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_train_batch_end` time: 0.0032s). Check your callbacks.\n",
      " — ate_err: 2.5251  — cate_err: 2.9561 — cate_nn_err: 3.4446 \n",
      "10/10 [==============================] - 1s 64ms/step - loss: 59.8954 - regression_loss: 52.5947 - val_loss: 62.0111 - val_regression_loss: 50.8706 - lr: 1.0000e-05\n",
      "Epoch 2/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 47.9761 - regression_loss: 43.2559 — ate_err: 1.4827  — cate_err: 2.0393 — cate_nn_err: 2.8053 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 45.8241 - regression_loss: 40.0535 - val_loss: 50.6571 - val_regression_loss: 41.0114 - lr: 1.0000e-05\n",
      "Epoch 3/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 45.2539 - regression_loss: 40.5335 — ate_err: 0.7540  — cate_err: 1.4696 — cate_nn_err: 2.5417 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 37.0066 - regression_loss: 31.1129 - val_loss: 42.6265 - val_regression_loss: 33.9059 - lr: 1.0000e-05\n",
      "Epoch 4/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 31.3281 - regression_loss: 26.6073 — ate_err: 0.3196  — cate_err: 1.1983 — cate_nn_err: 2.5182 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 31.1637 - regression_loss: 25.4754 - val_loss: 37.8268 - val_regression_loss: 29.6035 - lr: 1.0000e-05\n",
      "Epoch 5/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 21.9090 - regression_loss: 17.1877 — ate_err: 0.1688  — cate_err: 1.0789 — cate_nn_err: 2.5570 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 28.3932 - regression_loss: 22.4823 - val_loss: 34.2628 - val_regression_loss: 26.4063 - lr: 1.0000e-05\n",
      "Epoch 6/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 20.8503 - regression_loss: 16.1286 — ate_err: 0.1058  — cate_err: 1.0103 — cate_nn_err: 2.6026 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 26.1804 - regression_loss: 20.3474 - val_loss: 31.6867 - val_regression_loss: 24.1458 - lr: 1.0000e-05\n",
      "Epoch 7/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 26.0872 - regression_loss: 21.3651 — ate_err: 0.1357  — cate_err: 0.9770 — cate_nn_err: 2.6597 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 23.7645 - regression_loss: 18.8759 - val_loss: 29.8051 - val_regression_loss: 22.5052 - lr: 1.0000e-05\n",
      "Epoch 8/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 27.8201 - regression_loss: 23.0978 — ate_err: 0.1286  — cate_err: 0.9539 — cate_nn_err: 2.7034 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 23.6011 - regression_loss: 17.9373 - val_loss: 28.5454 - val_regression_loss: 21.4317 - lr: 1.0000e-05\n",
      "Epoch 9/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 30.0436 - regression_loss: 25.3211 — ate_err: 0.1850  — cate_err: 0.9457 — cate_nn_err: 2.7343 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 22.8386 - regression_loss: 17.2451 - val_loss: 27.5611 - val_regression_loss: 20.5527 - lr: 1.0000e-05\n",
      "Epoch 10/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 22.2119 - regression_loss: 17.4893 — ate_err: 0.1867  — cate_err: 0.9298 — cate_nn_err: 2.7497 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 22.2557 - regression_loss: 16.7501 - val_loss: 26.8489 - val_regression_loss: 19.9282 - lr: 1.0000e-05\n",
      "Epoch 11/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 26.9173 - regression_loss: 22.1946 — ate_err: 0.1335  — cate_err: 0.9057 — cate_nn_err: 2.7623 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 21.7972 - regression_loss: 16.3048 - val_loss: 26.2912 - val_regression_loss: 19.4578 - lr: 1.0000e-05\n",
      "Epoch 12/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.2801 - regression_loss: 12.5575 — ate_err: 0.1897  — cate_err: 0.8978 — cate_nn_err: 2.7733 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 21.3999 - regression_loss: 15.8532 - val_loss: 25.7775 - val_regression_loss: 18.9656 - lr: 1.0000e-05\n",
      "Epoch 13/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 25.3351 - regression_loss: 20.6125 — ate_err: 0.1596  — cate_err: 0.8730 — cate_nn_err: 2.7689 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 20.8849 - regression_loss: 15.5231 - val_loss: 25.3745 - val_regression_loss: 18.6114 - lr: 1.0000e-05\n",
      "Epoch 14/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.8917 - regression_loss: 13.1692 — ate_err: 0.1504  — cate_err: 0.8554 — cate_nn_err: 2.7715 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 20.7954 - regression_loss: 15.1921 - val_loss: 24.9567 - val_regression_loss: 18.2393 - lr: 1.0000e-05\n",
      "Epoch 15/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 19.8961 - regression_loss: 15.1737 — ate_err: 0.1472  — cate_err: 0.8406 — cate_nn_err: 2.7667 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 20.1335 - regression_loss: 14.8785 - val_loss: 24.6211 - val_regression_loss: 17.9350 - lr: 1.0000e-05\n",
      "Epoch 16/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 24.0174 - regression_loss: 19.2951 — ate_err: 0.1404  — cate_err: 0.8249 — cate_nn_err: 2.7591 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 20.0080 - regression_loss: 14.6375 - val_loss: 24.2852 - val_regression_loss: 17.6517 - lr: 1.0000e-05\n",
      "Epoch 17/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 20.8896 - regression_loss: 16.1673 — ate_err: 0.1613  — cate_err: 0.8174 — cate_nn_err: 2.7637 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 19.6444 - regression_loss: 14.3726 - val_loss: 23.9870 - val_regression_loss: 17.3667 - lr: 1.0000e-05\n",
      "Epoch 18/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 18.9191 - regression_loss: 14.1968 — ate_err: 0.1555  — cate_err: 0.8061 — cate_nn_err: 2.7710 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 19.4095 - regression_loss: 14.1275 - val_loss: 23.6796 - val_regression_loss: 17.0963 - lr: 1.0000e-05\n",
      "Epoch 19/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.3917 - regression_loss: 10.6695 — ate_err: 0.1812  — cate_err: 0.7968 — cate_nn_err: 2.7631 \n",
      "10/10 [==============================] - 0s 22ms/step - loss: 19.3900 - regression_loss: 13.8868 - val_loss: 23.4452 - val_regression_loss: 16.8724 - lr: 1.0000e-05\n",
      "Epoch 20/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.0836 - regression_loss: 10.3615 — ate_err: 0.1490  — cate_err: 0.7774 — cate_nn_err: 2.7635 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 19.2033 - regression_loss: 13.7045 - val_loss: 23.2148 - val_regression_loss: 16.6671 - lr: 1.0000e-05\n",
      "Epoch 21/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.9717 - regression_loss: 12.2496 — ate_err: 0.1290  — cate_err: 0.7670 — cate_nn_err: 2.7456 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 18.8419 - regression_loss: 13.5444 - val_loss: 23.0189 - val_regression_loss: 16.4945 - lr: 1.0000e-05\n",
      "Epoch 22/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 19.2605 - regression_loss: 14.5385 — ate_err: 0.1844  — cate_err: 0.7704 — cate_nn_err: 2.7471 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 18.6164 - regression_loss: 13.4044 - val_loss: 22.8364 - val_regression_loss: 16.3195 - lr: 1.0000e-05\n",
      "Epoch 23/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 20.3347 - regression_loss: 15.6128 — ate_err: 0.1883  — cate_err: 0.7637 — cate_nn_err: 2.7521 \n",
      "10/10 [==============================] - 0s 21ms/step - loss: 18.5930 - regression_loss: 13.2151 - val_loss: 22.6678 - val_regression_loss: 16.1594 - lr: 1.0000e-05\n",
      "Epoch 24/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.7029 - regression_loss: 12.9811 — ate_err: 0.1425  — cate_err: 0.7458 — cate_nn_err: 2.7494 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 18.3404 - regression_loss: 13.0718 - val_loss: 22.5577 - val_regression_loss: 16.0704 - lr: 1.0000e-05\n",
      "Epoch 25/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/10 [==>...........................] - ETA: 0s - loss: 18.2901 - regression_loss: 13.5684 — ate_err: 0.1340  — cate_err: 0.7391 — cate_nn_err: 2.7509 \n",
      "10/10 [==============================] - 0s 22ms/step - loss: 18.0441 - regression_loss: 12.9617 - val_loss: 22.3967 - val_regression_loss: 15.9422 - lr: 1.0000e-05\n",
      "Epoch 26/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 19.4008 - regression_loss: 14.6791 — ate_err: 0.1737  — cate_err: 0.7419 — cate_nn_err: 2.7557 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 18.2405 - regression_loss: 12.8378 - val_loss: 22.2512 - val_regression_loss: 15.7892 - lr: 1.0000e-05\n",
      "Epoch 27/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.0616 - regression_loss: 11.3400 — ate_err: 0.1504  — cate_err: 0.7309 — cate_nn_err: 2.7557 \n",
      "10/10 [==============================] - 0s 22ms/step - loss: 17.9881 - regression_loss: 12.7241 - val_loss: 22.1168 - val_regression_loss: 15.6691 - lr: 1.0000e-05\n",
      "Epoch 28/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.7370 - regression_loss: 10.0154 — ate_err: 0.1646  — cate_err: 0.7298 — cate_nn_err: 2.7596 \n",
      "10/10 [==============================] - 0s 22ms/step - loss: 18.0991 - regression_loss: 12.6860 - val_loss: 22.0464 - val_regression_loss: 15.6087 - lr: 1.0000e-05\n",
      "Epoch 29/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 22.3815 - regression_loss: 17.6601 — ate_err: 0.1674  — cate_err: 0.7287 — cate_nn_err: 2.7569 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 17.9725 - regression_loss: 12.5740 - val_loss: 21.9729 - val_regression_loss: 15.5479 - lr: 1.0000e-05\n",
      "Epoch 30/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 18.3723 - regression_loss: 13.6510 — ate_err: 0.1509  — cate_err: 0.7217 — cate_nn_err: 2.7613 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 17.9367 - regression_loss: 12.4892 - val_loss: 21.8565 - val_regression_loss: 15.4363 - lr: 1.0000e-05\n",
      "Epoch 31/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 22.0838 - regression_loss: 17.3625 — ate_err: 0.1170  — cate_err: 0.7119 — cate_nn_err: 2.7617 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 17.7706 - regression_loss: 12.4068 - val_loss: 21.7601 - val_regression_loss: 15.3479 - lr: 1.0000e-05\n",
      "Epoch 32/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.4652 - regression_loss: 12.7440 — ate_err: 0.1151  — cate_err: 0.7083 — cate_nn_err: 2.7578 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 17.6815 - regression_loss: 12.3365 - val_loss: 21.7745 - val_regression_loss: 15.3652 - lr: 1.0000e-05\n",
      "Epoch 33/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 20.9713 - regression_loss: 16.2503 — ate_err: 0.1235  — cate_err: 0.7055 — cate_nn_err: 2.7611 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 17.6959 - regression_loss: 12.3176 - val_loss: 21.6603 - val_regression_loss: 15.2589 - lr: 1.0000e-05\n",
      "Epoch 34/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 19.7736 - regression_loss: 15.0527 — ate_err: 0.1274  — cate_err: 0.7057 — cate_nn_err: 2.7642 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 17.4501 - regression_loss: 12.1956 - val_loss: 21.6307 - val_regression_loss: 15.2290 - lr: 1.0000e-05\n",
      "Epoch 35/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 18.2822 - regression_loss: 13.5614 — ate_err: 0.1276  — cate_err: 0.7029 — cate_nn_err: 2.7671 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 17.4334 - regression_loss: 12.1441 - val_loss: 21.5434 - val_regression_loss: 15.1551 - lr: 1.0000e-05\n",
      "Epoch 36/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 18.3320 - regression_loss: 13.6113 — ate_err: 0.1563  — cate_err: 0.7030 — cate_nn_err: 2.7633 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 17.3821 - regression_loss: 12.1511 - val_loss: 21.4686 - val_regression_loss: 15.0731 - lr: 1.0000e-05\n",
      "Epoch 37/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 18.8347 - regression_loss: 14.1141 — ate_err: 0.1696  — cate_err: 0.7056 — cate_nn_err: 2.7626 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 17.3335 - regression_loss: 12.0506 - val_loss: 21.4550 - val_regression_loss: 15.0628 - lr: 1.0000e-05\n",
      "Epoch 38/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.8956 - regression_loss: 13.1752 — ate_err: 0.1519  — cate_err: 0.7027 — cate_nn_err: 2.7684 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 17.3291 - regression_loss: 12.0220 - val_loss: 21.4041 - val_regression_loss: 15.0306 - lr: 1.0000e-05\n",
      "Epoch 39/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 18.3415 - regression_loss: 13.6212 — ate_err: 0.1448  — cate_err: 0.6973 — cate_nn_err: 2.7726 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 17.3020 - regression_loss: 11.9695 - val_loss: 21.3385 - val_regression_loss: 14.9647 - lr: 1.0000e-05\n",
      "Epoch 40/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.7256 - regression_loss: 13.0054 — ate_err: 0.1417  — cate_err: 0.6987 — cate_nn_err: 2.7777 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 17.3075 - regression_loss: 11.9788 - val_loss: 21.3116 - val_regression_loss: 14.9393 - lr: 1.0000e-05\n",
      "Epoch 41/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.1162 - regression_loss: 11.3961 — ate_err: 0.1545  — cate_err: 0.6963 — cate_nn_err: 2.7734 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 17.3272 - regression_loss: 11.9198 - val_loss: 21.2559 - val_regression_loss: 14.8856 - lr: 1.0000e-05\n",
      "Epoch 42/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.5943 - regression_loss: 11.8744 — ate_err: 0.1358  — cate_err: 0.6863 — cate_nn_err: 2.7675 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 17.1310 - regression_loss: 11.8832 - val_loss: 21.2390 - val_regression_loss: 14.8778 - lr: 1.0000e-05\n",
      "Epoch 43/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 18.7639 - regression_loss: 14.0441 — ate_err: 0.1367  — cate_err: 0.6886 — cate_nn_err: 2.7738 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 16.9843 - regression_loss: 11.8395 - val_loss: 21.1727 - val_regression_loss: 14.8124 - lr: 1.0000e-05\n",
      "Epoch 44/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 20.8271 - regression_loss: 16.1075 — ate_err: 0.1173  — cate_err: 0.6841 — cate_nn_err: 2.7748 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 17.0431 - regression_loss: 11.8121 - val_loss: 21.1209 - val_regression_loss: 14.7675 - lr: 1.0000e-05\n",
      "Epoch 45/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 18.6019 - regression_loss: 13.8824 — ate_err: 0.1454  — cate_err: 0.6891 — cate_nn_err: 2.7839 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 16.8980 - regression_loss: 11.7420 - val_loss: 21.0924 - val_regression_loss: 14.7479 - lr: 1.0000e-05\n",
      "Epoch 46/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 19.2526 - regression_loss: 14.5332 — ate_err: 0.1351  — cate_err: 0.6810 — cate_nn_err: 2.7711 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 17.1130 - regression_loss: 11.7808 - val_loss: 21.0811 - val_regression_loss: 14.7472 - lr: 1.0000e-05\n",
      "Epoch 47/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 19.5280 - regression_loss: 14.8088 — ate_err: 0.1128  — cate_err: 0.6752 — cate_nn_err: 2.7735 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 16.8936 - regression_loss: 11.6817 - val_loss: 21.0533 - val_regression_loss: 14.7175 - lr: 1.0000e-05\n",
      "Epoch 48/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 13.8643 - regression_loss: 9.1452 — ate_err: 0.1589  — cate_err: 0.6879 — cate_nn_err: 2.7853 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 16.9160 - regression_loss: 11.6720 - val_loss: 20.9855 - val_regression_loss: 14.6488 - lr: 1.0000e-05\n",
      "Epoch 49/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.4986 - regression_loss: 10.7797 — ate_err: 0.1280  — cate_err: 0.6776 — cate_nn_err: 2.7830 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 16.9850 - regression_loss: 11.6196 - val_loss: 20.9073 - val_regression_loss: 14.5851 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 12.6353 - regression_loss: 7.9165 — ate_err: 0.1355  — cate_err: 0.6755 — cate_nn_err: 2.7804 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 17.0384 - regression_loss: 11.6304 - val_loss: 20.8923 - val_regression_loss: 14.5701 - lr: 1.0000e-05\n",
      "Epoch 51/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.9760 - regression_loss: 10.2573 — ate_err: 0.1214  — cate_err: 0.6719 — cate_nn_err: 2.7783 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 17.0135 - regression_loss: 11.6222 - val_loss: 20.8806 - val_regression_loss: 14.5620 - lr: 1.0000e-05\n",
      "Epoch 52/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 19.0968 - regression_loss: 14.3783 — ate_err: 0.1260  — cate_err: 0.6735 — cate_nn_err: 2.7834 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 16.7202 - regression_loss: 11.5406 - val_loss: 20.8059 - val_regression_loss: 14.4976 - lr: 1.0000e-05\n",
      "Epoch 53/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.5012 - regression_loss: 10.7828 — ate_err: 0.1263  — cate_err: 0.6727 — cate_nn_err: 2.7845 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 16.8030 - regression_loss: 11.5558 - val_loss: 20.7463 - val_regression_loss: 14.4511 - lr: 1.0000e-05\n",
      "Epoch 54/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.3656 - regression_loss: 10.6474 — ate_err: 0.0924  — cate_err: 0.6641 — cate_nn_err: 2.7841 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 16.7883 - regression_loss: 11.5310 - val_loss: 20.7187 - val_regression_loss: 14.4249 - lr: 1.0000e-05\n",
      "Epoch 55/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.9631 - regression_loss: 10.2450 — ate_err: 0.1576  — cate_err: 0.6739 — cate_nn_err: 2.7858 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 16.7785 - regression_loss: 11.4362 - val_loss: 20.6924 - val_regression_loss: 14.3944 - lr: 1.0000e-05\n",
      "Epoch 56/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.5462 - regression_loss: 12.8283 — ate_err: 0.1461  — cate_err: 0.6732 — cate_nn_err: 2.7908 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 16.6942 - regression_loss: 11.4742 - val_loss: 20.6700 - val_regression_loss: 14.3833 - lr: 1.0000e-05\n",
      "Epoch 57/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.5559 - regression_loss: 10.8382 — ate_err: 0.1044  — cate_err: 0.6600 — cate_nn_err: 2.7820 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 16.7576 - regression_loss: 11.4488 - val_loss: 20.6746 - val_regression_loss: 14.3892 - lr: 1.0000e-05\n",
      "Epoch 58/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 13.5828 - regression_loss: 8.8653 — ate_err: 0.1503  — cate_err: 0.6672 — cate_nn_err: 2.7849 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 16.6275 - regression_loss: 11.4486 - val_loss: 20.6081 - val_regression_loss: 14.3265 - lr: 1.0000e-05\n",
      "Epoch 59/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.2563 - regression_loss: 9.5388 — ate_err: 0.1514  — cate_err: 0.6690 — cate_nn_err: 2.7944 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 16.5571 - regression_loss: 11.4217 - val_loss: 20.5443 - val_regression_loss: 14.2657 - lr: 1.0000e-05\n",
      "Epoch 60/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.9211 - regression_loss: 11.2037 — ate_err: 0.1408  — cate_err: 0.6629 — cate_nn_err: 2.7901 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 16.7238 - regression_loss: 11.3348 - val_loss: 20.4976 - val_regression_loss: 14.2280 - lr: 1.0000e-05\n",
      "Epoch 61/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 18.3009 - regression_loss: 13.5837 — ate_err: 0.1053  — cate_err: 0.6536 — cate_nn_err: 2.7870 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 16.5013 - regression_loss: 11.3169 - val_loss: 20.4791 - val_regression_loss: 14.2126 - lr: 1.0000e-05\n",
      "Epoch 62/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.8014 - regression_loss: 10.0844 — ate_err: 0.1681  — cate_err: 0.6645 — cate_nn_err: 2.7854 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 16.5349 - regression_loss: 11.3132 - val_loss: 20.4561 - val_regression_loss: 14.1915 - lr: 1.0000e-05\n",
      "Epoch 63/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 13.7192 - regression_loss: 9.0024 — ate_err: 0.1373  — cate_err: 0.6562 — cate_nn_err: 2.7835 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 16.6863 - regression_loss: 11.3536 - val_loss: 20.4056 - val_regression_loss: 14.1544 - lr: 1.0000e-05\n",
      "Epoch 64/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.7683 - regression_loss: 13.0516 — ate_err: 0.1858  — cate_err: 0.6668 — cate_nn_err: 2.7885 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 16.4433 - regression_loss: 11.2589 - val_loss: 20.3701 - val_regression_loss: 14.1174 - lr: 1.0000e-05\n",
      "Epoch 65/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 20.0693 - regression_loss: 15.3528 — ate_err: 0.1181  — cate_err: 0.6495 — cate_nn_err: 2.7815 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 16.4357 - regression_loss: 11.2717 - val_loss: 20.3334 - val_regression_loss: 14.0937 - lr: 1.0000e-05\n",
      "Epoch 66/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 20.0082 - regression_loss: 15.2918 — ate_err: 0.1773  — cate_err: 0.6663 — cate_nn_err: 2.7845 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 16.3702 - regression_loss: 11.2193 - val_loss: 20.2418 - val_regression_loss: 14.0078 - lr: 1.0000e-05\n",
      "Epoch 67/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.4669 - regression_loss: 10.7507 — ate_err: 0.1359  — cate_err: 0.6491 — cate_nn_err: 2.7822 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 16.5162 - regression_loss: 11.2149 - val_loss: 20.2722 - val_regression_loss: 14.0364 - lr: 1.0000e-05\n",
      "Epoch 68/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.8220 - regression_loss: 10.1060 — ate_err: 0.1239  — cate_err: 0.6460 — cate_nn_err: 2.7762 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 16.3373 - regression_loss: 11.2009 - val_loss: 20.2607 - val_regression_loss: 14.0280 - lr: 1.0000e-05\n",
      "Epoch 69/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.8245 - regression_loss: 11.1086 — ate_err: 0.1410  — cate_err: 0.6470 — cate_nn_err: 2.7740 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 16.3823 - regression_loss: 11.1698 - val_loss: 20.1849 - val_regression_loss: 13.9614 - lr: 1.0000e-05\n",
      "Epoch 70/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.7844 - regression_loss: 13.0687 — ate_err: 0.1260  — cate_err: 0.6421 — cate_nn_err: 2.7638 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 16.3794 - regression_loss: 11.1823 - val_loss: 20.1691 - val_regression_loss: 13.9483 - lr: 1.0000e-05\n",
      "Epoch 71/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.8765 - regression_loss: 11.1609 — ate_err: 0.1421  — cate_err: 0.6441 — cate_nn_err: 2.7650 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 16.3673 - regression_loss: 11.0967 - val_loss: 20.1042 - val_regression_loss: 13.8881 - lr: 1.0000e-05\n",
      "Epoch 72/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.5509 - regression_loss: 9.8354 — ate_err: 0.1431  — cate_err: 0.6416 — cate_nn_err: 2.7663 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 16.2456 - regression_loss: 11.0986 - val_loss: 20.0625 - val_regression_loss: 13.8470 - lr: 1.0000e-05\n",
      "Epoch 73/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.8756 - regression_loss: 13.1603 — ate_err: 0.1530  — cate_err: 0.6452 — cate_nn_err: 2.7695 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 16.2582 - regression_loss: 11.0649 - val_loss: 20.0082 - val_regression_loss: 13.7977 - lr: 1.0000e-05\n",
      "Epoch 74/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.8430 - regression_loss: 13.1278 — ate_err: 0.1459  — cate_err: 0.6429 — cate_nn_err: 2.7653 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 16.2841 - regression_loss: 11.0389 - val_loss: 20.0076 - val_regression_loss: 13.8076 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.8521 - regression_loss: 11.1371 — ate_err: 0.1610  — cate_err: 0.6438 — cate_nn_err: 2.7623 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 16.2489 - regression_loss: 11.0651 - val_loss: 19.9621 - val_regression_loss: 13.7638 - lr: 1.0000e-05\n",
      "Epoch 76/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 18.0281 - regression_loss: 13.3132 — ate_err: 0.1657  — cate_err: 0.6465 — cate_nn_err: 2.7700 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 16.1099 - regression_loss: 11.0038 - val_loss: 19.9507 - val_regression_loss: 13.7572 - lr: 1.0000e-05\n",
      "Epoch 77/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.5006 - regression_loss: 9.7859 — ate_err: 0.1439  — cate_err: 0.6369 — cate_nn_err: 2.7636 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 16.3529 - regression_loss: 11.0005 - val_loss: 19.9442 - val_regression_loss: 13.7468 - lr: 1.0000e-05\n",
      "Epoch 78/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.3213 - regression_loss: 11.6068 — ate_err: 0.1765  — cate_err: 0.6409 — cate_nn_err: 2.7616 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 16.0849 - regression_loss: 10.9569 - val_loss: 19.8976 - val_regression_loss: 13.7075 - lr: 1.0000e-05\n",
      "Epoch 79/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.5961 - regression_loss: 9.8817 — ate_err: 0.1227  — cate_err: 0.6284 — cate_nn_err: 2.7612 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 16.2524 - regression_loss: 10.9727 - val_loss: 19.8652 - val_regression_loss: 13.6863 - lr: 1.0000e-05\n",
      "Epoch 80/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.4156 - regression_loss: 11.7013 — ate_err: 0.1253  — cate_err: 0.6289 — cate_nn_err: 2.7639 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 16.0072 - regression_loss: 10.9175 - val_loss: 19.8200 - val_regression_loss: 13.6443 - lr: 1.0000e-05\n",
      "Epoch 81/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.4531 - regression_loss: 9.7389 — ate_err: 0.1650  — cate_err: 0.6367 — cate_nn_err: 2.7657 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 16.1350 - regression_loss: 10.8954 - val_loss: 19.7312 - val_regression_loss: 13.5632 - lr: 1.0000e-05\n",
      "Epoch 82/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.0351 - regression_loss: 12.3212 — ate_err: 0.1332  — cate_err: 0.6262 — cate_nn_err: 2.7645 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 15.8733 - regression_loss: 10.8905 - val_loss: 19.7412 - val_regression_loss: 13.5812 - lr: 1.0000e-05\n",
      "Epoch 83/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.5416 - regression_loss: 10.8278 — ate_err: 0.1556  — cate_err: 0.6324 — cate_nn_err: 2.7698 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 16.1298 - regression_loss: 10.8540 - val_loss: 19.6848 - val_regression_loss: 13.5268 - lr: 1.0000e-05\n",
      "Epoch 84/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 13.0160 - regression_loss: 8.3023 — ate_err: 0.1485  — cate_err: 0.6274 — cate_nn_err: 2.7630 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 16.0217 - regression_loss: 10.8497 - val_loss: 19.6795 - val_regression_loss: 13.5267 - lr: 1.0000e-05\n",
      "Epoch 85/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.8346 - regression_loss: 10.1211 — ate_err: 0.1204  — cate_err: 0.6202 — cate_nn_err: 2.7626 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 16.0987 - regression_loss: 10.8641 - val_loss: 19.6460 - val_regression_loss: 13.4973 - lr: 1.0000e-05\n",
      "Epoch 86/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 21.5374 - regression_loss: 16.8240 — ate_err: 0.1013  — cate_err: 0.6160 — cate_nn_err: 2.7608 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 16.1028 - regression_loss: 10.7995 - val_loss: 19.6153 - val_regression_loss: 13.4685 - lr: 1.0000e-05\n",
      "Epoch 87/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 18.1232 - regression_loss: 13.4100\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      " — ate_err: 0.1167  — cate_err: 0.6148 — cate_nn_err: 2.7654 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 16.0886 - regression_loss: 10.8345 - val_loss: 19.5828 - val_regression_loss: 13.4364 - lr: 1.0000e-05\n",
      "Epoch 88/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.4701 - regression_loss: 12.7570 — ate_err: 0.1571  — cate_err: 0.6250 — cate_nn_err: 2.7708 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 16.0591 - regression_loss: 10.7496 - val_loss: 19.5154 - val_regression_loss: 13.3761 - lr: 5.0000e-06\n",
      "Epoch 89/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 19.3614 - regression_loss: 14.6484 — ate_err: 0.1454  — cate_err: 0.6205 — cate_nn_err: 2.7729 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 15.9643 - regression_loss: 10.7373 - val_loss: 19.4971 - val_regression_loss: 13.3630 - lr: 5.0000e-06\n",
      "Epoch 90/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.1620 - regression_loss: 9.4491 — ate_err: 0.1441  — cate_err: 0.6195 — cate_nn_err: 2.7706 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 15.8422 - regression_loss: 10.7407 - val_loss: 19.4941 - val_regression_loss: 13.3605 - lr: 5.0000e-06\n",
      "Epoch 91/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.3197 - regression_loss: 11.6068 — ate_err: 0.1566  — cate_err: 0.6223 — cate_nn_err: 2.7673 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.9379 - regression_loss: 10.7234 - val_loss: 19.4727 - val_regression_loss: 13.3427 - lr: 5.0000e-06\n",
      "Epoch 92/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 13.7771 - regression_loss: 9.0644 — ate_err: 0.1369  — cate_err: 0.6176 — cate_nn_err: 2.7675 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 15.8275 - regression_loss: 10.7152 - val_loss: 19.4627 - val_regression_loss: 13.3347 - lr: 5.0000e-06\n",
      "Epoch 93/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 18.6437 - regression_loss: 13.9311 — ate_err: 0.1517  — cate_err: 0.6196 — cate_nn_err: 2.7672 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 15.8913 - regression_loss: 10.6876 - val_loss: 19.4609 - val_regression_loss: 13.3363 - lr: 5.0000e-06\n",
      "Epoch 94/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.6149 - regression_loss: 9.9023 — ate_err: 0.1394  — cate_err: 0.6149 — cate_nn_err: 2.7666 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 15.8689 - regression_loss: 10.6947 - val_loss: 19.4576 - val_regression_loss: 13.3314 - lr: 5.0000e-06\n",
      "Epoch 95/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.9336 - regression_loss: 13.2211 — ate_err: 0.1442  — cate_err: 0.6169 — cate_nn_err: 2.7669 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 15.9664 - regression_loss: 10.6766 - val_loss: 19.4456 - val_regression_loss: 13.3221 - lr: 5.0000e-06\n",
      "Epoch 96/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.0570 - regression_loss: 11.3445 — ate_err: 0.1570  — cate_err: 0.6188 — cate_nn_err: 2.7702 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.8058 - regression_loss: 10.6717 - val_loss: 19.3992 - val_regression_loss: 13.2777 - lr: 5.0000e-06\n",
      "Epoch 97/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.6367 - regression_loss: 11.9243 — ate_err: 0.1429  — cate_err: 0.6148 — cate_nn_err: 2.7670 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.9196 - regression_loss: 10.6822 - val_loss: 19.4080 - val_regression_loss: 13.2900 - lr: 5.0000e-06\n",
      "Epoch 98/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 13.4133 - regression_loss: 8.7010 — ate_err: 0.1504  — cate_err: 0.6177 — cate_nn_err: 2.7724 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.8436 - regression_loss: 10.6673 - val_loss: 19.3611 - val_regression_loss: 13.2473 - lr: 5.0000e-06\n",
      "Epoch 99/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.1618 - regression_loss: 11.4496 — ate_err: 0.1903  — cate_err: 0.6266 — cate_nn_err: 2.7702 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.9120 - regression_loss: 10.6854 - val_loss: 19.3539 - val_regression_loss: 13.2392 - lr: 5.0000e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 19.4226 - regression_loss: 14.7105 — ate_err: 0.1185  — cate_err: 0.6076 — cate_nn_err: 2.7631 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.9009 - regression_loss: 10.6471 - val_loss: 19.3754 - val_regression_loss: 13.2645 - lr: 5.0000e-06\n",
      "Epoch 101/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.1236 - regression_loss: 12.4115\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      " — ate_err: 0.1505  — cate_err: 0.6140 — cate_nn_err: 2.7654 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.8243 - regression_loss: 10.6514 - val_loss: 19.3325 - val_regression_loss: 13.2261 - lr: 5.0000e-06\n",
      "Epoch 102/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 12.6427 - regression_loss: 7.9307 — ate_err: 0.1569  — cate_err: 0.6151 — cate_nn_err: 2.7665 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.8394 - regression_loss: 10.6016 - val_loss: 19.3227 - val_regression_loss: 13.2157 - lr: 2.5000e-06\n",
      "Epoch 103/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.0944 - regression_loss: 11.3825 — ate_err: 0.1512  — cate_err: 0.6131 — cate_nn_err: 2.7662 \n",
      "10/10 [==============================] - 0s 27ms/step - loss: 15.7857 - regression_loss: 10.6024 - val_loss: 19.3127 - val_regression_loss: 13.2059 - lr: 2.5000e-06\n",
      "Epoch 104/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 19.2792 - regression_loss: 14.5673 — ate_err: 0.1517  — cate_err: 0.6130 — cate_nn_err: 2.7673 \n",
      "10/10 [==============================] - 0s 27ms/step - loss: 15.8319 - regression_loss: 10.5930 - val_loss: 19.2955 - val_regression_loss: 13.1907 - lr: 2.5000e-06\n",
      "Epoch 105/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.0169 - regression_loss: 11.3050 — ate_err: 0.1471  — cate_err: 0.6110 — cate_nn_err: 2.7683 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.8240 - regression_loss: 10.5880 - val_loss: 19.2912 - val_regression_loss: 13.1864 - lr: 2.5000e-06\n",
      "Epoch 106/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 19.0658 - regression_loss: 14.3540 — ate_err: 0.1397  — cate_err: 0.6087 — cate_nn_err: 2.7695 \n",
      "10/10 [==============================] - 0s 28ms/step - loss: 15.5555 - regression_loss: 10.5877 - val_loss: 19.2859 - val_regression_loss: 13.1817 - lr: 2.5000e-06\n",
      "Epoch 107/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.1350 - regression_loss: 11.4232 — ate_err: 0.1392  — cate_err: 0.6079 — cate_nn_err: 2.7693 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.7732 - regression_loss: 10.5857 - val_loss: 19.2744 - val_regression_loss: 13.1716 - lr: 2.5000e-06\n",
      "Epoch 108/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.5905 - regression_loss: 10.8787 — ate_err: 0.1624  — cate_err: 0.6137 — cate_nn_err: 2.7706 \n",
      "10/10 [==============================] - 0s 27ms/step - loss: 15.7673 - regression_loss: 10.6066 - val_loss: 19.2624 - val_regression_loss: 13.1602 - lr: 2.5000e-06\n",
      "Epoch 109/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.1376 - regression_loss: 10.4258 — ate_err: 0.1425  — cate_err: 0.6087 — cate_nn_err: 2.7689 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.8506 - regression_loss: 10.5789 - val_loss: 19.2575 - val_regression_loss: 13.1589 - lr: 2.5000e-06\n",
      "Epoch 110/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.0105 - regression_loss: 12.2988 — ate_err: 0.1423  — cate_err: 0.6080 — cate_nn_err: 2.7690 \n",
      "10/10 [==============================] - 0s 28ms/step - loss: 15.8569 - regression_loss: 10.5695 - val_loss: 19.2463 - val_regression_loss: 13.1475 - lr: 2.5000e-06\n",
      "Epoch 111/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.4518 - regression_loss: 10.7402 — ate_err: 0.1433  — cate_err: 0.6082 — cate_nn_err: 2.7698 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 15.5462 - regression_loss: 10.5720 - val_loss: 19.2347 - val_regression_loss: 13.1377 - lr: 2.5000e-06\n",
      "Epoch 112/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.5248 - regression_loss: 11.8132 — ate_err: 0.1448  — cate_err: 0.6083 — cate_nn_err: 2.7681 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.6040 - regression_loss: 10.5703 - val_loss: 19.2449 - val_regression_loss: 13.1483 - lr: 2.5000e-06\n",
      "Epoch 113/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.2451 - regression_loss: 10.5335 — ate_err: 0.1406  — cate_err: 0.6074 — cate_nn_err: 2.7681 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.6128 - regression_loss: 10.5581 - val_loss: 19.2401 - val_regression_loss: 13.1440 - lr: 2.5000e-06\n",
      "Epoch 114/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.8675 - regression_loss: 10.1560 — ate_err: 0.1458  — cate_err: 0.6082 — cate_nn_err: 2.7699 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.7570 - regression_loss: 10.5533 - val_loss: 19.2180 - val_regression_loss: 13.1233 - lr: 2.5000e-06\n",
      "Epoch 115/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 13.3652 - regression_loss: 8.6537 — ate_err: 0.1515  — cate_err: 0.6097 — cate_nn_err: 2.7711 \n",
      "10/10 [==============================] - 0s 28ms/step - loss: 15.5934 - regression_loss: 10.5555 - val_loss: 19.2127 - val_regression_loss: 13.1181 - lr: 2.5000e-06\n",
      "Epoch 116/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.0867 - regression_loss: 9.3752\n",
      "Epoch 00116: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      " — ate_err: 0.1491  — cate_err: 0.6084 — cate_nn_err: 2.7693 \n",
      "10/10 [==============================] - 0s 27ms/step - loss: 15.6274 - regression_loss: 10.5450 - val_loss: 19.2067 - val_regression_loss: 13.1141 - lr: 2.5000e-06\n",
      "Epoch 117/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.9995 - regression_loss: 12.2880 — ate_err: 0.1419  — cate_err: 0.6061 — cate_nn_err: 2.7690 \n",
      "10/10 [==============================] - 0s 28ms/step - loss: 15.7708 - regression_loss: 10.5393 - val_loss: 19.1988 - val_regression_loss: 13.1076 - lr: 1.2500e-06\n",
      "Epoch 118/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.1701 - regression_loss: 12.4587 — ate_err: 0.1394  — cate_err: 0.6052 — cate_nn_err: 2.7712 \n",
      "10/10 [==============================] - 0s 29ms/step - loss: 15.8616 - regression_loss: 10.5320 - val_loss: 19.1985 - val_regression_loss: 13.1068 - lr: 1.2500e-06\n",
      "Epoch 119/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.0255 - regression_loss: 12.3141 — ate_err: 0.1451  — cate_err: 0.6064 — cate_nn_err: 2.7708 \n",
      "10/10 [==============================] - 0s 31ms/step - loss: 15.6405 - regression_loss: 10.5325 - val_loss: 19.1910 - val_regression_loss: 13.0999 - lr: 1.2500e-06\n",
      "Epoch 120/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.0408 - regression_loss: 10.3294 — ate_err: 0.1467  — cate_err: 0.6068 — cate_nn_err: 2.7711 \n",
      "10/10 [==============================] - 0s 27ms/step - loss: 15.7397 - regression_loss: 10.5282 - val_loss: 19.1866 - val_regression_loss: 13.0957 - lr: 1.2500e-06\n",
      "Epoch 121/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 18.9443 - regression_loss: 14.2330\n",
      "Epoch 00121: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      " — ate_err: 0.1459  — cate_err: 0.6066 — cate_nn_err: 2.7712 \n",
      "10/10 [==============================] - 0s 27ms/step - loss: 15.7958 - regression_loss: 10.5252 - val_loss: 19.1861 - val_regression_loss: 13.0951 - lr: 1.2500e-06\n",
      "Epoch 122/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 13.7681 - regression_loss: 9.0567 — ate_err: 0.1409  — cate_err: 0.6053 — cate_nn_err: 2.7707 \n",
      "10/10 [==============================] - 0s 28ms/step - loss: 15.7633 - regression_loss: 10.5220 - val_loss: 19.1862 - val_regression_loss: 13.0959 - lr: 6.2500e-07\n",
      "Epoch 123/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.6271 - regression_loss: 12.9158 — ate_err: 0.1412  — cate_err: 0.6054 — cate_nn_err: 2.7710 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.7038 - regression_loss: 10.5189 - val_loss: 19.1816 - val_regression_loss: 13.0921 - lr: 6.2500e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.1126 - regression_loss: 11.4013 — ate_err: 0.1412  — cate_err: 0.6054 — cate_nn_err: 2.7724 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.7593 - regression_loss: 10.5176 - val_loss: 19.1802 - val_regression_loss: 13.0910 - lr: 6.2500e-07\n",
      "Epoch 125/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.6339 - regression_loss: 10.9226 — ate_err: 0.1427  — cate_err: 0.6056 — cate_nn_err: 2.7724 \n",
      "10/10 [==============================] - 0s 27ms/step - loss: 15.8041 - regression_loss: 10.5167 - val_loss: 19.1775 - val_regression_loss: 13.0885 - lr: 6.2500e-07\n",
      "Epoch 126/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.3555 - regression_loss: 11.6442\n",
      "Epoch 00126: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-07.\n",
      " — ate_err: 0.1414  — cate_err: 0.6055 — cate_nn_err: 2.7727 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.7562 - regression_loss: 10.5169 - val_loss: 19.1764 - val_regression_loss: 13.0876 - lr: 6.2500e-07\n",
      "Epoch 127/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.9389 - regression_loss: 12.2276 — ate_err: 0.1426  — cate_err: 0.6056 — cate_nn_err: 2.7729 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.7293 - regression_loss: 10.5142 - val_loss: 19.1730 - val_regression_loss: 13.0845 - lr: 3.1250e-07\n",
      "Epoch 128/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.2655 - regression_loss: 10.5543 — ate_err: 0.1448  — cate_err: 0.6062 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.7512 - regression_loss: 10.5137 - val_loss: 19.1698 - val_regression_loss: 13.0818 - lr: 3.1250e-07\n",
      "Epoch 129/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.1583 - regression_loss: 9.4471 — ate_err: 0.1430  — cate_err: 0.6055 — cate_nn_err: 2.7728 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.6495 - regression_loss: 10.5140 - val_loss: 19.1702 - val_regression_loss: 13.0821 - lr: 3.1250e-07\n",
      "Epoch 130/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.8271 - regression_loss: 12.1158 — ate_err: 0.1457  — cate_err: 0.6062 — cate_nn_err: 2.7730 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.7911 - regression_loss: 10.5119 - val_loss: 19.1687 - val_regression_loss: 13.0808 - lr: 3.1250e-07\n",
      "Epoch 131/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.6641 - regression_loss: 9.9529\n",
      "Epoch 00131: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      " — ate_err: 0.1466  — cate_err: 0.6064 — cate_nn_err: 2.7731 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.6611 - regression_loss: 10.5119 - val_loss: 19.1666 - val_regression_loss: 13.0787 - lr: 3.1250e-07\n",
      "Epoch 132/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.1939 - regression_loss: 11.4826 — ate_err: 0.1465  — cate_err: 0.6064 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.7557 - regression_loss: 10.5104 - val_loss: 19.1661 - val_regression_loss: 13.0784 - lr: 1.5625e-07\n",
      "Epoch 133/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.3255 - regression_loss: 10.6142 — ate_err: 0.1466  — cate_err: 0.6064 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.6720 - regression_loss: 10.5102 - val_loss: 19.1658 - val_regression_loss: 13.0782 - lr: 1.5625e-07\n",
      "Epoch 134/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.0937 - regression_loss: 11.3824 — ate_err: 0.1466  — cate_err: 0.6063 — cate_nn_err: 2.7731 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.6157 - regression_loss: 10.5100 - val_loss: 19.1661 - val_regression_loss: 13.0784 - lr: 1.5625e-07\n",
      "Epoch 135/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 13.7703 - regression_loss: 9.0591 — ate_err: 0.1453  — cate_err: 0.6060 — cate_nn_err: 2.7729 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.7425 - regression_loss: 10.5098 - val_loss: 19.1662 - val_regression_loss: 13.0787 - lr: 1.5625e-07\n",
      "Epoch 136/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.7789 - regression_loss: 10.0677\n",
      "Epoch 00136: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-08.\n",
      " — ate_err: 0.1455  — cate_err: 0.6061 — cate_nn_err: 2.7730 \n",
      "10/10 [==============================] - 0s 27ms/step - loss: 15.6739 - regression_loss: 10.5089 - val_loss: 19.1656 - val_regression_loss: 13.0782 - lr: 1.5625e-07\n",
      "Epoch 137/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.0916 - regression_loss: 12.3804 — ate_err: 0.1458  — cate_err: 0.6061 — cate_nn_err: 2.7730 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.7340 - regression_loss: 10.5087 - val_loss: 19.1649 - val_regression_loss: 13.0775 - lr: 7.8125e-08\n",
      "Epoch 138/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.7729 - regression_loss: 11.0617 — ate_err: 0.1457  — cate_err: 0.6061 — cate_nn_err: 2.7731 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.6810 - regression_loss: 10.5086 - val_loss: 19.1641 - val_regression_loss: 13.0768 - lr: 7.8125e-08\n",
      "Epoch 139/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.3677 - regression_loss: 11.6565 — ate_err: 0.1453  — cate_err: 0.6060 — cate_nn_err: 2.7731 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.7560 - regression_loss: 10.5084 - val_loss: 19.1643 - val_regression_loss: 13.0770 - lr: 7.8125e-08\n",
      "Epoch 140/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.2623 - regression_loss: 9.5511 — ate_err: 0.1455  — cate_err: 0.6060 — cate_nn_err: 2.7731 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.5360 - regression_loss: 10.5081 - val_loss: 19.1641 - val_regression_loss: 13.0767 - lr: 7.8125e-08\n",
      "Epoch 141/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 12.4237 - regression_loss: 7.7125 — ate_err: 0.1453  — cate_err: 0.6060 — cate_nn_err: 2.7731 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.6506 - regression_loss: 10.5081 - val_loss: 19.1640 - val_regression_loss: 13.0767 - lr: 7.8125e-08\n",
      "Epoch 142/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 19.9235 - regression_loss: 15.2122 — ate_err: 0.1453  — cate_err: 0.6059 — cate_nn_err: 2.7731 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.5710 - regression_loss: 10.5078 - val_loss: 19.1636 - val_regression_loss: 13.0764 - lr: 7.8125e-08\n",
      "Epoch 143/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.5690 - regression_loss: 10.8577 — ate_err: 0.1452  — cate_err: 0.6059 — cate_nn_err: 2.7731 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.8700 - regression_loss: 10.5082 - val_loss: 19.1631 - val_regression_loss: 13.0759 - lr: 7.8125e-08\n",
      "Epoch 144/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.5271 - regression_loss: 11.8159 — ate_err: 0.1451  — cate_err: 0.6059 — cate_nn_err: 2.7731 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.5258 - regression_loss: 10.5075 - val_loss: 19.1630 - val_regression_loss: 13.0759 - lr: 7.8125e-08\n",
      "Epoch 145/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 12.3237 - regression_loss: 7.6124 — ate_err: 0.1460  — cate_err: 0.6061 — cate_nn_err: 2.7731 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.7038 - regression_loss: 10.5079 - val_loss: 19.1625 - val_regression_loss: 13.0753 - lr: 7.8125e-08\n",
      "Epoch 146/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.3185 - regression_loss: 10.6072 — ate_err: 0.1460  — cate_err: 0.6060 — cate_nn_err: 2.7731 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.7042 - regression_loss: 10.5073 - val_loss: 19.1624 - val_regression_loss: 13.0752 - lr: 7.8125e-08\n",
      "Epoch 147/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 19.6840 - regression_loss: 14.9728 — ate_err: 0.1462  — cate_err: 0.6061 — cate_nn_err: 2.7731 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 15.6457 - regression_loss: 10.5074 - val_loss: 19.1622 - val_regression_loss: 13.0751 - lr: 7.8125e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.5034 - regression_loss: 12.7922 — ate_err: 0.1457  — cate_err: 0.6060 — cate_nn_err: 2.7731 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.6003 - regression_loss: 10.5073 - val_loss: 19.1623 - val_regression_loss: 13.0752 - lr: 7.8125e-08\n",
      "Epoch 149/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 13.8858 - regression_loss: 9.1745 — ate_err: 0.1454  — cate_err: 0.6059 — cate_nn_err: 2.7731 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.4642 - regression_loss: 10.5069 - val_loss: 19.1622 - val_regression_loss: 13.0751 - lr: 7.8125e-08\n",
      "Epoch 150/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.2754 - regression_loss: 11.5641 — ate_err: 0.1451  — cate_err: 0.6058 — cate_nn_err: 2.7730 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.6612 - regression_loss: 10.5071 - val_loss: 19.1623 - val_regression_loss: 13.0753 - lr: 7.8125e-08\n",
      "Epoch 151/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.5818 - regression_loss: 11.8706 — ate_err: 0.1458  — cate_err: 0.6060 — cate_nn_err: 2.7731 \n",
      "10/10 [==============================] - 0s 27ms/step - loss: 15.7651 - regression_loss: 10.5069 - val_loss: 19.1613 - val_regression_loss: 13.0743 - lr: 7.8125e-08\n",
      "Epoch 152/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 13.9074 - regression_loss: 9.1962 — ate_err: 0.1460  — cate_err: 0.6060 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 27ms/step - loss: 15.5958 - regression_loss: 10.5065 - val_loss: 19.1612 - val_regression_loss: 13.0742 - lr: 7.8125e-08\n",
      "Epoch 153/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 13.1128 - regression_loss: 8.4015 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7731 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.5759 - regression_loss: 10.5063 - val_loss: 19.1609 - val_regression_loss: 13.0740 - lr: 7.8125e-08\n",
      "Epoch 154/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.3894 - regression_loss: 12.6782\n",
      "Epoch 00154: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-08.\n",
      " — ate_err: 0.1457  — cate_err: 0.6059 — cate_nn_err: 2.7731 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.7725 - regression_loss: 10.5063 - val_loss: 19.1608 - val_regression_loss: 13.0739 - lr: 7.8125e-08\n",
      "Epoch 155/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 18.3024 - regression_loss: 13.5912 — ate_err: 0.1456  — cate_err: 0.6059 — cate_nn_err: 2.7731 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 15.7147 - regression_loss: 10.5060 - val_loss: 19.1608 - val_regression_loss: 13.0739 - lr: 3.9062e-08\n",
      "Epoch 156/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 13.1230 - regression_loss: 8.4118 — ate_err: 0.1456  — cate_err: 0.6059 — cate_nn_err: 2.7731 \n",
      "10/10 [==============================] - 0s 22ms/step - loss: 15.5206 - regression_loss: 10.5060 - val_loss: 19.1605 - val_regression_loss: 13.0737 - lr: 3.9062e-08\n",
      "Epoch 157/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.7598 - regression_loss: 12.0486 — ate_err: 0.1459  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 15.7039 - regression_loss: 10.5059 - val_loss: 19.1602 - val_regression_loss: 13.0734 - lr: 3.9062e-08\n",
      "Epoch 158/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 19.0425 - regression_loss: 14.3313 — ate_err: 0.1459  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 27ms/step - loss: 15.7127 - regression_loss: 10.5059 - val_loss: 19.1601 - val_regression_loss: 13.0733 - lr: 3.9062e-08\n",
      "Epoch 159/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 13.2237 - regression_loss: 8.5125\n",
      "Epoch 00159: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-08.\n",
      " — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.6407 - regression_loss: 10.5058 - val_loss: 19.1600 - val_regression_loss: 13.0732 - lr: 3.9062e-08\n",
      "Epoch 160/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.0706 - regression_loss: 10.3594 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 15.7582 - regression_loss: 10.5056 - val_loss: 19.1599 - val_regression_loss: 13.0731 - lr: 1.9531e-08\n",
      "Epoch 161/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.7017 - regression_loss: 9.9904 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 15.6242 - regression_loss: 10.5055 - val_loss: 19.1599 - val_regression_loss: 13.0731 - lr: 1.9531e-08\n",
      "Epoch 162/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.3037 - regression_loss: 12.5925 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 15.7573 - regression_loss: 10.5055 - val_loss: 19.1598 - val_regression_loss: 13.0731 - lr: 1.9531e-08\n",
      "Epoch 163/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 18.6658 - regression_loss: 13.9546 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 15.7599 - regression_loss: 10.5055 - val_loss: 19.1597 - val_regression_loss: 13.0730 - lr: 1.9531e-08\n",
      "Epoch 164/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.1726 - regression_loss: 10.4614\n",
      "Epoch 00164: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-09.\n",
      " — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.5660 - regression_loss: 10.5055 - val_loss: 19.1596 - val_regression_loss: 13.0729 - lr: 1.9531e-08\n",
      "Epoch 165/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 18.3199 - regression_loss: 13.6087 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 22ms/step - loss: 15.6965 - regression_loss: 10.5054 - val_loss: 19.1596 - val_regression_loss: 13.0729 - lr: 9.7656e-09\n",
      "Epoch 166/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.2661 - regression_loss: 9.5549 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 15.7482 - regression_loss: 10.5054 - val_loss: 19.1596 - val_regression_loss: 13.0729 - lr: 9.7656e-09\n",
      "Epoch 167/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.3484 - regression_loss: 9.6372 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 15.7552 - regression_loss: 10.5054 - val_loss: 19.1596 - val_regression_loss: 13.0728 - lr: 9.7656e-09\n",
      "Epoch 168/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.3278 - regression_loss: 12.6166 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.7840 - regression_loss: 10.5053 - val_loss: 19.1595 - val_regression_loss: 13.0728 - lr: 9.7656e-09\n",
      "Epoch 169/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 13.5355 - regression_loss: 8.8243\n",
      "Epoch 00169: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-09.\n",
      " — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.6553 - regression_loss: 10.5053 - val_loss: 19.1595 - val_regression_loss: 13.0728 - lr: 9.7656e-09\n",
      "Epoch 170/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 12.8422 - regression_loss: 8.1310 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 15.7259 - regression_loss: 10.5053 - val_loss: 19.1595 - val_regression_loss: 13.0728 - lr: 4.8828e-09\n",
      "Epoch 171/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.6533 - regression_loss: 10.9420 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.5761 - regression_loss: 10.5053 - val_loss: 19.1595 - val_regression_loss: 13.0727 - lr: 4.8828e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 13.1818 - regression_loss: 8.4706 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.6234 - regression_loss: 10.5053 - val_loss: 19.1595 - val_regression_loss: 13.0727 - lr: 4.8828e-09\n",
      "Epoch 173/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 13.2641 - regression_loss: 8.5529 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.6619 - regression_loss: 10.5053 - val_loss: 19.1595 - val_regression_loss: 13.0727 - lr: 4.8828e-09\n",
      "Epoch 174/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.2125 - regression_loss: 10.5013\n",
      "Epoch 00174: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-09.\n",
      " — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.7084 - regression_loss: 10.5053 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 4.8828e-09\n",
      "Epoch 175/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.8680 - regression_loss: 12.1568 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.7484 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 2.4414e-09\n",
      "Epoch 176/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.4679 - regression_loss: 11.7567 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 15.7760 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 2.4414e-09\n",
      "Epoch 177/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.4252 - regression_loss: 11.7140 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.6461 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 2.4414e-09\n",
      "Epoch 178/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.8624 - regression_loss: 13.1512 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 15.7363 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 2.4414e-09\n",
      "Epoch 179/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.5932 - regression_loss: 12.8819\n",
      "Epoch 00179: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-09.\n",
      " — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.6830 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 2.4414e-09\n",
      "Epoch 180/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.5726 - regression_loss: 11.8614 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.7645 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 1.2207e-09\n",
      "Epoch 181/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.0588 - regression_loss: 9.3475 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 15.7911 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 1.2207e-09\n",
      "Epoch 182/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.2670 - regression_loss: 11.5558 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 15.6800 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 1.2207e-09\n",
      "Epoch 183/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.8605 - regression_loss: 10.1493 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 15.7204 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 1.2207e-09\n",
      "Epoch 184/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.4529 - regression_loss: 10.7417\n",
      "Epoch 00184: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-10.\n",
      " — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 15.7888 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 1.2207e-09\n",
      "Epoch 185/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.6465 - regression_loss: 9.9353 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.7222 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 6.1035e-10\n",
      "Epoch 186/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.5333 - regression_loss: 9.8220 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 15.7658 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 6.1035e-10\n",
      "Epoch 187/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 12.9550 - regression_loss: 8.2438 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 15.6304 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 6.1035e-10\n",
      "Epoch 188/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 18.4383 - regression_loss: 13.7270 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 15.5804 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 6.1035e-10\n",
      "Epoch 189/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.9413 - regression_loss: 12.2301\n",
      "Epoch 00189: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-10.\n",
      " — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 15.6530 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 6.1035e-10\n",
      "Epoch 190/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.1422 - regression_loss: 11.4309 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 15.6986 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 3.0518e-10\n",
      "Epoch 191/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.7905 - regression_loss: 13.0793 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 15.7540 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 3.0518e-10\n",
      "Epoch 192/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.5802 - regression_loss: 11.8690 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 15.8181 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 3.0518e-10\n",
      "Epoch 193/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.2056 - regression_loss: 9.4944 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 15.8192 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 3.0518e-10\n",
      "Epoch 194/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.8700 - regression_loss: 11.1588\n",
      "Epoch 00194: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-10.\n",
      " — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 15.7012 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 3.0518e-10\n",
      "Epoch 195/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.2790 - regression_loss: 12.5677 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 24ms/step - loss: 15.6711 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 1.5259e-10\n",
      "Epoch 196/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 13.4385 - regression_loss: 8.7272 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.7072 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 1.5259e-10\n",
      "Epoch 197/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.8321 - regression_loss: 11.1209 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 22ms/step - loss: 15.8245 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 1.5259e-10\n",
      "Epoch 198/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 13.8924 - regression_loss: 9.1812 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 23ms/step - loss: 15.6222 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 1.5259e-10\n",
      "Epoch 199/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 13.3034 - regression_loss: 8.5922\n",
      "Epoch 00199: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-11.\n",
      " — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 15.7169 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 1.5259e-10\n",
      "Epoch 200/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.7950 - regression_loss: 12.0838 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.8258 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 7.6294e-11\n",
      "Epoch 201/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 13.5443 - regression_loss: 8.8330 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.7774 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 7.6294e-11\n",
      "Epoch 202/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.1853 - regression_loss: 9.4741 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.8220 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 7.6294e-11\n",
      "Epoch 203/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 18.7548 - regression_loss: 14.0436 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.6209 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 7.6294e-11\n",
      "Epoch 204/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 13.6938 - regression_loss: 8.9826\n",
      "Epoch 00204: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-11.\n",
      " — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 27ms/step - loss: 15.7701 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 7.6294e-11\n",
      "Epoch 205/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.1326 - regression_loss: 12.4214 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.7510 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 3.8147e-11\n",
      "Epoch 206/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.5213 - regression_loss: 9.8101 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.6771 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 3.8147e-11\n",
      "Epoch 207/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 13.3179 - regression_loss: 8.6067 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.7289 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 3.8147e-11\n",
      "Epoch 208/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 19.3535 - regression_loss: 14.6422 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.8095 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 3.8147e-11\n",
      "Epoch 209/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.5647 - regression_loss: 11.8535\n",
      "Epoch 00209: ReduceLROnPlateau reducing learning rate to 1.9073485846288207e-11.\n",
      " — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 27ms/step - loss: 15.6741 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 3.8147e-11\n",
      "Epoch 210/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 18.4473 - regression_loss: 13.7361 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.7880 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 1.9073e-11\n",
      "Epoch 211/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.8007 - regression_loss: 11.0894 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.7774 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 1.9073e-11\n",
      "Epoch 212/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.2631 - regression_loss: 11.5519 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.6726 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 1.9073e-11\n",
      "Epoch 213/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.7724 - regression_loss: 13.0612 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.7730 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 1.9073e-11\n",
      "Epoch 214/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.2638 - regression_loss: 10.5526\n",
      "Epoch 00214: ReduceLROnPlateau reducing learning rate to 9.536742923144104e-12.\n",
      " — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.6333 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 1.9073e-11\n",
      "Epoch 215/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.1952 - regression_loss: 12.4840 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.5620 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 9.5367e-12\n",
      "Epoch 216/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 17.6398 - regression_loss: 12.9285 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 15.6835 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 9.5367e-12\n",
      "Epoch 217/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.8537 - regression_loss: 12.1425 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.6883 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 9.5367e-12\n",
      "Epoch 218/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.6378 - regression_loss: 9.9265 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.8240 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 9.5367e-12\n",
      "Epoch 219/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/10 [==>...........................] - ETA: 0s - loss: 13.9894 - regression_loss: 9.2782\n",
      "Epoch 00219: ReduceLROnPlateau reducing learning rate to 4.768371461572052e-12.\n",
      " — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 26ms/step - loss: 15.6958 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 9.5367e-12\n",
      "Epoch 220/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 16.6130 - regression_loss: 11.9018 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.7335 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 4.7684e-12\n",
      "Epoch 221/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 14.4670 - regression_loss: 9.7558 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.6645 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 4.7684e-12\n",
      "Epoch 222/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 18.0596 - regression_loss: 13.3484 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 25ms/step - loss: 15.6515 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 4.7684e-12\n",
      "Epoch 223/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 18.7516 - regression_loss: 14.0404 — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 24ms/step - loss: 15.7272 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 4.7684e-12\n",
      "Epoch 224/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 15.9205 - regression_loss: 11.2093\n",
      "Epoch 00224: ReduceLROnPlateau reducing learning rate to 2.384185730786026e-12.\n",
      " — ate_err: 0.1458  — cate_err: 0.6059 — cate_nn_err: 2.7732 \n",
      "10/10 [==============================] - 0s 27ms/step - loss: 15.6853 - regression_loss: 10.5052 - val_loss: 19.1594 - val_regression_loss: 13.0727 - lr: 4.7684e-12\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# every loss function in TF2 takes 2 arguments, a vector of true values and a vector predictions\n",
    "def regression_loss(concat_true, concat_pred):\n",
    "    #computes a standard MSE loss for TARNet\n",
    "    y_true = concat_true[:, 0] #get individual vectors\n",
    "    t_true = concat_true[:, 1]\n",
    " \n",
    "    y0_pred = concat_pred[:, 0]\n",
    "    y1_pred = concat_pred[:, 1]\n",
    " \n",
    "    #Each head outputs a prediction for both potential outcomes\n",
    "    #We use t_true as a switch to only calculate the factual loss\n",
    "    loss0 = tf.reduce_sum((1. - t_true) * tf.square(y_true - y0_pred))\n",
    "    loss1 = tf.reduce_sum(t_true * tf.square(y_true - y1_pred))\n",
    "    #note Shi uses tf.reduce_sum for her losses even though mathematically we should be using the mean\n",
    "    #tf.reduce_mean and tf.reduce_sum should be equivalent, but maybe having larger error gradients makes training easier?\n",
    "    return loss0 + loss1\n",
    " \n",
    "### MAIN CODE ####\n",
    " \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD\n",
    " \n",
    "val_split=0.2\n",
    "batch_size=64\n",
    "verbose=True\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    "yt = np.concatenate([data['ys'], data['t']], 1) #we'll use both y and t to compute the loss\n",
    " \n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    " \n",
    "sgd_callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='val_loss', patience=40, min_delta=0), \n",
    "        #40 is Shi's recommendation patience for this dataset, but you should tune for your data \n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0, cooldown=0, min_lr=0),\n",
    "        #This learning rate scheduling is quite agressive which seems good for this dataset\n",
    "        Full_Metrics(data,verbose),\n",
    "        tensorboard_callback\n",
    "    ]\n",
    "#optimizer hyperparameters\n",
    "sgd_lr = 1e-5\n",
    "momentum = 0.9\n",
    "tarnet_model.compile(optimizer=SGD(lr=sgd_lr, momentum=momentum, nesterov=True),\n",
    "                    loss=regression_loss,\n",
    "                    metrics=regression_loss)\n",
    " \n",
    "tarnet_model.fit(x=data['x'],y=yt,\n",
    "                callbacks=sgd_callbacks,\n",
    "                validation_split=val_split,\n",
    "                epochs=300,\n",
    "                batch_size=batch_size,\n",
    "                verbose=verbose)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 11635), started 4:48:47 ago. (Use '!kill 11635' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9e69740cbf8650ac\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9e69740cbf8650ac\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(747, 25)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.65613806, -1.0024741 , -0.360898  , ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.7153288 , -1.0024741 , -0.733261  , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.5543657 ,  0.19681813, -0.360898  , ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       ...,\n",
       "       [-0.24543142, -0.20294595,  0.38382798, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [ 0.53274953,  0.5965822 , -1.105624  , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.2830061 , -1.0024741 , -0.360898  , ...,  0.        ,\n",
       "         0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
