{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"CEVAE moargs.latent_dimensionel on IHargs.latent_dimensionP\n",
    "\"\"\"\n",
    "##################### Neeargs.latent_dimension TF1\n",
    "# import edward as ed\n",
    "# import tensorflow as tf\n",
    "# from utils import fc_net, get_y0_y1\n",
    "# from edward.models import Bernoulli, Normal\n",
    "# from progressbar import ETA, Bar, Percentage, ProgressBar\n",
    "###############################################################\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.stats import sem\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from datasets import IHDP\n",
    "from evaluation import Evaluator\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser()\n",
    "################### CEVAE\n",
    "parser.add_argument('-reps', type=int, default=10)\n",
    "parser.add_argument('-earl', type=int, default=10)\n",
    "parser.add_argument('-lr', type=float, default=0.001)\n",
    "parser.add_argument('-opt', choices=['adam', 'adamax'], default='adam')\n",
    "parser.add_argument('-epochs', type=int, default=100)\n",
    "parser.add_argument('-print_every', type=int, default=10)\n",
    "################### Reckon Need\n",
    "parser.add_argument('--model_path',  type = str, default='./',help = 'Model save path.')\n",
    "parser.add_argument('--train_paths', type = str, default='',  help = 'HDFS paths to input train files.')\n",
    "parser.add_argument('--test_paths',  type = str, default='',  help = 'HDFS paths to input test files.')\n",
    "################### Data Path Self\n",
    "parser.add_argument('--train_data_paths',  type = str, \n",
    "                    default=\"hdfs://haruna/user/yanyiming.work/coupon/new_user/rp_label_30day_all_convert_split_v1.1/tfrecord/train\")\n",
    "parser.add_argument('--test_data_paths',   type = str, \n",
    "                    default=\"hdfs://haruna/user/yanyiming.work/coupon/new_user/rp_label_30day_all_convert_split_v1.1/tfrecord/test\")\n",
    "parser.add_argument('--date_interval',     type = str, default=\"20211114,20211124\")\n",
    "################### Load Model Path\n",
    "parser.add_argument('--last_model_path',   type = str, default='', help='last model path.')\n",
    "parser.add_argument('--load_latest_model', type = str, default='', help='load from the latest dump of this model')\n",
    "################### Treatment And Label\n",
    "parser.add_argument('--label', type = str, default='rpg30', help = '')\n",
    "parser.add_argument('--target_treat',  type = str, default = 'threshold_cents', help = 'target treat')\n",
    "parser.add_argument('--treatment_col', type = str, default = 'credit_cents,threshold_cents', help = 'all treatment')\n",
    "parser.add_argument('--control_treatment_val', type = float, default = 0.0,   help = 'control id')\n",
    "parser.add_argument('--experim_treatment_val', type = float, default = 800.0, help = 'experiment id')\n",
    "################### BASIC\n",
    "parser.add_argument('--num_parallel_reads', type = int, default = 10, help = 'random seed')\n",
    "parser.add_argument('--random_seed', type = int,default = 9418, help = 'random seed')\n",
    "parser.add_argument('--mode', type = str, default = 'train', help = 'train or evaluate')\n",
    "args = parser.parse_args([])\n",
    "args.true_post = True\n",
    "\n",
    "#################################From CEVAE\n",
    "dataset = IHDP(replications=args.reps)\n",
    "dimx = 25\n",
    "scores = np.zeros((args.reps, 3))\n",
    "scores_test = np.zeros((args.reps, 3))\n",
    "\n",
    "M = None  # batch size during training\n",
    "d = 20  # latent dimension\n",
    "lamba = 1e-4  # weight decay\n",
    "nh, h = 3, 200  # number and size of hidden layers\n",
    "#################################IHDP Data\n",
    "binfeats = [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
    "numfeats = [i for i in range(25) if i not in binfeats]\n",
    "#################################\n",
    "\n",
    "def findfile(start, name):\n",
    "    ans = []\n",
    "    for relpath, dirs, files in os.walk(start):\n",
    "        for file in files:\n",
    "            if name in file:\n",
    "                full_path = os.path.join(start, file)\n",
    "                ans.append(full_path)\n",
    "                # print(os.path.normpath(os.path.abspath(full_path)))\n",
    "    return ans\n",
    "\n",
    "def ihdp_fn(data):\n",
    "    data = tf.strings.to_number(tf.strings.split(data, sep=','))\n",
    "    t, y, y_cf = data[0], data[1], data[2]\n",
    "    mu_0, mu_1, x = data[3], data[4], data[5:]\n",
    "    # this binary feature is in {1, 2}\n",
    "    tmp = [0 for i in range(25)]\n",
    "    tmp[13] = 1\n",
    "    x = x - tmp\n",
    "    x_bin,x_num = x[6:],x[:6]\n",
    "    return {\"features\":{\"x_bin\":x_bin, \"x_num\":x_num, \"t\":t, \"y\":y}, \n",
    "            \"labels\":{\"y_cf\":y_cf, \"mu_0\":mu_0, \"mu_1\":mu_1}}\n",
    "    \n",
    "def input_fn_csv(path,name, batch_size=128, shuffle_size=0):\n",
    "    all_files = findfile(path,name)\n",
    "    ds = tf.data.TextLineDataset(\n",
    "        all_files,\n",
    "        compression_type= None,\n",
    "        num_parallel_reads=int(args.num_parallel_reads),\n",
    "        buffer_size=64*1024*1024# 可选参数，多线程并行读多个文件\n",
    "    ).repeat(1)\n",
    "    if shuffle_size > 0:\n",
    "        ds = ds.shuffle(shuffle_size)\n",
    "    ds = ds.map(ihdp_fn).batch(batch_size)\n",
    "    return ds\n",
    "\n",
    "class ExpandDims(tf.keras.layers.Layer):\n",
    "    def __init__(self, axis=1, **kwargs):\n",
    "        super(ExpandDims, self).__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ExpandDims, self).build(input_shape)\n",
    "\n",
    "    def call(self, input):\n",
    "        out = tf.expand_dims(input, axis=self.axis)\n",
    "        return out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [input_shape[0], 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'x_bin': <tf.Tensor: shape=(2, 19), dtype=float32, numpy=\n",
       "  array([[1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
       "          0., 0., 0.]], dtype=float32)>,\n",
       "  'x_num': <tf.Tensor: shape=(2, 6), dtype=float32, numpy=\n",
       "  array([[-0.52860284, -0.3434545 ,  1.128554  ,  0.16170253, -0.31660318,\n",
       "           1.295216  ],\n",
       "         [-0.52860284, -0.3434545 ,  1.128554  ,  0.16170253, -0.31660318,\n",
       "           1.295216  ]], dtype=float32)>,\n",
       "  't': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>,\n",
       "  'y': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([11.267344, 12.698154], dtype=float32)>},\n",
       " {'y_cf': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([6.31402 , 7.944765], dtype=float32)>,\n",
       "  'mu_0': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([6.1513076, 7.9621396], dtype=float32)>,\n",
       "  'mu_1': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([11.160778, 12.358606], dtype=float32)>})"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for data in input_fn_csv(\"./datasets/IHDP/csv/\",\".csv\",batch_size=2):\n",
    "    break\n",
    "data['features'],data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEVAE(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CEVAE, self).__init__()\n",
    "        ########################################\n",
    "        self.activation = 'elu'\n",
    "        # IHDP input part\n",
    "        \n",
    "        # CEVAE Model (decoder)\n",
    "        # p(x|z)\n",
    "        self.px_z_shared = self.fc_net((nh - 1) * [h], [], 'px_z_shared', lamba=lamba, activation=self.activation)\n",
    "        self.px_z_bin_logits = self.fc_net([h], [[len(binfeats), None]], 'px_z_bin', lamba=lamba, activation=self.activation)\n",
    "        self.px_z_num_mu = self.fc_net([h],[[len(numfeats), None]], 'px_z_cont', lamba=lamba, activation=self.activation)\n",
    "        self.px_z_num_sigma = self.fc_net([h],[[len(numfeats), tf.nn.softplus]], 'px_z_cont', lamba=lamba, activation=self.activation)\n",
    "        # p(t|z)\n",
    "        self.pt_z_logits = self.fc_net([h], [[1, None]], 'pt_z', lamba=lamba, activation = self.activation)\n",
    "        # p(y|t,z)\n",
    "        self.py_t0z_mu2_t0 = self.fc_net(nh * [h], [[1, None]], 'py_t0z', lamba=lamba, activation=self.activation)\n",
    "        self.py_t1z_mu2_t1 = self.fc_net(nh * [h], [[1, None]], 'py_t1z', lamba=lamba, activation=self.activation)\n",
    "        \n",
    "        # CEVAE Model (encoder)\n",
    "        # q(t|x)\n",
    "        self.qt_logits_t = self.fc_net([d], [[1, None]], 'qt', lamba=lamba, activation=self.activation)\n",
    "        # q(y|x,t)\n",
    "        self.qy_xt_shared_hqy = self.fc_net((nh - 1) * [h], [], 'qy_xt_shared', lamba=lamba, activation=self.activation)\n",
    "        self.qy_xt0_mu_qy_t0 = self.fc_net([h], [[1, None]], 'qy_xt0', lamba=lamba, activation=self.activation)\n",
    "        self.qy_xt1_mu_qy_t1 = self.fc_net([h], [[1, None]], 'qy_xt1', lamba=lamba, activation=self.activation)\n",
    "        # q(z|x,t,y)\n",
    "        self.qz_xty_shared_hqz = self.fc_net((nh - 1) * [h], [], 'qz_xty_shared', lamba=lamba, activation=self.activation)\n",
    "        self.qz_xt0_muq_t0 = self.fc_net([h], [[d, None]], 'qz_xt0', lamba=lamba,activation=self.activation), \n",
    "        self.qz_xt1_sigmaq_t0 = self.fc_net([h], [[d, tf.nn.softplus]], 'qz_xt0', lamba=lamba,activation=self.activation)\n",
    "        self.qz_xt1_muq_t1 = self.fc_net([h], [[d, None]], 'qz_xt1', lamba=lamba,activation=self.activation), \n",
    "        self.qz_xt1_sigmaq_t1 = self.fc_net([h], [[d, tf.nn.softplus]], 'qz_xt1', lamba=lamba,activation=self.activation)\n",
    "\n",
    "    def fc_net(self, layers, out_layers, scope, lamba=1e-3, activation=tf.nn.relu):\n",
    "        net = tf.keras.Sequential()\n",
    "        for hidden in layers:\n",
    "            net.add(tf.keras.layers.Dense(hidden, activation = self.activation))\n",
    "            if not out_layers:\n",
    "                return net\n",
    "        for i, (outdim, activation) in enumerate(out_layers):\n",
    "            net.add(tf.keras.layers.Dense(outdim, activation = activation))\n",
    "        return net\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, features, training=False, serving=False):\n",
    "        x_ph_bin = features['x_bin']  # binary inputs\n",
    "        x_ph_num = features['x_num'] # continuous inputs\n",
    "        t_ph = features['t']\n",
    "        y_ph = features['y']\n",
    "\n",
    "        x_ph = tf.concat([x_ph_bin, x_ph_num], 1)\n",
    "        # p(z)\n",
    "        z_prior = tfp.distributions.Normal(loc=tf.zeros([tf.shape(x_ph)[0], d]), scale=tf.ones([tf.shape(x_ph)[0], d]))\n",
    "        \n",
    "        # CEVAE variational approximation (encoder)\n",
    "        # q(t|x)\n",
    "        logits_t = self.qt_logits_t(x_ph)\n",
    "        qt = tfp.distributions.Bernoulli(logits=logits_t, dtype=tf.float32)\n",
    "        # q(y|x,t)\n",
    "        hqy = self.qy_xt_shared_hqy(x_ph)\n",
    "        mu_qy_t0 = self.qy_xt0_mu_qy_t0(hqy)\n",
    "        mu_qy_t1 = self.qy_xt1_mu_qy_t1(hqy)\n",
    "        qy = tfp.distributions.Normal(loc=qt * mu_qy_t1 + (1. - qt) * mu_qy_t0, scale=tf.ones_like(mu_qy_t0))\n",
    "        # q(z|x,t,y)\n",
    "        inpt2 = tf.concat([x_ph, qy], 1)\n",
    "        hqz = self.qz_xty_shared_hqz(inpt2)\n",
    "        muq_t0 = self.qz_xt0_muq_t0(hqz)\n",
    "        sigmaq_t0 = self.qz_xt1_sigmaq_t0(hqz)\n",
    "        muq_t1 = self.qz_xt0_muq_t1(hqz)\n",
    "        sigmaq_t1 = self.qz_xt1_sigmaq_t1(hqz)\n",
    "        qz = tfp.distributions.Normal(loc=qt * muq_t1 + (1. - qt) * muq_t0, scale=qt * sigmaq_t1 + (1. - qt) * sigmaq_t0)\n",
    "\n",
    "        # CEVAE model (decoder)\n",
    "        # p(x|z)\n",
    "        hx = self.px_z_shared(qz)\n",
    "        logits = self.px_z_bin_logits(hx)\n",
    "        x1 = tfp.distributions.Bernoulli(logits = logits, dtype = tf.float32)\n",
    "\n",
    "        mu = self.px_z_num_mu(hx)\n",
    "        sigma = self.px_z_num_sigma(hx)\n",
    "        x2 = tfp.distributions.Normal(loc = mu, sacle = sigma)\n",
    "\n",
    "        # p(t|z)\n",
    "        logits = self.pt_z_logits(z)\n",
    "        t = tfp.distributions.Bernouli(logits = logits, dtype = tf.float32)\n",
    "\n",
    "        # p(y|t,z)\n",
    "        mu2_t0 = self.py_t0z_mu2_t0(z)\n",
    "        mu2_t1 = self.py_t0z_mu2_t1(z)\n",
    "        y = tfp.distributions.Normal(loc=t * mu2_t1 + (1. - t) * mu2_t0, scale=tf.ones_like(mu2_t0))\n",
    "        # data = {x1: x_ph_bin, x2: x_ph_num, y: y_ph, qt: t_ph, t: t_ph, qy: y_ph}\n",
    "        return \n",
    "\n",
    "    @tf.function\n",
    "    def serve(self, input):\n",
    "        return self(input, serving=True)\n",
    "\n",
    "def build_model():\n",
    "    model = CEVAE()\n",
    "    features = {\"x_bin\":tf.keras.Input(1,dtype=tf.float32), \n",
    "                \"x_num\":tf.keras.Input(1,dtype=tf.float32), \n",
    "                \"t\":tf.keras.Input(1,dtype=tf.float32), \n",
    "                \"y\":tf.keras.Input(1,dtype=tf.float32)}\n",
    "    labels = {\"y_cf\":tf.keras.Input(1,dtype=tf.float32), \n",
    "              \"mu_0\":tf.keras.Input(1,dtype=tf.float32), \n",
    "              \"mu_1\":tf.keras.Input(1,dtype=tf.float32)}\n",
    "    fake_inputs = features\n",
    "    model(fake_inputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adagrad(args.learning_rate),\n",
    "                  loss= \"mse\",\n",
    "                  metrics=[])\n",
    "    return model\n",
    "\n",
    "def loss(labels, preds):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling layer \"cevae_11\" (type CEVAE).\n\nin user code:\n\n    File \"/var/folders/p0/kb961x5d6wd79gfp2h6t_8500000gn/T/ipykernel_47808/3851423365.py\", line 64, in call  *\n        qy = tfp.distributions.Normal(loc=qt * mu_qy_t1 + (1. - qt) * mu_qy_t0, scale=tf.ones_like(mu_qy_t0))\n\n    TypeError: Expected float32, but got tfp.distributions.Bernoulli(\"Bernoulli\", batch_shape=[?, 1], event_shape=[], dtype=float32) of type 'Bernoulli'.\n\n\nCall arguments received:\n  • features={'x_bin': 'tf.Tensor(shape=(None, 1), dtype=float32)', 'x_num': 'tf.Tensor(shape=(None, 1), dtype=float32)', 't': 'tf.Tensor(shape=(None, 1), dtype=float32)', 'y': 'tf.Tensor(shape=(None, 1), dtype=float32)'}\n  • training=False\n  • serving=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p0/kb961x5d6wd79gfp2h6t_8500000gn/T/ipykernel_47808/1335155156.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/p0/kb961x5d6wd79gfp2h6t_8500000gn/T/ipykernel_47808/3851423365.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m               \"mu_1\":tf.keras.Input(1,dtype=tf.float32)}\n\u001b[1;32m    108\u001b[0m     \u001b[0mfake_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     model.compile(optimizer=tf.keras.optimizers.Adagrad(args.learning_rate),\n\u001b[1;32m    111\u001b[0m                   \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"mse\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling layer \"cevae_11\" (type CEVAE).\n\nin user code:\n\n    File \"/var/folders/p0/kb961x5d6wd79gfp2h6t_8500000gn/T/ipykernel_47808/3851423365.py\", line 64, in call  *\n        qy = tfp.distributions.Normal(loc=qt * mu_qy_t1 + (1. - qt) * mu_qy_t0, scale=tf.ones_like(mu_qy_t0))\n\n    TypeError: Expected float32, but got tfp.distributions.Bernoulli(\"Bernoulli\", batch_shape=[?, 1], event_shape=[], dtype=float32) of type 'Bernoulli'.\n\n\nCall arguments received:\n  • features={'x_bin': 'tf.Tensor(shape=(None, 1), dtype=float32)', 'x_num': 'tf.Tensor(shape=(None, 1), dtype=float32)', 't': 'tf.Tensor(shape=(None, 1), dtype=float32)', 'y': 'tf.Tensor(shape=(None, 1), dtype=float32)'}\n  • training=False\n  • serving=False"
     ]
    }
   ],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['normal_likelihood_1/mean:0', 'normal_likelihood_1/std:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['normal_likelihood_1/mean:0', 'normal_likelihood_1/std:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 648.8309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13aaccdc0>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "EPS = 1e-5\n",
    "\n",
    "batch_size = 4\n",
    "N = 100\n",
    "x = np.random.randn(batch_size, N)\n",
    "\n",
    "class NormalLikelihood(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(NormalLikelihood, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.mu = self.add_weight(\"mean\", shape=[2], initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1), dtype=tf.float32)\n",
    "        self.sigma = self.add_weight(\"std\", shape=[2], initializer=tf.keras.initializers.RandomUniform(minval=EPS, maxval=5.0, seed=None), constraint=tf.keras.constraints.non_neg(), dtype=tf.float32)\n",
    "        self.pi = self.add_weight(\"weights\", shape=[2], initializer=tf.keras.initializers.RandomUniform(minval=EPS, maxval=5.0, seed=None), constraint=tf.keras.constraints.non_neg(), dtype=tf.float32)\n",
    "        self.distributions = []\n",
    "        for k in range(2):\n",
    "            self.distributions.append(tfp.distributions.Normal(self.mu[k], self.sigma[k]))\n",
    "\n",
    "    def call(self, input):\n",
    "        # Mixture of gaussian model\n",
    "        weights = self.pi / (tf.reduce_sum(self.pi) + 1e-7)\n",
    "        r = weights[0] * self.distributions[0].prob(input)\n",
    "        r = r + weights[1] * self.distributions[1].prob(input)\n",
    "        r = tf.clip_by_value(r, 1e-3, 1-1e-3)\n",
    "        return r\n",
    "\n",
    "input_layer = tf.keras.layers.Input(shape=(100,))\n",
    "r = NormalLikelihood()(input_layer)\n",
    "r = -tf.reduce_sum(tf.math.log(r))\n",
    "model = tf.keras.models.Model(input_layer, r)\n",
    "model.add_loss(r)\n",
    "model.compile(optimizer='rmsprop', loss=None)\n",
    "model.fit(x, y=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "\n",
    "# Create a stochastic encoder -- e.g., for use in a variational auto-encoder.\n",
    "input_shape = [28, 28, 1]\n",
    "encoded_shape = 2\n",
    "encoder = tfk.Sequential([\n",
    "  tfkl.InputLayer(input_shape=input_shape),\n",
    "  tfkl.Flatten(),\n",
    "  tfkl.Dense(10, activation='relu'),\n",
    "  tfkl.Dense(tfpl.IndependentNormal.params_size(encoded_shape)),\n",
    "  tfpl.IndependentNormal(encoded_shape)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p0/kb961x5d6wd79gfp2h6t_8500000gn/T/ipykernel_47808/2686285229.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'show'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
