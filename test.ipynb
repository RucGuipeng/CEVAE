{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "CEVAE moargs.latent_dimensionel on IHargs.latent_dimensionP\n",
    "\"\"\"\n",
    "##################### Neeargs.latent_dimension TF1\n",
    "# import edward as ed\n",
    "# import tensorflow as tf\n",
    "# from utils import fc_net, get_y0_y1\n",
    "# from edward.models import Bernoulli, Normal\n",
    "# from progressbar import ETA, Bar, Percentage, ProgressBar\n",
    "###############################################################\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.enable_v2_behavior()\n",
    "from scipy.stats import sem\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.layers as tfkl\n",
    "tfd,tfpl = tfp.distributions,tfp.layers\n",
    "import tensorflow.keras.backend as tfkb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from datasets import IHDP\n",
    "from evaluation import Evaluator\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser()\n",
    "################### CEVAE\n",
    "parser.add_argument('-reps', type=int, default=10)\n",
    "parser.add_argument('-earl', type=int, default=10)\n",
    "parser.add_argument('-lr', type=float, default=0.001)\n",
    "parser.add_argument('-opt', choices=['adam', 'adamax'], default='adam')\n",
    "parser.add_argument('-epochs', type=int, default=100)\n",
    "parser.add_argument('-print_every', type=int, default=10)\n",
    "################### Reckon Need\n",
    "parser.add_argument('--model_path',  type = str, default='./',help = 'Model save path.')\n",
    "parser.add_argument('--train_paths', type = str, default='',  help = 'HDFS paths to input train files.')\n",
    "parser.add_argument('--test_paths',  type = str, default='',  help = 'HDFS paths to input test files.')\n",
    "################### Data Path Self\n",
    "parser.add_argument('--train_data_paths',  type = str, \n",
    "                    default=\"hdfs://haruna/user/yanyiming.work/coupon/new_user/rp_label_30day_all_convert_split_v1.1/tfrecord/train\")\n",
    "parser.add_argument('--test_data_paths',   type = str, \n",
    "                    default=\"hdfs://haruna/user/yanyiming.work/coupon/new_user/rp_label_30day_all_convert_split_v1.1/tfrecord/test\")\n",
    "parser.add_argument('--date_interval',     type = str, default=\"20211114,20211124\")\n",
    "################### Load Model Path\n",
    "parser.add_argument('--last_model_path',   type = str, default='', help='last model path.')\n",
    "parser.add_argument('--load_latest_model', type = str, default='', help='load from the latest dump of this model')\n",
    "################### Treatment And Label\n",
    "parser.add_argument('--label', type = str, default='rpg30', help = '')\n",
    "parser.add_argument('--target_treat',  type = str, default = 'threshold_cents', help = 'target treat')\n",
    "parser.add_argument('--treatment_col', type = str, default = 'credit_cents,threshold_cents', help = 'all treatment')\n",
    "parser.add_argument('--control_treatment_val', type = float, default = 0.0,   help = 'control id')\n",
    "parser.add_argument('--experim_treatment_val', type = float, default = 800.0, help = 'experiment id')\n",
    "################### BASIC\n",
    "parser.add_argument('--num_parallel_reads', type = int, default = 10, help = 'random seed')\n",
    "parser.add_argument('--random_seed', type = int,default = 9418, help = 'random seed')\n",
    "parser.add_argument('--mode', type = str, default = 'train', help = 'train or evaluate')\n",
    "args = parser.parse_args([])\n",
    "args.true_post = True\n",
    "\n",
    "#################################From CEVAE\n",
    "dataset = IHDP(replications=args.reps)\n",
    "dimx = 25\n",
    "scores = np.zeros((args.reps, 3))\n",
    "scores_test = np.zeros((args.reps, 3))\n",
    "\n",
    "#################################IHDP Data\n",
    "\n",
    "################################# some functions needed\n",
    "\n",
    "def findfile(start, name):\n",
    "    ans = []\n",
    "    for relpath, dirs, files in os.walk(start):\n",
    "        for file in files:\n",
    "            if name in file:\n",
    "                full_path = os.path.join(start, file)\n",
    "                ans.append(full_path)\n",
    "                # print(os.path.normpath(os.path.abspath(full_path)))\n",
    "    return ans\n",
    "\n",
    "class ExpandDims(tf.keras.layers.Layer):\n",
    "    def __init__(self, axis=1, **kwargs):\n",
    "        super(ExpandDims, self).__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ExpandDims, self).build(input_shape)\n",
    "\n",
    "    def call(self, input):\n",
    "        out = tf.expand_dims(input, axis=self.axis)\n",
    "        return out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [input_shape[0], 1]\n",
    "\n",
    "# for data in input_fn_csv(\"./datasets/IHDP/csv/\",\".csv\",batch_size=2):\n",
    "#     break\n",
    "# data['features'],data['labels']\n",
    "\n",
    "# data information \n",
    "t_bin_dim = 1\n",
    "y_dim, default_y_scale = 1,tf.exp(0.)\n",
    "M = None        # batch size during training\n",
    "z_dim = 20          # latent z dimension\n",
    "lamba = 1e-4    # weight decay\n",
    "nh, h = 3, 200  # number and size of hidden layers\n",
    "binfeats = [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
    "numfeats = [i for i in range(25) if i not in binfeats]\n",
    "x_bin_dim = len(binfeats)\n",
    "x_num_dim = len(numfeats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ihdp_fn(data):\n",
    "    data = tf.strings.to_number(tf.strings.split(data, sep=','))\n",
    "    t, y, y_cf = data[0:1], data[1:2], data[2:3]\n",
    "    mu_0, mu_1, x = data[3:4], data[4:5], data[5:]\n",
    "    # this binary feature is in {1, 2}\n",
    "    tmp = [0 for i in range(25)]\n",
    "    tmp[13] = 1\n",
    "    # print(x,t,y)\n",
    "    return [\n",
    "        tf.concat([x,t,y], axis=-1),\n",
    "        tf.concat([y_cf, mu_0, mu_1], axis=-1)\n",
    "        ]\n",
    "def input_fn_csv(path,name, batch_size=128, shuffle_size=0):\n",
    "    all_files = findfile(path,name)\n",
    "    ds = tf.data.TextLineDataset(\n",
    "        all_files,\n",
    "        compression_type= None,\n",
    "        num_parallel_reads=int(args.num_parallel_reads),\n",
    "        buffer_size=64*1024*1024# 可选参数，多线程并行读多个文件\n",
    "    ).repeat(1)\n",
    "    if shuffle_size > 0:\n",
    "        ds = ds.shuffle(shuffle_size)\n",
    "    ds = ds.map(ihdp_fn).batch(batch_size)\n",
    "    return ds\n",
    "\n",
    "class CEVAE(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CEVAE, self).__init__()\n",
    "        ########################################\n",
    "        self.activation = 'elu'\n",
    "        # CEVAE Model (decoder)\n",
    "        # p(z)\n",
    "        self.z = tfd.Normal(loc = [0]*z_dim, scale = [1]*z_dim)\n",
    "        # p(x|z)\n",
    "        self.p_x_z_shared = self.fc_net(z_dim,(nh - 1) * [h], [], 'px_z_shared', lamba=lamba, activation=self.activation)\n",
    "        self.p_x_z_bin = tfk.Sequential(\n",
    "            [tfkl.InputLayer(input_shape=[h]),\n",
    "            tfkl.Dense(h,activation=self.activation),\n",
    "            tfkl.Dense(tfpl.IndependentBernoulli.params_size(x_bin_dim),activation=None),\n",
    "            tfkl.Flatten(),\n",
    "            tfpl.IndependentBernoulli(x_bin_dim,tfd.Bernoulli.logits)\n",
    "            ])\n",
    "        \n",
    "        self.p_x_z_num = tfk.Sequential(\n",
    "            [tfkl.InputLayer(input_shape=[h]),\n",
    "            tfkl.Dense(h,activation=self.activation),\n",
    "            tfkl.Flatten(),\n",
    "            tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(z_dim),\n",
    "                       activation=None),\n",
    "            tfpl.MultivariateNormalTriL(\n",
    "                z_dim,\n",
    "                # activity_regularizer=tfpl.KLDivergenceRegularizer(self.z)\n",
    "                ),\n",
    "        ])\n",
    "\n",
    "            \n",
    "            \n",
    "        # q(z|x,t,y)\n",
    "        self.q_z_x = tfk.Sequential(\n",
    "            [self.fc_net(x_bin_dim + x_num_dim,(nh-1) * [h], [], 'qz_x_shared', lamba=lamba, activation=self.activation),\n",
    "            tfkl.Dense(h,activation=self.activation),\n",
    "            tfkl.Dense(tfpl.IndependentNormal.params_size(z_dim),activation=None),\n",
    "            tfkl.Flatten(),\n",
    "            tfpl.IndependentNormal(z_dim)\n",
    "            ])\n",
    "\n",
    "    def fc_net(self,input_shape, layers, out_layers, scope, lamba=1e-3, activation=tf.nn.relu):\n",
    "        net = tfk.Sequential([tfkl.InputLayer([input_shape])])\n",
    "        for hidden in layers:\n",
    "            net.add(tfkl.Dense(hidden, activation = self.activation))\n",
    "        if out_layers:\n",
    "            [outdim, activation] = out_layers\n",
    "            net.add(tfkl.Dense(outdim, activation = activation))\n",
    "        return net\n",
    "\n",
    "    # @tf.function\n",
    "    def call(self, features, training=False, serving=False):\n",
    "        # input data preprocess\n",
    "        x_ph_bin = features[...,0:x_bin_dim]  # binary inputs\n",
    "        x_ph_num = features[...,x_bin_dim:x_bin_dim+x_num_dim]  # continuous inputs\n",
    "\n",
    "        x_ph = tf.concat([x_ph_bin, x_ph_num], -1)\n",
    "        # p(z)\n",
    "        z = self.z.sample(tf.shape(x_ph)[0])\n",
    "        # print(\"**** z ****\",z)\n",
    "\n",
    "        # CEVAE variational approximation (encoder)\n",
    "        # q(z|x)\n",
    "        q_z_x_dist = self.q_z_x(x_ph)\n",
    "        # print(\"**** qz ****\",q_z_x_dist)\n",
    "\n",
    "        # CEVAE model (decoder)\n",
    "        # p(x|z)\n",
    "        hx = self.p_x_z_shared(q_z_x_dist)\n",
    "        x_bin = self.p_x_z_bin(hx)\n",
    "        x_num = self.p_x_z_num(hx)\n",
    "\n",
    "        output = x_bin\n",
    "        # print(\"**** output ****\", output)\n",
    "        return output\n",
    "\n",
    "    # @tf.function\n",
    "    # def serve(self, input):\n",
    "    #     return self(input, serving=True)\n",
    "\n",
    "def cevae_loss(labels,preds):\n",
    "    print(\"**** loss output ****\", preds)\n",
    "    print(\"cevae loss\")\n",
    "\n",
    "    # read data\n",
    "    x_bin_real = labels[..., 0:x_bin_dim]  # binary inputs\n",
    "    x_num_real = labels[..., (x_bin_dim):(x_bin_dim+x_num_dim)]  # continuous inputs\n",
    "    t_real     = labels[..., (x_bin_dim+x_num_dim):(x_bin_dim+x_num_dim+t_bin_dim)]\n",
    "    y_real     = labels[..., (x_bin_dim+x_num_dim+t_bin_dim):(x_bin_dim+x_num_dim+t_bin_dim+y_dim)]\n",
    "    # print(\"**** t_real ****\",t_real)\n",
    "\n",
    "    return 1\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    model = CEVAE()\n",
    "    features = tf.keras.Input(x_bin_dim + x_num_dim + t_bin_dim + y_dim,dtype=tf.float32)\n",
    "    fake_inputs = features\n",
    "    model(fake_inputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(args.lr),\n",
    "                  loss= cevae_loss,\n",
    "                  metrics=[])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "**** loss output **** tfp.distributions._TensorCoercible(\"tensor_coercible\", batch_shape=[32], event_shape=[19], dtype=float32)\n",
      "cevae loss\n",
      "WARNING:tensorflow:The dtype of the target tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 816, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/usr/local/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 532, in minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    File \"/usr/local/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 633, in apply_gradients\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n    File \"/usr/local/lib/python3.9/site-packages/keras/optimizer_v2/utils.py\", line 73, in filter_empty_gradients\n        raise ValueError(f\"No gradients provided for any variable: {variable}. \"\n\n    ValueError: No gradients provided for any variable: (['dense_119/kernel:0', 'dense_119/bias:0', 'dense_120/kernel:0', 'dense_120/bias:0', 'dense_121/kernel:0', 'dense_121/bias:0', 'dense_122/kernel:0', 'dense_122/bias:0', 'dense_123/kernel:0', 'dense_123/bias:0', 'dense_124/kernel:0', 'dense_124/bias:0', 'dense_125/kernel:0', 'dense_125/bias:0', 'dense_126/kernel:0', 'dense_126/bias:0', 'dense_127/kernel:0', 'dense_127/bias:0', 'dense_128/kernel:0', 'dense_128/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'dense_119/kernel:0' shape=(20, 200) dtype=float32>), (None, <tf.Variable 'dense_119/bias:0' shape=(200,) dtype=float32>), (None, <tf.Variable 'dense_120/kernel:0' shape=(200, 200) dtype=float32>), (None, <tf.Variable 'dense_120/bias:0' shape=(200,) dtype=float32>), (None, <tf.Variable 'dense_121/kernel:0' shape=(200, 200) dtype=float32>), (None, <tf.Variable 'dense_121/bias:0' shape=(200,) dtype=float32>), (None, <tf.Variable 'dense_122/kernel:0' shape=(200, 19) dtype=float32>), (None, <tf.Variable 'dense_122/bias:0' shape=(19,) dtype=float32>), (None, <tf.Variable 'dense_123/kernel:0' shape=(200, 200) dtype=float32>), (None, <tf.Variable 'dense_123/bias:0' shape=(200,) dtype=float32>), (None, <tf.Variable 'dense_124/kernel:0' shape=(200, 230) dtype=float32>), (None, <tf.Variable 'dense_124/bias:0' shape=(230,) dtype=float32>), (None, <tf.Variable 'dense_125/kernel:0' shape=(25, 200) dtype=float32>), (None, <tf.Variable 'dense_125/bias:0' shape=(200,) dtype=float32>), (None, <tf.Variable 'dense_126/kernel:0' shape=(200, 200) dtype=float32>), (None, <tf.Variable 'dense_126/bias:0' shape=(200,) dtype=float32>), (None, <tf.Variable 'dense_127/kernel:0' shape=(200, 200) dtype=float32>), (None, <tf.Variable 'dense_127/bias:0' shape=(200,) dtype=float32>), (None, <tf.Variable 'dense_128/kernel:0' shape=(200, 40) dtype=float32>), (None, <tf.Variable 'dense_128/bias:0' shape=(40,) dtype=float32>)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p0/kb961x5d6wd79gfp2h6t_8500000gn/T/ipykernel_96573/3510247826.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 816, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/usr/local/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 532, in minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    File \"/usr/local/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 633, in apply_gradients\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n    File \"/usr/local/lib/python3.9/site-packages/keras/optimizer_v2/utils.py\", line 73, in filter_empty_gradients\n        raise ValueError(f\"No gradients provided for any variable: {variable}. \"\n\n    ValueError: No gradients provided for any variable: (['dense_119/kernel:0', 'dense_119/bias:0', 'dense_120/kernel:0', 'dense_120/bias:0', 'dense_121/kernel:0', 'dense_121/bias:0', 'dense_122/kernel:0', 'dense_122/bias:0', 'dense_123/kernel:0', 'dense_123/bias:0', 'dense_124/kernel:0', 'dense_124/bias:0', 'dense_125/kernel:0', 'dense_125/bias:0', 'dense_126/kernel:0', 'dense_126/bias:0', 'dense_127/kernel:0', 'dense_127/bias:0', 'dense_128/kernel:0', 'dense_128/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'dense_119/kernel:0' shape=(20, 200) dtype=float32>), (None, <tf.Variable 'dense_119/bias:0' shape=(200,) dtype=float32>), (None, <tf.Variable 'dense_120/kernel:0' shape=(200, 200) dtype=float32>), (None, <tf.Variable 'dense_120/bias:0' shape=(200,) dtype=float32>), (None, <tf.Variable 'dense_121/kernel:0' shape=(200, 200) dtype=float32>), (None, <tf.Variable 'dense_121/bias:0' shape=(200,) dtype=float32>), (None, <tf.Variable 'dense_122/kernel:0' shape=(200, 19) dtype=float32>), (None, <tf.Variable 'dense_122/bias:0' shape=(19,) dtype=float32>), (None, <tf.Variable 'dense_123/kernel:0' shape=(200, 200) dtype=float32>), (None, <tf.Variable 'dense_123/bias:0' shape=(200,) dtype=float32>), (None, <tf.Variable 'dense_124/kernel:0' shape=(200, 230) dtype=float32>), (None, <tf.Variable 'dense_124/bias:0' shape=(230,) dtype=float32>), (None, <tf.Variable 'dense_125/kernel:0' shape=(25, 200) dtype=float32>), (None, <tf.Variable 'dense_125/bias:0' shape=(200,) dtype=float32>), (None, <tf.Variable 'dense_126/kernel:0' shape=(200, 200) dtype=float32>), (None, <tf.Variable 'dense_126/bias:0' shape=(200,) dtype=float32>), (None, <tf.Variable 'dense_127/kernel:0' shape=(200, 200) dtype=float32>), (None, <tf.Variable 'dense_127/bias:0' shape=(200,) dtype=float32>), (None, <tf.Variable 'dense_128/kernel:0' shape=(200, 40) dtype=float32>), (None, <tf.Variable 'dense_128/bias:0' shape=(40,) dtype=float32>)).\n"
     ]
    }
   ],
   "source": [
    "for i in input_fn_csv(\"./datasets/IHDP/csv\",'.csv'):\n",
    "    break\n",
    "model = build_model()\n",
    "model.fit(\n",
    "    x = i[0],\n",
    "    y = i[0],\n",
    "    epochs = 5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
