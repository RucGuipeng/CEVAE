{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 “ihdp_npci_1-100.train.npz” 已经存在；不获取。\n",
      "\n",
      "文件 “ihdp_npci_1-100.test.npz” 已经存在；不获取。\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([5.26691518]), array([2.59847927]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.layers as tfkl\n",
    "tfd,tfpl = tfp.distributions,tfp.layers\n",
    "import tensorflow.keras.backend as tfkb\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from evaluation import *\n",
    "from cevae_networks import *\n",
    "################################################\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='')\n",
    "parser.add_argument('--scale_penalize',    type = float, default = 0.001,  help = '')\n",
    "parser.add_argument('--learning_rate',     type = float, default = 0.001,  help = '')\n",
    "parser.add_argument('--default_y_scale',   type = float, default = 1.,  help = '')\n",
    "parser.add_argument('--t_dim',     type = int, default = 1,  help = '')\n",
    "parser.add_argument('--y_dim',     type = int, default = 1,  help = '')\n",
    "parser.add_argument('--x_dim',     type = int, default = 25, help = '')\n",
    "parser.add_argument('--z_dim',     type = int, default = 20, help = '')\n",
    "parser.add_argument('--x_num_dim', type = int, default = 6,  help = '')\n",
    "parser.add_argument('--x_bin_dim', type = int, default = 19, help = '')\n",
    "parser.add_argument('--nh', type = int, default = 3, help = 'number of hidden layers')\n",
    "parser.add_argument('--h',  type = int, default = 200, help = 'number of hidden units')\n",
    "args = parser.parse_args([])\n",
    "################################################\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz \n",
    "\n",
    "def load_IHDP_data(training_data,testing_data,i):\n",
    "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "        train_data=np.load(trf); test_data=np.load(tef)\n",
    "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
    "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
    "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
    "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    "        ycf=np.concatenate((train_data['ycf'][:,i],  test_data['ycf'][:,i])).astype('float32')\n",
    "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "        data['y']=data['y'].reshape(-1,1)\n",
    "        data['ycf'] = ycf.reshape(-1,1)\n",
    "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "    return data\n",
    "\n",
    "ind = 7\n",
    "rep = 1\n",
    "data = load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.test.npz',i = ind)\n",
    "for key in data:\n",
    "    if key != 'y_scaler':\n",
    "        data[key] = np.repeat(data[key],repeats = rep, axis = 0)\n",
    "data['y_scaler'].mean_, data['y_scaler'].scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonLayer(tfkl.Layer):\n",
    "    def __init__(self):\n",
    "        super(EpsilonLayer, self).__init__()\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.epsilon = self.add_weight(name='epsilon',\n",
    "                                       shape=[1, 1],\n",
    "                                       initializer='RandomNormal',\n",
    "                                       #  initializer='ones',\n",
    "                                       trainable=True)\n",
    "        super(EpsilonLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        #note there is only one epsilon were just duplicating it for conformability\n",
    "        return self.epsilon * tf.ones_like(inputs)[:, 0:1]\n",
    "\n",
    "class CEVAE(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CEVAE, self).__init__()\n",
    "        ########################################\n",
    "        # networks\n",
    "        self.activation = 'elu'\n",
    "        # CEVAE Model \n",
    "        ## (encoder)\n",
    "        self.q_y_tx = q_y_tx(args.x_bin_dim, args.x_num_dim, args.y_dim, args.t_dim, args.nh, args.h)\n",
    "        self.q_t_x = q_t_x(args.x_bin_dim, args.x_num_dim, args.t_dim, args.nh, args.h)\n",
    "        self.q_z_txy = q_z_txy(args.x_bin_dim, args.x_num_dim, args.y_dim, args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        ## (decoder)\n",
    "        self.p_x_z = p_x_z(args.x_bin_dim, args.x_num_dim, args.z_dim, args.nh, args.h)\n",
    "        self.p_t_z = p_t_z(args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        self.p_y_tz = p_y_tz(args.y_dim, args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        self.epsilon_layer = EpsilonLayer()\n",
    "        self.beta = 1\n",
    "\n",
    "    def call(self, data, training=False):\n",
    "        if training:\n",
    "            x_train,t_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "            \n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            \n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.sample()\n",
    "            # decoder\n",
    "            ## p(x|z)\n",
    "            x_num,x_bin = self.p_x_z(z_infer_sample)\n",
    "            ## p(t|z)\n",
    "            t = self.p_t_z(z_infer_sample)\n",
    "            ## p(y|t,z)\n",
    "            y = self.p_y_tz(tf.concat([t_train,z_infer_sample],-1) )\n",
    "            epsilon = self.epsilon_layer(t_infer_sample)\n",
    "            \n",
    "            return y_infer,t_infer,z_infer,y,t,x_num,x_bin,epsilon\n",
    "        else:\n",
    "            x_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.loc\n",
    "\n",
    "            t1z = tf.concat([tf.ones_like(t_infer_sample),z_infer_sample],-1)\n",
    "            t0z = tf.concat([tf.zeros_like(t_infer_sample),z_infer_sample],-1)\n",
    "            y0 = self.p_y_tz(t0z)\n",
    "            y1 = self.p_y_tz(t1z)\n",
    "            y = [y0,y1]\n",
    "            return y,t_infer,z_infer\n",
    "\n",
    "\n",
    "    def cevae_loss(self, data, pred, training = False):\n",
    "        x_train, t_train, y_train = data[0],data[1],data[2]\n",
    "        x_train_num, x_train_bin = x_train[:,:args.x_num_dim],x_train[:,args.x_num_dim:]\n",
    "        y_infer,t_infer,z_infer,y,t,x_num,x_bin,epsilon = pred\n",
    "        y0,y1 = y_infer\n",
    "        # reconstruct loss\n",
    "        recon_x_num = tfkb.sum(x_num.log_prob(x_train_num), 1)\n",
    "        recon_x_bin = tfkb.sum(x_bin.log_prob(x_train_bin), 1)\n",
    "        recon_y = tfkb.sum(y.log_prob(y_train), 1)\n",
    "        recon_t = tfkb.sum(t.log_prob(t_train), 1)\n",
    "        # kl loss\n",
    "        z_infer_sample = z_infer.sample()\n",
    "        z = tfd.Normal(loc = [0] * 20, scale = [1]*20)\n",
    "        kl_z = tfkb.sum((z.log_prob(z_infer_sample) - z_infer.log_prob(z_infer_sample)), -1)\n",
    "        # aux loss\n",
    "        aux_y = tfkb.sum(y0.log_prob(y_train)*(1-t_train) + y1.log_prob(y_train)* t_train, 1)\n",
    "        aux_t = tfkb.sum(t_infer.log_prob(t_train), 1)\n",
    "        loss = -tfkb.mean(recon_x_bin + recon_x_num + recon_y + recon_t + aux_y + aux_t + kl_z)\n",
    "\n",
    "        # target regularization\n",
    "        y_pred = y0.loc * (1-t_train) + y1.loc * t_train\n",
    "        t_pred = tf.math.sigmoid(t.logits)\n",
    "        cc = t_train/t_pred - (1-t_train) / (1-t_pred)\n",
    "        t_reg = tf.math.square(y_pred + epsilon * cc - y_train)\n",
    "        loss += tfkb.mean(t_reg) * self.beta\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, data):\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "            loss = self.cevae_loss(data = data, pred = pred, training = True)\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        metrics = {\"loss\": loss}\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "        y_infer = pred[0]\n",
    "        loss = self.cevae_loss(data = data, pred = pred, training = False)\n",
    "        y0, y1 = y_infer[0].sample(),y_infer[1].sample()\n",
    "        ate = tfkb.mean(y1) - tfkb.mean(y0)\n",
    "        metrics = {\"loss\":loss,\"y0\": tfkb.mean(y0),\"y1\": tfkb.mean(y1),'ate_afte_scaled': ate}\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-24 21:52:42.151796: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/3 [=========>....................] - ETA: 11s - loss: 30.1364 — ite: 4.7124  — ate: 3.7950 — pehe: 5.6014 \n",
      "3/3 [==============================] - 7s 675ms/step - loss: 30.0015 - val_loss: 30.6958 - val_y0: -0.1441 - val_y1: -0.0386 - val_ate_afte_scaled: 0.1055 - lr: 5.0000e-05\n",
      "Epoch 2/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 30.3288 — ite: 4.6736  — ate: 3.7796 — pehe: 5.3685 \n",
      "3/3 [==============================] - 1s 288ms/step - loss: 29.8698 - val_loss: 31.0439 - val_y0: -0.0499 - val_y1: 0.0705 - val_ate_afte_scaled: 0.1204 - lr: 5.0000e-05\n",
      "Epoch 3/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 29.4329WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0147s vs `on_train_batch_end` time: 0.0185s). Check your callbacks.\n",
      " — ite: 4.5933  — ate: 3.6626 — pehe: 5.4606 \n",
      "3/3 [==============================] - 1s 289ms/step - loss: 29.5210 - val_loss: 30.8767 - val_y0: -0.0591 - val_y1: 0.2130 - val_ate_afte_scaled: 0.2721 - lr: 5.0000e-05\n",
      "Epoch 4/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 29.4994 — ite: 4.6433  — ate: 3.6748 — pehe: 5.3437 \n",
      "3/3 [==============================] - 1s 259ms/step - loss: 29.5097 - val_loss: 30.5513 - val_y0: -0.1253 - val_y1: 0.2317 - val_ate_afte_scaled: 0.3570 - lr: 5.0000e-05\n",
      "Epoch 5/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 29.3865 — ite: 4.5826  — ate: 3.5985 — pehe: 5.3538 \n",
      "3/3 [==============================] - 1s 253ms/step - loss: 29.1481 - val_loss: 30.3301 - val_y0: -0.3193 - val_y1: 0.2116 - val_ate_afte_scaled: 0.5309 - lr: 5.0000e-05\n",
      "Epoch 6/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.7174 — ite: 4.6295  — ate: 3.4952 — pehe: 5.3377 \n",
      "3/3 [==============================] - 1s 264ms/step - loss: 28.8765 - val_loss: 29.8262 - val_y0: -0.1954 - val_y1: 0.3750 - val_ate_afte_scaled: 0.5704 - lr: 5.0000e-05\n",
      "Epoch 7/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.8480 — ite: 4.4990  — ate: 3.3002 — pehe: 4.9999 \n",
      "3/3 [==============================] - 1s 252ms/step - loss: 28.7290 - val_loss: 29.2655 - val_y0: -0.2449 - val_y1: 0.4112 - val_ate_afte_scaled: 0.6562 - lr: 5.0000e-05\n",
      "Epoch 8/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.9170 — ite: 4.6453  — ate: 3.5871 — pehe: 5.3701 \n",
      "3/3 [==============================] - 1s 326ms/step - loss: 28.4520 - val_loss: 29.2666 - val_y0: -0.0449 - val_y1: 0.5248 - val_ate_afte_scaled: 0.5697 - lr: 5.0000e-05\n",
      "Epoch 9/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.5645 — ite: 4.5548  — ate: 3.4537 — pehe: 5.2112 \n",
      "3/3 [==============================] - 0s 237ms/step - loss: 28.2337 - val_loss: 29.2103 - val_y0: -0.1693 - val_y1: 0.6527 - val_ate_afte_scaled: 0.8220 - lr: 5.0000e-05\n",
      "Epoch 10/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.7991 — ite: 4.6331  — ate: 3.4582 — pehe: 5.2892 \n",
      "3/3 [==============================] - 0s 230ms/step - loss: 27.9782 - val_loss: 28.7595 - val_y0: -0.2331 - val_y1: 0.5954 - val_ate_afte_scaled: 0.8285 - lr: 5.0000e-05\n",
      "Epoch 11/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.3237 — ite: 4.6756  — ate: 3.5061 — pehe: 5.4554 \n",
      "3/3 [==============================] - 0s 234ms/step - loss: 27.8988 - val_loss: 28.6777 - val_y0: -0.0409 - val_y1: 0.6442 - val_ate_afte_scaled: 0.6851 - lr: 5.0000e-05\n",
      "Epoch 12/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.8425 — ite: 4.5632  — ate: 3.3278 — pehe: 5.2422 \n",
      "3/3 [==============================] - 0s 229ms/step - loss: 27.8093 - val_loss: 28.3208 - val_y0: -0.2025 - val_y1: 0.6706 - val_ate_afte_scaled: 0.8731 - lr: 5.0000e-05\n",
      "Epoch 13/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.6683 — ite: 4.5839  — ate: 3.4659 — pehe: 5.3455 \n",
      "3/3 [==============================] - 0s 231ms/step - loss: 27.6011 - val_loss: 28.4712 - val_y0: -0.2638 - val_y1: 0.8379 - val_ate_afte_scaled: 1.1017 - lr: 5.0000e-05\n",
      "Epoch 14/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.5338 — ite: 4.5177  — ate: 3.2196 — pehe: 5.1562 \n",
      "3/3 [==============================] - 0s 229ms/step - loss: 27.2830 - val_loss: 28.1095 - val_y0: -0.1909 - val_y1: 0.7798 - val_ate_afte_scaled: 0.9707 - lr: 5.0000e-05\n",
      "Epoch 15/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.5751 — ite: 4.3731  — ate: 3.0019 — pehe: 4.9347 \n",
      "3/3 [==============================] - 0s 233ms/step - loss: 27.0694 - val_loss: 27.9700 - val_y0: -0.1327 - val_y1: 0.8590 - val_ate_afte_scaled: 0.9917 - lr: 5.0000e-05\n",
      "Epoch 16/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.4432 — ite: 4.3752  — ate: 2.6999 — pehe: 4.6933 \n",
      "3/3 [==============================] - 0s 244ms/step - loss: 26.8501 - val_loss: 27.8533 - val_y0: -0.0620 - val_y1: 0.9215 - val_ate_afte_scaled: 0.9835 - lr: 5.0000e-05\n",
      "Epoch 17/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 26.8657 — ite: 4.3709  — ate: 2.8067 — pehe: 4.9468 \n",
      "3/3 [==============================] - 0s 240ms/step - loss: 26.7042 - val_loss: 27.5226 - val_y0: -0.2586 - val_y1: 0.9033 - val_ate_afte_scaled: 1.1619 - lr: 5.0000e-05\n",
      "Epoch 18/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 26.8745 — ite: 4.3659  — ate: 2.9808 — pehe: 4.8633 \n",
      "3/3 [==============================] - 0s 229ms/step - loss: 26.5683 - val_loss: 27.3210 - val_y0: -0.1858 - val_y1: 0.9593 - val_ate_afte_scaled: 1.1451 - lr: 5.0000e-05\n",
      "Epoch 19/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 26.6298 — ite: 4.3362  — ate: 2.7882 — pehe: 4.7942 \n",
      "3/3 [==============================] - 0s 218ms/step - loss: 26.3733 - val_loss: 27.1633 - val_y0: -0.1638 - val_y1: 0.9423 - val_ate_afte_scaled: 1.1061 - lr: 5.0000e-05\n",
      "Epoch 20/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 26.5598 — ite: 4.3174  — ate: 2.4959 — pehe: 4.6833 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: 26.4377 - val_loss: 27.0167 - val_y0: -0.2978 - val_y1: 1.1330 - val_ate_afte_scaled: 1.4308 - lr: 5.0000e-05\n",
      "Epoch 21/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.8988 — ite: 4.3694  — ate: 2.7283 — pehe: 4.9115 \n",
      "3/3 [==============================] - 0s 205ms/step - loss: 25.9584 - val_loss: 26.6693 - val_y0: -0.3175 - val_y1: 0.8712 - val_ate_afte_scaled: 1.1888 - lr: 5.0000e-05\n",
      "Epoch 22/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.9282 — ite: 4.2482  — ate: 2.5871 — pehe: 4.8272 \n",
      "3/3 [==============================] - 0s 207ms/step - loss: 26.1058 - val_loss: 26.2618 - val_y0: -0.2174 - val_y1: 1.0527 - val_ate_afte_scaled: 1.2701 - lr: 5.0000e-05\n",
      "Epoch 23/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.8078 — ite: 4.2601  — ate: 2.6538 — pehe: 4.8062 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: 25.7158 - val_loss: 26.2515 - val_y0: -0.4400 - val_y1: 0.9886 - val_ate_afte_scaled: 1.4286 - lr: 5.0000e-05\n",
      "Epoch 24/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.5617 — ite: 4.2480  — ate: 2.1675 — pehe: 4.6130 \n",
      "3/3 [==============================] - 0s 204ms/step - loss: 25.4695 - val_loss: 26.3993 - val_y0: -0.0657 - val_y1: 1.0234 - val_ate_afte_scaled: 1.0891 - lr: 5.0000e-05\n",
      "Epoch 25/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.5166 — ite: 4.1664  — ate: 2.0024 — pehe: 4.4828 \n",
      "3/3 [==============================] - 0s 207ms/step - loss: 25.4349 - val_loss: 26.1801 - val_y0: -0.2010 - val_y1: 1.1881 - val_ate_afte_scaled: 1.3890 - lr: 5.0000e-05\n",
      "Epoch 26/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.4086 — ite: 4.0382  — ate: 1.9888 — pehe: 4.2795 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: 25.1358 - val_loss: 26.2043 - val_y0: -0.2622 - val_y1: 0.9867 - val_ate_afte_scaled: 1.2488 - lr: 5.0000e-05\n",
      "Epoch 27/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.0221 — ite: 4.0792  — ate: 1.7847 — pehe: 4.4482 \n",
      "3/3 [==============================] - 0s 227ms/step - loss: 25.0717 - val_loss: 25.5083 - val_y0: -0.2831 - val_y1: 0.9433 - val_ate_afte_scaled: 1.2264 - lr: 5.0000e-05\n",
      "Epoch 28/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.6760 — ite: 4.0057  — ate: 1.9385 — pehe: 4.2832 \n",
      "3/3 [==============================] - 0s 226ms/step - loss: 24.7910 - val_loss: 25.6053 - val_y0: -0.2757 - val_y1: 1.0891 - val_ate_afte_scaled: 1.3648 - lr: 5.0000e-05\n",
      "Epoch 29/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.9858 — ite: 4.0834  — ate: 1.8075 — pehe: 4.4438 \n",
      "3/3 [==============================] - 0s 226ms/step - loss: 24.7440 - val_loss: 25.4034 - val_y0: -0.1599 - val_y1: 0.9020 - val_ate_afte_scaled: 1.0619 - lr: 5.0000e-05\n",
      "Epoch 30/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.8610 — ite: 3.9362  — ate: 1.5346 — pehe: 4.2519 \n",
      "3/3 [==============================] - 0s 213ms/step - loss: 24.5541 - val_loss: 25.1512 - val_y0: -0.2510 - val_y1: 1.0531 - val_ate_afte_scaled: 1.3042 - lr: 5.0000e-05\n",
      "Epoch 31/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.5779 — ite: 3.9656  — ate: 1.6778 — pehe: 4.2922 \n",
      "3/3 [==============================] - 0s 205ms/step - loss: 24.3725 - val_loss: 25.2147 - val_y0: -0.1746 - val_y1: 1.0840 - val_ate_afte_scaled: 1.2587 - lr: 5.0000e-05\n",
      "Epoch 32/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.1539 — ite: 3.8160  — ate: 1.3133 — pehe: 4.1702 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: 24.2647 - val_loss: 24.6190 - val_y0: -0.1621 - val_y1: 0.9917 - val_ate_afte_scaled: 1.1538 - lr: 5.0000e-05\n",
      "Epoch 33/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.3831 — ite: 3.8082  — ate: 1.3175 — pehe: 4.1155 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: 24.0732 - val_loss: 24.2090 - val_y0: -0.2622 - val_y1: 0.8974 - val_ate_afte_scaled: 1.1596 - lr: 5.0000e-05\n",
      "Epoch 34/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.1536 — ite: 3.8623  — ate: 1.1942 — pehe: 4.1042 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: 23.7692 - val_loss: 24.9489 - val_y0: -0.2698 - val_y1: 0.9982 - val_ate_afte_scaled: 1.2680 - lr: 5.0000e-05\n",
      "Epoch 35/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.6997 — ite: 3.7770  — ate: 0.9126 — pehe: 4.0986 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: 23.5509 - val_loss: 24.4501 - val_y0: -0.1415 - val_y1: 0.8474 - val_ate_afte_scaled: 0.9889 - lr: 5.0000e-05\n",
      "Epoch 36/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.3164 — ite: 3.7788  — ate: 0.7874 — pehe: 4.0675 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: 23.4441 - val_loss: 24.1682 - val_y0: -0.0920 - val_y1: 0.9901 - val_ate_afte_scaled: 1.0821 - lr: 5.0000e-05\n",
      "Epoch 37/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.9844 — ite: 3.7704  — ate: 0.9103 — pehe: 4.1348 \n",
      "3/3 [==============================] - 1s 248ms/step - loss: 23.6431 - val_loss: 24.1840 - val_y0: -0.2605 - val_y1: 1.0473 - val_ate_afte_scaled: 1.3078 - lr: 5.0000e-05\n",
      "Epoch 38/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.5259 — ite: 3.8108  — ate: 0.9559 — pehe: 4.0737 \n",
      "3/3 [==============================] - 1s 242ms/step - loss: 23.2772 - val_loss: 23.5754 - val_y0: -0.3071 - val_y1: 1.0368 - val_ate_afte_scaled: 1.3438 - lr: 5.0000e-05\n",
      "Epoch 39/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.8771 — ite: 3.7563  — ate: 0.9338 — pehe: 3.9801 \n",
      "3/3 [==============================] - 0s 240ms/step - loss: 22.9890 - val_loss: 23.5385 - val_y0: -0.2693 - val_y1: 1.0075 - val_ate_afte_scaled: 1.2768 - lr: 5.0000e-05\n",
      "Epoch 40/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.0374 — ite: 3.8455  — ate: 0.7479 — pehe: 4.1279 \n",
      "3/3 [==============================] - 0s 202ms/step - loss: 22.8216 - val_loss: 23.3966 - val_y0: -0.2399 - val_y1: 0.8707 - val_ate_afte_scaled: 1.1106 - lr: 5.0000e-05\n",
      "Epoch 41/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.6578 — ite: 3.8842  — ate: 0.9568 — pehe: 4.1566 \n",
      "3/3 [==============================] - 0s 207ms/step - loss: 22.7540 - val_loss: 23.1387 - val_y0: -0.1622 - val_y1: 0.9236 - val_ate_afte_scaled: 1.0858 - lr: 5.0000e-05\n",
      "Epoch 42/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.0947 — ite: 3.8457  — ate: 0.7520 — pehe: 4.0114 \n",
      "3/3 [==============================] - 0s 206ms/step - loss: 22.7925 - val_loss: 23.3951 - val_y0: -0.2896 - val_y1: 0.8757 - val_ate_afte_scaled: 1.1654 - lr: 5.0000e-05\n",
      "Epoch 43/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.5104 — ite: 3.8020  — ate: 0.9953 — pehe: 4.2104 \n",
      "3/3 [==============================] - 0s 214ms/step - loss: 22.5804 - val_loss: 22.5767 - val_y0: -0.1994 - val_y1: 0.9472 - val_ate_afte_scaled: 1.1465 - lr: 5.0000e-05\n",
      "Epoch 44/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.3387 — ite: 3.7302  — ate: 0.7785 — pehe: 3.9776 \n",
      "3/3 [==============================] - 0s 205ms/step - loss: 22.1284 - val_loss: 22.9797 - val_y0: -0.0452 - val_y1: 1.0773 - val_ate_afte_scaled: 1.1225 - lr: 5.0000e-05\n",
      "Epoch 45/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.1253 — ite: 3.8656  — ate: 0.9933 — pehe: 4.1261 \n",
      "3/3 [==============================] - 0s 210ms/step - loss: 22.0205 - val_loss: 22.2256 - val_y0: -0.2884 - val_y1: 0.8462 - val_ate_afte_scaled: 1.1347 - lr: 5.0000e-05\n",
      "Epoch 46/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.3210 — ite: 3.6247  — ate: 0.7711 — pehe: 3.8713 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: 21.9648 - val_loss: 22.2033 - val_y0: -0.2871 - val_y1: 0.9239 - val_ate_afte_scaled: 1.2110 - lr: 5.0000e-05\n",
      "Epoch 47/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.8204 — ite: 3.7471  — ate: 0.7668 — pehe: 3.9670 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: 21.8404 - val_loss: 21.9900 - val_y0: -0.0928 - val_y1: 1.0018 - val_ate_afte_scaled: 1.0946 - lr: 5.0000e-05\n",
      "Epoch 48/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.3299 — ite: 3.8220  — ate: 0.9769 — pehe: 4.0951 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: 21.5383 - val_loss: 22.3990 - val_y0: -0.2686 - val_y1: 0.9859 - val_ate_afte_scaled: 1.2545 - lr: 5.0000e-05\n",
      "Epoch 49/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.9908 — ite: 3.7584  — ate: 0.6670 — pehe: 4.0229 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: 21.3008 - val_loss: 22.0497 - val_y0: -0.2228 - val_y1: 0.9652 - val_ate_afte_scaled: 1.1880 - lr: 5.0000e-05\n",
      "Epoch 50/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.7982 — ite: 3.7871  — ate: 0.8509 — pehe: 3.9571 \n",
      "3/3 [==============================] - 0s 210ms/step - loss: 21.2897 - val_loss: 21.9049 - val_y0: -0.2403 - val_y1: 0.9239 - val_ate_afte_scaled: 1.1642 - lr: 5.0000e-05\n",
      "Epoch 51/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.1922 — ite: 3.8066  — ate: 0.6503 — pehe: 4.0631 \n",
      "3/3 [==============================] - 0s 210ms/step - loss: 21.2072 - val_loss: 21.5049 - val_y0: -0.1339 - val_y1: 0.9654 - val_ate_afte_scaled: 1.0993 - lr: 5.0000e-05\n",
      "Epoch 52/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.1011 — ite: 3.7668  — ate: 0.8399 — pehe: 4.0687 \n",
      "3/3 [==============================] - 0s 221ms/step - loss: 21.0328 - val_loss: 21.8196 - val_y0: -0.2767 - val_y1: 0.8726 - val_ate_afte_scaled: 1.1494 - lr: 5.0000e-05\n",
      "Epoch 53/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.0447 — ite: 3.8132  — ate: 0.6569 — pehe: 4.0210 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: 20.9889 - val_loss: 21.4560 - val_y0: -0.2611 - val_y1: 0.9598 - val_ate_afte_scaled: 1.2208 - lr: 5.0000e-05\n",
      "Epoch 54/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.2684 — ite: 3.7304  — ate: 0.7226 — pehe: 4.0896 \n",
      "3/3 [==============================] - 0s 207ms/step - loss: 20.8784 - val_loss: 21.1349 - val_y0: -0.1956 - val_y1: 1.1430 - val_ate_afte_scaled: 1.3386 - lr: 5.0000e-05\n",
      "Epoch 55/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.2718 — ite: 3.7691  — ate: 0.8875 — pehe: 3.9589 \n",
      "3/3 [==============================] - 0s 206ms/step - loss: 20.7222 - val_loss: 21.0823 - val_y0: -0.3598 - val_y1: 0.8910 - val_ate_afte_scaled: 1.2508 - lr: 5.0000e-05\n",
      "Epoch 56/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.4450 — ite: 3.7332  — ate: 0.6763 — pehe: 4.0892 \n",
      "3/3 [==============================] - 0s 210ms/step - loss: 20.4426 - val_loss: 20.9896 - val_y0: -0.0966 - val_y1: 0.8178 - val_ate_afte_scaled: 0.9145 - lr: 5.0000e-05\n",
      "Epoch 57/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.5989 — ite: 3.7531  — ate: 0.7673 — pehe: 4.0762 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: 20.4187 - val_loss: 20.5534 - val_y0: -0.1501 - val_y1: 0.8612 - val_ate_afte_scaled: 1.0113 - lr: 5.0000e-05\n",
      "Epoch 58/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.1199 — ite: 3.7677  — ate: 0.7193 — pehe: 4.0662 \n",
      "3/3 [==============================] - 0s 218ms/step - loss: 20.4001 - val_loss: 21.0647 - val_y0: -0.1536 - val_y1: 0.9814 - val_ate_afte_scaled: 1.1349 - lr: 5.0000e-05\n",
      "Epoch 59/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.4355 — ite: 3.8211  — ate: 0.7142 — pehe: 3.9757 \n",
      "3/3 [==============================] - 0s 222ms/step - loss: 20.0837 - val_loss: 21.0619 - val_y0: -0.2267 - val_y1: 0.9296 - val_ate_afte_scaled: 1.1563 - lr: 5.0000e-05\n",
      "Epoch 60/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.7863 — ite: 3.7392  — ate: 0.6273 — pehe: 4.1391 \n",
      "3/3 [==============================] - 0s 206ms/step - loss: 19.8680 - val_loss: 21.0056 - val_y0: -0.1763 - val_y1: 0.9408 - val_ate_afte_scaled: 1.1171 - lr: 5.0000e-05\n",
      "Epoch 61/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.8538 — ite: 3.7878  — ate: 0.9581 — pehe: 3.9652 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: 19.9623 - val_loss: 20.4034 - val_y0: -0.0970 - val_y1: 0.9451 - val_ate_afte_scaled: 1.0420 - lr: 5.0000e-05\n",
      "Epoch 62/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.5243 — ite: 3.6785  — ate: 0.5160 — pehe: 3.9330 \n",
      "3/3 [==============================] - 0s 218ms/step - loss: 19.6220 - val_loss: 20.4070 - val_y0: -0.2027 - val_y1: 1.1084 - val_ate_afte_scaled: 1.3111 - lr: 5.0000e-05\n",
      "Epoch 63/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.3975 — ite: 3.7272  — ate: 0.8242 — pehe: 3.8679 \n",
      "3/3 [==============================] - 0s 217ms/step - loss: 19.6855 - val_loss: 20.3095 - val_y0: -0.2455 - val_y1: 0.9824 - val_ate_afte_scaled: 1.2279 - lr: 5.0000e-05\n",
      "Epoch 64/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.5535 — ite: 3.7611  — ate: 0.7175 — pehe: 4.0056 \n",
      "3/3 [==============================] - 0s 219ms/step - loss: 19.6170 - val_loss: 20.2103 - val_y0: -0.1547 - val_y1: 1.0678 - val_ate_afte_scaled: 1.2225 - lr: 5.0000e-05\n",
      "Epoch 65/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.8498 — ite: 3.7969  — ate: 0.6933 — pehe: 4.0602 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: 19.2702 - val_loss: 19.6758 - val_y0: -0.1935 - val_y1: 0.9115 - val_ate_afte_scaled: 1.1050 - lr: 5.0000e-05\n",
      "Epoch 66/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.5774 — ite: 3.8150  — ate: 0.7870 — pehe: 4.1759 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: 19.2275 - val_loss: 19.8989 - val_y0: -0.1659 - val_y1: 0.9068 - val_ate_afte_scaled: 1.0728 - lr: 5.0000e-05\n",
      "Epoch 67/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.1023 — ite: 3.8006  — ate: 0.8545 — pehe: 4.1432 \n",
      "3/3 [==============================] - 0s 228ms/step - loss: 19.5160 - val_loss: 19.8245 - val_y0: -0.1038 - val_y1: 0.9321 - val_ate_afte_scaled: 1.0359 - lr: 5.0000e-05\n",
      "Epoch 68/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.5523 — ite: 3.7023  — ate: 0.5796 — pehe: 3.8997 \n",
      "3/3 [==============================] - 0s 213ms/step - loss: 19.0354 - val_loss: 20.4963 - val_y0: -0.2037 - val_y1: 0.9415 - val_ate_afte_scaled: 1.1451 - lr: 5.0000e-05\n",
      "Epoch 69/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.8917 — ite: 3.7987  — ate: 0.9245 — pehe: 4.1033 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: 19.0458 - val_loss: 19.4806 - val_y0: -0.1384 - val_y1: 0.9821 - val_ate_afte_scaled: 1.1205 - lr: 5.0000e-05\n",
      "Epoch 70/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.2742 — ite: 3.7341  — ate: 0.7462 — pehe: 3.9329 \n",
      "3/3 [==============================] - 0s 204ms/step - loss: 19.0227 - val_loss: 19.4432 - val_y0: -0.1950 - val_y1: 0.9779 - val_ate_afte_scaled: 1.1728 - lr: 5.0000e-05\n",
      "Epoch 71/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.4416 — ite: 3.7006  — ate: 1.0521 — pehe: 3.8551 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: 18.5942 - val_loss: 19.6548 - val_y0: -0.1495 - val_y1: 1.0388 - val_ate_afte_scaled: 1.1883 - lr: 5.0000e-05\n",
      "Epoch 72/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.2420 — ite: 3.7562  — ate: 0.6076 — pehe: 4.0158 \n",
      "3/3 [==============================] - 0s 207ms/step - loss: 18.1639 - val_loss: 18.9198 - val_y0: -0.1985 - val_y1: 0.9806 - val_ate_afte_scaled: 1.1791 - lr: 5.0000e-05\n",
      "Epoch 73/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.9776 — ite: 3.8181  — ate: 0.6747 — pehe: 4.1741 \n",
      "3/3 [==============================] - 0s 218ms/step - loss: 18.2261 - val_loss: 18.7790 - val_y0: -0.2405 - val_y1: 0.9860 - val_ate_afte_scaled: 1.2264 - lr: 5.0000e-05\n",
      "Epoch 74/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.8607 — ite: 3.7661  — ate: 0.4572 — pehe: 3.9918 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: 18.1170 - val_loss: 18.6645 - val_y0: -0.2334 - val_y1: 0.9714 - val_ate_afte_scaled: 1.2048 - lr: 5.0000e-05\n",
      "Epoch 75/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.0386 — ite: 3.7678  — ate: 1.0064 — pehe: 3.9981 \n",
      "3/3 [==============================] - 0s 205ms/step - loss: 18.2588 - val_loss: 18.4713 - val_y0: -0.3899 - val_y1: 1.0017 - val_ate_afte_scaled: 1.3916 - lr: 5.0000e-05\n",
      "Epoch 76/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.2429 — ite: 3.7013  — ate: 0.3778 — pehe: 3.8029 \n",
      "3/3 [==============================] - 0s 227ms/step - loss: 17.7483 - val_loss: 18.9012 - val_y0: -0.4595 - val_y1: 1.0117 - val_ate_afte_scaled: 1.4711 - lr: 5.0000e-05\n",
      "Epoch 77/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.2661 — ite: 3.8851  — ate: 0.7606 — pehe: 4.2903 \n",
      "3/3 [==============================] - 0s 235ms/step - loss: 17.7969 - val_loss: 18.6206 - val_y0: -0.1646 - val_y1: 0.9564 - val_ate_afte_scaled: 1.1210 - lr: 5.0000e-05\n",
      "Epoch 78/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.1329 — ite: 3.8652  — ate: 0.7266 — pehe: 4.1163 \n",
      "3/3 [==============================] - 1s 247ms/step - loss: 17.3640 - val_loss: 18.2945 - val_y0: -0.3652 - val_y1: 1.1001 - val_ate_afte_scaled: 1.4653 - lr: 5.0000e-05\n",
      "Epoch 79/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.2407 — ite: 3.8723  — ate: 0.7497 — pehe: 4.1582 \n",
      "3/3 [==============================] - 0s 206ms/step - loss: 17.6188 - val_loss: 18.1374 - val_y0: -0.2100 - val_y1: 0.9291 - val_ate_afte_scaled: 1.1391 - lr: 5.0000e-05\n",
      "Epoch 80/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17.0347 — ite: 3.7690  — ate: 0.7003 — pehe: 3.9185 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: 17.3050 - val_loss: 17.8574 - val_y0: -0.1242 - val_y1: 0.9055 - val_ate_afte_scaled: 1.0297 - lr: 5.0000e-05\n",
      "Epoch 81/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17.1858 — ite: 3.8786  — ate: 0.6751 — pehe: 4.2674 \n",
      "3/3 [==============================] - 0s 207ms/step - loss: 17.0517 - val_loss: 17.8367 - val_y0: -0.1082 - val_y1: 1.1083 - val_ate_afte_scaled: 1.2165 - lr: 5.0000e-05\n",
      "Epoch 82/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17.1492 — ite: 3.8123  — ate: 0.9506 — pehe: 4.1455 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: 16.7218 - val_loss: 17.5844 - val_y0: -0.2739 - val_y1: 0.8910 - val_ate_afte_scaled: 1.1649 - lr: 5.0000e-05\n",
      "Epoch 83/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15.9500 — ite: 3.8125  — ate: 0.8369 — pehe: 4.0092 \n",
      "3/3 [==============================] - 0s 218ms/step - loss: 16.9390 - val_loss: 16.9545 - val_y0: -0.3278 - val_y1: 1.0685 - val_ate_afte_scaled: 1.3963 - lr: 5.0000e-05\n",
      "Epoch 84/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16.6217 — ite: 3.9024  — ate: 0.9188 — pehe: 4.1592 \n",
      "3/3 [==============================] - 0s 218ms/step - loss: 16.6084 - val_loss: 16.7084 - val_y0: -0.2191 - val_y1: 1.0372 - val_ate_afte_scaled: 1.2564 - lr: 5.0000e-05\n",
      "Epoch 85/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15.6204 — ite: 3.7112  — ate: 0.5238 — pehe: 3.9724 \n",
      "3/3 [==============================] - 0s 224ms/step - loss: 15.6859 - val_loss: 16.7102 - val_y0: -0.2419 - val_y1: 0.9285 - val_ate_afte_scaled: 1.1705 - lr: 5.0000e-05\n",
      "Epoch 86/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15.7395 — ite: 3.8223  — ate: 0.7386 — pehe: 4.2670 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: 15.7910 - val_loss: 16.5334 - val_y0: -0.1953 - val_y1: 0.9723 - val_ate_afte_scaled: 1.1676 - lr: 5.0000e-05\n",
      "Epoch 87/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15.6811 — ite: 3.8565  — ate: 0.7844 — pehe: 4.1716 \n",
      "3/3 [==============================] - 0s 210ms/step - loss: 15.6671 - val_loss: 16.5732 - val_y0: -0.1986 - val_y1: 0.8982 - val_ate_afte_scaled: 1.0967 - lr: 5.0000e-05\n",
      "Epoch 88/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15.8080 — ite: 3.7847  — ate: 0.6279 — pehe: 4.0678 \n",
      "3/3 [==============================] - 0s 224ms/step - loss: 15.6627 - val_loss: 15.7290 - val_y0: -0.1942 - val_y1: 1.0477 - val_ate_afte_scaled: 1.2419 - lr: 5.0000e-05\n",
      "Epoch 89/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14.4342 — ite: 3.8097  — ate: 0.8594 — pehe: 4.0197 \n",
      "3/3 [==============================] - 0s 224ms/step - loss: 15.0236 - val_loss: 15.2840 - val_y0: -0.4577 - val_y1: 0.9105 - val_ate_afte_scaled: 1.3682 - lr: 5.0000e-05\n",
      "Epoch 90/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14.5301 — ite: 3.8096  — ate: 0.5322 — pehe: 4.0302 \n",
      "3/3 [==============================] - 0s 223ms/step - loss: 15.0041 - val_loss: 15.2669 - val_y0: -0.1545 - val_y1: 1.0309 - val_ate_afte_scaled: 1.1853 - lr: 5.0000e-05\n",
      "Epoch 91/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14.3540 — ite: 3.8146  — ate: 0.5050 — pehe: 4.0992 \n",
      "3/3 [==============================] - 0s 241ms/step - loss: 14.4744 - val_loss: 15.1395 - val_y0: -0.1392 - val_y1: 0.9086 - val_ate_afte_scaled: 1.0478 - lr: 5.0000e-05\n",
      "Epoch 92/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14.8510 — ite: 3.8255  — ate: 0.9589 — pehe: 4.1515 \n",
      "3/3 [==============================] - 0s 231ms/step - loss: 14.4585 - val_loss: 15.1027 - val_y0: -0.2456 - val_y1: 1.0691 - val_ate_afte_scaled: 1.3147 - lr: 5.0000e-05\n",
      "Epoch 93/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14.0488 — ite: 3.7541  — ate: 0.6402 — pehe: 3.9808 \n",
      "3/3 [==============================] - 0s 229ms/step - loss: 13.6741 - val_loss: 14.4435 - val_y0: -0.2717 - val_y1: 1.0709 - val_ate_afte_scaled: 1.3426 - lr: 5.0000e-05\n",
      "Epoch 94/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 12.8853 — ite: 3.8818  — ate: 0.7625 — pehe: 4.1835 \n",
      "3/3 [==============================] - 0s 230ms/step - loss: 13.5309 - val_loss: 14.3912 - val_y0: -0.2612 - val_y1: 0.9380 - val_ate_afte_scaled: 1.1992 - lr: 5.0000e-05\n",
      "Epoch 95/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13.4528 — ite: 3.7687  — ate: 0.5567 — pehe: 3.9256 \n",
      "3/3 [==============================] - 0s 235ms/step - loss: 13.2808 - val_loss: 14.0853 - val_y0: -0.3360 - val_y1: 1.0525 - val_ate_afte_scaled: 1.3885 - lr: 5.0000e-05\n",
      "Epoch 96/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14.2400 — ite: 3.8056  — ate: 0.8958 — pehe: 4.2050 \n",
      "3/3 [==============================] - 1s 278ms/step - loss: 12.3081 - val_loss: 13.4921 - val_y0: -0.2141 - val_y1: 1.0824 - val_ate_afte_scaled: 1.2965 - lr: 5.0000e-05\n",
      "Epoch 97/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10.7548 — ite: 3.7461  — ate: 0.8413 — pehe: 4.0746 \n",
      "3/3 [==============================] - 1s 366ms/step - loss: 12.3897 - val_loss: 12.5064 - val_y0: -0.2418 - val_y1: 1.0049 - val_ate_afte_scaled: 1.2467 - lr: 5.0000e-05\n",
      "Epoch 98/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11.8072 — ite: 3.8420  — ate: 0.6332 — pehe: 4.0342 \n",
      "3/3 [==============================] - 1s 347ms/step - loss: 12.1259 - val_loss: 12.0860 - val_y0: -0.2675 - val_y1: 1.0643 - val_ate_afte_scaled: 1.3317 - lr: 5.0000e-05\n",
      "Epoch 99/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 11.6200 — ite: 3.7729  — ate: 0.7502 — pehe: 4.0398 \n",
      "3/3 [==============================] - 1s 359ms/step - loss: 11.3642 - val_loss: 12.1368 - val_y0: -0.2853 - val_y1: 0.8476 - val_ate_afte_scaled: 1.1329 - lr: 5.0000e-05\n",
      "Epoch 100/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 10.7754 — ite: 3.8282  — ate: 0.8657 — pehe: 4.1044 \n",
      "3/3 [==============================] - 1s 325ms/step - loss: 10.4568 - val_loss: 11.0694 - val_y0: -0.2837 - val_y1: 0.9371 - val_ate_afte_scaled: 1.2208 - lr: 5.0000e-05\n",
      "Epoch 101/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 10.4238 — ite: 3.8167  — ate: 0.8150 — pehe: 4.1595 \n",
      "3/3 [==============================] - 1s 422ms/step - loss: 10.5828 - val_loss: 10.9183 - val_y0: -0.2355 - val_y1: 0.8384 - val_ate_afte_scaled: 1.0739 - lr: 5.0000e-05\n",
      "Epoch 102/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.6648 — ite: 3.8222  — ate: 0.6349 — pehe: 3.8989 \n",
      "3/3 [==============================] - 1s 279ms/step - loss: 9.6107 - val_loss: 10.3326 - val_y0: -0.3386 - val_y1: 1.0454 - val_ate_afte_scaled: 1.3840 - lr: 5.0000e-05\n",
      "Epoch 103/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 9.1711 — ite: 3.9125  — ate: 0.6917 — pehe: 4.1691 \n",
      "3/3 [==============================] - 1s 259ms/step - loss: 9.0258 - val_loss: 9.2712 - val_y0: -0.1367 - val_y1: 1.0466 - val_ate_afte_scaled: 1.1833 - lr: 5.0000e-05\n",
      "Epoch 104/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 9.1478 — ite: 3.8382  — ate: 0.3937 — pehe: 4.0685 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: 9.2304 - val_loss: 8.3788 - val_y0: -0.2025 - val_y1: 1.0102 - val_ate_afte_scaled: 1.2127 - lr: 5.0000e-05\n",
      "Epoch 105/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.5843 — ite: 3.7931  — ate: 0.6130 — pehe: 4.0982 \n",
      "3/3 [==============================] - 0s 225ms/step - loss: 7.3413 - val_loss: 7.4872 - val_y0: -0.3018 - val_y1: 0.8257 - val_ate_afte_scaled: 1.1276 - lr: 5.0000e-05\n",
      "Epoch 106/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.3880 — ite: 3.9212  — ate: 1.0086 — pehe: 4.3726 \n",
      "3/3 [==============================] - 1s 348ms/step - loss: 7.4443 - val_loss: 6.9203 - val_y0: -0.1006 - val_y1: 1.0528 - val_ate_afte_scaled: 1.1533 - lr: 5.0000e-05\n",
      "Epoch 107/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.0274 — ite: 3.7167  — ate: 0.7537 — pehe: 3.8705 \n",
      "3/3 [==============================] - 0s 197ms/step - loss: 6.4479 - val_loss: 6.9066 - val_y0: -0.1278 - val_y1: 1.0721 - val_ate_afte_scaled: 1.1999 - lr: 5.0000e-05\n",
      "Epoch 108/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.7941 — ite: 3.8099  — ate: 0.6776 — pehe: 4.0115 \n",
      "3/3 [==============================] - 1s 269ms/step - loss: 6.2595 - val_loss: 4.6169 - val_y0: -0.3343 - val_y1: 0.9875 - val_ate_afte_scaled: 1.3219 - lr: 5.0000e-05\n",
      "Epoch 109/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 4.0793 — ite: 3.7679  — ate: 0.7618 — pehe: 4.0544 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: 5.3118 - val_loss: 5.0598 - val_y0: -0.3069 - val_y1: 0.9074 - val_ate_afte_scaled: 1.2142 - lr: 5.0000e-05\n",
      "Epoch 110/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.7264 — ite: 3.7528  — ate: 0.8710 — pehe: 4.0549 \n",
      "3/3 [==============================] - 1s 255ms/step - loss: 4.3790 - val_loss: 4.0242 - val_y0: -0.2987 - val_y1: 1.1194 - val_ate_afte_scaled: 1.4181 - lr: 5.0000e-05\n",
      "Epoch 111/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.0362 — ite: 3.8375  — ate: 0.8982 — pehe: 4.1813 \n",
      "3/3 [==============================] - 1s 259ms/step - loss: 2.9701 - val_loss: 3.4691 - val_y0: -0.2734 - val_y1: 1.0258 - val_ate_afte_scaled: 1.2992 - lr: 5.0000e-05\n",
      "Epoch 112/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4203 — ite: 3.8212  — ate: 0.6146 — pehe: 4.0486 \n",
      "3/3 [==============================] - 1s 392ms/step - loss: 1.1834 - val_loss: 1.8679 - val_y0: -0.2539 - val_y1: 0.9532 - val_ate_afte_scaled: 1.2070 - lr: 5.0000e-05\n",
      "Epoch 113/300\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9923 — ite: 3.8526  — ate: 0.5680 — pehe: 3.9942 \n",
      "3/3 [==============================] - 1s 390ms/step - loss: 0.5162 - val_loss: 0.5559 - val_y0: -0.2126 - val_y1: 1.0849 - val_ate_afte_scaled: 1.2975 - lr: 5.0000e-05\n",
      "Epoch 114/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 2.7953 — ite: 3.8239  — ate: 0.6921 — pehe: 4.2078 \n",
      "3/3 [==============================] - 1s 306ms/step - loss: -0.6586 - val_loss: 0.1707 - val_y0: -0.3234 - val_y1: 0.9951 - val_ate_afte_scaled: 1.3184 - lr: 5.0000e-05\n",
      "Epoch 115/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.9772 — ite: 4.0015  — ate: 0.8663 — pehe: 4.4282 \n",
      "3/3 [==============================] - 1s 307ms/step - loss: -1.8616 - val_loss: -2.0005 - val_y0: -0.3066 - val_y1: 1.0268 - val_ate_afte_scaled: 1.3334 - lr: 5.0000e-05\n",
      "Epoch 116/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1.3722 — ite: 3.8871  — ate: 0.8764 — pehe: 4.2367 \n",
      "3/3 [==============================] - 1s 309ms/step - loss: -3.4088 - val_loss: -3.1440 - val_y0: -0.1825 - val_y1: 0.9771 - val_ate_afte_scaled: 1.1597 - lr: 5.0000e-05\n",
      "Epoch 117/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -6.4532 — ite: 3.7275  — ate: 0.8311 — pehe: 3.9140 \n",
      "3/3 [==============================] - 1s 316ms/step - loss: -3.7254 - val_loss: -4.1456 - val_y0: -0.4083 - val_y1: 1.1099 - val_ate_afte_scaled: 1.5182 - lr: 5.0000e-05\n",
      "Epoch 118/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -5.9207 — ite: 3.8561  — ate: 0.8432 — pehe: 4.1074 \n",
      "3/3 [==============================] - 1s 310ms/step - loss: -5.6031 - val_loss: -6.1017 - val_y0: -0.2273 - val_y1: 0.9525 - val_ate_afte_scaled: 1.1798 - lr: 5.0000e-05\n",
      "Epoch 119/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -6.2816 — ite: 3.8886  — ate: 0.7008 — pehe: 4.0512 \n",
      "3/3 [==============================] - 1s 340ms/step - loss: -7.9980 - val_loss: -8.7031 - val_y0: -0.2871 - val_y1: 0.8847 - val_ate_afte_scaled: 1.1717 - lr: 5.0000e-05\n",
      "Epoch 120/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -10.7574 — ite: 3.8250  — ate: 0.9789 — pehe: 4.1613 \n",
      "3/3 [==============================] - 1s 370ms/step - loss: -8.4898 - val_loss: -10.0725 - val_y0: -0.3766 - val_y1: 0.9330 - val_ate_afte_scaled: 1.3097 - lr: 5.0000e-05\n",
      "Epoch 121/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -10.7914 — ite: 3.8806  — ate: 0.8335 — pehe: 4.1822 \n",
      "3/3 [==============================] - 1s 301ms/step - loss: -11.8856 - val_loss: -11.7365 - val_y0: -0.1718 - val_y1: 0.9770 - val_ate_afte_scaled: 1.1488 - lr: 5.0000e-05\n",
      "Epoch 122/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -12.9527 — ite: 3.7485  — ate: 0.8130 — pehe: 4.0656 \n",
      "3/3 [==============================] - 1s 411ms/step - loss: -13.4114 - val_loss: -13.0282 - val_y0: -0.3212 - val_y1: 0.9190 - val_ate_afte_scaled: 1.2402 - lr: 5.0000e-05\n",
      "Epoch 123/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -14.1316 — ite: 3.7586  — ate: 0.6733 — pehe: 3.8377 \n",
      "3/3 [==============================] - 1s 330ms/step - loss: -14.4485 - val_loss: -15.9770 - val_y0: -0.2057 - val_y1: 1.0427 - val_ate_afte_scaled: 1.2484 - lr: 5.0000e-05\n",
      "Epoch 124/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -16.3577 — ite: 3.7292  — ate: 0.7573 — pehe: 3.8777 \n",
      "3/3 [==============================] - 1s 292ms/step - loss: -18.0000 - val_loss: -17.6278 - val_y0: -0.0920 - val_y1: 1.0968 - val_ate_afte_scaled: 1.1888 - lr: 5.0000e-05\n",
      "Epoch 125/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20.5558 — ite: 3.8611  — ate: 0.7461 — pehe: 4.1565 \n",
      "3/3 [==============================] - 1s 342ms/step - loss: -19.4372 - val_loss: -18.6699 - val_y0: -0.3006 - val_y1: 1.0044 - val_ate_afte_scaled: 1.3050 - lr: 5.0000e-05\n",
      "Epoch 126/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20.5390 — ite: 3.7383  — ate: 0.6890 — pehe: 3.8726 \n",
      "3/3 [==============================] - 1s 266ms/step - loss: -22.5329 - val_loss: -23.3964 - val_y0: -0.0416 - val_y1: 1.1182 - val_ate_afte_scaled: 1.1599 - lr: 5.0000e-05\n",
      "Epoch 127/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -20.0073 — ite: 3.8842  — ate: 0.6499 — pehe: 4.1722 \n",
      "3/3 [==============================] - 1s 238ms/step - loss: -24.3819 - val_loss: -26.1183 - val_y0: -0.1774 - val_y1: 0.9332 - val_ate_afte_scaled: 1.1105 - lr: 5.0000e-05\n",
      "Epoch 128/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -30.9146 — ite: 3.8729  — ate: 0.4253 — pehe: 4.0409 \n",
      "3/3 [==============================] - 1s 255ms/step - loss: -28.8894 - val_loss: -29.6761 - val_y0: -0.1797 - val_y1: 0.9988 - val_ate_afte_scaled: 1.1785 - lr: 5.0000e-05\n",
      "Epoch 129/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -31.3765 — ite: 3.9243  — ate: 0.6088 — pehe: 4.1544 \n",
      "3/3 [==============================] - 1s 301ms/step - loss: -31.3178 - val_loss: -31.8781 - val_y0: -0.2501 - val_y1: 0.9591 - val_ate_afte_scaled: 1.2092 - lr: 5.0000e-05\n",
      "Epoch 130/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -33.6093 — ite: 3.8162  — ate: 0.8735 — pehe: 3.9543 \n",
      "3/3 [==============================] - 1s 354ms/step - loss: -33.3741 - val_loss: -34.7382 - val_y0: -0.3588 - val_y1: 1.0079 - val_ate_afte_scaled: 1.3667 - lr: 5.0000e-05\n",
      "Epoch 131/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -34.0555 — ite: 3.7634  — ate: 0.9071 — pehe: 4.0193 \n",
      "3/3 [==============================] - 1s 317ms/step - loss: -38.1057 - val_loss: -39.0388 - val_y0: -0.2124 - val_y1: 0.9010 - val_ate_afte_scaled: 1.1134 - lr: 5.0000e-05\n",
      "Epoch 132/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -40.7871 — ite: 3.8393  — ate: 0.7870 — pehe: 4.0704 \n",
      "3/3 [==============================] - 1s 347ms/step - loss: -42.6130 - val_loss: -43.2003 - val_y0: -0.2443 - val_y1: 1.0804 - val_ate_afte_scaled: 1.3246 - lr: 5.0000e-05\n",
      "Epoch 133/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -44.7326 — ite: 3.8605  — ate: 1.0571 — pehe: 4.0827 \n",
      "3/3 [==============================] - 1s 342ms/step - loss: -44.5477 - val_loss: -48.0993 - val_y0: -0.2641 - val_y1: 1.1223 - val_ate_afte_scaled: 1.3864 - lr: 5.0000e-05\n",
      "Epoch 134/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -44.5603 — ite: 3.9718  — ate: 0.8869 — pehe: 4.2703 \n",
      "3/3 [==============================] - 1s 372ms/step - loss: -48.7596 - val_loss: -51.5430 - val_y0: -0.1544 - val_y1: 0.9821 - val_ate_afte_scaled: 1.1364 - lr: 5.0000e-05\n",
      "Epoch 135/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -57.9334 — ite: 3.8936  — ate: 0.7658 — pehe: 4.2417 \n",
      "3/3 [==============================] - 1s 271ms/step - loss: -53.6695 - val_loss: -57.8516 - val_y0: -0.1771 - val_y1: 0.9928 - val_ate_afte_scaled: 1.1699 - lr: 5.0000e-05\n",
      "Epoch 136/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -64.5061 — ite: 3.8667  — ate: 0.9097 — pehe: 4.0947 \n",
      "3/3 [==============================] - 0s 222ms/step - loss: -59.6282 - val_loss: -62.7428 - val_y0: -0.2382 - val_y1: 0.8915 - val_ate_afte_scaled: 1.1297 - lr: 5.0000e-05\n",
      "Epoch 137/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -64.4827 — ite: 3.8323  — ate: 0.7338 — pehe: 4.0686 \n",
      "3/3 [==============================] - 1s 251ms/step - loss: -64.2554 - val_loss: -67.9853 - val_y0: -0.2620 - val_y1: 1.0762 - val_ate_afte_scaled: 1.3382 - lr: 5.0000e-05\n",
      "Epoch 138/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -66.1791 — ite: 3.8645  — ate: 0.7914 — pehe: 4.1030 \n",
      "3/3 [==============================] - 0s 242ms/step - loss: -73.8818 - val_loss: -75.5235 - val_y0: -0.3090 - val_y1: 0.9094 - val_ate_afte_scaled: 1.2184 - lr: 5.0000e-05\n",
      "Epoch 139/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -67.1101 — ite: 3.9289  — ate: 0.9111 — pehe: 4.2085 \n",
      "3/3 [==============================] - 1s 281ms/step - loss: -74.7290 - val_loss: -81.2826 - val_y0: -0.2276 - val_y1: 0.9477 - val_ate_afte_scaled: 1.1753 - lr: 5.0000e-05\n",
      "Epoch 140/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -82.5423 — ite: 3.7947  — ate: 0.6615 — pehe: 4.0407 \n",
      "3/3 [==============================] - 1s 322ms/step - loss: -81.8927 - val_loss: -87.9010 - val_y0: -0.0507 - val_y1: 0.9574 - val_ate_afte_scaled: 1.0081 - lr: 5.0000e-05\n",
      "Epoch 141/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -100.7455 — ite: 3.8138  — ate: 0.9824 — pehe: 3.9079 \n",
      "3/3 [==============================] - 1s 284ms/step - loss: -90.1380 - val_loss: -95.7349 - val_y0: -0.2141 - val_y1: 0.9977 - val_ate_afte_scaled: 1.2118 - lr: 5.0000e-05\n",
      "Epoch 142/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -92.2414 — ite: 3.8241  — ate: 0.6893 — pehe: 4.0661 \n",
      "3/3 [==============================] - 1s 250ms/step - loss: -96.3450 - val_loss: -100.2446 - val_y0: -0.2425 - val_y1: 0.9195 - val_ate_afte_scaled: 1.1620 - lr: 5.0000e-05\n",
      "Epoch 143/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -99.3338 — ite: 3.8879  — ate: 0.4747 — pehe: 4.0250 \n",
      "3/3 [==============================] - 1s 333ms/step - loss: -103.0326 - val_loss: -111.4813 - val_y0: -0.1700 - val_y1: 0.9713 - val_ate_afte_scaled: 1.1413 - lr: 5.0000e-05\n",
      "Epoch 144/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -114.7651 — ite: 3.8294  — ate: 0.6357 — pehe: 4.1220 \n",
      "3/3 [==============================] - 1s 427ms/step - loss: -117.0897 - val_loss: -120.6468 - val_y0: -0.3024 - val_y1: 0.9252 - val_ate_afte_scaled: 1.2276 - lr: 5.0000e-05\n",
      "Epoch 145/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -123.0055 — ite: 3.8555  — ate: 0.7560 — pehe: 4.0421 \n",
      "3/3 [==============================] - 1s 386ms/step - loss: -124.3876 - val_loss: -129.1524 - val_y0: -0.2227 - val_y1: 0.9942 - val_ate_afte_scaled: 1.2169 - lr: 5.0000e-05\n",
      "Epoch 146/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -132.9328 — ite: 3.9425  — ate: 0.5083 — pehe: 4.1762 \n",
      "3/3 [==============================] - 1s 365ms/step - loss: -135.4408 - val_loss: -142.2653 - val_y0: -0.2139 - val_y1: 0.9495 - val_ate_afte_scaled: 1.1633 - lr: 5.0000e-05\n",
      "Epoch 147/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -123.6175 — ite: 3.7919  — ate: 0.8061 — pehe: 4.1075 \n",
      "3/3 [==============================] - 0s 232ms/step - loss: -153.4701 - val_loss: -150.4053 - val_y0: -0.1127 - val_y1: 0.8451 - val_ate_afte_scaled: 0.9578 - lr: 5.0000e-05\n",
      "Epoch 148/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -152.9368 — ite: 3.8269  — ate: 0.6570 — pehe: 4.1389 \n",
      "3/3 [==============================] - 1s 268ms/step - loss: -157.7682 - val_loss: -162.9496 - val_y0: -0.1954 - val_y1: 0.9304 - val_ate_afte_scaled: 1.1258 - lr: 5.0000e-05\n",
      "Epoch 149/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -161.2315 — ite: 3.7854  — ate: 0.5025 — pehe: 3.9271 \n",
      "3/3 [==============================] - 1s 249ms/step - loss: -169.2659 - val_loss: -176.6418 - val_y0: -0.2960 - val_y1: 0.9714 - val_ate_afte_scaled: 1.2674 - lr: 5.0000e-05\n",
      "Epoch 150/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -179.3697 — ite: 3.8068  — ate: 0.6222 — pehe: 4.0494 \n",
      "3/3 [==============================] - 1s 245ms/step - loss: -184.4470 - val_loss: -184.2508 - val_y0: -0.2074 - val_y1: 1.0251 - val_ate_afte_scaled: 1.2326 - lr: 5.0000e-05\n",
      "Epoch 151/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -191.7135 — ite: 3.7530  — ate: 0.9738 — pehe: 4.0060 \n",
      "3/3 [==============================] - 1s 265ms/step - loss: -190.6072 - val_loss: -201.3588 - val_y0: -0.2381 - val_y1: 0.9667 - val_ate_afte_scaled: 1.2048 - lr: 5.0000e-05\n",
      "Epoch 152/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -201.3853 — ite: 3.7951  — ate: 0.4291 — pehe: 3.8358 \n",
      "3/3 [==============================] - 1s 311ms/step - loss: -204.1183 - val_loss: -216.2616 - val_y0: -0.2065 - val_y1: 1.1335 - val_ate_afte_scaled: 1.3399 - lr: 5.0000e-05\n",
      "Epoch 153/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -222.8766 — ite: 3.8613  — ate: 0.4003 — pehe: 4.0308 \n",
      "3/3 [==============================] - 1s 304ms/step - loss: -221.3714 - val_loss: -234.0084 - val_y0: -0.2915 - val_y1: 0.8799 - val_ate_afte_scaled: 1.1715 - lr: 5.0000e-05\n",
      "Epoch 154/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -240.8236 — ite: 3.9117  — ate: 0.9674 — pehe: 4.1786 \n",
      "3/3 [==============================] - 1s 282ms/step - loss: -250.9419 - val_loss: -258.6183 - val_y0: -0.2880 - val_y1: 0.9567 - val_ate_afte_scaled: 1.2446 - lr: 5.0000e-05\n",
      "Epoch 155/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -256.5416 — ite: 3.8156  — ate: 0.6361 — pehe: 4.1398 \n",
      "3/3 [==============================] - 1s 259ms/step - loss: -255.6554 - val_loss: -211.2943 - val_y0: -0.0046 - val_y1: 0.9683 - val_ate_afte_scaled: 0.9729 - lr: 5.0000e-05\n",
      "Epoch 156/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -194.8223 — ite: 3.7712  — ate: 0.3384 — pehe: 4.0232 \n",
      "3/3 [==============================] - 1s 320ms/step - loss: -265.7293 - val_loss: -283.3559 - val_y0: -0.1902 - val_y1: 0.7405 - val_ate_afte_scaled: 0.9307 - lr: 5.0000e-05\n",
      "Epoch 157/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -287.7473 — ite: 3.8039  — ate: 0.8451 — pehe: 4.1301 \n",
      "3/3 [==============================] - 1s 263ms/step - loss: -288.8174 - val_loss: -295.8163 - val_y0: -0.2231 - val_y1: 0.9549 - val_ate_afte_scaled: 1.1780 - lr: 5.0000e-05\n",
      "Epoch 158/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -261.3734 — ite: 3.8979  — ate: 1.0372 — pehe: 4.2569 \n",
      "3/3 [==============================] - 1s 269ms/step - loss: -298.7712 - val_loss: -312.8900 - val_y0: -0.2476 - val_y1: 0.7888 - val_ate_afte_scaled: 1.0364 - lr: 5.0000e-05\n",
      "Epoch 159/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -317.2962 — ite: 3.7743  — ate: 0.3154 — pehe: 4.1111 \n",
      "3/3 [==============================] - 1s 254ms/step - loss: -320.9955 - val_loss: -325.5427 - val_y0: -0.2020 - val_y1: 0.9610 - val_ate_afte_scaled: 1.1630 - lr: 5.0000e-05\n",
      "Epoch 160/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -323.0498 — ite: 3.8975  — ate: 0.8476 — pehe: 4.1306 \n",
      "3/3 [==============================] - 1s 275ms/step - loss: -349.9459 - val_loss: -345.8365 - val_y0: -0.2801 - val_y1: 0.9767 - val_ate_afte_scaled: 1.2568 - lr: 5.0000e-05\n",
      "Epoch 161/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -374.2304 — ite: 3.8216  — ate: 0.5196 — pehe: 4.0797 \n",
      "3/3 [==============================] - 0s 207ms/step - loss: -342.1526 - val_loss: -364.4077 - val_y0: -0.2391 - val_y1: 0.8131 - val_ate_afte_scaled: 1.0522 - lr: 5.0000e-05\n",
      "Epoch 162/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -385.2156 — ite: 3.8184  — ate: 0.3120 — pehe: 4.0245 \n",
      "3/3 [==============================] - 1s 281ms/step - loss: -374.5993 - val_loss: -386.5346 - val_y0: -0.0931 - val_y1: 0.8504 - val_ate_afte_scaled: 0.9434 - lr: 5.0000e-05\n",
      "Epoch 163/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -345.1466 — ite: 3.8813  — ate: 0.9365 — pehe: 4.0425 \n",
      "3/3 [==============================] - 0s 221ms/step - loss: -397.9869 - val_loss: -403.2402 - val_y0: -0.2049 - val_y1: 0.9454 - val_ate_afte_scaled: 1.1503 - lr: 5.0000e-05\n",
      "Epoch 164/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -408.4874 — ite: 3.7179  — ate: 0.8358 — pehe: 4.0566 \n",
      "3/3 [==============================] - 1s 291ms/step - loss: -415.9541 - val_loss: -428.6117 - val_y0: -0.0695 - val_y1: 0.7938 - val_ate_afte_scaled: 0.8633 - lr: 5.0000e-05\n",
      "Epoch 165/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -418.0439 — ite: 3.8755  — ate: 0.4353 — pehe: 4.2170 \n",
      "3/3 [==============================] - 1s 268ms/step - loss: -446.2113 - val_loss: -455.7572 - val_y0: -0.2392 - val_y1: 0.9893 - val_ate_afte_scaled: 1.2286 - lr: 5.0000e-05\n",
      "Epoch 166/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -488.0646 — ite: 3.8965  — ate: 0.5298 — pehe: 4.2094 \n",
      "3/3 [==============================] - 0s 228ms/step - loss: -471.0506 - val_loss: -485.0023 - val_y0: -0.2292 - val_y1: 1.0490 - val_ate_afte_scaled: 1.2782 - lr: 5.0000e-05\n",
      "Epoch 167/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -493.9083 — ite: 3.9187  — ate: 0.7840 — pehe: 4.3074 \n",
      "3/3 [==============================] - 1s 406ms/step - loss: -489.4225 - val_loss: -506.1409 - val_y0: -0.3811 - val_y1: 1.0068 - val_ate_afte_scaled: 1.3878 - lr: 5.0000e-05\n",
      "Epoch 168/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -519.5259 — ite: 3.8460  — ate: 0.5928 — pehe: 4.0856 \n",
      "3/3 [==============================] - 1s 269ms/step - loss: -525.0403 - val_loss: -540.1675 - val_y0: -0.1406 - val_y1: 1.0784 - val_ate_afte_scaled: 1.2191 - lr: 5.0000e-05\n",
      "Epoch 169/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -573.1475 — ite: 3.9929  — ate: 1.0317 — pehe: 4.4216 \n",
      "3/3 [==============================] - 1s 261ms/step - loss: -552.8110 - val_loss: -573.0139 - val_y0: -0.0947 - val_y1: 0.8097 - val_ate_afte_scaled: 0.9044 - lr: 5.0000e-05\n",
      "Epoch 170/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -558.0195 — ite: 3.9207  — ate: 1.1585 — pehe: 4.2619 \n",
      "3/3 [==============================] - 0s 214ms/step - loss: -585.1979 - val_loss: -606.6539 - val_y0: 0.0199 - val_y1: 1.0548 - val_ate_afte_scaled: 1.0349 - lr: 5.0000e-05\n",
      "Epoch 171/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -596.7856 — ite: 3.9136  — ate: 0.8907 — pehe: 4.2151 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: -631.5334 - val_loss: -646.3759 - val_y0: -0.1567 - val_y1: 1.0909 - val_ate_afte_scaled: 1.2476 - lr: 5.0000e-05\n",
      "Epoch 172/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -699.7795 — ite: 3.9077  — ate: 0.6596 — pehe: 4.2503 \n",
      "3/3 [==============================] - 0s 220ms/step - loss: -636.9185 - val_loss: -674.2802 - val_y0: -0.2427 - val_y1: 1.1505 - val_ate_afte_scaled: 1.3932 - lr: 5.0000e-05\n",
      "Epoch 173/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -571.5539 — ite: 4.0247  — ate: 1.2843 — pehe: 4.3648 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: -720.3975 - val_loss: -717.9575 - val_y0: -0.1991 - val_y1: 0.8623 - val_ate_afte_scaled: 1.0614 - lr: 5.0000e-05\n",
      "Epoch 174/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -785.7166 — ite: 3.8179  — ate: 0.7667 — pehe: 4.2026 \n",
      "3/3 [==============================] - 0s 229ms/step - loss: -742.7898 - val_loss: -761.0228 - val_y0: -0.0797 - val_y1: 1.1218 - val_ate_afte_scaled: 1.2015 - lr: 5.0000e-05\n",
      "Epoch 175/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -735.0177 — ite: 4.0027  — ate: 0.9071 — pehe: 4.3891 \n",
      "3/3 [==============================] - 1s 312ms/step - loss: -771.3798 - val_loss: -806.7876 - val_y0: -0.1272 - val_y1: 1.2106 - val_ate_afte_scaled: 1.3378 - lr: 5.0000e-05\n",
      "Epoch 176/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -816.9890 — ite: 3.8921  — ate: 1.0729 — pehe: 4.2638 \n",
      "3/3 [==============================] - 1s 294ms/step - loss: -799.5336 - val_loss: -849.7177 - val_y0: -0.1802 - val_y1: 1.0538 - val_ate_afte_scaled: 1.2340 - lr: 5.0000e-05\n",
      "Epoch 177/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -881.7031 — ite: 3.8767  — ate: 0.8971 — pehe: 4.0542 \n",
      "3/3 [==============================] - 1s 243ms/step - loss: -867.6030 - val_loss: -911.3522 - val_y0: -0.2570 - val_y1: 1.1138 - val_ate_afte_scaled: 1.3708 - lr: 5.0000e-05\n",
      "Epoch 178/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -926.8099 — ite: 3.8685  — ate: 0.8071 — pehe: 4.2133 \n",
      "3/3 [==============================] - 1s 300ms/step - loss: -919.2212 - val_loss: -954.8465 - val_y0: -0.4156 - val_y1: 1.1010 - val_ate_afte_scaled: 1.5166 - lr: 5.0000e-05\n",
      "Epoch 179/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1105.4222 — ite: 3.7928  — ate: 1.1368 — pehe: 4.1001 \n",
      "3/3 [==============================] - 1s 265ms/step - loss: -955.1306 - val_loss: -1022.9396 - val_y0: -0.1114 - val_y1: 1.0590 - val_ate_afte_scaled: 1.1704 - lr: 5.0000e-05\n",
      "Epoch 180/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -960.9684 — ite: 3.9123  — ate: 0.8613 — pehe: 4.2577 \n",
      "3/3 [==============================] - 0s 219ms/step - loss: -1047.0369 - val_loss: -1076.2528 - val_y0: -0.0369 - val_y1: 0.8490 - val_ate_afte_scaled: 0.8859 - lr: 5.0000e-05\n",
      "Epoch 181/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1162.4105 — ite: 3.8649  — ate: 1.0192 — pehe: 4.0338 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: -1136.1854 - val_loss: -1137.9216 - val_y0: -0.2785 - val_y1: 0.8064 - val_ate_afte_scaled: 1.0849 - lr: 5.0000e-05\n",
      "Epoch 182/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1177.7590 — ite: 3.8621  — ate: 0.9133 — pehe: 4.1234 \n",
      "3/3 [==============================] - 1s 259ms/step - loss: -1173.3774 - val_loss: -1218.8550 - val_y0: -0.2216 - val_y1: 0.6267 - val_ate_afte_scaled: 0.8483 - lr: 5.0000e-05\n",
      "Epoch 183/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -1246.8450 — ite: 3.8488  — ate: 0.6991 — pehe: 4.0610 \n",
      "3/3 [==============================] - 1s 250ms/step - loss: -1281.3716 - val_loss: -1290.6952 - val_y0: -0.0483 - val_y1: 0.6212 - val_ate_afte_scaled: 0.6695 - lr: 5.0000e-05\n",
      "Epoch 184/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1281.2860 — ite: 3.8401  — ate: 0.7583 — pehe: 3.9921 \n",
      "3/3 [==============================] - 1s 256ms/step - loss: -1340.3814 - val_loss: -1376.4912 - val_y0: -0.1865 - val_y1: 0.6519 - val_ate_afte_scaled: 0.8384 - lr: 5.0000e-05\n",
      "Epoch 185/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1508.3501 — ite: 3.9253  — ate: 0.7665 — pehe: 4.2150 \n",
      "3/3 [==============================] - 0s 213ms/step - loss: -1370.7215 - val_loss: -1462.5698 - val_y0: -0.3430 - val_y1: 0.5834 - val_ate_afte_scaled: 0.9264 - lr: 5.0000e-05\n",
      "Epoch 186/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1477.3657 — ite: 3.7810  — ate: 0.2368 — pehe: 4.0224 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: -1473.8123 - val_loss: -1556.3269 - val_y0: -0.3217 - val_y1: 0.5304 - val_ate_afte_scaled: 0.8521 - lr: 5.0000e-05\n",
      "Epoch 187/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1558.3240 — ite: 3.7685  — ate: 0.8360 — pehe: 4.0087 \n",
      "3/3 [==============================] - 0s 220ms/step - loss: -1586.0179 - val_loss: -1669.9554 - val_y0: -0.2871 - val_y1: 0.4823 - val_ate_afte_scaled: 0.7694 - lr: 5.0000e-05\n",
      "Epoch 188/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1571.1825 — ite: 3.9918  — ate: 0.4670 — pehe: 4.2751 \n",
      "3/3 [==============================] - 1s 276ms/step - loss: -1707.4692 - val_loss: -1756.2628 - val_y0: -0.5312 - val_y1: 0.5910 - val_ate_afte_scaled: 1.1222 - lr: 5.0000e-05\n",
      "Epoch 189/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -1797.5284 — ite: 3.8116  — ate: 0.2438 — pehe: 4.0571 \n",
      "3/3 [==============================] - 1s 332ms/step - loss: -1833.7671 - val_loss: -1873.2745 - val_y0: -0.4231 - val_y1: 0.4977 - val_ate_afte_scaled: 0.9208 - lr: 5.0000e-05\n",
      "Epoch 190/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -1845.0769 — ite: 3.8809  — ate: 1.1860 — pehe: 4.1891 \n",
      "3/3 [==============================] - 1s 340ms/step - loss: -1878.7335 - val_loss: -1998.8357 - val_y0: -0.5594 - val_y1: 0.5613 - val_ate_afte_scaled: 1.1207 - lr: 5.0000e-05\n",
      "Epoch 191/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -2079.4663 — ite: 3.7731  — ate: 0.1131 — pehe: 4.1066 \n",
      "3/3 [==============================] - 1s 244ms/step - loss: -2060.0341 - val_loss: -2128.6592 - val_y0: -0.4476 - val_y1: 0.5442 - val_ate_afte_scaled: 0.9918 - lr: 5.0000e-05\n",
      "Epoch 192/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -2175.1172 — ite: 3.7352  — ate: 1.1432 — pehe: 3.9735 \n",
      "3/3 [==============================] - 1s 314ms/step - loss: -2229.8163 - val_loss: -2271.4597 - val_y0: -0.5843 - val_y1: 0.2453 - val_ate_afte_scaled: 0.8296 - lr: 5.0000e-05\n",
      "Epoch 193/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -2308.8542 — ite: 3.6522  — ate: 0.0074 — pehe: 3.8456 \n",
      "3/3 [==============================] - 1s 341ms/step - loss: -2347.4467 - val_loss: -2413.1567 - val_y0: -0.6595 - val_y1: 0.3175 - val_ate_afte_scaled: 0.9770 - lr: 5.0000e-05\n",
      "Epoch 194/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -2366.4751 — ite: 3.7807  — ate: 0.4871 — pehe: 3.9467 \n",
      "3/3 [==============================] - 1s 320ms/step - loss: -2477.6505 - val_loss: -2568.1287 - val_y0: -0.4671 - val_y1: 0.2873 - val_ate_afte_scaled: 0.7544 - lr: 5.0000e-05\n",
      "Epoch 195/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -2608.8888 — ite: 3.8245  — ate: 0.7121 — pehe: 3.9342 \n",
      "3/3 [==============================] - 1s 317ms/step - loss: -2616.1169 - val_loss: -2754.8308 - val_y0: -0.5394 - val_y1: 0.5092 - val_ate_afte_scaled: 1.0486 - lr: 5.0000e-05\n",
      "Epoch 196/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -2777.1501 — ite: 3.7858  — ate: 0.1267 — pehe: 3.9145 \n",
      "3/3 [==============================] - 0s 239ms/step - loss: -2871.4018 - val_loss: -2907.9919 - val_y0: -0.3607 - val_y1: 0.3927 - val_ate_afte_scaled: 0.7535 - lr: 5.0000e-05\n",
      "Epoch 197/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -2561.9832 — ite: 3.6952  — ate: 1.0099 — pehe: 3.9844 \n",
      "3/3 [==============================] - 0s 239ms/step - loss: -3014.0709 - val_loss: -3103.0300 - val_y0: -0.4644 - val_y1: 0.2652 - val_ate_afte_scaled: 0.7296 - lr: 5.0000e-05\n",
      "Epoch 198/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -3095.7158 — ite: 3.8901  — ate: 0.5911 — pehe: 4.0555 \n",
      "3/3 [==============================] - 0s 230ms/step - loss: -3207.3643 - val_loss: -3285.2256 - val_y0: -0.4620 - val_y1: 0.2623 - val_ate_afte_scaled: 0.7243 - lr: 5.0000e-05\n",
      "Epoch 199/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -3026.8057 — ite: 3.8313  — ate: 0.4338 — pehe: 4.1055 \n",
      "3/3 [==============================] - 0s 201ms/step - loss: -3369.2155 - val_loss: -3484.5247 - val_y0: -0.4122 - val_y1: 0.2844 - val_ate_afte_scaled: 0.6965 - lr: 5.0000e-05\n",
      "Epoch 200/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -3697.2910 — ite: 3.7775  — ate: 0.6667 — pehe: 4.0796 \n",
      "3/3 [==============================] - 0s 190ms/step - loss: -3550.3198 - val_loss: -3687.0710 - val_y0: -0.0777 - val_y1: 0.4264 - val_ate_afte_scaled: 0.5041 - lr: 5.0000e-05\n",
      "Epoch 201/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -3983.7500 — ite: 3.8120  — ate: 0.6727 — pehe: 4.0288 \n",
      "3/3 [==============================] - 0s 183ms/step - loss: -3698.2873 - val_loss: -3944.8108 - val_y0: 0.0853 - val_y1: 0.5528 - val_ate_afte_scaled: 0.4675 - lr: 5.0000e-05\n",
      "Epoch 202/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -3728.2241 — ite: 3.7690  — ate: 0.7418 — pehe: 3.9847 \n",
      "3/3 [==============================] - 0s 241ms/step - loss: -3948.7700 - val_loss: -4164.8247 - val_y0: 0.0939 - val_y1: 0.5454 - val_ate_afte_scaled: 0.4515 - lr: 5.0000e-05\n",
      "Epoch 203/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -3752.3870 — ite: 3.8409  — ate: 0.6603 — pehe: 4.0916 \n",
      "3/3 [==============================] - 1s 263ms/step - loss: -4415.9246 - val_loss: -4421.0869 - val_y0: 0.1898 - val_y1: 0.5755 - val_ate_afte_scaled: 0.3857 - lr: 5.0000e-05\n",
      "Epoch 204/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -4455.0062 — ite: 3.8569  — ate: 0.5395 — pehe: 4.3067 \n",
      "3/3 [==============================] - 0s 239ms/step - loss: -4371.7809 - val_loss: -4671.5137 - val_y0: 0.1734 - val_y1: 0.7205 - val_ate_afte_scaled: 0.5471 - lr: 5.0000e-05\n",
      "Epoch 205/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -4723.8289 — ite: 3.8312  — ate: 0.3776 — pehe: 4.0976 \n",
      "3/3 [==============================] - 1s 259ms/step - loss: -4623.2948 - val_loss: -4949.4116 - val_y0: 0.3150 - val_y1: 0.6863 - val_ate_afte_scaled: 0.3714 - lr: 5.0000e-05\n",
      "Epoch 206/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -5019.6177 — ite: 3.8691  — ate: 0.5133 — pehe: 4.1265 \n",
      "3/3 [==============================] - 1s 308ms/step - loss: -5174.1036 - val_loss: -5222.6572 - val_y0: 0.3391 - val_y1: 0.6954 - val_ate_afte_scaled: 0.3564 - lr: 5.0000e-05\n",
      "Epoch 207/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -5557.9565 — ite: 3.8760  — ate: 0.9794 — pehe: 4.1142 \n",
      "3/3 [==============================] - 1s 289ms/step - loss: -5320.1400 - val_loss: -5517.7471 - val_y0: 0.6829 - val_y1: 0.8702 - val_ate_afte_scaled: 0.1873 - lr: 5.0000e-05\n",
      "Epoch 208/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -5272.1182 — ite: 3.7642  — ate: 0.4292 — pehe: 3.9670 \n",
      "3/3 [==============================] - 1s 255ms/step - loss: -5616.7216 - val_loss: -5870.8066 - val_y0: 0.5935 - val_y1: 0.9746 - val_ate_afte_scaled: 0.3811 - lr: 5.0000e-05\n",
      "Epoch 209/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -5917.1636 — ite: 3.8259  — ate: 0.7559 — pehe: 4.1908 \n",
      "3/3 [==============================] - 1s 303ms/step - loss: -5978.8589 - val_loss: -6199.2734 - val_y0: 0.7264 - val_y1: 0.9142 - val_ate_afte_scaled: 0.1878 - lr: 5.0000e-05\n",
      "Epoch 210/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -6239.4224 — ite: 3.8830  — ate: 0.4607 — pehe: 4.2179 \n",
      "3/3 [==============================] - 1s 277ms/step - loss: -6246.1636 - val_loss: -6520.2021 - val_y0: 0.7319 - val_y1: 1.0312 - val_ate_afte_scaled: 0.2992 - lr: 5.0000e-05\n",
      "Epoch 211/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -6486.6567 — ite: 3.9389  — ate: 1.3261 — pehe: 4.2711 \n",
      "3/3 [==============================] - 0s 217ms/step - loss: -6544.2885 - val_loss: -6795.2002 - val_y0: 0.5900 - val_y1: 1.0106 - val_ate_afte_scaled: 0.4206 - lr: 5.0000e-05\n",
      "Epoch 212/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -6838.5439 — ite: 3.9038  — ate: 0.4622 — pehe: 4.1181 \n",
      "3/3 [==============================] - 0s 227ms/step - loss: -7015.5688 - val_loss: -7164.3911 - val_y0: 0.5336 - val_y1: 0.9585 - val_ate_afte_scaled: 0.4249 - lr: 5.0000e-05\n",
      "Epoch 213/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -7724.7524 — ite: 3.7952  — ate: 0.5562 — pehe: 4.0285 \n",
      "3/3 [==============================] - 0s 230ms/step - loss: -7068.5347 - val_loss: -7526.4844 - val_y0: 0.7330 - val_y1: 1.1919 - val_ate_afte_scaled: 0.4589 - lr: 5.0000e-05\n",
      "Epoch 214/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -7129.2100 — ite: 3.7934  — ate: 0.4101 — pehe: 3.9601 \n",
      "3/3 [==============================] - 0s 222ms/step - loss: -7616.8125 - val_loss: -7878.8516 - val_y0: 0.9641 - val_y1: 1.4294 - val_ate_afte_scaled: 0.4652 - lr: 5.0000e-05\n",
      "Epoch 215/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -7981.2261 — ite: 3.7915  — ate: 0.9349 — pehe: 4.0694 \n",
      "3/3 [==============================] - 0s 218ms/step - loss: -7867.7460 - val_loss: -8264.0205 - val_y0: 0.9793 - val_y1: 1.4566 - val_ate_afte_scaled: 0.4774 - lr: 5.0000e-05\n",
      "Epoch 216/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -7678.7632 — ite: 3.9512  — ate: 0.3631 — pehe: 4.3549 \n",
      "3/3 [==============================] - 0s 227ms/step - loss: -8430.5812 - val_loss: -8594.6445 - val_y0: 0.9201 - val_y1: 1.3731 - val_ate_afte_scaled: 0.4531 - lr: 5.0000e-05\n",
      "Epoch 217/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -7954.5381 — ite: 3.8403  — ate: 0.9664 — pehe: 4.1012 \n",
      "3/3 [==============================] - 0s 236ms/step - loss: -8857.0312 - val_loss: -9063.2070 - val_y0: 1.0911 - val_y1: 1.1202 - val_ate_afte_scaled: 0.0291 - lr: 5.0000e-05\n",
      "Epoch 218/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -9142.0664 — ite: 3.8692  — ate: 0.1034 — pehe: 4.0484 \n",
      "3/3 [==============================] - 0s 226ms/step - loss: -9006.2627 - val_loss: -9522.1875 - val_y0: 1.1358 - val_y1: 1.6437 - val_ate_afte_scaled: 0.5079 - lr: 5.0000e-05\n",
      "Epoch 219/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -10088.2002 — ite: 3.7709  — ate: 1.2758 — pehe: 4.0715 \n",
      "3/3 [==============================] - 1s 269ms/step - loss: -9486.1382 - val_loss: -9962.0547 - val_y0: 1.1656 - val_y1: 1.4078 - val_ate_afte_scaled: 0.2422 - lr: 5.0000e-05\n",
      "Epoch 220/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -9211.3027 — ite: 3.8459  — ate: 0.2109 — pehe: 4.1987 \n",
      "3/3 [==============================] - 0s 235ms/step - loss: -10106.3628 - val_loss: -10442.0693 - val_y0: 1.1640 - val_y1: 1.3726 - val_ate_afte_scaled: 0.2085 - lr: 5.0000e-05\n",
      "Epoch 221/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -9911.6162 — ite: 3.8062  — ate: 0.9262 — pehe: 4.1217 \n",
      "3/3 [==============================] - 1s 259ms/step - loss: -10311.3562 - val_loss: -10871.5176 - val_y0: 1.1191 - val_y1: 1.3471 - val_ate_afte_scaled: 0.2280 - lr: 5.0000e-05\n",
      "Epoch 222/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -10881.2559 — ite: 3.7589  — ate: 0.2626 — pehe: 3.7838 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: -11231.4409 - val_loss: -11395.1807 - val_y0: 1.4406 - val_y1: 1.7109 - val_ate_afte_scaled: 0.2703 - lr: 5.0000e-05\n",
      "Epoch 223/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -11105.3564 — ite: 3.6894  — ate: 0.8561 — pehe: 4.0055 \n",
      "3/3 [==============================] - 1s 273ms/step - loss: -11523.4785 - val_loss: -11924.4717 - val_y0: 1.6043 - val_y1: 1.8682 - val_ate_afte_scaled: 0.2639 - lr: 5.0000e-05\n",
      "Epoch 224/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -12026.7920 — ite: 3.8829  — ate: 0.6283 — pehe: 4.1170 \n",
      "3/3 [==============================] - 1s 340ms/step - loss: -12006.8667 - val_loss: -12463.2734 - val_y0: 1.5191 - val_y1: 1.6255 - val_ate_afte_scaled: 0.1064 - lr: 5.0000e-05\n",
      "Epoch 225/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -12582.9632 — ite: 3.7987  — ate: 0.2294 — pehe: 4.2476 \n",
      "3/3 [==============================] - 1s 363ms/step - loss: -12727.3901 - val_loss: -13089.9014 - val_y0: 1.4014 - val_y1: 1.7591 - val_ate_afte_scaled: 0.3576 - lr: 5.0000e-05\n",
      "Epoch 226/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -13162.2705 — ite: 3.7673  — ate: 0.9054 — pehe: 4.1294 \n",
      "3/3 [==============================] - 1s 332ms/step - loss: -13162.6731 - val_loss: -13670.0254 - val_y0: 1.7781 - val_y1: 1.8969 - val_ate_afte_scaled: 0.1188 - lr: 5.0000e-05\n",
      "Epoch 227/300\n",
      "2/3 [===================>..........] - ETA: 0s - loss: -13360.0137 — ite: 3.7340  — ate: 0.2261 — pehe: 3.9424 \n",
      "3/3 [==============================] - 1s 316ms/step - loss: -13961.7852 - val_loss: -14255.4580 - val_y0: 1.5997 - val_y1: 1.8700 - val_ate_afte_scaled: 0.2703 - lr: 5.0000e-05\n",
      "Epoch 228/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -14854.8115 — ite: 3.7355  — ate: 0.8319 — pehe: 3.9365 \n",
      "3/3 [==============================] - 1s 372ms/step - loss: -14212.1025 - val_loss: -14949.3701 - val_y0: 1.5380 - val_y1: 1.5891 - val_ate_afte_scaled: 0.0510 - lr: 5.0000e-05\n",
      "Epoch 229/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -14385.2432 — ite: 3.8242  — ate: 0.2101 — pehe: 4.0984 \n",
      "3/3 [==============================] - 1s 311ms/step - loss: -15383.4722 - val_loss: -15590.2402 - val_y0: 1.6189 - val_y1: 1.6675 - val_ate_afte_scaled: 0.0486 - lr: 5.0000e-05\n",
      "Epoch 230/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -13390.9795 — ite: 3.8114  — ate: 1.0269 — pehe: 4.2783 \n",
      "3/3 [==============================] - 1s 284ms/step - loss: -16047.5579 - val_loss: -16319.8477 - val_y0: 1.9427 - val_y1: 1.9663 - val_ate_afte_scaled: 0.0236 - lr: 5.0000e-05\n",
      "Epoch 231/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -16393.9671 — ite: 3.7791  — ate: 0.4599 — pehe: 3.9164 \n",
      "3/3 [==============================] - 1s 254ms/step - loss: -16678.3069 - val_loss: -16988.4746 - val_y0: 1.9024 - val_y1: 1.9464 - val_ate_afte_scaled: 0.0440 - lr: 5.0000e-05\n",
      "Epoch 232/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -18050.8887 — ite: 3.8269  — ate: 0.3322 — pehe: 4.0553 \n",
      "3/3 [==============================] - 1s 353ms/step - loss: -17169.3218 - val_loss: -17724.0586 - val_y0: 1.8029 - val_y1: 1.9789 - val_ate_afte_scaled: 0.1760 - lr: 5.0000e-05\n",
      "Epoch 233/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -16937.5508 — ite: 3.7914  — ate: 0.9927 — pehe: 4.0783 \n",
      "3/3 [==============================] - 1s 296ms/step - loss: -18262.7144 - val_loss: -18581.9590 - val_y0: 1.9887 - val_y1: 2.0473 - val_ate_afte_scaled: 0.0586 - lr: 5.0000e-05\n",
      "Epoch 234/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -21129.7324 — ite: 3.8208  — ate: 0.1964 — pehe: 4.2023 \n",
      "3/3 [==============================] - 1s 273ms/step - loss: -18396.4551 - val_loss: -19384.4941 - val_y0: 2.0526 - val_y1: 2.2240 - val_ate_afte_scaled: 0.1714 - lr: 5.0000e-05\n",
      "Epoch 235/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -18644.4922 — ite: 3.7390  — ate: 1.0559 — pehe: 4.1722 \n",
      "3/3 [==============================] - 1s 247ms/step - loss: -19699.0415 - val_loss: -20168.3164 - val_y0: 1.9462 - val_y1: 2.0088 - val_ate_afte_scaled: 0.0626 - lr: 5.0000e-05\n",
      "Epoch 236/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -20311.1960 — ite: 3.7557  — ate: 0.0309 — pehe: 4.1517 \n",
      "3/3 [==============================] - 1s 339ms/step - loss: -20426.8618 - val_loss: -21111.4961 - val_y0: 2.0075 - val_y1: 1.9402 - val_ate_afte_scaled: -0.0673 - lr: 5.0000e-05\n",
      "Epoch 237/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -21194.1387 — ite: 3.8163  — ate: 1.0073 — pehe: 4.1279 \n",
      "3/3 [==============================] - 1s 341ms/step - loss: -21223.0010 - val_loss: -21971.5293 - val_y0: 2.1874 - val_y1: 2.2752 - val_ate_afte_scaled: 0.0878 - lr: 5.0000e-05\n",
      "Epoch 238/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -21173.2363 — ite: 3.8929  — ate: 0.0594 — pehe: 4.2173 \n",
      "3/3 [==============================] - 1s 297ms/step - loss: -22132.4951 - val_loss: -22958.2246 - val_y0: 2.1915 - val_y1: 2.1415 - val_ate_afte_scaled: -0.0501 - lr: 5.0000e-05\n",
      "Epoch 239/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -22887.8828 — ite: 3.8152  — ate: 1.4091 — pehe: 4.1786 \n",
      "3/3 [==============================] - 1s 263ms/step - loss: -23272.9243 - val_loss: -23930.9746 - val_y0: 1.9359 - val_y1: 2.0234 - val_ate_afte_scaled: 0.0875 - lr: 5.0000e-05\n",
      "Epoch 240/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -24358.5137 — ite: 3.8405  — ate: 0.5112 — pehe: 4.4043 \n",
      "3/3 [==============================] - 1s 350ms/step - loss: -23542.6655 - val_loss: -24921.8906 - val_y0: 2.3835 - val_y1: 2.1421 - val_ate_afte_scaled: -0.2414 - lr: 5.0000e-05\n",
      "Epoch 241/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -25952.4160 — ite: 3.7762  — ate: 1.5979 — pehe: 4.2324 \n",
      "3/3 [==============================] - 1s 323ms/step - loss: -24685.3052 - val_loss: -25942.3027 - val_y0: 2.4397 - val_y1: 2.3110 - val_ate_afte_scaled: -0.1288 - lr: 5.0000e-05\n",
      "Epoch 242/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -25886.0176 — ite: 3.7903  — ate: 0.5490 — pehe: 4.1643 \n",
      "3/3 [==============================] - 1s 308ms/step - loss: -26371.9897 - val_loss: -27049.8555 - val_y0: 2.4270 - val_y1: 2.2875 - val_ate_afte_scaled: -0.1395 - lr: 5.0000e-05\n",
      "Epoch 243/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -26642.4805 — ite: 3.8989  — ate: 1.7534 — pehe: 4.4279 \n",
      "3/3 [==============================] - 1s 284ms/step - loss: -27107.9272 - val_loss: -28146.2480 - val_y0: 2.4286 - val_y1: 2.6510 - val_ate_afte_scaled: 0.2224 - lr: 5.0000e-05\n",
      "Epoch 244/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -28316.1146 — ite: 3.7977  — ate: 0.7949 — pehe: 4.3318 \n",
      "3/3 [==============================] - 1s 256ms/step - loss: -28334.7573 - val_loss: -29400.3711 - val_y0: 2.6110 - val_y1: 2.4929 - val_ate_afte_scaled: -0.1182 - lr: 5.0000e-05\n",
      "Epoch 245/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -28581.2012 — ite: 3.8030  — ate: 1.3245 — pehe: 4.2745 \n",
      "3/3 [==============================] - 0s 224ms/step - loss: -30000.4399 - val_loss: -30503.2227 - val_y0: 2.6258 - val_y1: 2.6873 - val_ate_afte_scaled: 0.0614 - lr: 5.0000e-05\n",
      "Epoch 246/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -29494.7402 — ite: 3.8354  — ate: 0.2333 — pehe: 4.0941 \n",
      "3/3 [==============================] - 0s 232ms/step - loss: -30974.3096 - val_loss: -31787.8594 - val_y0: 2.7168 - val_y1: 2.4626 - val_ate_afte_scaled: -0.2542 - lr: 5.0000e-05\n",
      "Epoch 247/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -30050.5371 — ite: 3.7024  — ate: 0.7873 — pehe: 3.8163 \n",
      "3/3 [==============================] - 0s 231ms/step - loss: -32486.4346 - val_loss: -33108.0039 - val_y0: 2.7169 - val_y1: 2.3271 - val_ate_afte_scaled: -0.3898 - lr: 5.0000e-05\n",
      "Epoch 248/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -33415.7656 — ite: 3.7318  — ate: 0.0675 — pehe: 3.9476 \n",
      "3/3 [==============================] - 1s 242ms/step - loss: -34311.8823 - val_loss: -34402.9219 - val_y0: 2.6758 - val_y1: 2.4725 - val_ate_afte_scaled: -0.2033 - lr: 5.0000e-05\n",
      "Epoch 249/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -31695.4551 — ite: 3.7821  — ate: 0.5810 — pehe: 3.9894 \n",
      "3/3 [==============================] - 0s 241ms/step - loss: -33751.9526 - val_loss: -35551.9102 - val_y0: 2.5179 - val_y1: 2.3251 - val_ate_afte_scaled: -0.1928 - lr: 5.0000e-05\n",
      "Epoch 250/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -34326.6055 — ite: 3.7000  — ate: 0.9018 — pehe: 4.0327 \n",
      "3/3 [==============================] - 0s 232ms/step - loss: -35651.5342 - val_loss: -36268.9609 - val_y0: 2.2116 - val_y1: 2.0048 - val_ate_afte_scaled: -0.2068 - lr: 5.0000e-05\n",
      "Epoch 251/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -36150.9180 — ite: 3.6867  — ate: 0.3703 — pehe: 3.9608 \n",
      "3/3 [==============================] - 0s 228ms/step - loss: -36307.8750 - val_loss: -37191.0508 - val_y0: 2.4871 - val_y1: 2.3117 - val_ate_afte_scaled: -0.1754 - lr: 5.0000e-05\n",
      "Epoch 252/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -39126.1406 — ite: 3.7533  — ate: 0.6565 — pehe: 3.9502 \n",
      "3/3 [==============================] - 1s 246ms/step - loss: -38074.7349 - val_loss: -37993.4805 - val_y0: 3.1277 - val_y1: 2.8515 - val_ate_afte_scaled: -0.2763 - lr: 5.0000e-05\n",
      "Epoch 253/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -37171.0195 — ite: 3.7689  — ate: 0.3371 — pehe: 4.1991 \n",
      "3/3 [==============================] - 0s 236ms/step - loss: -38995.2705 - val_loss: -38833.6602 - val_y0: 3.2486 - val_y1: 3.0442 - val_ate_afte_scaled: -0.2044 - lr: 5.0000e-05\n",
      "Epoch 254/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -40494.5625 — ite: 3.6978  — ate: 0.8214 — pehe: 3.9506 \n",
      "3/3 [==============================] - 0s 235ms/step - loss: -39246.0801 - val_loss: -39770.8086 - val_y0: 3.0834 - val_y1: 2.9216 - val_ate_afte_scaled: -0.1618 - lr: 5.0000e-05\n",
      "Epoch 255/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -37957.0820 — ite: 3.7640  — ate: 0.2865 — pehe: 3.9722 \n",
      "3/3 [==============================] - 1s 248ms/step - loss: -40332.4287 - val_loss: -40659.8906 - val_y0: 2.8857 - val_y1: 2.7016 - val_ate_afte_scaled: -0.1840 - lr: 5.0000e-05\n",
      "Epoch 256/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -42201.5625 — ite: 3.7343  — ate: 0.3399 — pehe: 4.0966 \n",
      "3/3 [==============================] - 1s 245ms/step - loss: -40588.4912 - val_loss: -41526.6016 - val_y0: 2.6157 - val_y1: 2.4723 - val_ate_afte_scaled: -0.1434 - lr: 5.0000e-05\n",
      "Epoch 257/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -41833.5586 — ite: 3.6725  — ate: 0.0925 — pehe: 3.8903 \n",
      "3/3 [==============================] - 1s 250ms/step - loss: -42187.5088 - val_loss: -42372.1328 - val_y0: 2.7466 - val_y1: 2.5554 - val_ate_afte_scaled: -0.1912 - lr: 5.0000e-05\n",
      "Epoch 258/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -40913.3906 — ite: 3.8377  — ate: 0.4394 — pehe: 4.1838 \n",
      "3/3 [==============================] - 1s 253ms/step - loss: -42804.6064 - val_loss: -43337.3125 - val_y0: 2.8684 - val_y1: 2.8323 - val_ate_afte_scaled: -0.0360 - lr: 5.0000e-05\n",
      "Epoch 259/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -41779.1523 — ite: 3.7286  — ate: 0.5464 — pehe: 3.8843 \n",
      "3/3 [==============================] - 1s 250ms/step - loss: -44086.6484 - val_loss: -44235.4727 - val_y0: 3.2212 - val_y1: 2.9087 - val_ate_afte_scaled: -0.3125 - lr: 5.0000e-05\n",
      "Epoch 260/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -47542.1445 — ite: 3.7201  — ate: 0.1621 — pehe: 3.9400 \n",
      "3/3 [==============================] - 0s 232ms/step - loss: -44104.2168 - val_loss: -45030.5820 - val_y0: 3.2531 - val_y1: 2.7930 - val_ate_afte_scaled: -0.4601 - lr: 5.0000e-05\n",
      "Epoch 261/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -46003.8164 — ite: 3.7784  — ate: 0.9064 — pehe: 3.9615 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: -44835.4121 - val_loss: -46225.3750 - val_y0: 2.9659 - val_y1: 2.9167 - val_ate_afte_scaled: -0.0492 - lr: 5.0000e-05\n",
      "Epoch 262/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -46064.8867 — ite: 3.7498  — ate: 0.1830 — pehe: 3.8932 \n",
      "3/3 [==============================] - 0s 220ms/step - loss: -46390.2598 - val_loss: -47036.2305 - val_y0: 3.1216 - val_y1: 2.7461 - val_ate_afte_scaled: -0.3755 - lr: 5.0000e-05\n",
      "Epoch 263/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -48011.6328 — ite: 3.7032  — ate: 0.4703 — pehe: 3.8086 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: -46996.1777 - val_loss: -48185.9453 - val_y0: 3.1925 - val_y1: 3.2031 - val_ate_afte_scaled: 0.0105 - lr: 5.0000e-05\n",
      "Epoch 264/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -49107.5859 — ite: 3.7477  — ate: 0.4608 — pehe: 4.0284 \n",
      "3/3 [==============================] - 0s 224ms/step - loss: -48021.7764 - val_loss: -49111.3125 - val_y0: 3.1926 - val_y1: 3.0491 - val_ate_afte_scaled: -0.1435 - lr: 5.0000e-05\n",
      "Epoch 265/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -50055.5000 — ite: 3.7867  — ate: 0.6304 — pehe: 4.0732 \n",
      "3/3 [==============================] - 0s 230ms/step - loss: -48712.5576 - val_loss: -50288.8086 - val_y0: 3.2356 - val_y1: 3.1846 - val_ate_afte_scaled: -0.0510 - lr: 5.0000e-05\n",
      "Epoch 266/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -46175.9766 — ite: 3.7034  — ate: 0.0347 — pehe: 3.9900 \n",
      "3/3 [==============================] - 0s 224ms/step - loss: -51585.3701 - val_loss: -51234.6406 - val_y0: 3.4702 - val_y1: 2.9818 - val_ate_afte_scaled: -0.4883 - lr: 5.0000e-05\n",
      "Epoch 267/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -47805.7812 — ite: 3.6562  — ate: 0.7090 — pehe: 3.8797 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: -52256.8877 - val_loss: -52407.3867 - val_y0: 3.4274 - val_y1: 3.0295 - val_ate_afte_scaled: -0.3979 - lr: 5.0000e-05\n",
      "Epoch 268/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -51010.4570 — ite: 3.7236  — ate: 0.0320 — pehe: 4.1076 \n",
      "3/3 [==============================] - 0s 229ms/step - loss: -51927.3213 - val_loss: -53374.7656 - val_y0: 3.2513 - val_y1: 3.0342 - val_ate_afte_scaled: -0.2171 - lr: 5.0000e-05\n",
      "Epoch 269/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -57314.4688 — ite: 3.7599  — ate: 0.8647 — pehe: 3.9865 \n",
      "3/3 [==============================] - 0s 230ms/step - loss: -52723.6494 - val_loss: -54537.5586 - val_y0: 3.5182 - val_y1: 3.0438 - val_ate_afte_scaled: -0.4744 - lr: 5.0000e-05\n",
      "Epoch 270/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -61352.7148 — ite: 3.7491  — ate: 0.4682 — pehe: 3.9501 \n",
      "3/3 [==============================] - 0s 238ms/step - loss: -54159.6416 - val_loss: -55668.3672 - val_y0: 3.4685 - val_y1: 3.2471 - val_ate_afte_scaled: -0.2213 - lr: 5.0000e-05\n",
      "Epoch 271/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -57356.9531 — ite: 3.8224  — ate: 0.5358 — pehe: 4.0561 \n",
      "3/3 [==============================] - 1s 281ms/step - loss: -55826.4336 - val_loss: -56773.2461 - val_y0: 3.6550 - val_y1: 3.2837 - val_ate_afte_scaled: -0.3713 - lr: 5.0000e-05\n",
      "Epoch 272/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -56713.5664 — ite: 3.8078  — ate: 0.4441 — pehe: 4.0017 \n",
      "3/3 [==============================] - 1s 252ms/step - loss: -57587.5088 - val_loss: -58031.6641 - val_y0: 3.7458 - val_y1: 3.3028 - val_ate_afte_scaled: -0.4430 - lr: 5.0000e-05\n",
      "Epoch 273/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -57865.1875 — ite: 3.8494  — ate: 0.5744 — pehe: 4.0627 \n",
      "3/3 [==============================] - 1s 279ms/step - loss: -57333.3770 - val_loss: -59130.5195 - val_y0: 3.7636 - val_y1: 3.3418 - val_ate_afte_scaled: -0.4217 - lr: 5.0000e-05\n",
      "Epoch 274/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -62158.3359 — ite: 3.7903  — ate: 0.2323 — pehe: 4.0648 \n",
      "3/3 [==============================] - 0s 236ms/step - loss: -58595.3857 - val_loss: -60305.0547 - val_y0: 3.9222 - val_y1: 3.4550 - val_ate_afte_scaled: -0.4672 - lr: 5.0000e-05\n",
      "Epoch 275/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -58866.0547 — ite: 3.8203  — ate: 0.4868 — pehe: 4.2368 \n",
      "3/3 [==============================] - 0s 242ms/step - loss: -59911.5186 - val_loss: -61498.5508 - val_y0: 3.7221 - val_y1: 3.5067 - val_ate_afte_scaled: -0.2154 - lr: 5.0000e-05\n",
      "Epoch 276/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -67300.3438 — ite: 3.7570  — ate: 0.4037 — pehe: 4.1686 \n",
      "3/3 [==============================] - 1s 262ms/step - loss: -61495.9482 - val_loss: -62904.3945 - val_y0: 3.8703 - val_y1: 3.4291 - val_ate_afte_scaled: -0.4412 - lr: 5.0000e-05\n",
      "Epoch 277/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -62732.9154 — ite: 3.7585  — ate: 0.1409 — pehe: 4.0160 \n",
      "3/3 [==============================] - 1s 316ms/step - loss: -63317.5166 - val_loss: -63783.6367 - val_y0: 3.9822 - val_y1: 3.5993 - val_ate_afte_scaled: -0.3829 - lr: 5.0000e-05\n",
      "Epoch 278/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -64004.4831 — ite: 3.7494  — ate: 1.0316 — pehe: 4.0464 \n",
      "3/3 [==============================] - 1s 365ms/step - loss: -64455.8838 - val_loss: -65346.2539 - val_y0: 3.8956 - val_y1: 3.4620 - val_ate_afte_scaled: -0.4336 - lr: 5.0000e-05\n",
      "Epoch 279/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -63058.4453 — ite: 3.8095  — ate: 0.0101 — pehe: 4.2220 \n",
      "3/3 [==============================] - 1s 250ms/step - loss: -66925.4814 - val_loss: -66620.9609 - val_y0: 3.8506 - val_y1: 3.3318 - val_ate_afte_scaled: -0.5188 - lr: 5.0000e-05\n",
      "Epoch 280/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -66598.0469 — ite: 3.7795  — ate: 0.7265 — pehe: 3.9981 \n",
      "3/3 [==============================] - 1s 304ms/step - loss: -66317.2500 - val_loss: -68034.9609 - val_y0: 3.8521 - val_y1: 3.3713 - val_ate_afte_scaled: -0.4808 - lr: 5.0000e-05\n",
      "Epoch 281/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -72983.0156 — ite: 3.8162  — ate: 0.8296 — pehe: 4.0254 \n",
      "3/3 [==============================] - 1s 298ms/step - loss: -65830.1758 - val_loss: -69462.0703 - val_y0: 3.8696 - val_y1: 3.3846 - val_ate_afte_scaled: -0.4850 - lr: 5.0000e-05\n",
      "Epoch 282/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -69162.7656 — ite: 3.7656  — ate: 0.1879 — pehe: 4.0233 \n",
      "3/3 [==============================] - 1s 275ms/step - loss: -68497.8379 - val_loss: -70895.9531 - val_y0: 3.9809 - val_y1: 3.4555 - val_ate_afte_scaled: -0.5254 - lr: 5.0000e-05\n",
      "Epoch 283/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -67429.8984 — ite: 3.7560  — ate: 1.3659 — pehe: 4.0535 \n",
      "3/3 [==============================] - 1s 291ms/step - loss: -70888.7676 - val_loss: -72126.8203 - val_y0: 4.1858 - val_y1: 3.6329 - val_ate_afte_scaled: -0.5529 - lr: 5.0000e-05\n",
      "Epoch 284/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -72031.5065 — ite: 3.8349  — ate: 0.2288 — pehe: 4.1068 \n",
      "3/3 [==============================] - 1s 272ms/step - loss: -73106.3486 - val_loss: -73820.7500 - val_y0: 4.1202 - val_y1: 3.6518 - val_ate_afte_scaled: -0.4684 - lr: 5.0000e-05\n",
      "Epoch 285/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -66394.6094 — ite: 3.7811  — ate: 0.3703 — pehe: 4.0066 \n",
      "3/3 [==============================] - 1s 324ms/step - loss: -74172.9238 - val_loss: -75201.0000 - val_y0: 4.1294 - val_y1: 3.8522 - val_ate_afte_scaled: -0.2772 - lr: 5.0000e-05\n",
      "Epoch 286/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -78162.8828 — ite: 3.8137  — ate: 0.2701 — pehe: 4.0365 \n",
      "3/3 [==============================] - 1s 266ms/step - loss: -76325.3574 - val_loss: -76710.2812 - val_y0: 4.3105 - val_y1: 3.8119 - val_ate_afte_scaled: -0.4987 - lr: 5.0000e-05\n",
      "Epoch 287/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -83013.0000 — ite: 3.7132  — ate: 1.0675 — pehe: 4.0892 \n",
      "3/3 [==============================] - 1s 240ms/step - loss: -75852.5137 - val_loss: -78236.5391 - val_y0: 4.4261 - val_y1: 3.7208 - val_ate_afte_scaled: -0.7054 - lr: 5.0000e-05\n",
      "Epoch 288/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -82119.5078 — ite: 3.7220  — ate: 0.1936 — pehe: 3.9771 \n",
      "3/3 [==============================] - 1s 243ms/step - loss: -78304.6191 - val_loss: -79785.6484 - val_y0: 4.1677 - val_y1: 3.6876 - val_ate_afte_scaled: -0.4800 - lr: 5.0000e-05\n",
      "Epoch 289/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -85511.1016 — ite: 3.8080  — ate: 0.5753 — pehe: 4.1226 \n",
      "3/3 [==============================] - 1s 241ms/step - loss: -78571.8223 - val_loss: -81371.0703 - val_y0: 4.3150 - val_y1: 3.8140 - val_ate_afte_scaled: -0.5010 - lr: 5.0000e-05\n",
      "Epoch 290/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -82928.1953 — ite: 3.7923  — ate: 0.2043 — pehe: 4.1534 \n",
      "3/3 [==============================] - 1s 254ms/step - loss: -81543.8574 - val_loss: -82999.9375 - val_y0: 4.3387 - val_y1: 3.8363 - val_ate_afte_scaled: -0.5024 - lr: 5.0000e-05\n",
      "Epoch 291/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -86399.0391 — ite: 3.7578  — ate: 1.0281 — pehe: 4.1783 \n",
      "3/3 [==============================] - 1s 248ms/step - loss: -80867.6484 - val_loss: -84474.1016 - val_y0: 4.4497 - val_y1: 3.9353 - val_ate_afte_scaled: -0.5143 - lr: 5.0000e-05\n",
      "Epoch 292/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -88054.4531 — ite: 3.7510  — ate: 0.2662 — pehe: 3.8607 \n",
      "3/3 [==============================] - 1s 257ms/step - loss: -84323.4277 - val_loss: -86282.5078 - val_y0: 4.4190 - val_y1: 3.9772 - val_ate_afte_scaled: -0.4418 - lr: 5.0000e-05\n",
      "Epoch 293/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -86235.9401 — ite: 3.7135  — ate: 0.3563 — pehe: 4.1054 \n",
      "3/3 [==============================] - 1s 305ms/step - loss: -86366.3125 - val_loss: -88197.1250 - val_y0: 4.6372 - val_y1: 3.9782 - val_ate_afte_scaled: -0.6590 - lr: 5.0000e-05\n",
      "Epoch 294/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -83921.9688 — ite: 3.6569  — ate: 0.4572 — pehe: 3.7976 \n",
      "3/3 [==============================] - 1s 273ms/step - loss: -88938.1621 - val_loss: -89848.9141 - val_y0: 4.4408 - val_y1: 3.9581 - val_ate_afte_scaled: -0.4827 - lr: 5.0000e-05\n",
      "Epoch 295/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -89612.7917 — ite: 3.7709  — ate: 0.0281 — pehe: 3.9836 \n",
      "3/3 [==============================] - 1s 354ms/step - loss: -89483.0469 - val_loss: -91635.4453 - val_y0: 4.5978 - val_y1: 4.0593 - val_ate_afte_scaled: -0.5385 - lr: 5.0000e-05\n",
      "Epoch 296/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -91367.1797 — ite: 3.8898  — ate: 1.3681 — pehe: 4.3484 \n",
      "3/3 [==============================] - 1s 374ms/step - loss: -91636.4961 - val_loss: -93468.2656 - val_y0: 4.5722 - val_y1: 3.9446 - val_ate_afte_scaled: -0.6276 - lr: 5.0000e-05\n",
      "Epoch 297/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -98990.5156 — ite: 3.8862  — ate: 0.2744 — pehe: 4.2557 \n",
      "3/3 [==============================] - 1s 350ms/step - loss: -93058.6836 - val_loss: -92758.2422 - val_y0: 4.5501 - val_y1: 3.9760 - val_ate_afte_scaled: -0.5741 - lr: 5.0000e-05\n",
      "Epoch 298/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -94035.3464 — ite: 3.8533  — ate: 0.9117 — pehe: 4.2966 \n",
      "3/3 [==============================] - 1s 296ms/step - loss: -93802.3867 - val_loss: -96629.6562 - val_y0: 4.0661 - val_y1: 3.5849 - val_ate_afte_scaled: -0.4813 - lr: 5.0000e-05\n",
      "Epoch 299/300\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -104574.4688 — ite: 3.7704  — ate: 0.3421 — pehe: 4.1987 \n",
      "3/3 [==============================] - 1s 331ms/step - loss: -93344.7559 - val_loss: -97912.7422 - val_y0: 4.0760 - val_y1: 3.6790 - val_ate_afte_scaled: -0.3970 - lr: 5.0000e-05\n",
      "Epoch 300/300\n",
      "3/3 [==============================] - ETA: 0s - loss: -97746.8359 — ite: 3.8147  — ate: 0.9358 — pehe: 4.0756 \n",
      "3/3 [==============================] - 1s 276ms/step - loss: -98688.2656 - val_loss: -99497.0859 - val_y0: 4.6824 - val_y1: 4.2398 - val_ate_afte_scaled: -0.4426 - lr: 5.0000e-05\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Colab command to allow us to run Colab in TF2\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "%reload_ext tensorboard \n",
    "\n",
    "model = CEVAE()\n",
    "### MAIN CODE ####\n",
    "val_split=0.2\n",
    "batch_size=64\n",
    "verbose=True\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    " \n",
    "callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='loss', patience=40, min_delta=0), \n",
    "        #40 is Shi's recommendation patience for this dataset, but you should tune for your data \n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0, cooldown=0, min_lr=0),\n",
    "        #This learning rate scheduling is quite agressive which seems good for this dataset\n",
    "        metrics_for_cevae(data,verbose),\n",
    "        tensorboard_callback\n",
    "    ]\n",
    "    \n",
    "#optimizer hyperparameters\n",
    "learning_rate = 5e-5\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate))\n",
    "\n",
    "model.fit(\n",
    "    [data['x'],data['t'],data['ys']],\n",
    "    callbacks=callbacks,\n",
    "    validation_split=val_split,\n",
    "    epochs=300,\n",
    "    batch_size=200,\n",
    "    verbose=verbose\n",
    "    )\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 45269), started 0:40:56 ago. (Use '!kill 45269' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2d44111be4a69954\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2d44111be4a69954\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
