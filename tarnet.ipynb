{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 “ihdp_npci_1-100.train.npz” 已经存在；不获取。\n",
      "\n",
      "文件 “ihdp_npci_1-100.test.npz” 已经存在；不获取。\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1344, 25)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title First load the data! (Click Play)\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz \n",
    "\n",
    "def load_IHDP_data(training_data,testing_data,i):\n",
    "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "        train_data=np.load(trf); test_data=np.load(tef)\n",
    "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
    "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
    "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
    "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    "\n",
    "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "        data['y']=data['y'].reshape(-1,1)\n",
    "        \n",
    "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "\n",
    "    return data\n",
    "\n",
    "ind = 7\n",
    "# rep = 5\n",
    "# rep = 1\n",
    "# data = load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.test.npz',i = ind)\n",
    "# for key in data:\n",
    "#     if key != 'y_scaler':\n",
    "#         data[key] = np.repeat(data[key],repeats = rep, axis = 0)\n",
    "# np.shape(data['x'])\n",
    "data_train = load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.train.npz',i = ind)\n",
    "data_valid = load_IHDP_data(training_data='./ihdp_npci_1-100.test.npz',testing_data='./ihdp_npci_1-100.test.npz',i = ind)\n",
    "np.shape(data_train['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-26 21:43:46.070050: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "#Colab command to allow us to run Colab in TF2\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "%reload_ext tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "def pdist2sq(A, B):\n",
    "    #helper for PEHEnn\n",
    "    #calculates squared euclidean distance between rows of two matrices  \n",
    "    #https://gist.github.com/mbsariyildiz/34cdc26afb630e8cae079048eef91865\n",
    "    # squared norms of each row in A and B\n",
    "    na = tf.reduce_sum(tf.square(A), 1)\n",
    "    nb = tf.reduce_sum(tf.square(B), 1)    \n",
    "    # na as a row and nb as a column vectors\n",
    "    na = tf.reshape(na, [-1, 1])\n",
    "    nb = tf.reshape(nb, [1, -1])\n",
    "    # return pairwise euclidean difference matrix\n",
    "    D=tf.reduce_sum((tf.expand_dims(A, 1)-tf.expand_dims(B, 0))**2,2) \n",
    "    return D\n",
    "\n",
    "from evaluation import Full_Metrics, metrics_for_cevae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.stats import sem\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.layers as tfkl\n",
    "tfd,tfpl = tfp.distributions,tfp.layers\n",
    "import tensorflow.keras.backend as tfkb\n",
    "\n",
    "class tarnet(tf.keras.Model):\n",
    "    def __init__(self, input_dim, reg_l2):\n",
    "        super(tarnet, self).__init__()\n",
    "        ########################################\n",
    "        # networks\n",
    "        self.activation = 'elu'\n",
    "        self.share_bottom = tfk.Sequential(\n",
    "            [\n",
    "                tfkl.InputLayer([input_dim]),\n",
    "                tfkl.Dense(100,activation = self.activation, kernel_initializer='RandomNormal'),\n",
    "                tfkl.Dense(100,activation = self.activation, kernel_initializer='RandomNormal'),\n",
    "                tfkl.Dense(100,activation = self.activation, kernel_initializer='RandomNormal'),\n",
    "            ])\n",
    "        self.tower_t0 = tfk.Sequential(\n",
    "            [\n",
    "                tfkl.InputLayer([100]),\n",
    "                tfkl.Dense(100,activation = self.activation, kernel_regularizer=regularizers.l2(reg_l2)),\n",
    "                tfkl.Dense(100,activation = self.activation, kernel_regularizer=regularizers.l2(reg_l2)),\n",
    "                tfkl.Dense(1,activation = None, kernel_regularizer=regularizers.l2(reg_l2)),\n",
    "            ])\n",
    "\n",
    "        self.tower_t1 = tfk.Sequential(\n",
    "            [\n",
    "                tfkl.InputLayer([100]),\n",
    "                tfkl.Dense(100,activation = self.activation, kernel_regularizer=regularizers.l2(reg_l2)),\n",
    "                tfkl.Dense(100,activation = self.activation, kernel_regularizer=regularizers.l2(reg_l2)),\n",
    "                tfkl.Dense(1,activation = None, kernel_regularizer=regularizers.l2(reg_l2)),\n",
    "            ])\n",
    "    def call(self, data, training=False, serving=False):\n",
    "        # Dataset_inp\n",
    "        hidden_bottom = self.share_bottom(data)\n",
    "        y_t0 = self.tower_t0(hidden_bottom)\n",
    "        y_t1 = self.tower_t1(hidden_bottom)\n",
    "    \n",
    "        output = tf.concat([y_t0,y_t1,hidden_bottom],-1)\n",
    "        return output\n",
    "\n",
    "#make model\n",
    "tarnet_model=tarnet(data_train['x'].shape[1],.01)\n",
    "# fake_inputs = tfk.Input(25,dtype = tf.float32)\n",
    "# tarnet_model(fake_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      " 1/11 [=>............................] - ETA: 10s - loss: 135.2939 - regression_loss: 131.2646WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_train_batch_end` time: 0.0056s). Check your callbacks.\n",
      "11/11 [==============================] - 2s 95ms/step - loss: 121.8382 - regression_loss: 115.3241 - val_loss: 102.8977 - val_regression_loss: 67.3580 - lr: 1.0000e-05\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 0s 45ms/step - loss: 96.6826 - regression_loss: 90.2155 - val_loss: 92.8227 - val_regression_loss: 60.1722 - lr: 1.0000e-05\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 0s 48ms/step - loss: 82.6461 - regression_loss: 76.6667 - val_loss: 83.8360 - val_regression_loss: 53.9516 - lr: 1.0000e-05\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - 0s 45ms/step - loss: 74.5042 - regression_loss: 68.4778 - val_loss: 74.2306 - val_regression_loss: 47.4680 - lr: 1.0000e-05\n",
      "Epoch 5/300\n",
      "11/11 [==============================] - 0s 41ms/step - loss: 66.7898 - regression_loss: 61.3513 - val_loss: 64.0097 - val_regression_loss: 40.6410 - lr: 1.0000e-05\n",
      "Epoch 6/300\n",
      "11/11 [==============================] - 0s 41ms/step - loss: 58.9701 - regression_loss: 53.4701 - val_loss: 53.2199 - val_regression_loss: 33.4936 - lr: 1.0000e-05\n",
      "Epoch 7/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 51.5301 - regression_loss: 46.0413 - val_loss: 43.2612 - val_regression_loss: 26.9293 - lr: 1.0000e-05\n",
      "Epoch 8/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 44.2717 - regression_loss: 39.5125 - val_loss: 37.2961 - val_regression_loss: 23.0159 - lr: 1.0000e-05\n",
      "Epoch 9/300\n",
      "11/11 [==============================] - 0s 41ms/step - loss: 40.1075 - regression_loss: 35.4783 - val_loss: 34.8021 - val_regression_loss: 21.4097 - lr: 1.0000e-05\n",
      "Epoch 10/300\n",
      "11/11 [==============================] - 0s 40ms/step - loss: 38.3479 - regression_loss: 33.4677 - val_loss: 33.9819 - val_regression_loss: 20.8677 - lr: 1.0000e-05\n",
      "Epoch 11/300\n",
      "11/11 [==============================] - 0s 41ms/step - loss: 37.1909 - regression_loss: 32.4043 - val_loss: 33.5817 - val_regression_loss: 20.5708 - lr: 1.0000e-05\n",
      "Epoch 12/300\n",
      "11/11 [==============================] - 0s 41ms/step - loss: 36.3903 - regression_loss: 31.5393 - val_loss: 33.0337 - val_regression_loss: 20.1764 - lr: 1.0000e-05\n",
      "Epoch 13/300\n",
      "11/11 [==============================] - 0s 42ms/step - loss: 35.6488 - regression_loss: 30.6846 - val_loss: 32.7342 - val_regression_loss: 19.9192 - lr: 1.0000e-05\n",
      "Epoch 14/300\n",
      "11/11 [==============================] - 0s 43ms/step - loss: 34.8428 - regression_loss: 29.9810 - val_loss: 32.2230 - val_regression_loss: 19.5459 - lr: 1.0000e-05\n",
      "Epoch 15/300\n",
      "11/11 [==============================] - 0s 41ms/step - loss: 33.9274 - regression_loss: 29.3111 - val_loss: 31.8962 - val_regression_loss: 19.2979 - lr: 1.0000e-05\n",
      "Epoch 16/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 33.5205 - regression_loss: 28.7338 - val_loss: 31.4449 - val_regression_loss: 18.9702 - lr: 1.0000e-05\n",
      "Epoch 17/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 33.0828 - regression_loss: 28.2531 - val_loss: 31.5986 - val_regression_loss: 19.0272 - lr: 1.0000e-05\n",
      "Epoch 18/300\n",
      "11/11 [==============================] - 1s 55ms/step - loss: 32.3276 - regression_loss: 27.7170 - val_loss: 31.1996 - val_regression_loss: 18.7557 - lr: 1.0000e-05\n",
      "Epoch 19/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 32.1524 - regression_loss: 27.3626 - val_loss: 31.1986 - val_regression_loss: 18.7313 - lr: 1.0000e-05\n",
      "Epoch 20/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 31.6412 - regression_loss: 26.9955 - val_loss: 31.3743 - val_regression_loss: 18.8109 - lr: 1.0000e-05\n",
      "Epoch 21/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 31.4050 - regression_loss: 26.7404 - val_loss: 30.8511 - val_regression_loss: 18.4673 - lr: 1.0000e-05\n",
      "Epoch 22/300\n",
      "11/11 [==============================] - 0s 40ms/step - loss: 30.9755 - regression_loss: 26.4118 - val_loss: 31.3729 - val_regression_loss: 18.7724 - lr: 1.0000e-05\n",
      "Epoch 23/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 30.8714 - regression_loss: 26.2966 - val_loss: 30.7779 - val_regression_loss: 18.3822 - lr: 1.0000e-05\n",
      "Epoch 24/300\n",
      "11/11 [==============================] - 0s 40ms/step - loss: 30.5828 - regression_loss: 26.0391 - val_loss: 31.0882 - val_regression_loss: 18.5641 - lr: 1.0000e-05\n",
      "Epoch 25/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 30.4044 - regression_loss: 25.7814 - val_loss: 31.2966 - val_regression_loss: 18.6804 - lr: 1.0000e-05\n",
      "Epoch 26/300\n",
      "11/11 [==============================] - 0s 40ms/step - loss: 30.4501 - regression_loss: 25.6498 - val_loss: 31.1319 - val_regression_loss: 18.5705 - lr: 1.0000e-05\n",
      "Epoch 27/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 30.1286 - regression_loss: 25.5367 - val_loss: 31.2696 - val_regression_loss: 18.6413 - lr: 1.0000e-05\n",
      "Epoch 28/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 30.0397 - regression_loss: 25.3941 - val_loss: 31.0278 - val_regression_loss: 18.4854 - lr: 1.0000e-05\n",
      "Epoch 29/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 29.9443 - regression_loss: 25.2574 - val_loss: 31.2277 - val_regression_loss: 18.6000 - lr: 1.0000e-05\n",
      "Epoch 30/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 29.6872 - regression_loss: 25.1911 - val_loss: 31.2885 - val_regression_loss: 18.6344 - lr: 1.0000e-05\n",
      "Epoch 31/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 29.6074 - regression_loss: 25.0720 - val_loss: 31.1824 - val_regression_loss: 18.5668 - lr: 1.0000e-05\n",
      "Epoch 32/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 29.5880 - regression_loss: 24.9493 - val_loss: 30.7595 - val_regression_loss: 18.2828 - lr: 1.0000e-05\n",
      "Epoch 33/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 29.4105 - regression_loss: 24.8924 - val_loss: 31.2798 - val_regression_loss: 18.6023 - lr: 1.0000e-05\n",
      "Epoch 34/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 29.4387 - regression_loss: 24.8196 - val_loss: 30.9711 - val_regression_loss: 18.4031 - lr: 1.0000e-05\n",
      "Epoch 35/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 29.4172 - regression_loss: 24.6774 - val_loss: 31.3293 - val_regression_loss: 18.6185 - lr: 1.0000e-05\n",
      "Epoch 36/300\n",
      "11/11 [==============================] - 0s 40ms/step - loss: 29.2838 - regression_loss: 24.6048 - val_loss: 30.9695 - val_regression_loss: 18.3845 - lr: 1.0000e-05\n",
      "Epoch 37/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 29.1962 - regression_loss: 24.4751 - val_loss: 31.0049 - val_regression_loss: 18.4056 - lr: 1.0000e-05\n",
      "Epoch 38/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 29.0888 - regression_loss: 24.4007 - val_loss: 30.8501 - val_regression_loss: 18.3079 - lr: 1.0000e-05\n",
      "Epoch 39/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 28.7591 - regression_loss: 24.3892 - val_loss: 30.8493 - val_regression_loss: 18.3015 - lr: 1.0000e-05\n",
      "Epoch 40/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 28.7866 - regression_loss: 24.2641 - val_loss: 31.3354 - val_regression_loss: 18.6158 - lr: 1.0000e-05\n",
      "Epoch 41/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 28.8970 - regression_loss: 24.2400 - val_loss: 30.4607 - val_regression_loss: 18.0481 - lr: 1.0000e-05\n",
      "Epoch 42/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 28.6621 - regression_loss: 24.1782 - val_loss: 30.6844 - val_regression_loss: 18.1917 - lr: 1.0000e-05\n",
      "Epoch 43/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 28.7502 - regression_loss: 24.1056 - val_loss: 30.6884 - val_regression_loss: 18.1858 - lr: 1.0000e-05\n",
      "Epoch 44/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 28.6412 - regression_loss: 24.0001 - val_loss: 30.9894 - val_regression_loss: 18.3770 - lr: 1.0000e-05\n",
      "Epoch 45/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 28.6535 - regression_loss: 23.9373 - val_loss: 30.3660 - val_regression_loss: 17.9549 - lr: 1.0000e-05\n",
      "Epoch 46/300\n",
      "11/11 [==============================] - 0s 42ms/step - loss: 28.4088 - regression_loss: 23.8634 - val_loss: 30.5400 - val_regression_loss: 18.0789 - lr: 1.0000e-05\n",
      "Epoch 47/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 28.3013 - regression_loss: 23.7977 - val_loss: 30.5576 - val_regression_loss: 18.0923 - lr: 1.0000e-05\n",
      "Epoch 48/300\n",
      "11/11 [==============================] - 0s 46ms/step - loss: 28.3907 - regression_loss: 23.7362 - val_loss: 30.6560 - val_regression_loss: 18.1491 - lr: 1.0000e-05\n",
      "Epoch 49/300\n",
      "11/11 [==============================] - 0s 43ms/step - loss: 28.2451 - regression_loss: 23.6676 - val_loss: 30.5842 - val_regression_loss: 18.1035 - lr: 1.0000e-05\n",
      "Epoch 50/300\n",
      "11/11 [==============================] - 0s 40ms/step - loss: 28.1491 - regression_loss: 23.6817 - val_loss: 30.7143 - val_regression_loss: 18.1770 - lr: 1.0000e-05\n",
      "Epoch 51/300\n",
      "11/11 [==============================] - 0s 43ms/step - loss: 28.1779 - regression_loss: 23.5689 - val_loss: 30.1594 - val_regression_loss: 17.8249 - lr: 1.0000e-05\n",
      "Epoch 52/300\n",
      "11/11 [==============================] - 0s 41ms/step - loss: 28.2301 - regression_loss: 23.4842 - val_loss: 30.1132 - val_regression_loss: 17.7848 - lr: 1.0000e-05\n",
      "Epoch 53/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 28.1014 - regression_loss: 23.3956 - val_loss: 30.1037 - val_regression_loss: 17.7748 - lr: 1.0000e-05\n",
      "Epoch 54/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 28.0506 - regression_loss: 23.4194 - val_loss: 29.8918 - val_regression_loss: 17.6405 - lr: 1.0000e-05\n",
      "Epoch 55/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 27.8797 - regression_loss: 23.3163 - val_loss: 30.0097 - val_regression_loss: 17.7058 - lr: 1.0000e-05\n",
      "Epoch 56/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 27.7742 - regression_loss: 23.2280 - val_loss: 29.9279 - val_regression_loss: 17.6669 - lr: 1.0000e-05\n",
      "Epoch 57/300\n",
      "11/11 [==============================] - 0s 41ms/step - loss: 27.6144 - regression_loss: 23.2287 - val_loss: 29.7570 - val_regression_loss: 17.5441 - lr: 1.0000e-05\n",
      "Epoch 58/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 27.6603 - regression_loss: 23.1495 - val_loss: 30.0377 - val_regression_loss: 17.7267 - lr: 1.0000e-05\n",
      "Epoch 59/300\n",
      "11/11 [==============================] - 0s 49ms/step - loss: 27.6723 - regression_loss: 23.0759 - val_loss: 29.7623 - val_regression_loss: 17.5493 - lr: 1.0000e-05\n",
      "Epoch 60/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 27.7699 - regression_loss: 23.0597 - val_loss: 30.0835 - val_regression_loss: 17.7454 - lr: 1.0000e-05\n",
      "Epoch 61/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 27.6015 - regression_loss: 22.9701 - val_loss: 29.4390 - val_regression_loss: 17.3347 - lr: 1.0000e-05\n",
      "Epoch 62/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 27.5817 - regression_loss: 22.9605 - val_loss: 29.7277 - val_regression_loss: 17.5203 - lr: 1.0000e-05\n",
      "Epoch 63/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 27.3880 - regression_loss: 22.8845 - val_loss: 29.8761 - val_regression_loss: 17.6110 - lr: 1.0000e-05\n",
      "Epoch 64/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 27.4527 - regression_loss: 22.8580 - val_loss: 29.8946 - val_regression_loss: 17.6246 - lr: 1.0000e-05\n",
      "Epoch 65/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 27.3937 - regression_loss: 22.7984 - val_loss: 29.9268 - val_regression_loss: 17.6420 - lr: 1.0000e-05\n",
      "Epoch 66/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 27.3403 - regression_loss: 22.7579 - val_loss: 29.2461 - val_regression_loss: 17.1971 - lr: 1.0000e-05\n",
      "Epoch 67/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 27.1144 - regression_loss: 22.6478 - val_loss: 29.4322 - val_regression_loss: 17.3192 - lr: 1.0000e-05\n",
      "Epoch 68/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 27.1397 - regression_loss: 22.5946 - val_loss: 29.4624 - val_regression_loss: 17.3224 - lr: 1.0000e-05\n",
      "Epoch 69/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 27.1686 - regression_loss: 22.5700 - val_loss: 29.4455 - val_regression_loss: 17.3077 - lr: 1.0000e-05\n",
      "Epoch 70/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 27.0404 - regression_loss: 22.5065 - val_loss: 29.1320 - val_regression_loss: 17.1214 - lr: 1.0000e-05\n",
      "Epoch 71/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 26.8483 - regression_loss: 22.4637 - val_loss: 28.9546 - val_regression_loss: 16.9958 - lr: 1.0000e-05\n",
      "Epoch 72/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 26.7507 - regression_loss: 22.4207 - val_loss: 29.1281 - val_regression_loss: 17.1068 - lr: 1.0000e-05\n",
      "Epoch 73/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 26.8264 - regression_loss: 22.4009 - val_loss: 29.1397 - val_regression_loss: 17.1095 - lr: 1.0000e-05\n",
      "Epoch 74/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 26.7658 - regression_loss: 22.3430 - val_loss: 29.0715 - val_regression_loss: 17.0546 - lr: 1.0000e-05\n",
      "Epoch 75/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 26.8748 - regression_loss: 22.2610 - val_loss: 29.1351 - val_regression_loss: 17.1060 - lr: 1.0000e-05\n",
      "Epoch 76/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 26.8574 - regression_loss: 22.2499 - val_loss: 28.5056 - val_regression_loss: 16.6933 - lr: 1.0000e-05\n",
      "Epoch 77/300\n",
      "11/11 [==============================] - 0s 43ms/step - loss: 26.6118 - regression_loss: 22.2126 - val_loss: 29.0641 - val_regression_loss: 17.0567 - lr: 1.0000e-05\n",
      "Epoch 78/300\n",
      "11/11 [==============================] - 0s 40ms/step - loss: 26.7805 - regression_loss: 22.1969 - val_loss: 29.2126 - val_regression_loss: 17.1410 - lr: 1.0000e-05\n",
      "Epoch 79/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 26.6684 - regression_loss: 22.1200 - val_loss: 28.5994 - val_regression_loss: 16.7399 - lr: 1.0000e-05\n",
      "Epoch 80/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 26.6971 - regression_loss: 22.0728 - val_loss: 28.7879 - val_regression_loss: 16.8701 - lr: 1.0000e-05\n",
      "Epoch 81/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 26.7153 - regression_loss: 22.0426 - val_loss: 28.6131 - val_regression_loss: 16.7454 - lr: 1.0000e-05\n",
      "Epoch 82/300\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 29.7994 - regression_loss: 25.7687\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "11/11 [==============================] - 0s 42ms/step - loss: 26.6321 - regression_loss: 21.9929 - val_loss: 28.4498 - val_regression_loss: 16.6432 - lr: 1.0000e-05\n",
      "Epoch 83/300\n",
      "11/11 [==============================] - 0s 41ms/step - loss: 26.5240 - regression_loss: 21.9121 - val_loss: 28.7556 - val_regression_loss: 16.8307 - lr: 5.0000e-06\n",
      "Epoch 84/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 26.2608 - regression_loss: 21.8827 - val_loss: 28.7369 - val_regression_loss: 16.8126 - lr: 5.0000e-06\n",
      "Epoch 85/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 26.1849 - regression_loss: 21.8617 - val_loss: 28.6939 - val_regression_loss: 16.7893 - lr: 5.0000e-06\n",
      "Epoch 86/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 26.2418 - regression_loss: 21.8657 - val_loss: 28.7312 - val_regression_loss: 16.8213 - lr: 5.0000e-06\n",
      "Epoch 87/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 26.3702 - regression_loss: 21.8205 - val_loss: 28.4940 - val_regression_loss: 16.6625 - lr: 5.0000e-06\n",
      "Epoch 88/300\n",
      "11/11 [==============================] - 0s 41ms/step - loss: 26.2194 - regression_loss: 21.8232 - val_loss: 28.4704 - val_regression_loss: 16.6365 - lr: 5.0000e-06\n",
      "Epoch 89/300\n",
      "11/11 [==============================] - 0s 47ms/step - loss: 26.3227 - regression_loss: 21.7796 - val_loss: 28.7195 - val_regression_loss: 16.8076 - lr: 5.0000e-06\n",
      "Epoch 90/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 26.1303 - regression_loss: 21.7687 - val_loss: 28.6009 - val_regression_loss: 16.7301 - lr: 5.0000e-06\n",
      "Epoch 91/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 26.3440 - regression_loss: 21.8209 - val_loss: 28.4008 - val_regression_loss: 16.6046 - lr: 5.0000e-06\n",
      "Epoch 92/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 26.2608 - regression_loss: 21.7553 - val_loss: 28.2108 - val_regression_loss: 16.4651 - lr: 5.0000e-06\n",
      "Epoch 93/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 26.2664 - regression_loss: 21.7079 - val_loss: 28.6124 - val_regression_loss: 16.7350 - lr: 5.0000e-06\n",
      "Epoch 94/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 26.1516 - regression_loss: 21.7291 - val_loss: 28.6848 - val_regression_loss: 16.7756 - lr: 5.0000e-06\n",
      "Epoch 95/300\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 25.0649 - regression_loss: 21.0350\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 26.2031 - regression_loss: 21.6711 - val_loss: 28.2889 - val_regression_loss: 16.5113 - lr: 5.0000e-06\n",
      "Epoch 96/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 26.2504 - regression_loss: 21.6496 - val_loss: 28.1873 - val_regression_loss: 16.4460 - lr: 2.5000e-06\n",
      "Epoch 97/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 26.2640 - regression_loss: 21.6338 - val_loss: 28.4065 - val_regression_loss: 16.5869 - lr: 2.5000e-06\n",
      "Epoch 98/300\n",
      "11/11 [==============================] - 0s 40ms/step - loss: 26.2290 - regression_loss: 21.6293 - val_loss: 28.6082 - val_regression_loss: 16.7227 - lr: 2.5000e-06\n",
      "Epoch 99/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 26.2509 - regression_loss: 21.6214 - val_loss: 28.3665 - val_regression_loss: 16.5614 - lr: 2.5000e-06\n",
      "Epoch 100/300\n",
      "11/11 [==============================] - 0s 40ms/step - loss: 26.1160 - regression_loss: 21.5940 - val_loss: 28.4263 - val_regression_loss: 16.6008 - lr: 2.5000e-06\n",
      "Epoch 101/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 26.1338 - regression_loss: 21.5913 - val_loss: 28.3613 - val_regression_loss: 16.5543 - lr: 2.5000e-06\n",
      "Epoch 102/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 26.1536 - regression_loss: 21.5813 - val_loss: 28.3423 - val_regression_loss: 16.5430 - lr: 2.5000e-06\n",
      "Epoch 103/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 26.2042 - regression_loss: 21.5794 - val_loss: 28.3010 - val_regression_loss: 16.5155 - lr: 2.5000e-06\n",
      "Epoch 104/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 26.1248 - regression_loss: 21.5891 - val_loss: 28.4576 - val_regression_loss: 16.6239 - lr: 2.5000e-06\n",
      "Epoch 105/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 26.0774 - regression_loss: 21.5520 - val_loss: 28.2930 - val_regression_loss: 16.5107 - lr: 2.5000e-06\n",
      "Epoch 106/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 26.0873 - regression_loss: 21.5421 - val_loss: 28.2228 - val_regression_loss: 16.4658 - lr: 2.5000e-06\n",
      "Epoch 107/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 26.0788 - regression_loss: 21.5490 - val_loss: 28.3056 - val_regression_loss: 16.5170 - lr: 2.5000e-06\n",
      "Epoch 108/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 25.9756 - regression_loss: 21.5254 - val_loss: 28.3563 - val_regression_loss: 16.5492 - lr: 2.5000e-06\n",
      "Epoch 109/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 25.8832 - regression_loss: 21.5149 - val_loss: 28.3243 - val_regression_loss: 16.5290 - lr: 2.5000e-06\n",
      "Epoch 110/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 26.0670 - regression_loss: 21.5101 - val_loss: 28.2145 - val_regression_loss: 16.4567 - lr: 2.5000e-06\n",
      "Epoch 111/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 26.1597 - regression_loss: 21.5072 - val_loss: 28.2678 - val_regression_loss: 16.4926 - lr: 2.5000e-06\n",
      "Epoch 112/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 25.9667 - regression_loss: 21.4910 - val_loss: 28.3188 - val_regression_loss: 16.5250 - lr: 2.5000e-06\n",
      "Epoch 113/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 25.9917 - regression_loss: 21.4845 - val_loss: 28.2832 - val_regression_loss: 16.5020 - lr: 2.5000e-06\n",
      "Epoch 114/300\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 23.4926 - regression_loss: 19.4632\n",
      "Epoch 00114: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 25.9529 - regression_loss: 21.4741 - val_loss: 28.2312 - val_regression_loss: 16.4668 - lr: 2.5000e-06\n",
      "Epoch 115/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 25.9155 - regression_loss: 21.4592 - val_loss: 28.3436 - val_regression_loss: 16.5409 - lr: 1.2500e-06\n",
      "Epoch 116/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 25.9937 - regression_loss: 21.4562 - val_loss: 28.3353 - val_regression_loss: 16.5349 - lr: 1.2500e-06\n",
      "Epoch 117/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 26.0556 - regression_loss: 21.4549 - val_loss: 28.1853 - val_regression_loss: 16.4354 - lr: 1.2500e-06\n",
      "Epoch 118/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 25.8853 - regression_loss: 21.4500 - val_loss: 28.2629 - val_regression_loss: 16.4877 - lr: 1.2500e-06\n",
      "Epoch 119/300\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 20.7535 - regression_loss: 16.7242\n",
      "Epoch 00119: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "11/11 [==============================] - 0s 44ms/step - loss: 25.9512 - regression_loss: 21.4421 - val_loss: 28.2337 - val_regression_loss: 16.4669 - lr: 1.2500e-06\n",
      "Epoch 120/300\n",
      "11/11 [==============================] - 0s 40ms/step - loss: 26.0318 - regression_loss: 21.4361 - val_loss: 28.1492 - val_regression_loss: 16.4099 - lr: 6.2500e-07\n",
      "Epoch 121/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 26.0510 - regression_loss: 21.4280 - val_loss: 28.1846 - val_regression_loss: 16.4327 - lr: 6.2500e-07\n",
      "Epoch 122/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 25.9943 - regression_loss: 21.4247 - val_loss: 28.1893 - val_regression_loss: 16.4358 - lr: 6.2500e-07\n",
      "Epoch 123/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 26.1281 - regression_loss: 21.4251 - val_loss: 28.2168 - val_regression_loss: 16.4537 - lr: 6.2500e-07\n",
      "Epoch 124/300\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 28.0041 - regression_loss: 23.9749\n",
      "Epoch 00124: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-07.\n",
      "11/11 [==============================] - 0s 40ms/step - loss: 25.9453 - regression_loss: 21.4205 - val_loss: 28.2137 - val_regression_loss: 16.4516 - lr: 6.2500e-07\n",
      "Epoch 125/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 25.7754 - regression_loss: 21.4170 - val_loss: 28.2253 - val_regression_loss: 16.4594 - lr: 3.1250e-07\n",
      "Epoch 126/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 25.9500 - regression_loss: 21.4170 - val_loss: 28.2130 - val_regression_loss: 16.4511 - lr: 3.1250e-07\n",
      "Epoch 127/300\n",
      "11/11 [==============================] - 0s 40ms/step - loss: 25.8247 - regression_loss: 21.4145 - val_loss: 28.2272 - val_regression_loss: 16.4600 - lr: 3.1250e-07\n",
      "Epoch 128/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 25.7470 - regression_loss: 21.4131 - val_loss: 28.2164 - val_regression_loss: 16.4532 - lr: 3.1250e-07\n",
      "Epoch 129/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 25.9612 - regression_loss: 21.4119 - val_loss: 28.2082 - val_regression_loss: 16.4479 - lr: 3.1250e-07\n",
      "Epoch 130/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 25.8980 - regression_loss: 21.4104 - val_loss: 28.2197 - val_regression_loss: 16.4550 - lr: 3.1250e-07\n",
      "Epoch 131/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 25.9001 - regression_loss: 21.4093 - val_loss: 28.2053 - val_regression_loss: 16.4456 - lr: 3.1250e-07\n",
      "Epoch 132/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 25.8202 - regression_loss: 21.4106 - val_loss: 28.2260 - val_regression_loss: 16.4595 - lr: 3.1250e-07\n",
      "Epoch 133/300\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 25.1824 - regression_loss: 21.1532\n",
      "Epoch 00133: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 25.8619 - regression_loss: 21.4083 - val_loss: 28.2213 - val_regression_loss: 16.4562 - lr: 3.1250e-07\n",
      "Epoch 134/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 25.9173 - regression_loss: 21.4056 - val_loss: 28.2172 - val_regression_loss: 16.4532 - lr: 1.5625e-07\n",
      "Epoch 135/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 25.8567 - regression_loss: 21.4050 - val_loss: 28.2052 - val_regression_loss: 16.4453 - lr: 1.5625e-07\n",
      "Epoch 136/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 26.0600 - regression_loss: 21.4045 - val_loss: 28.2095 - val_regression_loss: 16.4482 - lr: 1.5625e-07\n",
      "Epoch 137/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 25.8517 - regression_loss: 21.4049 - val_loss: 28.2096 - val_regression_loss: 16.4482 - lr: 1.5625e-07\n",
      "Epoch 138/300\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 25.5035 - regression_loss: 21.4743\n",
      "Epoch 00138: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-08.\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 25.8277 - regression_loss: 21.4032 - val_loss: 28.2031 - val_regression_loss: 16.4438 - lr: 1.5625e-07\n",
      "Epoch 139/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 26.0748 - regression_loss: 21.4023 - val_loss: 28.2034 - val_regression_loss: 16.4440 - lr: 7.8125e-08\n",
      "Epoch 140/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 25.9076 - regression_loss: 21.4023 - val_loss: 28.2009 - val_regression_loss: 16.4424 - lr: 7.8125e-08\n",
      "Epoch 141/300\n",
      "11/11 [==============================] - 0s 41ms/step - loss: 26.0174 - regression_loss: 21.4022 - val_loss: 28.1968 - val_regression_loss: 16.4395 - lr: 7.8125e-08\n",
      "Epoch 142/300\n",
      "11/11 [==============================] - 0s 41ms/step - loss: 25.9169 - regression_loss: 21.4014 - val_loss: 28.1996 - val_regression_loss: 16.4415 - lr: 7.8125e-08\n",
      "Epoch 143/300\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 25.5626 - regression_loss: 21.5334\n",
      "Epoch 00143: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-08.\n",
      "11/11 [==============================] - 0s 43ms/step - loss: 25.9325 - regression_loss: 21.4016 - val_loss: 28.1992 - val_regression_loss: 16.4411 - lr: 7.8125e-08\n",
      "Epoch 144/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 26.0505 - regression_loss: 21.4007 - val_loss: 28.2008 - val_regression_loss: 16.4422 - lr: 3.9062e-08\n",
      "Epoch 145/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 25.9120 - regression_loss: 21.4006 - val_loss: 28.1995 - val_regression_loss: 16.4414 - lr: 3.9062e-08\n",
      "Epoch 146/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 25.8612 - regression_loss: 21.4006 - val_loss: 28.1985 - val_regression_loss: 16.4407 - lr: 3.9062e-08\n",
      "Epoch 147/300\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 25.7448 - regression_loss: 21.4004 - val_loss: 28.2006 - val_regression_loss: 16.4421 - lr: 3.9062e-08\n",
      "Epoch 148/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 25.9051 - regression_loss: 21.4001 - val_loss: 28.2016 - val_regression_loss: 16.4428 - lr: 3.9062e-08\n",
      "Epoch 149/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 26.0687 - regression_loss: 21.4002 - val_loss: 28.2040 - val_regression_loss: 16.4443 - lr: 3.9062e-08\n",
      "Epoch 150/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 25.9997 - regression_loss: 21.4000 - val_loss: 28.2005 - val_regression_loss: 16.4420 - lr: 3.9062e-08\n",
      "Epoch 151/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 25.9019 - regression_loss: 21.4000 - val_loss: 28.2023 - val_regression_loss: 16.4432 - lr: 3.9062e-08\n",
      "Epoch 152/300\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 24.2008 - regression_loss: 20.1715\n",
      "Epoch 00152: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-08.\n",
      "11/11 [==============================] - 0s 41ms/step - loss: 25.8991 - regression_loss: 21.3997 - val_loss: 28.2026 - val_regression_loss: 16.4434 - lr: 3.9062e-08\n",
      "Epoch 153/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 26.0803 - regression_loss: 21.3995 - val_loss: 28.2010 - val_regression_loss: 16.4423 - lr: 1.9531e-08\n",
      "Epoch 154/300\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 25.9746 - regression_loss: 21.3994 - val_loss: 28.2007 - val_regression_loss: 16.4421 - lr: 1.9531e-08\n",
      "Epoch 155/300\n",
      "11/11 [==============================] - 1s 51ms/step - loss: 25.8062 - regression_loss: 21.3992 - val_loss: 28.2010 - val_regression_loss: 16.4423 - lr: 1.9531e-08\n",
      "Epoch 156/300\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 25.9582 - regression_loss: 21.3992 - val_loss: 28.2011 - val_regression_loss: 16.4423 - lr: 1.9531e-08\n",
      "Epoch 157/300\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 26.5236 - regression_loss: 22.4944\n",
      "Epoch 00157: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-09.\n",
      "11/11 [==============================] - 0s 42ms/step - loss: 25.9809 - regression_loss: 21.3993 - val_loss: 28.2025 - val_regression_loss: 16.4433 - lr: 1.9531e-08\n",
      "Epoch 158/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 26.0681 - regression_loss: 21.3990 - val_loss: 28.2014 - val_regression_loss: 16.4425 - lr: 9.7656e-09\n",
      "Epoch 159/300\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 25.8253 - regression_loss: 21.3990 - val_loss: 28.2016 - val_regression_loss: 16.4426 - lr: 9.7656e-09\n",
      "Epoch 160/300\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 25.9932 - regression_loss: 21.3989 - val_loss: 28.2014 - val_regression_loss: 16.4425 - lr: 9.7656e-09\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# every loss function in TF2 takes 2 arguments, a vector of true values and a vector predictions\n",
    "def regression_loss(concat_true, concat_pred):\n",
    "    #computes a standard MSE loss for TARNet\n",
    "    y_true = concat_true[:, 0] #get individual vectors\n",
    "    t_true = concat_true[:, 1]\n",
    " \n",
    "    y0_pred = concat_pred[:, 0]\n",
    "    y1_pred = concat_pred[:, 1]\n",
    " \n",
    "    #Each head outputs a prediction for both potential outcomes\n",
    "    #We use t_true as a switch to only calculate the factual loss\n",
    "    loss0 = tf.reduce_sum((1. - t_true) * tf.square(y_true - y0_pred))\n",
    "    loss1 = tf.reduce_sum(t_true * tf.square(y_true - y1_pred))\n",
    "    #note Shi uses tf.reduce_sum for her losses even though mathematically we should be using the mean\n",
    "    #tf.reduce_mean and tf.reduce_sum should be equivalent, but maybe having larger error gradients makes training easier?\n",
    "    return loss0 + loss1\n",
    " \n",
    "### MAIN CODE ####\n",
    " \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD\n",
    " \n",
    "# val_split=0.2\n",
    "batch_size=128\n",
    "verbose=True\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    "yt = np.concatenate([data_train['ys'], data_train['t']], 1) #we'll use both y and t to compute the loss\n",
    " \n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    " \n",
    "sgd_callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='val_loss', patience=40, min_delta=0), \n",
    "        #40 is Shi's recommendation patience for this dataset, but you should tune for your data \n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0, cooldown=0, min_lr=0),\n",
    "        #This learning rate scheduling is quite agressive which seems good for this dataset\n",
    "        Full_Metrics(data_train,'train',verbose),\n",
    "        Full_Metrics(data_valid,'valid',verbose),\n",
    "        tensorboard_callback\n",
    "    ]\n",
    "#optimizer hyperparameters\n",
    "sgd_lr = 1e-5\n",
    "momentum = 0.9\n",
    "tarnet_model.compile(optimizer=SGD(lr=sgd_lr, momentum=momentum, nesterov=True),\n",
    "                    loss=regression_loss,\n",
    "                    metrics=regression_loss)\n",
    " \n",
    "tarnet_model.fit(x=data_train['x'],y=yt,\n",
    "                callbacks=sgd_callbacks,\n",
    "                validation_data=[data_valid['x'], np.concatenate([data_valid['ys'], data_valid['t']], 1)],\n",
    "                epochs=300,\n",
    "                batch_size=batch_size,\n",
    "                verbose=verbose)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 45269), started 2 days, 0:30:17 ago. (Use '!kill 45269' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6843ca6b53796c88\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6843ca6b53796c88\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
