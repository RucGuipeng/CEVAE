{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 “ihdp_npci_1-100.train.npz” 已经存在；不获取。\n",
      "\n",
      "文件 “ihdp_npci_1-100.test.npz” 已经存在；不获取。\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(747, 25)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title First load the data! (Click Play)\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz \n",
    "\n",
    "def load_IHDP_data(training_data,testing_data,i):\n",
    "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "        train_data=np.load(trf); test_data=np.load(tef)\n",
    "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
    "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
    "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
    "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    "\n",
    "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "        data['y']=data['y'].reshape(-1,1)\n",
    "        \n",
    "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "\n",
    "    return data\n",
    "\n",
    "ind = 7\n",
    "# rep = 5\n",
    "# rep = 1\n",
    "# data = load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.test.npz',i = ind)\n",
    "# for key in data:\n",
    "#     if key != 'y_scaler':\n",
    "#         data[key] = np.repeat(data[key],repeats = rep, axis = 0)\n",
    "# np.shape(data['x'])\n",
    "data_train = load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.train.npz',i = ind)\n",
    "data_valid = load_IHDP_data(training_data='./ihdp_npci_1-100.test.npz',testing_data='./ihdp_npci_1-100.test.npz',i = ind)\n",
    "np.shape(data_train['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-26 21:38:59.011742: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "#Colab command to allow us to run Colab in TF2\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "%reload_ext tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "def pdist2sq(A, B):\n",
    "    #helper for PEHEnn\n",
    "    #calculates squared euclidean distance between rows of two matrices  \n",
    "    #https://gist.github.com/mbsariyildiz/34cdc26afb630e8cae079048eef91865\n",
    "    # squared norms of each row in A and B\n",
    "    na = tf.reduce_sum(tf.square(A), 1)\n",
    "    nb = tf.reduce_sum(tf.square(B), 1)    \n",
    "    # na as a row and nb as a column vectors\n",
    "    na = tf.reshape(na, [-1, 1])\n",
    "    nb = tf.reshape(nb, [1, -1])\n",
    "    # return pairwise euclidean difference matrix\n",
    "    D=tf.reduce_sum((tf.expand_dims(A, 1)-tf.expand_dims(B, 0))**2,2) \n",
    "    return D\n",
    "\n",
    "from evaluation import Full_Metrics, metrics_for_cevae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.stats import sem\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.layers as tfkl\n",
    "tfd,tfpl = tfp.distributions,tfp.layers\n",
    "import tensorflow.keras.backend as tfkb\n",
    "\n",
    "class tarnet(tf.keras.Model):\n",
    "    def __init__(self, input_dim, reg_l2):\n",
    "        super(tarnet, self).__init__()\n",
    "        ########################################\n",
    "        # networks\n",
    "        self.activation = 'elu'\n",
    "        self.share_bottom = tfk.Sequential(\n",
    "            [\n",
    "                tfkl.InputLayer([input_dim]),\n",
    "                tfkl.Dense(100,activation = self.activation, kernel_initializer='RandomNormal'),\n",
    "                tfkl.Dense(100,activation = self.activation, kernel_initializer='RandomNormal'),\n",
    "                tfkl.Dense(100,activation = self.activation, kernel_initializer='RandomNormal'),\n",
    "            ])\n",
    "        self.tower_t0 = tfk.Sequential(\n",
    "            [\n",
    "                tfkl.InputLayer([100]),\n",
    "                tfkl.Dense(100,activation = self.activation, kernel_regularizer=regularizers.l2(reg_l2)),\n",
    "                tfkl.Dense(100,activation = self.activation, kernel_regularizer=regularizers.l2(reg_l2)),\n",
    "                tfkl.Dense(1,activation = None, kernel_regularizer=regularizers.l2(reg_l2)),\n",
    "            ])\n",
    "\n",
    "        self.tower_t1 = tfk.Sequential(\n",
    "            [\n",
    "                tfkl.InputLayer([100]),\n",
    "                tfkl.Dense(100,activation = self.activation, kernel_regularizer=regularizers.l2(reg_l2)),\n",
    "                tfkl.Dense(100,activation = self.activation, kernel_regularizer=regularizers.l2(reg_l2)),\n",
    "                tfkl.Dense(1,activation = None, kernel_regularizer=regularizers.l2(reg_l2)),\n",
    "            ])\n",
    "    def call(self, data, training=False, serving=False):\n",
    "        # Dataset_inp\n",
    "        hidden_bottom = self.share_bottom(data)\n",
    "        y_t0 = self.tower_t0(hidden_bottom)\n",
    "        y_t1 = self.tower_t1(hidden_bottom)\n",
    "    \n",
    "        output = tf.concat([y_t0,y_t1,hidden_bottom],-1)\n",
    "        return output\n",
    "\n",
    "#make model\n",
    "tarnet_model=tarnet(data_train['x'].shape[1],.01)\n",
    "# fake_inputs = tfk.Input(25,dtype = tf.float32)\n",
    "# tarnet_model(fake_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/6 [====>.........................] - ETA: 4s - loss: 158.3189 - regression_loss: 154.2744WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_train_batch_end` time: 0.0037s). Check your callbacks.\n",
      " - train_rmse: 2.5231 - train_rmse_ite:3.7793 — train_ate_err: 3.4608  — train_cate_err: 3.7793 — train_cate_nn_err: 4.1913 \n",
      " - valid_rmse: 2.5231 - valid_rmse_ite:3.7793 — valid_ate_err: 3.4608  — valid_cate_err: 3.7793 — valid_cate_nn_err: 4.1913 \n",
      "6/6 [==============================] - 2s 222ms/step - loss: 128.2760 - regression_loss: 123.7725 - val_loss: 120.2053 - val_regression_loss: 115.9875 - lr: 1.0000e-05\n",
      "Epoch 2/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 122.1193 - regression_loss: 118.0749 - train_rmse: 2.3789 - train_rmse_ite:2.9725 — train_ate_err: 2.5668  — train_cate_err: 2.9725 — train_cate_nn_err: 3.4952 \n",
      " - valid_rmse: 2.3789 - valid_rmse_ite:2.9725 — valid_ate_err: 2.5668  — valid_cate_err: 2.9725 — valid_cate_nn_err: 3.4952 \n",
      "6/6 [==============================] - 0s 93ms/step - loss: 114.6337 - regression_loss: 110.1408 - val_loss: 105.6151 - val_regression_loss: 101.3829 - lr: 1.0000e-05\n",
      "Epoch 3/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 104.2565 - regression_loss: 100.2121 - train_rmse: 2.2858 - train_rmse_ite:2.2898 — train_ate_err: 1.7510  — train_cate_err: 2.2898 — train_cate_nn_err: 2.9788 \n",
      " - valid_rmse: 2.2858 - valid_rmse_ite:2.2898 — valid_ate_err: 1.7510  — valid_cate_err: 2.2898 — valid_cate_nn_err: 2.9788 \n",
      "6/6 [==============================] - 1s 111ms/step - loss: 101.5553 - regression_loss: 97.2381 - val_loss: 95.6972 - val_regression_loss: 91.4568 - lr: 1.0000e-05\n",
      "Epoch 4/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 136.9850 - regression_loss: 132.9404 - train_rmse: 2.2343 - train_rmse_ite:1.8479 — train_ate_err: 1.1493  — train_cate_err: 1.8479 — train_cate_nn_err: 2.6759 \n",
      " - valid_rmse: 2.2343 - valid_rmse_ite:1.8479 — valid_ate_err: 1.1493  — valid_cate_err: 1.8479 — valid_cate_nn_err: 2.6759 \n",
      "6/6 [==============================] - 0s 83ms/step - loss: 93.1956 - regression_loss: 88.6110 - val_loss: 89.1963 - val_regression_loss: 84.9654 - lr: 1.0000e-05\n",
      "Epoch 5/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 107.1991 - regression_loss: 103.1542 - train_rmse: 2.2059 - train_rmse_ite:1.6116 — train_ate_err: 0.7734  — train_cate_err: 1.6116 — train_cate_nn_err: 2.5419 \n",
      " - valid_rmse: 2.2059 - valid_rmse_ite:1.6116 — valid_ate_err: 0.7734  — valid_cate_err: 1.6116 — valid_cate_nn_err: 2.5419 \n",
      "6/6 [==============================] - 0s 81ms/step - loss: 87.5696 - regression_loss: 82.8328 - val_loss: 84.4334 - val_regression_loss: 80.2297 - lr: 1.0000e-05\n",
      "Epoch 6/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 57.3277 - regression_loss: 53.2825 - train_rmse: 2.1948 - train_rmse_ite:1.4618 — train_ate_err: 0.4894  — train_cate_err: 1.4618 — train_cate_nn_err: 2.4572 \n",
      " - valid_rmse: 2.1948 - valid_rmse_ite:1.4618 — valid_ate_err: 0.4894  — valid_cate_err: 1.4618 — valid_cate_nn_err: 2.4572 \n",
      "6/6 [==============================] - 0s 94ms/step - loss: 82.8017 - regression_loss: 78.7268 - val_loss: 80.8505 - val_regression_loss: 76.6712 - lr: 1.0000e-05\n",
      "Epoch 7/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 71.5921 - regression_loss: 67.5465 - train_rmse: 2.1947 - train_rmse_ite:1.3802 — train_ate_err: 0.3511  — train_cate_err: 1.3802 — train_cate_nn_err: 2.4285 \n",
      " - valid_rmse: 2.1947 - valid_rmse_ite:1.3802 — valid_ate_err: 0.3511  — valid_cate_err: 1.3802 — valid_cate_nn_err: 2.4285 \n",
      "6/6 [==============================] - 0s 75ms/step - loss: 79.5854 - regression_loss: 75.3564 - val_loss: 77.6291 - val_regression_loss: 73.4777 - lr: 1.0000e-05\n",
      "Epoch 8/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 77.5323 - regression_loss: 73.4864 - train_rmse: 2.2024 - train_rmse_ite:1.3165 — train_ate_err: 0.2833  — train_cate_err: 1.3165 — train_cate_nn_err: 2.4188 \n",
      " - valid_rmse: 2.2024 - valid_rmse_ite:1.3165 — valid_ate_err: 0.2833  — valid_cate_err: 1.3165 — valid_cate_nn_err: 2.4188 \n",
      "6/6 [==============================] - 0s 74ms/step - loss: 76.5143 - regression_loss: 72.1984 - val_loss: 74.4199 - val_regression_loss: 70.2880 - lr: 1.0000e-05\n",
      "Epoch 9/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 77.1507 - regression_loss: 73.1044 - train_rmse: 2.2153 - train_rmse_ite:1.2548 — train_ate_err: 0.2456  — train_cate_err: 1.2548 — train_cate_nn_err: 2.4133 \n",
      " - valid_rmse: 2.2153 - valid_rmse_ite:1.2548 — valid_ate_err: 0.2456  — valid_cate_err: 1.2548 — valid_cate_nn_err: 2.4133 \n",
      "6/6 [==============================] - 0s 61ms/step - loss: 73.2797 - regression_loss: 68.9192 - val_loss: 70.8741 - val_regression_loss: 66.7550 - lr: 1.0000e-05\n",
      "Epoch 10/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 76.6190 - regression_loss: 72.5722 - train_rmse: 2.2341 - train_rmse_ite:1.1915 — train_ate_err: 0.2313  — train_cate_err: 1.1915 — train_cate_nn_err: 2.4108 \n",
      " - valid_rmse: 2.2341 - valid_rmse_ite:1.1915 — valid_ate_err: 0.2313  — valid_cate_err: 1.1915 — valid_cate_nn_err: 2.4108 \n",
      "6/6 [==============================] - 0s 65ms/step - loss: 69.7011 - regression_loss: 65.3569 - val_loss: 66.8665 - val_regression_loss: 62.7577 - lr: 1.0000e-05\n",
      "Epoch 11/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 72.9332 - regression_loss: 68.8860 - train_rmse: 2.2599 - train_rmse_ite:1.1209 — train_ate_err: 0.1827  — train_cate_err: 1.1209 — train_cate_nn_err: 2.4145 \n",
      " - valid_rmse: 2.2599 - valid_rmse_ite:1.1209 — valid_ate_err: 0.1827  — valid_cate_err: 1.1209 — valid_cate_nn_err: 2.4145 \n",
      "6/6 [==============================] - 0s 66ms/step - loss: 65.2883 - regression_loss: 61.1235 - val_loss: 62.6457 - val_regression_loss: 58.5417 - lr: 1.0000e-05\n",
      "Epoch 12/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 69.7207 - regression_loss: 65.6730 - train_rmse: 2.2969 - train_rmse_ite:1.0601 — train_ate_err: 0.2031  — train_cate_err: 1.0601 — train_cate_nn_err: 2.4626 \n",
      " - valid_rmse: 2.2969 - valid_rmse_ite:1.0601 — valid_ate_err: 0.2031  — valid_cate_err: 1.0601 — valid_cate_nn_err: 2.4626 \n",
      "6/6 [==============================] - 0s 63ms/step - loss: 61.0662 - regression_loss: 56.7948 - val_loss: 58.1575 - val_regression_loss: 54.0621 - lr: 1.0000e-05\n",
      "Epoch 13/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 59.1980 - regression_loss: 55.1496 - train_rmse: 2.3455 - train_rmse_ite:1.0104 — train_ate_err: 0.2353  — train_cate_err: 1.0104 — train_cate_nn_err: 2.5206 \n",
      " - valid_rmse: 2.3455 - valid_rmse_ite:1.0104 — valid_ate_err: 0.2353  — valid_cate_err: 1.0104 — valid_cate_nn_err: 2.5206 \n",
      "6/6 [==============================] - 0s 62ms/step - loss: 56.8812 - regression_loss: 52.3027 - val_loss: 53.7437 - val_regression_loss: 49.6616 - lr: 1.0000e-05\n",
      "Epoch 14/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 57.5223 - regression_loss: 53.4733 - train_rmse: 2.4079 - train_rmse_ite:0.9800 — train_ate_err: 0.2770  — train_cate_err: 0.9800 — train_cate_nn_err: 2.5481 \n",
      " - valid_rmse: 2.4079 - valid_rmse_ite:0.9800 — valid_ate_err: 0.2770  — valid_cate_err: 0.9800 — valid_cate_nn_err: 2.5481 \n",
      "6/6 [==============================] - 0s 66ms/step - loss: 52.3864 - regression_loss: 48.1806 - val_loss: 49.6678 - val_regression_loss: 45.5963 - lr: 1.0000e-05\n",
      "Epoch 15/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 46.8589 - regression_loss: 42.8092 - train_rmse: 2.4766 - train_rmse_ite:0.9613 — train_ate_err: 0.2650  — train_cate_err: 0.9613 — train_cate_nn_err: 2.6038 \n",
      " - valid_rmse: 2.4766 - valid_rmse_ite:0.9613 — valid_ate_err: 0.2650  — valid_cate_err: 0.9613 — valid_cate_nn_err: 2.6038 \n",
      "6/6 [==============================] - 0s 64ms/step - loss: 48.7179 - regression_loss: 44.2543 - val_loss: 46.4025 - val_regression_loss: 42.3371 - lr: 1.0000e-05\n",
      "Epoch 16/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 53.2738 - regression_loss: 49.2234 - train_rmse: 2.5494 - train_rmse_ite:0.9619 — train_ate_err: 0.2511  — train_cate_err: 0.9619 — train_cate_nn_err: 2.6714 \n",
      " - valid_rmse: 2.5494 - valid_rmse_ite:0.9619 — valid_ate_err: 0.2511  — valid_cate_err: 0.9619 — valid_cate_nn_err: 2.6714 \n",
      "6/6 [==============================] - 0s 64ms/step - loss: 45.6270 - regression_loss: 41.3215 - val_loss: 43.9226 - val_regression_loss: 39.8582 - lr: 1.0000e-05\n",
      "Epoch 17/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 56.7974 - regression_loss: 52.7464 - train_rmse: 2.6188 - train_rmse_ite:0.9752 — train_ate_err: 0.2338  — train_cate_err: 0.9752 — train_cate_nn_err: 2.7371 \n",
      " - valid_rmse: 2.6188 - valid_rmse_ite:0.9752 — valid_ate_err: 0.2338  — valid_cate_err: 0.9752 — valid_cate_nn_err: 2.7371 \n",
      "6/6 [==============================] - 0s 64ms/step - loss: 43.5470 - regression_loss: 39.2537 - val_loss: 42.2392 - val_regression_loss: 38.1727 - lr: 1.0000e-05\n",
      "Epoch 18/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 51.9840 - regression_loss: 47.9324 - train_rmse: 2.6737 - train_rmse_ite:0.9857 — train_ate_err: 0.2033  — train_cate_err: 0.9857 — train_cate_nn_err: 2.7900 \n",
      " - valid_rmse: 2.6737 - valid_rmse_ite:0.9857 — valid_ate_err: 0.2033  — valid_cate_err: 0.9857 — valid_cate_nn_err: 2.7900 \n",
      "6/6 [==============================] - 0s 68ms/step - loss: 41.8546 - regression_loss: 37.7593 - val_loss: 41.1536 - val_regression_loss: 37.0824 - lr: 1.0000e-05\n",
      "Epoch 19/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 34.2654 - regression_loss: 30.2134 - train_rmse: 2.7089 - train_rmse_ite:0.9899 — train_ate_err: 0.1936  — train_cate_err: 0.9899 — train_cate_nn_err: 2.8349 \n",
      " - valid_rmse: 2.7089 - valid_rmse_ite:0.9899 — valid_ate_err: 0.1936  — valid_cate_err: 0.9899 — valid_cate_nn_err: 2.8349 \n",
      "6/6 [==============================] - 0s 72ms/step - loss: 40.8615 - regression_loss: 36.7318 - val_loss: 40.3909 - val_regression_loss: 36.3180 - lr: 1.0000e-05\n",
      "Epoch 20/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 37.6449 - regression_loss: 33.5926 - train_rmse: 2.7362 - train_rmse_ite:0.9868 — train_ate_err: 0.1865  — train_cate_err: 0.9868 — train_cate_nn_err: 2.8604 \n",
      " - valid_rmse: 2.7362 - valid_rmse_ite:0.9868 — valid_ate_err: 0.1865  — valid_cate_err: 0.9868 — valid_cate_nn_err: 2.8604 \n",
      "6/6 [==============================] - 0s 61ms/step - loss: 40.1725 - regression_loss: 36.1183 - val_loss: 39.7374 - val_regression_loss: 35.6613 - lr: 1.0000e-05\n",
      "Epoch 21/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 40.0866 - regression_loss: 36.0342 - train_rmse: 2.7455 - train_rmse_ite:0.9691 — train_ate_err: 0.1761  — train_cate_err: 0.9691 — train_cate_nn_err: 2.8519 \n",
      " - valid_rmse: 2.7455 - valid_rmse_ite:0.9691 — valid_ate_err: 0.1761  — valid_cate_err: 0.9691 — valid_cate_nn_err: 2.8519 \n",
      "6/6 [==============================] - 0s 64ms/step - loss: 39.5953 - regression_loss: 35.4309 - val_loss: 39.1271 - val_regression_loss: 35.0503 - lr: 1.0000e-05\n",
      "Epoch 22/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 51.7879 - regression_loss: 47.7354 - train_rmse: 2.7507 - train_rmse_ite:0.9548 — train_ate_err: 0.1922  — train_cate_err: 0.9548 — train_cate_nn_err: 2.8337 \n",
      " - valid_rmse: 2.7507 - valid_rmse_ite:0.9548 — valid_ate_err: 0.1922  — valid_cate_err: 0.9548 — valid_cate_nn_err: 2.8337 \n",
      "6/6 [==============================] - 0s 65ms/step - loss: 39.1642 - regression_loss: 34.9829 - val_loss: 38.5589 - val_regression_loss: 34.4861 - lr: 1.0000e-05\n",
      "Epoch 23/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 31.8390 - regression_loss: 27.7864 - train_rmse: 2.7503 - train_rmse_ite:0.9183 — train_ate_err: 0.1314  — train_cate_err: 0.9183 — train_cate_nn_err: 2.8142 \n",
      " - valid_rmse: 2.7503 - valid_rmse_ite:0.9183 — valid_ate_err: 0.1314  — valid_cate_err: 0.9183 — valid_cate_nn_err: 2.8142 \n",
      "6/6 [==============================] - 0s 64ms/step - loss: 38.3111 - regression_loss: 34.3451 - val_loss: 38.0429 - val_regression_loss: 33.9619 - lr: 1.0000e-05\n",
      "Epoch 24/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 39.9113 - regression_loss: 35.8588 - train_rmse: 2.7424 - train_rmse_ite:0.8954 — train_ate_err: 0.1538  — train_cate_err: 0.8954 — train_cate_nn_err: 2.8059 \n",
      " - valid_rmse: 2.7424 - valid_rmse_ite:0.8954 — valid_ate_err: 0.1538  — valid_cate_err: 0.8954 — valid_cate_nn_err: 2.8059 \n",
      "6/6 [==============================] - 0s 67ms/step - loss: 38.0954 - regression_loss: 33.8269 - val_loss: 37.5128 - val_regression_loss: 33.4394 - lr: 1.0000e-05\n",
      "Epoch 25/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 37.7513 - regression_loss: 33.6988 - train_rmse: 2.7421 - train_rmse_ite:0.8720 — train_ate_err: 0.1446  — train_cate_err: 0.8720 — train_cate_nn_err: 2.7971 \n",
      " - valid_rmse: 2.7421 - valid_rmse_ite:0.8720 — valid_ate_err: 0.1446  — valid_cate_err: 0.8720 — valid_cate_nn_err: 2.7971 \n",
      "6/6 [==============================] - 0s 64ms/step - loss: 37.4999 - regression_loss: 33.3329 - val_loss: 37.0527 - val_regression_loss: 32.9769 - lr: 1.0000e-05\n",
      "Epoch 26/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 41.3355 - regression_loss: 37.2830 - train_rmse: 2.7449 - train_rmse_ite:0.8613 — train_ate_err: 0.1791  — train_cate_err: 0.8613 — train_cate_nn_err: 2.7913 \n",
      " - valid_rmse: 2.7449 - valid_rmse_ite:0.8613 — valid_ate_err: 0.1791  — valid_cate_err: 0.8613 — valid_cate_nn_err: 2.7913 \n",
      "6/6 [==============================] - 0s 66ms/step - loss: 37.1278 - regression_loss: 32.8980 - val_loss: 36.6227 - val_regression_loss: 32.5534 - lr: 1.0000e-05\n",
      "Epoch 27/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 37.9483 - regression_loss: 33.8958 - train_rmse: 2.7498 - train_rmse_ite:0.8423 — train_ate_err: 0.1566  — train_cate_err: 0.8423 — train_cate_nn_err: 2.7831 \n",
      " - valid_rmse: 2.7498 - valid_rmse_ite:0.8423 — valid_ate_err: 0.1566  — valid_cate_err: 0.8423 — valid_cate_nn_err: 2.7831 \n",
      "6/6 [==============================] - 0s 91ms/step - loss: 36.7602 - regression_loss: 32.4325 - val_loss: 36.2118 - val_regression_loss: 32.1389 - lr: 1.0000e-05\n",
      "Epoch 28/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 35.9557 - regression_loss: 31.9032 - train_rmse: 2.7529 - train_rmse_ite:0.8249 — train_ate_err: 0.1398  — train_cate_err: 0.8249 — train_cate_nn_err: 2.7680 \n",
      " - valid_rmse: 2.7529 - valid_rmse_ite:0.8249 — valid_ate_err: 0.1398  — valid_cate_err: 0.8249 — valid_cate_nn_err: 2.7680 \n",
      "6/6 [==============================] - 0s 63ms/step - loss: 36.4218 - regression_loss: 32.0323 - val_loss: 35.8391 - val_regression_loss: 31.7647 - lr: 1.0000e-05\n",
      "Epoch 29/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 43.4594 - regression_loss: 39.4069 - train_rmse: 2.7561 - train_rmse_ite:0.8061 — train_ate_err: 0.1203  — train_cate_err: 0.8061 — train_cate_nn_err: 2.7607 \n",
      " - valid_rmse: 2.7561 - valid_rmse_ite:0.8061 — valid_ate_err: 0.1203  — valid_cate_err: 0.8061 — valid_cate_nn_err: 2.7607 \n",
      "6/6 [==============================] - 0s 59ms/step - loss: 35.8181 - regression_loss: 31.7104 - val_loss: 35.5099 - val_regression_loss: 31.4325 - lr: 1.0000e-05\n",
      "Epoch 30/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 40.7074 - regression_loss: 36.6549 - train_rmse: 2.7569 - train_rmse_ite:0.7952 — train_ate_err: 0.1262  — train_cate_err: 0.7952 — train_cate_nn_err: 2.7461 \n",
      " - valid_rmse: 2.7569 - valid_rmse_ite:0.7952 — valid_ate_err: 0.1262  — valid_cate_err: 0.7952 — valid_cate_nn_err: 2.7461 \n",
      "6/6 [==============================] - 0s 58ms/step - loss: 35.3545 - regression_loss: 31.3817 - val_loss: 35.1827 - val_regression_loss: 31.1076 - lr: 1.0000e-05\n",
      "Epoch 31/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 42.9019 - regression_loss: 38.8494 - train_rmse: 2.7666 - train_rmse_ite:0.7940 — train_ate_err: 0.1658  — train_cate_err: 0.7940 — train_cate_nn_err: 2.7525 \n",
      " - valid_rmse: 2.7666 - valid_rmse_ite:0.7940 — valid_ate_err: 0.1658  — valid_cate_err: 0.7940 — valid_cate_nn_err: 2.7525 \n",
      "6/6 [==============================] - 0s 57ms/step - loss: 35.3808 - regression_loss: 31.1230 - val_loss: 34.8574 - val_regression_loss: 30.7873 - lr: 1.0000e-05\n",
      "Epoch 32/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 35.1155 - regression_loss: 31.0629 - train_rmse: 2.7680 - train_rmse_ite:0.7812 — train_ate_err: 0.1668  — train_cate_err: 0.7812 — train_cate_nn_err: 2.7437 \n",
      " - valid_rmse: 2.7680 - valid_rmse_ite:0.7812 — valid_ate_err: 0.1668  — valid_cate_err: 0.7812 — valid_cate_nn_err: 2.7437 \n",
      "6/6 [==============================] - 0s 63ms/step - loss: 35.0062 - regression_loss: 30.7234 - val_loss: 34.5766 - val_regression_loss: 30.5065 - lr: 1.0000e-05\n",
      "Epoch 33/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 30.4567 - regression_loss: 26.4041 - train_rmse: 2.7709 - train_rmse_ite:0.7609 — train_ate_err: 0.1222  — train_cate_err: 0.7609 — train_cate_nn_err: 2.7394 \n",
      " - valid_rmse: 2.7709 - valid_rmse_ite:0.7609 — valid_ate_err: 0.1222  — valid_cate_err: 0.7609 — valid_cate_nn_err: 2.7394 \n",
      "6/6 [==============================] - 0s 58ms/step - loss: 34.5530 - regression_loss: 30.4484 - val_loss: 34.3102 - val_regression_loss: 30.2356 - lr: 1.0000e-05\n",
      "Epoch 34/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 27.6613 - regression_loss: 23.6087 - train_rmse: 2.7744 - train_rmse_ite:0.7532 — train_ate_err: 0.1308  — train_cate_err: 0.7532 — train_cate_nn_err: 2.7376 \n",
      " - valid_rmse: 2.7744 - valid_rmse_ite:0.7532 — valid_ate_err: 0.1308  — valid_cate_err: 0.7532 — valid_cate_nn_err: 2.7376 \n",
      "6/6 [==============================] - 0s 58ms/step - loss: 34.5530 - regression_loss: 30.2411 - val_loss: 34.0581 - val_regression_loss: 29.9842 - lr: 1.0000e-05\n",
      "Epoch 35/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 37.9973 - regression_loss: 33.9447 - train_rmse: 2.7761 - train_rmse_ite:0.7487 — train_ate_err: 0.1527  — train_cate_err: 0.7487 — train_cate_nn_err: 2.7318 \n",
      " - valid_rmse: 2.7761 - valid_rmse_ite:0.7487 — valid_ate_err: 0.1527  — valid_cate_err: 0.7487 — valid_cate_nn_err: 2.7318 \n",
      "6/6 [==============================] - 0s 70ms/step - loss: 34.1099 - regression_loss: 29.9256 - val_loss: 33.8330 - val_regression_loss: 29.7633 - lr: 1.0000e-05\n",
      "Epoch 36/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 32.1899 - regression_loss: 28.1373 - train_rmse: 2.7755 - train_rmse_ite:0.7324 — train_ate_err: 0.1282  — train_cate_err: 0.7324 — train_cate_nn_err: 2.7223 \n",
      " - valid_rmse: 2.7755 - valid_rmse_ite:0.7324 — valid_ate_err: 0.1282  — valid_cate_err: 0.7324 — valid_cate_nn_err: 2.7223 \n",
      "6/6 [==============================] - 0s 78ms/step - loss: 33.6829 - regression_loss: 29.7358 - val_loss: 33.6160 - val_regression_loss: 29.5426 - lr: 1.0000e-05\n",
      "Epoch 37/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 30.3254 - regression_loss: 26.2728 - train_rmse: 2.7799 - train_rmse_ite:0.7244 — train_ate_err: 0.1305  — train_cate_err: 0.7244 — train_cate_nn_err: 2.7253 \n",
      " - valid_rmse: 2.7799 - valid_rmse_ite:0.7244 — valid_ate_err: 0.1305  — valid_cate_err: 0.7244 — valid_cate_nn_err: 2.7253 \n",
      "6/6 [==============================] - 0s 59ms/step - loss: 33.7584 - regression_loss: 29.5272 - val_loss: 33.4157 - val_regression_loss: 29.3426 - lr: 1.0000e-05\n",
      "Epoch 38/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 37.9470 - regression_loss: 33.8944 - train_rmse: 2.7847 - train_rmse_ite:0.7127 — train_ate_err: 0.1137  — train_cate_err: 0.7127 — train_cate_nn_err: 2.7235 \n",
      " - valid_rmse: 2.7847 - valid_rmse_ite:0.7127 — valid_ate_err: 0.1137  — valid_cate_err: 0.7127 — valid_cate_nn_err: 2.7235 \n",
      "6/6 [==============================] - 0s 59ms/step - loss: 33.5145 - regression_loss: 29.3853 - val_loss: 33.2272 - val_regression_loss: 29.1513 - lr: 1.0000e-05\n",
      "Epoch 39/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 35.1139 - regression_loss: 31.0613 - train_rmse: 2.7888 - train_rmse_ite:0.7170 — train_ate_err: 0.1512  — train_cate_err: 0.7170 — train_cate_nn_err: 2.7169 \n",
      " - valid_rmse: 2.7888 - valid_rmse_ite:0.7170 — valid_ate_err: 0.1512  — valid_cate_err: 0.7170 — valid_cate_nn_err: 2.7169 \n",
      "6/6 [==============================] - 0s 71ms/step - loss: 33.4603 - regression_loss: 29.1685 - val_loss: 33.0433 - val_regression_loss: 28.9743 - lr: 1.0000e-05\n",
      "Epoch 40/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 34.9442 - regression_loss: 30.8916 - train_rmse: 2.7883 - train_rmse_ite:0.7058 — train_ate_err: 0.1416  — train_cate_err: 0.7058 — train_cate_nn_err: 2.7118 \n",
      " - valid_rmse: 2.7883 - valid_rmse_ite:0.7058 — valid_ate_err: 0.1416  — valid_cate_err: 0.7058 — valid_cate_nn_err: 2.7118 \n",
      "6/6 [==============================] - 0s 68ms/step - loss: 33.2342 - regression_loss: 28.9836 - val_loss: 32.8687 - val_regression_loss: 28.7981 - lr: 1.0000e-05\n",
      "Epoch 41/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 34.1456 - regression_loss: 30.0930 - train_rmse: 2.7903 - train_rmse_ite:0.6876 — train_ate_err: 0.0919  — train_cate_err: 0.6876 — train_cate_nn_err: 2.7037 \n",
      " - valid_rmse: 2.7903 - valid_rmse_ite:0.6876 — valid_ate_err: 0.0919  — valid_cate_err: 0.6876 — valid_cate_nn_err: 2.7037 \n",
      "6/6 [==============================] - 0s 56ms/step - loss: 33.0957 - regression_loss: 28.7737 - val_loss: 32.7335 - val_regression_loss: 28.6556 - lr: 1.0000e-05\n",
      "Epoch 42/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 38.2946 - regression_loss: 34.2420 - train_rmse: 2.7906 - train_rmse_ite:0.6838 — train_ate_err: 0.1130  — train_cate_err: 0.6838 — train_cate_nn_err: 2.7073 \n",
      " - valid_rmse: 2.7906 - valid_rmse_ite:0.6838 — valid_ate_err: 0.1130  — valid_cate_err: 0.6838 — valid_cate_nn_err: 2.7073 \n",
      "6/6 [==============================] - 0s 57ms/step - loss: 32.6152 - regression_loss: 28.6738 - val_loss: 32.5623 - val_regression_loss: 28.4863 - lr: 1.0000e-05\n",
      "Epoch 43/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 29.4519 - regression_loss: 25.3993 - train_rmse: 2.7944 - train_rmse_ite:0.6851 — train_ate_err: 0.1414  — train_cate_err: 0.6851 — train_cate_nn_err: 2.7112 \n",
      " - valid_rmse: 2.7944 - valid_rmse_ite:0.6851 — valid_ate_err: 0.1414  — valid_cate_err: 0.6851 — valid_cate_nn_err: 2.7112 \n",
      "6/6 [==============================] - 0s 57ms/step - loss: 32.5288 - regression_loss: 28.4750 - val_loss: 32.4224 - val_regression_loss: 28.3504 - lr: 1.0000e-05\n",
      "Epoch 44/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 28.0498 - regression_loss: 23.9973 - train_rmse: 2.7985 - train_rmse_ite:0.6824 — train_ate_err: 0.1419  — train_cate_err: 0.6824 — train_cate_nn_err: 2.7082 \n",
      " - valid_rmse: 2.7985 - valid_rmse_ite:0.6824 — valid_ate_err: 0.1419  — valid_cate_err: 0.6824 — valid_cate_nn_err: 2.7082 \n",
      "6/6 [==============================] - 0s 56ms/step - loss: 32.5286 - regression_loss: 28.3574 - val_loss: 32.2871 - val_regression_loss: 28.2161 - lr: 1.0000e-05\n",
      "Epoch 45/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 33.7778 - regression_loss: 29.7252 - train_rmse: 2.8025 - train_rmse_ite:0.6730 — train_ate_err: 0.1205  — train_cate_err: 0.6730 — train_cate_nn_err: 2.7032 \n",
      " - valid_rmse: 2.8025 - valid_rmse_ite:0.6730 — valid_ate_err: 0.1205  — valid_cate_err: 0.6730 — valid_cate_nn_err: 2.7032 \n",
      "6/6 [==============================] - 0s 58ms/step - loss: 32.4866 - regression_loss: 28.2354 - val_loss: 32.1610 - val_regression_loss: 28.0877 - lr: 1.0000e-05\n",
      "Epoch 46/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 32.1618 - regression_loss: 28.1093 - train_rmse: 2.7987 - train_rmse_ite:0.6595 — train_ate_err: 0.1011  — train_cate_err: 0.6595 — train_cate_nn_err: 2.6905 \n",
      " - valid_rmse: 2.7987 - valid_rmse_ite:0.6595 — valid_ate_err: 0.1011  — valid_cate_err: 0.6595 — valid_cate_nn_err: 2.6905 \n",
      "6/6 [==============================] - 0s 57ms/step - loss: 32.3561 - regression_loss: 28.1011 - val_loss: 32.0502 - val_regression_loss: 27.9741 - lr: 1.0000e-05\n",
      "Epoch 47/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 37.9071 - regression_loss: 33.8547 - train_rmse: 2.7984 - train_rmse_ite:0.6601 — train_ate_err: 0.1327  — train_cate_err: 0.6601 — train_cate_nn_err: 2.6920 \n",
      " - valid_rmse: 2.7984 - valid_rmse_ite:0.6601 — valid_ate_err: 0.1327  — valid_cate_err: 0.6601 — valid_cate_nn_err: 2.6920 \n",
      "6/6 [==============================] - 0s 58ms/step - loss: 32.1026 - regression_loss: 28.0050 - val_loss: 31.9296 - val_regression_loss: 27.8573 - lr: 1.0000e-05\n",
      "Epoch 48/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 32.5343 - regression_loss: 28.4819 - train_rmse: 2.7964 - train_rmse_ite:0.6500 — train_ate_err: 0.1170  — train_cate_err: 0.6500 — train_cate_nn_err: 2.6912 \n",
      " - valid_rmse: 2.7964 - valid_rmse_ite:0.6500 — valid_ate_err: 0.1170  — valid_cate_err: 0.6500 — valid_cate_nn_err: 2.6912 \n",
      "6/6 [==============================] - 0s 57ms/step - loss: 32.0255 - regression_loss: 27.8401 - val_loss: 31.8241 - val_regression_loss: 27.7492 - lr: 1.0000e-05\n",
      "Epoch 49/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 36.7777 - regression_loss: 32.7254 - train_rmse: 2.8001 - train_rmse_ite:0.6466 — train_ate_err: 0.1090  — train_cate_err: 0.6466 — train_cate_nn_err: 2.6891 \n",
      " - valid_rmse: 2.8001 - valid_rmse_ite:0.6466 — valid_ate_err: 0.1090  — valid_cate_err: 0.6466 — valid_cate_nn_err: 2.6891 \n",
      "6/6 [==============================] - 0s 58ms/step - loss: 31.9857 - regression_loss: 27.7488 - val_loss: 31.7185 - val_regression_loss: 27.6421 - lr: 1.0000e-05\n",
      "Epoch 50/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 27.4103 - regression_loss: 23.3580 - train_rmse: 2.8014 - train_rmse_ite:0.6444 — train_ate_err: 0.1176  — train_cate_err: 0.6444 — train_cate_nn_err: 2.6873 \n",
      " - valid_rmse: 2.8014 - valid_rmse_ite:0.6444 — valid_ate_err: 0.1176  — valid_cate_err: 0.6444 — valid_cate_nn_err: 2.6873 \n",
      "6/6 [==============================] - 0s 84ms/step - loss: 31.6636 - regression_loss: 27.6687 - val_loss: 31.6190 - val_regression_loss: 27.5425 - lr: 1.0000e-05\n",
      "Epoch 51/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 30.3757 - regression_loss: 26.3234 - train_rmse: 2.8046 - train_rmse_ite:0.6449 — train_ate_err: 0.1336  — train_cate_err: 0.6449 — train_cate_nn_err: 2.6889 \n",
      " - valid_rmse: 2.8046 - valid_rmse_ite:0.6449 — valid_ate_err: 0.1336  — valid_cate_err: 0.6449 — valid_cate_nn_err: 2.6889 \n",
      "6/6 [==============================] - 0s 89ms/step - loss: 31.6290 - regression_loss: 27.5466 - val_loss: 31.5183 - val_regression_loss: 27.4439 - lr: 1.0000e-05\n",
      "Epoch 52/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 37.6381 - regression_loss: 33.5858 - train_rmse: 2.8107 - train_rmse_ite:0.6428 — train_ate_err: 0.1301  — train_cate_err: 0.6428 — train_cate_nn_err: 2.7005 \n",
      " - valid_rmse: 2.8107 - valid_rmse_ite:0.6428 — valid_ate_err: 0.1301  — valid_cate_err: 0.6428 — valid_cate_nn_err: 2.7005 \n",
      "6/6 [==============================] - 0s 68ms/step - loss: 31.8068 - regression_loss: 27.5600 - val_loss: 31.4268 - val_regression_loss: 27.3525 - lr: 1.0000e-05\n",
      "Epoch 53/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 30.6961 - regression_loss: 26.6439 - train_rmse: 2.8096 - train_rmse_ite:0.6286 — train_ate_err: 0.0862  — train_cate_err: 0.6286 — train_cate_nn_err: 2.6966 \n",
      " - valid_rmse: 2.8096 - valid_rmse_ite:0.6286 — valid_ate_err: 0.0862  — valid_cate_err: 0.6286 — valid_cate_nn_err: 2.6966 \n",
      "6/6 [==============================] - 0s 57ms/step - loss: 31.4227 - regression_loss: 27.3691 - val_loss: 31.3579 - val_regression_loss: 27.2797 - lr: 1.0000e-05\n",
      "Epoch 54/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 29.1629 - regression_loss: 25.1107 - train_rmse: 2.8071 - train_rmse_ite:0.6292 — train_ate_err: 0.1116  — train_cate_err: 0.6292 — train_cate_nn_err: 2.6927 \n",
      " - valid_rmse: 2.8071 - valid_rmse_ite:0.6292 — valid_ate_err: 0.1116  — valid_cate_err: 0.6292 — valid_cate_nn_err: 2.6927 \n",
      "6/6 [==============================] - 0s 66ms/step - loss: 31.4251 - regression_loss: 27.2892 - val_loss: 31.2523 - val_regression_loss: 27.1778 - lr: 1.0000e-05\n",
      "Epoch 55/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 35.4937 - regression_loss: 31.4415 - train_rmse: 2.8062 - train_rmse_ite:0.6244 — train_ate_err: 0.1060  — train_cate_err: 0.6244 — train_cate_nn_err: 2.6872 \n",
      " - valid_rmse: 2.8062 - valid_rmse_ite:0.6244 — valid_ate_err: 0.1060  — valid_cate_err: 0.6244 — valid_cate_nn_err: 2.6872 \n",
      "6/6 [==============================] - 0s 66ms/step - loss: 31.4929 - regression_loss: 27.2077 - val_loss: 31.1726 - val_regression_loss: 27.0969 - lr: 1.0000e-05\n",
      "Epoch 56/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 37.7344 - regression_loss: 33.6823 - train_rmse: 2.8077 - train_rmse_ite:0.6198 — train_ate_err: 0.1022  — train_cate_err: 0.6198 — train_cate_nn_err: 2.6915 \n",
      " - valid_rmse: 2.8077 - valid_rmse_ite:0.6198 — valid_ate_err: 0.1022  — valid_cate_err: 0.6198 — valid_cate_nn_err: 2.6915 \n",
      "6/6 [==============================] - 0s 56ms/step - loss: 31.2750 - regression_loss: 27.1012 - val_loss: 31.0923 - val_regression_loss: 27.0169 - lr: 1.0000e-05\n",
      "Epoch 57/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 33.3085 - regression_loss: 29.2565 - train_rmse: 2.8090 - train_rmse_ite:0.6148 — train_ate_err: 0.0956  — train_cate_err: 0.6148 — train_cate_nn_err: 2.6884 \n",
      " - valid_rmse: 2.8090 - valid_rmse_ite:0.6148 — valid_ate_err: 0.0956  — valid_cate_err: 0.6148 — valid_cate_nn_err: 2.6884 \n",
      "6/6 [==============================] - 0s 58ms/step - loss: 31.1492 - regression_loss: 27.0536 - val_loss: 31.0163 - val_regression_loss: 26.9400 - lr: 1.0000e-05\n",
      "Epoch 58/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 31.7639 - regression_loss: 27.7119 - train_rmse: 2.8104 - train_rmse_ite:0.6119 — train_ate_err: 0.1001  — train_cate_err: 0.6119 — train_cate_nn_err: 2.6887 \n",
      " - valid_rmse: 2.8104 - valid_rmse_ite:0.6119 — valid_ate_err: 0.1001  — valid_cate_err: 0.6119 — valid_cate_nn_err: 2.6887 \n",
      "6/6 [==============================] - 0s 57ms/step - loss: 31.2339 - regression_loss: 27.0186 - val_loss: 30.9424 - val_regression_loss: 26.8668 - lr: 1.0000e-05\n",
      "Epoch 59/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 31.9581 - regression_loss: 27.9061 - train_rmse: 2.8148 - train_rmse_ite:0.6145 — train_ate_err: 0.1098  — train_cate_err: 0.6145 — train_cate_nn_err: 2.6881 \n",
      " - valid_rmse: 2.8148 - valid_rmse_ite:0.6145 — valid_ate_err: 0.1098  — valid_cate_err: 0.6145 — valid_cate_nn_err: 2.6881 \n",
      "6/6 [==============================] - 0s 58ms/step - loss: 31.0996 - regression_loss: 26.9279 - val_loss: 30.8669 - val_regression_loss: 26.7924 - lr: 1.0000e-05\n",
      "Epoch 60/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 27.9273 - regression_loss: 23.8753 - train_rmse: 2.8156 - train_rmse_ite:0.6066 — train_ate_err: 0.0786  — train_cate_err: 0.6066 — train_cate_nn_err: 2.6812 \n",
      " - valid_rmse: 2.8156 - valid_rmse_ite:0.6066 — valid_ate_err: 0.0786  — valid_cate_err: 0.6066 — valid_cate_nn_err: 2.6812 \n",
      "6/6 [==============================] - 0s 57ms/step - loss: 31.0196 - regression_loss: 26.8533 - val_loss: 30.8125 - val_regression_loss: 26.7341 - lr: 1.0000e-05\n",
      "Epoch 61/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 32.5203 - regression_loss: 28.4684 - train_rmse: 2.8194 - train_rmse_ite:0.6099 — train_ate_err: 0.1112  — train_cate_err: 0.6099 — train_cate_nn_err: 2.6936 \n",
      " - valid_rmse: 2.8194 - valid_rmse_ite:0.6099 — valid_ate_err: 0.1112  — valid_cate_err: 0.6099 — valid_cate_nn_err: 2.6936 \n",
      "6/6 [==============================] - 0s 58ms/step - loss: 30.9340 - regression_loss: 26.7756 - val_loss: 30.7272 - val_regression_loss: 26.6520 - lr: 1.0000e-05\n",
      "Epoch 62/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 28.1495 - regression_loss: 24.0976 - train_rmse: 2.8167 - train_rmse_ite:0.6054 — train_ate_err: 0.1105  — train_cate_err: 0.6054 — train_cate_nn_err: 2.6845 \n",
      " - valid_rmse: 2.8167 - valid_rmse_ite:0.6054 — valid_ate_err: 0.1105  — valid_cate_err: 0.6054 — valid_cate_nn_err: 2.6845 \n",
      "6/6 [==============================] - 0s 60ms/step - loss: 30.7785 - regression_loss: 26.7032 - val_loss: 30.6598 - val_regression_loss: 26.5853 - lr: 1.0000e-05\n",
      "Epoch 63/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 29.4942 - regression_loss: 25.4424 - train_rmse: 2.8139 - train_rmse_ite:0.5967 — train_ate_err: 0.0763  — train_cate_err: 0.5967 — train_cate_nn_err: 2.6860 \n",
      " - valid_rmse: 2.8139 - valid_rmse_ite:0.5967 — valid_ate_err: 0.0763  — valid_cate_err: 0.5967 — valid_cate_nn_err: 2.6860 \n",
      "6/6 [==============================] - 0s 62ms/step - loss: 30.6201 - regression_loss: 26.6720 - val_loss: 30.6095 - val_regression_loss: 26.5305 - lr: 1.0000e-05\n",
      "Epoch 64/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 36.6463 - regression_loss: 32.5946 - train_rmse: 2.8165 - train_rmse_ite:0.6012 — train_ate_err: 0.1137  — train_cate_err: 0.6012 — train_cate_nn_err: 2.6715 \n",
      " - valid_rmse: 2.8165 - valid_rmse_ite:0.6012 — valid_ate_err: 0.1137  — valid_cate_err: 0.6012 — valid_cate_nn_err: 2.6715 \n",
      "6/6 [==============================] - 0s 63ms/step - loss: 30.8133 - regression_loss: 26.6138 - val_loss: 30.5313 - val_regression_loss: 26.4560 - lr: 1.0000e-05\n",
      "Epoch 65/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 34.5332 - regression_loss: 30.4816 - train_rmse: 2.8134 - train_rmse_ite:0.5915 — train_ate_err: 0.0883  — train_cate_err: 0.5915 — train_cate_nn_err: 2.6766 \n",
      " - valid_rmse: 2.8134 - valid_rmse_ite:0.5915 — valid_ate_err: 0.0883  — valid_cate_err: 0.5915 — valid_cate_nn_err: 2.6766 \n",
      "6/6 [==============================] - 0s 60ms/step - loss: 30.7985 - regression_loss: 26.5065 - val_loss: 30.4690 - val_regression_loss: 26.3911 - lr: 1.0000e-05\n",
      "Epoch 66/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 26.9418 - regression_loss: 22.8902 - train_rmse: 2.8143 - train_rmse_ite:0.5913 — train_ate_err: 0.0993  — train_cate_err: 0.5913 — train_cate_nn_err: 2.6793 \n",
      " - valid_rmse: 2.8143 - valid_rmse_ite:0.5913 — valid_ate_err: 0.0993  — valid_cate_err: 0.5913 — valid_cate_nn_err: 2.6793 \n",
      "6/6 [==============================] - 0s 57ms/step - loss: 30.6593 - regression_loss: 26.4060 - val_loss: 30.4085 - val_regression_loss: 26.3302 - lr: 1.0000e-05\n",
      "Epoch 67/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 30.8364 - regression_loss: 26.7849 - train_rmse: 2.8184 - train_rmse_ite:0.5938 — train_ate_err: 0.1172  — train_cate_err: 0.5938 — train_cate_nn_err: 2.6852 \n",
      " - valid_rmse: 2.8184 - valid_rmse_ite:0.5938 — valid_ate_err: 0.1172  — valid_cate_err: 0.5938 — valid_cate_nn_err: 2.6852 \n",
      "6/6 [==============================] - 0s 58ms/step - loss: 30.5000 - regression_loss: 26.3460 - val_loss: 30.3477 - val_regression_loss: 26.2705 - lr: 1.0000e-05\n",
      "Epoch 68/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 30.9730 - regression_loss: 26.9215 - train_rmse: 2.8194 - train_rmse_ite:0.5907 — train_ate_err: 0.1096  — train_cate_err: 0.5907 — train_cate_nn_err: 2.6791 \n",
      " - valid_rmse: 2.8194 - valid_rmse_ite:0.5907 — valid_ate_err: 0.1096  — valid_cate_err: 0.5907 — valid_cate_nn_err: 2.6791 \n",
      "6/6 [==============================] - 0s 58ms/step - loss: 30.4349 - regression_loss: 26.3252 - val_loss: 30.2858 - val_regression_loss: 26.2087 - lr: 1.0000e-05\n",
      "Epoch 69/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 31.3249 - regression_loss: 27.2734 - train_rmse: 2.8204 - train_rmse_ite:0.5857 — train_ate_err: 0.0894  — train_cate_err: 0.5857 — train_cate_nn_err: 2.6790 \n",
      " - valid_rmse: 2.8204 - valid_rmse_ite:0.5857 — valid_ate_err: 0.0894  — valid_cate_err: 0.5857 — valid_cate_nn_err: 2.6790 \n",
      "6/6 [==============================] - 0s 58ms/step - loss: 30.4237 - regression_loss: 26.2316 - val_loss: 30.2331 - val_regression_loss: 26.1539 - lr: 1.0000e-05\n",
      "Epoch 70/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 32.6294 - regression_loss: 28.5780 - train_rmse: 2.8220 - train_rmse_ite:0.5796 — train_ate_err: 0.0812  — train_cate_err: 0.5796 — train_cate_nn_err: 2.6813 \n",
      " - valid_rmse: 2.8220 - valid_rmse_ite:0.5796 — valid_ate_err: 0.0812  — valid_cate_err: 0.5796 — valid_cate_nn_err: 2.6813 \n",
      "6/6 [==============================] - 0s 62ms/step - loss: 30.3380 - regression_loss: 26.2678 - val_loss: 30.1764 - val_regression_loss: 26.0971 - lr: 1.0000e-05\n",
      "Epoch 71/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 34.5352 - regression_loss: 30.4839 - train_rmse: 2.8229 - train_rmse_ite:0.5867 — train_ate_err: 0.1182  — train_cate_err: 0.5867 — train_cate_nn_err: 2.6791 \n",
      " - valid_rmse: 2.8229 - valid_rmse_ite:0.5867 — valid_ate_err: 0.1182  — valid_cate_err: 0.5867 — valid_cate_nn_err: 2.6791 \n",
      "6/6 [==============================] - 0s 60ms/step - loss: 30.3181 - regression_loss: 26.1084 - val_loss: 30.1239 - val_regression_loss: 26.0477 - lr: 1.0000e-05\n",
      "Epoch 72/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 30.8889 - regression_loss: 26.8376 - train_rmse: 2.8246 - train_rmse_ite:0.5845 — train_ate_err: 0.1153  — train_cate_err: 0.5845 — train_cate_nn_err: 2.6752 \n",
      " - valid_rmse: 2.8246 - valid_rmse_ite:0.5845 — valid_ate_err: 0.1153  — valid_cate_err: 0.5845 — valid_cate_nn_err: 2.6752 \n",
      "6/6 [==============================] - 0s 59ms/step - loss: 30.2564 - regression_loss: 26.0986 - val_loss: 30.0612 - val_regression_loss: 25.9838 - lr: 1.0000e-05\n",
      "Epoch 73/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 26.3059 - regression_loss: 22.2547 - train_rmse: 2.8251 - train_rmse_ite:0.5759 — train_ate_err: 0.0803  — train_cate_err: 0.5759 — train_cate_nn_err: 2.6716 \n",
      " - valid_rmse: 2.8251 - valid_rmse_ite:0.5759 — valid_ate_err: 0.0803  — valid_cate_err: 0.5759 — valid_cate_nn_err: 2.6716 \n",
      "6/6 [==============================] - 0s 59ms/step - loss: 30.3078 - regression_loss: 26.0841 - val_loss: 30.0084 - val_regression_loss: 25.9290 - lr: 1.0000e-05\n",
      "Epoch 74/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 35.4283 - regression_loss: 31.3771 - train_rmse: 2.8262 - train_rmse_ite:0.5797 — train_ate_err: 0.0994  — train_cate_err: 0.5797 — train_cate_nn_err: 2.6690 \n",
      " - valid_rmse: 2.8262 - valid_rmse_ite:0.5797 — valid_ate_err: 0.0994  — valid_cate_err: 0.5797 — valid_cate_nn_err: 2.6690 \n",
      "6/6 [==============================] - 0s 59ms/step - loss: 30.0916 - regression_loss: 25.9822 - val_loss: 29.9543 - val_regression_loss: 25.8764 - lr: 1.0000e-05\n",
      "Epoch 75/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 29.7478 - regression_loss: 25.6967 - train_rmse: 2.8295 - train_rmse_ite:0.5762 — train_ate_err: 0.0951  — train_cate_err: 0.5762 — train_cate_nn_err: 2.6766 \n",
      " - valid_rmse: 2.8295 - valid_rmse_ite:0.5762 — valid_ate_err: 0.0951  — valid_cate_err: 0.5762 — valid_cate_nn_err: 2.6766 \n",
      "6/6 [==============================] - 0s 62ms/step - loss: 30.0918 - regression_loss: 25.9512 - val_loss: 29.9006 - val_regression_loss: 25.8217 - lr: 1.0000e-05\n",
      "Epoch 76/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 26.0782 - regression_loss: 22.0271 - train_rmse: 2.8282 - train_rmse_ite:0.5726 — train_ate_err: 0.0808  — train_cate_err: 0.5726 — train_cate_nn_err: 2.6663 \n",
      " - valid_rmse: 2.8282 - valid_rmse_ite:0.5726 — valid_ate_err: 0.0808  — valid_cate_err: 0.5726 — valid_cate_nn_err: 2.6663 \n",
      "6/6 [==============================] - 0s 58ms/step - loss: 29.9900 - regression_loss: 25.8724 - val_loss: 29.8514 - val_regression_loss: 25.7717 - lr: 1.0000e-05\n",
      "Epoch 77/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 31.0473 - regression_loss: 26.9963 - train_rmse: 2.8306 - train_rmse_ite:0.5690 — train_ate_err: 0.0720  — train_cate_err: 0.5690 — train_cate_nn_err: 2.6691 \n",
      " - valid_rmse: 2.8306 - valid_rmse_ite:0.5690 — valid_ate_err: 0.0720  — valid_cate_err: 0.5690 — valid_cate_nn_err: 2.6691 \n",
      "6/6 [==============================] - 0s 62ms/step - loss: 29.9240 - regression_loss: 25.8104 - val_loss: 29.8057 - val_regression_loss: 25.7251 - lr: 1.0000e-05\n",
      "Epoch 78/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 33.3948 - regression_loss: 29.3438 - train_rmse: 2.8244 - train_rmse_ite:0.5705 — train_ate_err: 0.1000  — train_cate_err: 0.5705 — train_cate_nn_err: 2.6663 \n",
      " - valid_rmse: 2.8244 - valid_rmse_ite:0.5705 — valid_ate_err: 0.1000  — valid_cate_err: 0.5705 — valid_cate_nn_err: 2.6663 \n",
      "6/6 [==============================] - 0s 61ms/step - loss: 29.8738 - regression_loss: 25.7706 - val_loss: 29.7484 - val_regression_loss: 25.6689 - lr: 1.0000e-05\n",
      "Epoch 79/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 32.8080 - regression_loss: 28.7572 - train_rmse: 2.8254 - train_rmse_ite:0.5725 — train_ate_err: 0.1141  — train_cate_err: 0.5725 — train_cate_nn_err: 2.6641 \n",
      " - valid_rmse: 2.8254 - valid_rmse_ite:0.5725 — valid_ate_err: 0.1141  — valid_cate_err: 0.5725 — valid_cate_nn_err: 2.6641 \n",
      "6/6 [==============================] - 0s 60ms/step - loss: 29.9485 - regression_loss: 25.7737 - val_loss: 29.7049 - val_regression_loss: 25.6273 - lr: 1.0000e-05\n",
      "Epoch 80/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 26.5912 - regression_loss: 22.5404 - train_rmse: 2.8217 - train_rmse_ite:0.5577 — train_ate_err: 0.0604  — train_cate_err: 0.5577 — train_cate_nn_err: 2.6651 \n",
      " - valid_rmse: 2.8217 - valid_rmse_ite:0.5577 — valid_ate_err: 0.0604  — valid_cate_err: 0.5577 — valid_cate_nn_err: 2.6651 \n",
      "6/6 [==============================] - 0s 61ms/step - loss: 29.7874 - regression_loss: 25.6666 - val_loss: 29.6812 - val_regression_loss: 25.5970 - lr: 1.0000e-05\n",
      "Epoch 81/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 27.6834 - regression_loss: 23.6327 - train_rmse: 2.8232 - train_rmse_ite:0.5611 — train_ate_err: 0.0933  — train_cate_err: 0.5611 — train_cate_nn_err: 2.6640 \n",
      " - valid_rmse: 2.8232 - valid_rmse_ite:0.5611 — valid_ate_err: 0.0933  — valid_cate_err: 0.5611 — valid_cate_nn_err: 2.6640 \n",
      "6/6 [==============================] - 0s 58ms/step - loss: 29.8771 - regression_loss: 25.6741 - val_loss: 29.6019 - val_regression_loss: 25.5214 - lr: 1.0000e-05\n",
      "Epoch 82/300\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 26.2326 - regression_loss: 22.1819 - train_rmse: 2.8307 - train_rmse_ite:0.5608 — train_ate_err: 0.0918  — train_cate_err: 0.5608 — train_cate_nn_err: 2.6622 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p0/kb961x5d6wd79gfp2h6t_8500000gn/T/ipykernel_18276/764417441.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m                     metrics=regression_loss)\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m tarnet_model.fit(x=data_train['x'],y=yt,\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgd_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ys'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1265\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code/git/CEVAE/evaluation.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m#Simulation Metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{self.name}_rmse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mrmse_ite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRMSE_ite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{self.name}_rmse_ite'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrmse_ite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mate_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mu_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mu_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code/git/CEVAE/evaluation.py\u001b[0m in \u001b[0;36mRMSE\u001b[0;34m(self, concat_pred)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0midx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y1_pred'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0midx1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y0_pred'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0midx0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# every loss function in TF2 takes 2 arguments, a vector of true values and a vector predictions\n",
    "def regression_loss(concat_true, concat_pred):\n",
    "    #computes a standard MSE loss for TARNet\n",
    "    y_true = concat_true[:, 0] #get individual vectors\n",
    "    t_true = concat_true[:, 1]\n",
    " \n",
    "    y0_pred = concat_pred[:, 0]\n",
    "    y1_pred = concat_pred[:, 1]\n",
    " \n",
    "    #Each head outputs a prediction for both potential outcomes\n",
    "    #We use t_true as a switch to only calculate the factual loss\n",
    "    loss0 = tf.reduce_sum((1. - t_true) * tf.square(y_true - y0_pred))\n",
    "    loss1 = tf.reduce_sum(t_true * tf.square(y_true - y1_pred))\n",
    "    #note Shi uses tf.reduce_sum for her losses even though mathematically we should be using the mean\n",
    "    #tf.reduce_mean and tf.reduce_sum should be equivalent, but maybe having larger error gradients makes training easier?\n",
    "    return loss0 + loss1\n",
    " \n",
    "### MAIN CODE ####\n",
    " \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD\n",
    " \n",
    "# val_split=0.2\n",
    "batch_size=128\n",
    "verbose=True\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    "yt = np.concatenate([data_train['ys'], data_train['t']], 1) #we'll use both y and t to compute the loss\n",
    " \n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    " \n",
    "sgd_callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='val_loss', patience=40, min_delta=0), \n",
    "        #40 is Shi's recommendation patience for this dataset, but you should tune for your data \n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0, cooldown=0, min_lr=0),\n",
    "        #This learning rate scheduling is quite agressive which seems good for this dataset\n",
    "        Full_Metrics(data_train,'train',verbose),\n",
    "        Full_Metrics(data_valid,'valid',verbose),\n",
    "        tensorboard_callback\n",
    "    ]\n",
    "#optimizer hyperparameters\n",
    "sgd_lr = 1e-5\n",
    "momentum = 0.9\n",
    "tarnet_model.compile(optimizer=SGD(lr=sgd_lr, momentum=momentum, nesterov=True),\n",
    "                    loss=regression_loss,\n",
    "                    metrics=regression_loss)\n",
    " \n",
    "tarnet_model.fit(x=data_train['x'],y=yt,\n",
    "                callbacks=sgd_callbacks,\n",
    "                validation_data=[data_valid['x'], np.concatenate([data_valid['ys'], data_valid['t']], 1)],\n",
    "                epochs=300,\n",
    "                batch_size=batch_size,\n",
    "                verbose=verbose)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.metrics.mean_squared_error([[1],[2]],[[2],[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
