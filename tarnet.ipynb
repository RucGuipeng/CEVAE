{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 “ihdp_npci_1-100.train.npz” 已经存在；不获取。\n",
      "\n",
      "文件 “ihdp_npci_1-100.test.npz” 已经存在；不获取。\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3735, 25)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title First load the data! (Click Play)\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz \n",
    "\n",
    "def load_IHDP_data(training_data,testing_data,i):\n",
    "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "        train_data=np.load(trf); test_data=np.load(tef)\n",
    "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
    "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
    "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
    "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    "\n",
    "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "        data['y']=data['y'].reshape(-1,1)\n",
    "        \n",
    "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "\n",
    "    return data\n",
    "\n",
    "ind = 7\n",
    "rep = 5\n",
    "data = load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.test.npz',i = ind)\n",
    "for key in data:\n",
    "    if key != 'y_scaler':\n",
    "        data[key] = np.repeat(data[key],repeats = rep, axis = 0)\n",
    "np.shape(data['x'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "#Colab command to allow us to run Colab in TF2\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "%reload_ext tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "def pdist2sq(A, B):\n",
    "    #helper for PEHEnn\n",
    "    #calculates squared euclidean distance between rows of two matrices  \n",
    "    #https://gist.github.com/mbsariyildiz/34cdc26afb630e8cae079048eef91865\n",
    "    # squared norms of each row in A and B\n",
    "    na = tf.reduce_sum(tf.square(A), 1)\n",
    "    nb = tf.reduce_sum(tf.square(B), 1)    \n",
    "    # na as a row and nb as a column vectors\n",
    "    na = tf.reshape(na, [-1, 1])\n",
    "    nb = tf.reshape(nb, [1, -1])\n",
    "    # return pairwise euclidean difference matrix\n",
    "    D=tf.reduce_sum((tf.expand_dims(A, 1)-tf.expand_dims(B, 0))**2,2) \n",
    "    return D\n",
    "\n",
    "from evaluation import Full_Metrics, metrics_for_cevae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.stats import sem\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.layers as tfkl\n",
    "tfd,tfpl = tfp.distributions,tfp.layers\n",
    "import tensorflow.keras.backend as tfkb\n",
    "\n",
    "class tarnet(tf.keras.Model):\n",
    "    def __init__(self, input_dim, reg_l2):\n",
    "        super(tarnet, self).__init__()\n",
    "        ########################################\n",
    "        # networks\n",
    "        self.activation = 'elu'\n",
    "        self.share_bottom = tfk.Sequential(\n",
    "            [\n",
    "                tfkl.InputLayer([input_dim]),\n",
    "                tfkl.Dense(100,activation = self.activation, kernel_initializer='RandomNormal'),\n",
    "                tfkl.Dense(100,activation = self.activation, kernel_initializer='RandomNormal'),\n",
    "                tfkl.Dense(100,activation = self.activation, kernel_initializer='RandomNormal'),\n",
    "            ])\n",
    "        self.tower_t0 = tfk.Sequential(\n",
    "            [\n",
    "                tfkl.InputLayer([100]),\n",
    "                tfkl.Dense(100,activation = self.activation, kernel_regularizer=regularizers.l2(reg_l2)),\n",
    "                tfkl.Dense(100,activation = self.activation, kernel_regularizer=regularizers.l2(reg_l2)),\n",
    "                tfkl.Dense(1,activation = None, kernel_regularizer=regularizers.l2(reg_l2)),\n",
    "            ])\n",
    "\n",
    "        self.tower_t1 = tfk.Sequential(\n",
    "            [\n",
    "                tfkl.InputLayer([100]),\n",
    "                tfkl.Dense(100,activation = self.activation, kernel_regularizer=regularizers.l2(reg_l2)),\n",
    "                tfkl.Dense(100,activation = self.activation, kernel_regularizer=regularizers.l2(reg_l2)),\n",
    "                tfkl.Dense(1,activation = None, kernel_regularizer=regularizers.l2(reg_l2)),\n",
    "            ])\n",
    "    def call(self, data, training=False, serving=False):\n",
    "        # Dataset_inp\n",
    "        hidden_bottom = self.share_bottom(data)\n",
    "        y_t0 = self.tower_t0(hidden_bottom)\n",
    "        y_t1 = self.tower_t1(hidden_bottom)\n",
    "    \n",
    "        output = tf.concat([y_t0,y_t1,hidden_bottom],-1)\n",
    "        return output\n",
    "\n",
    "#make model\n",
    "tarnet_model=tarnet(data['x'].shape[1],.01)\n",
    "# fake_inputs = tfk.Input(25,dtype = tf.float32)\n",
    "# tarnet_model(fake_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/24 [>.............................] - ETA: 16s - loss: 22.6674 - regression_loss: 18.7378WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0039s vs `on_train_batch_end` time: 0.0039s). Check your callbacks.\n",
      "12/24 [==============>...............] - ETA: 0s - loss: 22.6399 - regression_loss: 18.7103  — ate_err: 0.1441  — cate_err: 0.5742 — cate_nn_err: 2.7274 \n",
      "24/24 [==============================] - 2s 63ms/step - loss: 21.7563 - regression_loss: 17.5126 - val_loss: 28.7704 - val_regression_loss: 25.0095 - lr: 1.0000e-05\n",
      "Epoch 2/300\n",
      "14/24 [================>.............] - ETA: 0s - loss: 21.6592 - regression_loss: 17.7298 — ate_err: 0.1130  — cate_err: 0.5616 — cate_nn_err: 2.7174 \n",
      "24/24 [==============================] - 1s 49ms/step - loss: 21.7472 - regression_loss: 17.4976 - val_loss: 28.8649 - val_regression_loss: 25.1071 - lr: 1.0000e-05\n",
      "Epoch 3/300\n",
      "13/24 [===============>..............] - ETA: 0s - loss: 22.1629 - regression_loss: 18.2337 — ate_err: 0.1742  — cate_err: 0.5859 — cate_nn_err: 2.7084 \n",
      "24/24 [==============================] - 1s 54ms/step - loss: 21.8144 - regression_loss: 17.5501 - val_loss: 29.2426 - val_regression_loss: 25.4964 - lr: 1.0000e-05\n",
      "Epoch 4/300\n",
      "13/24 [===============>..............] - ETA: 0s - loss: 21.3805 - regression_loss: 17.4516 — ate_err: 0.1386  — cate_err: 0.5745 — cate_nn_err: 2.7122 \n",
      "24/24 [==============================] - 1s 51ms/step - loss: 21.7342 - regression_loss: 17.4572 - val_loss: 28.9093 - val_regression_loss: 25.1588 - lr: 1.0000e-05\n",
      "Epoch 5/300\n",
      "14/24 [================>.............] - ETA: 0s - loss: 21.3727 - regression_loss: 17.4441 — ate_err: 0.0846  — cate_err: 0.5773 — cate_nn_err: 2.7267 \n",
      "24/24 [==============================] - 1s 52ms/step - loss: 21.6962 - regression_loss: 17.4014 - val_loss: 28.9070 - val_regression_loss: 25.1570 - lr: 1.0000e-05\n",
      "Epoch 6/300\n",
      "16/24 [===================>..........] - ETA: 0s - loss: 22.3559 - regression_loss: 18.4275 — ate_err: 0.2381  — cate_err: 0.6140 — cate_nn_err: 2.7218 \n",
      "24/24 [==============================] - 1s 46ms/step - loss: 21.7094 - regression_loss: 17.4572 - val_loss: 29.1206 - val_regression_loss: 25.3767 - lr: 1.0000e-05\n",
      "Epoch 7/300\n",
      "16/24 [===================>..........] - ETA: 0s - loss: 21.9423 - regression_loss: 18.0142 — ate_err: 0.2222  — cate_err: 0.6088 — cate_nn_err: 2.7240 \n",
      "24/24 [==============================] - 1s 45ms/step - loss: 21.5561 - regression_loss: 17.3388 - val_loss: 29.0648 - val_regression_loss: 25.3073 - lr: 1.0000e-05\n",
      "Epoch 8/300\n",
      "18/24 [=====================>........] - ETA: 0s - loss: 21.8780 - regression_loss: 17.9501 — ate_err: 0.0691  — cate_err: 0.5692 — cate_nn_err: 2.7137 \n",
      "24/24 [==============================] - 1s 44ms/step - loss: 21.5864 - regression_loss: 17.3474 - val_loss: 29.3417 - val_regression_loss: 25.6008 - lr: 1.0000e-05\n",
      "Epoch 9/300\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 20.9665 - regression_loss: 17.0389 — ate_err: 0.1430  — cate_err: 0.5805 — cate_nn_err: 2.6934 \n",
      "24/24 [==============================] - 1s 45ms/step - loss: 21.6473 - regression_loss: 17.4633 - val_loss: 29.3813 - val_regression_loss: 25.6368 - lr: 1.0000e-05\n",
      "Epoch 10/300\n",
      "16/24 [===================>..........] - ETA: 0s - loss: 21.8389 - regression_loss: 17.9116 — ate_err: 0.1385  — cate_err: 0.5942 — cate_nn_err: 2.7224 \n",
      "24/24 [==============================] - 1s 45ms/step - loss: 21.6627 - regression_loss: 17.3912 - val_loss: 29.0070 - val_regression_loss: 25.2655 - lr: 1.0000e-05\n",
      "Epoch 11/300\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 21.6639 - regression_loss: 17.7368 — ate_err: 0.1423  — cate_err: 0.5831 — cate_nn_err: 2.6992 \n",
      "24/24 [==============================] - 1s 45ms/step - loss: 21.5742 - regression_loss: 17.3436 - val_loss: 29.0867 - val_regression_loss: 25.3368 - lr: 1.0000e-05\n",
      "Epoch 12/300\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 21.3150 - regression_loss: 17.3882 — ate_err: 0.2393  — cate_err: 0.6235 — cate_nn_err: 2.7332 \n",
      "24/24 [==============================] - 1s 44ms/step - loss: 21.5246 - regression_loss: 17.3016 - val_loss: 29.1694 - val_regression_loss: 25.4257 - lr: 1.0000e-05\n",
      "Epoch 13/300\n",
      "16/24 [===================>..........] - ETA: 0s - loss: 21.8672 - regression_loss: 17.9407 — ate_err: 0.0085  — cate_err: 0.5679 — cate_nn_err: 2.7233 \n",
      "24/24 [==============================] - 1s 44ms/step - loss: 21.5278 - regression_loss: 17.2592 - val_loss: 29.4110 - val_regression_loss: 25.6652 - lr: 1.0000e-05\n",
      "Epoch 14/300\n",
      "17/24 [====================>.........] - ETA: 0s - loss: 21.3455 - regression_loss: 17.4192 — ate_err: 0.2114  — cate_err: 0.6331 — cate_nn_err: 2.7429 \n",
      "24/24 [==============================] - 1s 44ms/step - loss: 21.4951 - regression_loss: 17.3322 - val_loss: 29.3518 - val_regression_loss: 25.6049 - lr: 1.0000e-05\n",
      "Epoch 15/300\n",
      "15/24 [=================>............] - ETA: 0s - loss: 21.6696 - regression_loss: 17.7435 — ate_err: 0.1547  — cate_err: 0.6006 — cate_nn_err: 2.7320 \n",
      "24/24 [==============================] - 1s 44ms/step - loss: 21.4490 - regression_loss: 17.2407 - val_loss: 29.0006 - val_regression_loss: 25.2546 - lr: 1.0000e-05\n",
      "Epoch 16/300\n",
      "16/24 [===================>..........] - ETA: 0s - loss: 21.8481 - regression_loss: 17.9223 — ate_err: 0.1771  — cate_err: 0.6145 — cate_nn_err: 2.7550 \n",
      "24/24 [==============================] - 1s 46ms/step - loss: 21.4145 - regression_loss: 17.1997 - val_loss: 29.0062 - val_regression_loss: 25.2546 - lr: 1.0000e-05\n",
      "Epoch 17/300\n",
      "10/24 [===========>..................] - ETA: 0s - loss: 21.5877 - regression_loss: 17.6621 — ate_err: 0.1373  — cate_err: 0.6035 — cate_nn_err: 2.7312 \n",
      "24/24 [==============================] - 1s 46ms/step - loss: 21.4334 - regression_loss: 17.1958 - val_loss: 29.2504 - val_regression_loss: 25.5156 - lr: 1.0000e-05\n",
      "Epoch 18/300\n",
      "14/24 [================>.............] - ETA: 0s - loss: 21.4794 - regression_loss: 17.5541 — ate_err: 0.1091  — cate_err: 0.5999 — cate_nn_err: 2.7448 \n",
      "24/24 [==============================] - 1s 47ms/step - loss: 21.4461 - regression_loss: 17.1736 - val_loss: 29.0755 - val_regression_loss: 25.3349 - lr: 1.0000e-05\n",
      "Epoch 19/300\n",
      "15/24 [=================>............] - ETA: 0s - loss: 21.2174 - regression_loss: 17.2923 — ate_err: 0.1605  — cate_err: 0.6053 — cate_nn_err: 2.7370 \n",
      "24/24 [==============================] - 1s 47ms/step - loss: 21.4122 - regression_loss: 17.1311 - val_loss: 29.0189 - val_regression_loss: 25.2744 - lr: 1.0000e-05\n",
      "Epoch 20/300\n",
      "13/24 [===============>..............] - ETA: 0s - loss: 21.8786 - regression_loss: 17.9538 — ate_err: 0.1063  — cate_err: 0.5923 — cate_nn_err: 2.7257 \n",
      "24/24 [==============================] - 1s 48ms/step - loss: 21.3940 - regression_loss: 17.1440 - val_loss: 29.4586 - val_regression_loss: 25.7279 - lr: 1.0000e-05\n",
      "Epoch 21/300\n",
      "16/24 [===================>..........] - ETA: 0s - loss: 21.8184 - regression_loss: 17.8939 — ate_err: 0.2190  — cate_err: 0.6317 — cate_nn_err: 2.7529 \n",
      "24/24 [==============================] - 1s 47ms/step - loss: 21.3678 - regression_loss: 17.1038 - val_loss: 29.0595 - val_regression_loss: 25.3154 - lr: 1.0000e-05\n",
      "Epoch 22/300\n",
      "13/24 [===============>..............] - ETA: 0s - loss: 21.8207 - regression_loss: 17.8963 — ate_err: 0.2252  — cate_err: 0.6295 — cate_nn_err: 2.7178 \n",
      "24/24 [==============================] - 1s 49ms/step - loss: 21.3364 - regression_loss: 17.1100 - val_loss: 29.6915 - val_regression_loss: 25.9682 - lr: 1.0000e-05\n",
      "Epoch 23/300\n",
      "15/24 [=================>............] - ETA: 0s - loss: 21.4936 - regression_loss: 17.5696 — ate_err: 0.0938  — cate_err: 0.6035 — cate_nn_err: 2.7419 \n",
      "24/24 [==============================] - 1s 47ms/step - loss: 21.3758 - regression_loss: 17.0840 - val_loss: 29.2259 - val_regression_loss: 25.4918 - lr: 1.0000e-05\n",
      "Epoch 24/300\n",
      "13/24 [===============>..............] - ETA: 0s - loss: 20.9725 - regression_loss: 17.0487 — ate_err: 0.1193  — cate_err: 0.6068 — cate_nn_err: 2.7430 \n",
      "24/24 [==============================] - 1s 48ms/step - loss: 21.2860 - regression_loss: 17.0314 - val_loss: 29.1932 - val_regression_loss: 25.4656 - lr: 1.0000e-05\n",
      "Epoch 25/300\n",
      "15/24 [=================>............] - ETA: 0s - loss: 21.2102 - regression_loss: 17.2866 — ate_err: 0.1758  — cate_err: 0.6318 — cate_nn_err: 2.7443 \n",
      "24/24 [==============================] - 1s 48ms/step - loss: 21.3246 - regression_loss: 17.0422 - val_loss: 29.4383 - val_regression_loss: 25.7027 - lr: 1.0000e-05\n",
      "Epoch 26/300\n",
      "16/24 [===================>..........] - ETA: 0s - loss: 20.9720 - regression_loss: 17.0487 — ate_err: 0.1257  — cate_err: 0.6126 — cate_nn_err: 2.7443 \n",
      "24/24 [==============================] - 1s 48ms/step - loss: 21.1640 - regression_loss: 16.9541 - val_loss: 29.2911 - val_regression_loss: 25.5638 - lr: 1.0000e-05\n",
      "Epoch 27/300\n",
      "16/24 [===================>..........] - ETA: 0s - loss: 21.1902 - regression_loss: 17.2671 — ate_err: 0.1384  — cate_err: 0.6150 — cate_nn_err: 2.7351 \n",
      "24/24 [==============================] - 1s 49ms/step - loss: 21.2560 - regression_loss: 17.0003 - val_loss: 29.4772 - val_regression_loss: 25.7488 - lr: 1.0000e-05\n",
      "Epoch 28/300\n",
      "18/24 [=====================>........] - ETA: 0s - loss: 21.0503 - regression_loss: 17.1275 — ate_err: 0.2520  — cate_err: 0.6571 — cate_nn_err: 2.7484 \n",
      "24/24 [==============================] - 1s 49ms/step - loss: 21.2499 - regression_loss: 16.9846 - val_loss: 29.7181 - val_regression_loss: 25.9952 - lr: 1.0000e-05\n",
      "Epoch 29/300\n",
      "16/24 [===================>..........] - ETA: 0s - loss: 21.2553 - regression_loss: 17.3328 — ate_err: 0.1624  — cate_err: 0.6292 — cate_nn_err: 2.7641 \n",
      "24/24 [==============================] - 1s 51ms/step - loss: 21.1762 - regression_loss: 16.9198 - val_loss: 29.2247 - val_regression_loss: 25.4958 - lr: 1.0000e-05\n",
      "Epoch 30/300\n",
      "14/24 [================>.............] - ETA: 0s - loss: 20.9006 - regression_loss: 16.9782 — ate_err: 0.1617  — cate_err: 0.6306 — cate_nn_err: 2.7292 \n",
      "24/24 [==============================] - 1s 51ms/step - loss: 21.0968 - regression_loss: 16.8951 - val_loss: 29.6648 - val_regression_loss: 25.9497 - lr: 1.0000e-05\n",
      "Epoch 31/300\n",
      "16/24 [===================>..........] - ETA: 0s - loss: 21.0145 - regression_loss: 17.0924 — ate_err: 0.2116  — cate_err: 0.6514 — cate_nn_err: 2.7691 \n",
      "24/24 [==============================] - 1s 52ms/step - loss: 21.1502 - regression_loss: 16.9049 - val_loss: 29.4270 - val_regression_loss: 25.6946 - lr: 1.0000e-05\n",
      "Epoch 32/300\n",
      "16/24 [===================>..........] - ETA: 0s - loss: 21.7131 - regression_loss: 17.7913 — ate_err: 0.1770  — cate_err: 0.6331 — cate_nn_err: 2.7541 \n",
      "24/24 [==============================] - 1s 48ms/step - loss: 21.2402 - regression_loss: 16.9815 - val_loss: 29.3039 - val_regression_loss: 25.5702 - lr: 1.0000e-05\n",
      "Epoch 33/300\n",
      "16/24 [===================>..........] - ETA: 0s - loss: 21.0538 - regression_loss: 17.1323 — ate_err: 0.1556  — cate_err: 0.6399 — cate_nn_err: 2.7470 \n",
      "24/24 [==============================] - 1s 48ms/step - loss: 21.0641 - regression_loss: 16.8886 - val_loss: 29.4803 - val_regression_loss: 25.7655 - lr: 1.0000e-05\n",
      "Epoch 34/300\n",
      "16/24 [===================>..........] - ETA: 0s - loss: 20.9977 - regression_loss: 17.0764 — ate_err: 0.2353  — cate_err: 0.6509 — cate_nn_err: 2.7455 \n",
      "24/24 [==============================] - 1s 49ms/step - loss: 21.0487 - regression_loss: 16.8225 - val_loss: 29.6615 - val_regression_loss: 25.9306 - lr: 1.0000e-05\n",
      "Epoch 35/300\n",
      "12/24 [==============>...............] - ETA: 0s - loss: 21.5064 - regression_loss: 17.5854 — ate_err: 0.1993  — cate_err: 0.6544 — cate_nn_err: 2.7831 \n",
      "24/24 [==============================] - 1s 51ms/step - loss: 20.9219 - regression_loss: 16.7557 - val_loss: 29.3117 - val_regression_loss: 25.5898 - lr: 1.0000e-05\n",
      "Epoch 36/300\n",
      "15/24 [=================>............] - ETA: 0s - loss: 21.9563 - regression_loss: 18.0354 — ate_err: 0.1497  — cate_err: 0.6394 — cate_nn_err: 2.7581 \n",
      "24/24 [==============================] - 1s 49ms/step - loss: 21.1135 - regression_loss: 16.8118 - val_loss: 29.5479 - val_regression_loss: 25.8271 - lr: 1.0000e-05\n",
      "Epoch 37/300\n",
      "15/24 [=================>............] - ETA: 0s - loss: 21.6208 - regression_loss: 17.7002 — ate_err: 0.1345  — cate_err: 0.6470 — cate_nn_err: 2.7598 \n",
      "24/24 [==============================] - 1s 51ms/step - loss: 21.1596 - regression_loss: 16.8754 - val_loss: 29.5314 - val_regression_loss: 25.8165 - lr: 1.0000e-05\n",
      "Epoch 38/300\n",
      "15/24 [=================>............] - ETA: 0s - loss: 21.1310 - regression_loss: 17.2107 — ate_err: 0.1712  — cate_err: 0.6444 — cate_nn_err: 2.7710 \n",
      "24/24 [==============================] - 1s 49ms/step - loss: 20.9340 - regression_loss: 16.7639 - val_loss: 29.4906 - val_regression_loss: 25.7714 - lr: 1.0000e-05\n",
      "Epoch 39/300\n",
      "16/24 [===================>..........] - ETA: 0s - loss: 21.0715 - regression_loss: 17.1515 — ate_err: 0.2630  — cate_err: 0.6859 — cate_nn_err: 2.7779 \n",
      "24/24 [==============================] - 1s 51ms/step - loss: 20.9967 - regression_loss: 16.7291 - val_loss: 29.8587 - val_regression_loss: 26.1343 - lr: 1.0000e-05\n",
      "Epoch 40/300\n",
      "14/24 [================>.............] - ETA: 0s - loss: 21.2837 - regression_loss: 17.3638 — ate_err: 0.3632  — cate_err: 0.7343 — cate_nn_err: 2.8091 \n",
      "24/24 [==============================] - 1s 53ms/step - loss: 20.8869 - regression_loss: 16.6835 - val_loss: 30.2323 - val_regression_loss: 26.5063 - lr: 1.0000e-05\n",
      "Epoch 41/300\n",
      "13/24 [===============>..............] - ETA: 0s - loss: 20.8784 - regression_loss: 16.9588 — ate_err: 0.0724  — cate_err: 0.6245 — cate_nn_err: 2.7648 \n",
      "24/24 [==============================] - 2s 70ms/step - loss: 20.8251 - regression_loss: 16.6877 - val_loss: 29.9225 - val_regression_loss: 26.2161 - lr: 1.0000e-05\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# every loss function in TF2 takes 2 arguments, a vector of true values and a vector predictions\n",
    "def regression_loss(concat_true, concat_pred):\n",
    "    #computes a standard MSE loss for TARNet\n",
    "    y_true = concat_true[:, 0] #get individual vectors\n",
    "    t_true = concat_true[:, 1]\n",
    " \n",
    "    y0_pred = concat_pred[:, 0]\n",
    "    y1_pred = concat_pred[:, 1]\n",
    " \n",
    "    #Each head outputs a prediction for both potential outcomes\n",
    "    #We use t_true as a switch to only calculate the factual loss\n",
    "    loss0 = tf.reduce_sum((1. - t_true) * tf.square(y_true - y0_pred))\n",
    "    loss1 = tf.reduce_sum(t_true * tf.square(y_true - y1_pred))\n",
    "    #note Shi uses tf.reduce_sum for her losses even though mathematically we should be using the mean\n",
    "    #tf.reduce_mean and tf.reduce_sum should be equivalent, but maybe having larger error gradients makes training easier?\n",
    "    return loss0 + loss1\n",
    " \n",
    "### MAIN CODE ####\n",
    " \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD\n",
    " \n",
    "val_split=0.2\n",
    "batch_size=128\n",
    "verbose=True\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    "yt = np.concatenate([data['ys'], data['t']], 1) #we'll use both y and t to compute the loss\n",
    " \n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    " \n",
    "sgd_callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='val_loss', patience=40, min_delta=0), \n",
    "        #40 is Shi's recommendation patience for this dataset, but you should tune for your data \n",
    "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0, cooldown=0, min_lr=0),\n",
    "        #This learning rate scheduling is quite agressive which seems good for this dataset\n",
    "        Full_Metrics(data,verbose),\n",
    "        tensorboard_callback\n",
    "    ]\n",
    "#optimizer hyperparameters\n",
    "sgd_lr = 1e-5\n",
    "momentum = 0.9\n",
    "tarnet_model.compile(optimizer=SGD(lr=sgd_lr, momentum=momentum, nesterov=True),\n",
    "                    loss=regression_loss,\n",
    "                    metrics=regression_loss)\n",
    " \n",
    "tarnet_model.fit(x=data['x'],y=yt,\n",
    "                callbacks=sgd_callbacks,\n",
    "                validation_split=val_split,\n",
    "                epochs=300,\n",
    "                batch_size=batch_size,\n",
    "                verbose=verbose)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 11635), started 1 day, 0:29:34 ago. (Use '!kill 11635' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9a1dccf66e4adae0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9a1dccf66e4adae0\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
