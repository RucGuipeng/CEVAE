{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 “ihdp_npci_1-100.train.npz” 已经存在；不获取。\n",
      "\n",
      "文件 “ihdp_npci_1-100.test.npz” 已经存在；不获取。\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1344, 25)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.layers as tfkl\n",
    "tfd,tfpl = tfp.distributions,tfp.layers\n",
    "import tensorflow.keras.backend as tfkb\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from evaluation import *\n",
    "from cevae_networks import *\n",
    "################################################\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='')\n",
    "parser.add_argument('--scale_penalize',    type = float, default = 0.001,  help = '')\n",
    "parser.add_argument('--learning_rate',     type = float, default = 0.001,  help = '')\n",
    "parser.add_argument('--default_y_scale',   type = float, default = 1.,  help = '')\n",
    "parser.add_argument('--t_dim',     type = int, default = 1,  help = '')\n",
    "parser.add_argument('--y_dim',     type = int, default = 1,  help = '')\n",
    "parser.add_argument('--x_dim',     type = int, default = 25, help = '')\n",
    "parser.add_argument('--z_dim',     type = int, default = 20, help = '')\n",
    "parser.add_argument('--x_num_dim', type = int, default = 6,  help = '')\n",
    "parser.add_argument('--x_bin_dim', type = int, default = 19, help = '')\n",
    "parser.add_argument('--nh', type = int, default = 3, help = 'number of hidden layers')\n",
    "parser.add_argument('--h',  type = int, default = 200, help = 'number of hidden units')\n",
    "args = parser.parse_args([])\n",
    "################################################\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz \n",
    "\n",
    "def load_IHDP_data(training_data,testing_data,i):\n",
    "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "        train_data=np.load(trf); test_data=np.load(tef)\n",
    "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
    "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
    "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
    "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    "        ycf=np.concatenate((train_data['ycf'][:,i],  test_data['ycf'][:,i])).astype('float32')\n",
    "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "        data['y']=data['y'].reshape(-1,1)\n",
    "        data['ycf'] = ycf.reshape(-1,1)\n",
    "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "    return data\n",
    "\n",
    "ind = 7\n",
    "# rep = 5\n",
    "# rep = 1\n",
    "# data = load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.test.npz',i = ind)\n",
    "# for key in data:\n",
    "#     if key != 'y_scaler':\n",
    "#         data[key] = np.repeat(data[key],repeats = rep, axis = 0)\n",
    "# np.shape(data['x'])\n",
    "data_train = load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.train.npz',i = ind)\n",
    "data_valid = load_IHDP_data(training_data='./ihdp_npci_1-100.test.npz',testing_data='./ihdp_npci_1-100.test.npz',i = ind)\n",
    "np.shape(data_train['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEVAE(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CEVAE, self).__init__()\n",
    "        ########################################\n",
    "        # networks\n",
    "        self.activation = 'elu'\n",
    "        # CEVAE Model \n",
    "        ## (encoder)\n",
    "        self.q_y_tx = q_y_tx(args.x_bin_dim, args.x_num_dim, args.y_dim, args.t_dim, args.nh, args.h)\n",
    "        self.q_t_x = q_t_x(args.x_bin_dim, args.x_num_dim, args.t_dim, args.nh, args.h)\n",
    "        self.q_z_txy = q_z_txy(args.x_bin_dim, args.x_num_dim, args.y_dim, args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        ## (decoder)\n",
    "        self.p_x_z = p_x_z(args.x_bin_dim, args.x_num_dim, args.z_dim, args.nh, args.h)\n",
    "        self.p_t_z = p_t_z(args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        self.p_y_tz = p_y_tz(args.y_dim, args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        \n",
    "\n",
    "    def call(self, data, training=False):\n",
    "        if training:\n",
    "            x_train,t_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "            \n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            \n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.sample()\n",
    "            # decoder\n",
    "            ## p(x|z)\n",
    "            x_num,x_bin = self.p_x_z(z_infer_sample)\n",
    "            ## p(t|z)\n",
    "            t = self.p_t_z(z_infer_sample)\n",
    "            ## p(y|t,z)\n",
    "            y = self.p_y_tz(tf.concat([t_train,z_infer_sample],-1) )\n",
    "            \n",
    "            return y_infer,t_infer,z_infer,y,t,x_num,x_bin\n",
    "        else:\n",
    "            x_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "        \n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            \n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.sample()\n",
    "            # decoder\n",
    "            ## p(x|z)\n",
    "            x_num,x_bin = self.p_x_z(z_infer_sample)\n",
    "            ## p(t|z)\n",
    "            t = self.p_t_z(z_infer_sample)\n",
    "            ## p(y|t,z)\n",
    "            y0 = self.p_y_tz(tf.concat([tf.zeros_like(t_infer_sample),z_infer_sample],-1) )\n",
    "            y1 = self.p_y_tz(tf.concat([tf.ones_like(t_infer_sample),z_infer_sample],-1) )\n",
    "            return [y0,y1],t,z_infer\n",
    "\n",
    "\n",
    "    def cevae_loss(self, data, pred, training = False):\n",
    "        x_train, t_train, y_train = data[0],data[1],data[2]\n",
    "        x_train_num, x_train_bin = x_train[:,:args.x_num_dim],x_train[:,args.x_num_dim:]\n",
    "        y_infer,t_infer,z_infer,y,t,x_num,x_bin = pred\n",
    "        y0,y1 = y_infer\n",
    "        # reconstruct loss\n",
    "        recon_x_num = tfkb.sum(x_num.log_prob(x_train_num), 1)\n",
    "        recon_x_bin = tfkb.sum(x_bin.log_prob(x_train_bin), 1)\n",
    "        recon_y = tfkb.sum(y.log_prob(y_train), 1)\n",
    "        recon_t = tfkb.sum(t.log_prob(t_train), 1)\n",
    "        # kl loss\n",
    "        z_infer_sample = z_infer.sample()\n",
    "        z = tfd.Normal(loc = [0] * 20, scale = [1]*20)\n",
    "        kl_z = tfkb.sum((z.log_prob(z_infer_sample) - z_infer.log_prob(z_infer_sample)), -1)\n",
    "        # aux loss\n",
    "        aux_y = tfkb.sum(y0.log_prob(y_train)*(1-t_train) + y1.log_prob(y_train)* t_train, 1)\n",
    "        aux_t = tfkb.sum(t_infer.log_prob(t_train), 1)\n",
    "        loss = -tfkb.mean(recon_x_bin + recon_x_num + recon_y + recon_t + aux_y + aux_t + kl_z)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, data):\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "            loss = self.cevae_loss(data = data, pred = pred, training = True)\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        metrics = {\"loss\": loss}\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "        y_infer = pred[0]\n",
    "        loss = self.cevae_loss(data = data, pred = pred, training = False)\n",
    "        y0, y1 = y_infer[0].sample(),y_infer[1].sample()\n",
    "        ate = tfkb.mean(y1) - tfkb.mean(y0)\n",
    "        metrics = {\"loss\":loss,\"y0\": tfkb.mean(y0),\"y1\": tfkb.mean(y1),'ate_after_scaled': ate}\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 29.2806 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0148s vs `on_train_batch_end` time: 0.0150s). Check your callbacks.\n",
      "WARNING:tensorflow:5 out of the last 194 calls to <function Model.make_test_function.<locals>.test_function at 0x13a9f2ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      " — ite: 5.5092  — ate: 3.8517 — pehe: 5.5092 \n",
      " — ite: 6.0923  — ate: 3.8824 — pehe: 6.0923 \n",
      "6/6 [==============================] - 5s 245ms/step - loss: 29.2919 - val_loss: 30.0170 - val_y0: -0.0134 - val_y1: 0.0703 - val_ate_after_scaled: 0.0837 - lr: 1.0000e-05\n",
      "Epoch 2/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 29.2443 — ite: 5.4548  — ate: 3.7792 — pehe: 5.4548 \n",
      " — ite: 5.6328  — ate: 3.2022 — pehe: 5.6328 \n",
      "6/6 [==============================] - 1s 150ms/step - loss: 29.3767 - val_loss: 30.4210 - val_y0: 0.0834 - val_y1: 0.1431 - val_ate_after_scaled: 0.0597 - lr: 1.0000e-05\n",
      "Epoch 3/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 29.3495 — ite: 5.4488  — ate: 3.8146 — pehe: 5.4488 \n",
      " — ite: 5.1359  — ate: 2.9802 — pehe: 5.1359 \n",
      "6/6 [==============================] - 1s 143ms/step - loss: 29.2230 - val_loss: 30.1692 - val_y0: 0.0768 - val_y1: 0.2497 - val_ate_after_scaled: 0.1729 - lr: 1.0000e-05\n",
      "Epoch 4/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 29.2475 — ite: 5.4857  — ate: 3.7749 — pehe: 5.4857 \n",
      " — ite: 5.6301  — ate: 3.5582 — pehe: 5.6301 \n",
      "6/6 [==============================] - 1s 134ms/step - loss: 29.1900 - val_loss: 29.8544 - val_y0: 0.0152 - val_y1: 0.2325 - val_ate_after_scaled: 0.2173 - lr: 1.0000e-05\n",
      "Epoch 5/300\n",
      "3/6 [==============>...............] - ETA: 0s - loss: 29.0838 — ite: 5.2844  — ate: 3.6589 — pehe: 5.2844 \n",
      " — ite: 5.4775  — ate: 3.2877 — pehe: 5.4775 \n",
      "6/6 [==============================] - 1s 129ms/step - loss: 29.0981 - val_loss: 30.0684 - val_y0: -0.1768 - val_y1: 0.1766 - val_ate_after_scaled: 0.3534 - lr: 1.0000e-05\n",
      "Epoch 6/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 29.0622 — ite: 5.4993  — ate: 3.8121 — pehe: 5.4993 \n",
      " — ite: 5.8742  — ate: 3.3530 — pehe: 5.8742 \n",
      "6/6 [==============================] - 1s 129ms/step - loss: 28.7809 - val_loss: 29.5598 - val_y0: -0.0508 - val_y1: 0.3055 - val_ate_after_scaled: 0.3563 - lr: 1.0000e-05\n",
      "Epoch 7/300\n",
      "3/6 [==============>...............] - ETA: 0s - loss: 28.7139 — ite: 5.2735  — ate: 3.5612 — pehe: 5.2735 \n",
      " — ite: 5.8670  — ate: 3.4787 — pehe: 5.8670 \n",
      "6/6 [==============================] - 1s 142ms/step - loss: 29.0276 - val_loss: 29.5855 - val_y0: -0.1013 - val_y1: 0.3075 - val_ate_after_scaled: 0.4088 - lr: 1.0000e-05\n",
      "Epoch 8/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 28.7846 — ite: 5.3470  — ate: 3.7103 — pehe: 5.3470 \n",
      " — ite: 6.0022  — ate: 3.7959 — pehe: 6.0022 \n",
      "6/6 [==============================] - 1s 128ms/step - loss: 28.9452 - val_loss: 29.7026 - val_y0: 0.0977 - val_y1: 0.3873 - val_ate_after_scaled: 0.2896 - lr: 1.0000e-05\n",
      "Epoch 9/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 28.8004 — ite: 5.4446  — ate: 3.7281 — pehe: 5.4446 \n",
      " — ite: 6.0056  — ate: 3.8039 — pehe: 6.0056 \n",
      "6/6 [==============================] - 1s 144ms/step - loss: 29.0511 - val_loss: 29.4741 - val_y0: -0.0292 - val_y1: 0.4803 - val_ate_after_scaled: 0.5095 - lr: 1.0000e-05\n",
      "Epoch 10/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 28.6190 — ite: 5.3250  — ate: 3.6517 — pehe: 5.3250 \n",
      " — ite: 5.8068  — ate: 3.5478 — pehe: 5.8068 \n",
      "6/6 [==============================] - 1s 142ms/step - loss: 28.7368 - val_loss: 29.2669 - val_y0: -0.0980 - val_y1: 0.3889 - val_ate_after_scaled: 0.4869 - lr: 1.0000e-05\n",
      "Epoch 11/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 28.5938 — ite: 5.2838  — ate: 3.5206 — pehe: 5.2838 \n",
      " — ite: 5.5824  — ate: 3.0854 — pehe: 5.5824 \n",
      "6/6 [==============================] - 1s 131ms/step - loss: 28.6197 - val_loss: 29.1500 - val_y0: 0.0889 - val_y1: 0.4042 - val_ate_after_scaled: 0.3153 - lr: 1.0000e-05\n",
      "Epoch 12/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 28.5957 — ite: 5.3152  — ate: 3.4780 — pehe: 5.3152 \n",
      " — ite: 5.7006  — ate: 3.4991 — pehe: 5.7006 \n",
      "6/6 [==============================] - 1s 133ms/step - loss: 28.4969 - val_loss: 29.3228 - val_y0: -0.0800 - val_y1: 0.3989 - val_ate_after_scaled: 0.4789 - lr: 1.0000e-05\n",
      "Epoch 13/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 28.5094 — ite: 5.4275  — ate: 3.6062 — pehe: 5.4275 \n",
      " — ite: 5.5725  — ate: 2.9264 — pehe: 5.5725 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 28.2949 - val_loss: 29.2655 - val_y0: -0.1488 - val_y1: 0.5365 - val_ate_after_scaled: 0.6853 - lr: 1.0000e-05\n",
      "Epoch 14/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 28.4208 — ite: 5.3356  — ate: 3.6484 — pehe: 5.3356 \n",
      " — ite: 5.4413  — ate: 2.4957 — pehe: 5.4413 \n",
      "6/6 [==============================] - 1s 136ms/step - loss: 28.1266 - val_loss: 29.0932 - val_y0: -0.0832 - val_y1: 0.4524 - val_ate_after_scaled: 0.5356 - lr: 1.0000e-05\n",
      "Epoch 15/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 28.1876 — ite: 5.2878  — ate: 3.5748 — pehe: 5.2878 \n",
      " — ite: 5.6108  — ate: 3.1730 — pehe: 5.6108 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 28.4780 - val_loss: 28.9438 - val_y0: -0.0324 - val_y1: 0.5076 - val_ate_after_scaled: 0.5400 - lr: 1.0000e-05\n",
      "Epoch 16/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 28.2876 — ite: 5.3628  — ate: 3.6002 — pehe: 5.3628 \n",
      " — ite: 5.9348  — ate: 3.2105 — pehe: 5.9348 \n",
      "6/6 [==============================] - 1s 135ms/step - loss: 28.3251 - val_loss: 29.1636 - val_y0: 0.0315 - val_y1: 0.5505 - val_ate_after_scaled: 0.5189 - lr: 1.0000e-05\n",
      "Epoch 17/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 28.2340 — ite: 5.2686  — ate: 3.5822 — pehe: 5.2686 \n",
      " — ite: 5.5429  — ate: 3.1938 — pehe: 5.5429 \n",
      "6/6 [==============================] - 1s 122ms/step - loss: 28.1984 - val_loss: 28.7757 - val_y0: -0.1723 - val_y1: 0.5153 - val_ate_after_scaled: 0.6876 - lr: 1.0000e-05\n",
      "Epoch 18/300\n",
      "3/6 [==============>...............] - ETA: 0s - loss: 28.0691 — ite: 5.3641  — ate: 3.7137 — pehe: 5.3641 \n",
      " — ite: 6.0637  — ate: 2.7402 — pehe: 6.0637 \n",
      "6/6 [==============================] - 1s 129ms/step - loss: 28.0068 - val_loss: 28.6063 - val_y0: -0.1041 - val_y1: 0.5589 - val_ate_after_scaled: 0.6630 - lr: 1.0000e-05\n",
      "Epoch 19/300\n",
      "3/6 [==============>...............] - ETA: 0s - loss: 28.0113 — ite: 5.0886  — ate: 3.3909 — pehe: 5.0886 \n",
      " — ite: 5.7187  — ate: 3.2827 — pehe: 5.7187 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 28.1096 - val_loss: 28.5755 - val_y0: -0.0887 - val_y1: 0.5352 - val_ate_after_scaled: 0.6238 - lr: 1.0000e-05\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 27.9058 — ite: 5.1204  — ate: 3.4089 — pehe: 5.1204 \n",
      " — ite: 5.6904  — ate: 2.9966 — pehe: 5.6904 \n",
      "6/6 [==============================] - 1s 133ms/step - loss: 27.9007 - val_loss: 28.7492 - val_y0: -0.2266 - val_y1: 0.7215 - val_ate_after_scaled: 0.9480 - lr: 1.0000e-05\n",
      "Epoch 21/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 27.7341 — ite: 5.1611  — ate: 3.3372 — pehe: 5.1611 \n",
      " — ite: 5.6391  — ate: 3.2124 — pehe: 5.6391 \n",
      "6/6 [==============================] - 1s 131ms/step - loss: 28.0170 - val_loss: 28.2697 - val_y0: -0.2485 - val_y1: 0.4609 - val_ate_after_scaled: 0.7095 - lr: 1.0000e-05\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 27.7572 — ite: 5.1923  — ate: 3.3862 — pehe: 5.1923 \n",
      " — ite: 5.3219  — ate: 2.2737 — pehe: 5.3219 \n",
      "6/6 [==============================] - 1s 137ms/step - loss: 27.7413 - val_loss: 28.4361 - val_y0: -0.1482 - val_y1: 0.6488 - val_ate_after_scaled: 0.7970 - lr: 1.0000e-05\n",
      "Epoch 23/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 27.7249 — ite: 5.1056  — ate: 3.2183 — pehe: 5.1056 \n",
      " — ite: 5.6090  — ate: 3.1177 — pehe: 5.6090 \n",
      "6/6 [==============================] - 1s 130ms/step - loss: 27.6676 - val_loss: 28.5243 - val_y0: -0.3733 - val_y1: 0.5946 - val_ate_after_scaled: 0.9679 - lr: 1.0000e-05\n",
      "Epoch 24/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 27.7412 — ite: 5.1523  — ate: 3.2562 — pehe: 5.1523 \n",
      " — ite: 5.7696  — ate: 3.3520 — pehe: 5.7696 \n",
      "6/6 [==============================] - 1s 126ms/step - loss: 27.6343 - val_loss: 28.4666 - val_y0: -0.0045 - val_y1: 0.6420 - val_ate_after_scaled: 0.6466 - lr: 1.0000e-05\n",
      "Epoch 25/300\n",
      "3/6 [==============>...............] - ETA: 0s - loss: 27.5293 — ite: 4.9282  — ate: 3.1373 — pehe: 4.9282 \n",
      " — ite: 5.2120  — ate: 2.5239 — pehe: 5.2120 \n",
      "6/6 [==============================] - 1s 135ms/step - loss: 27.7282 - val_loss: 28.3706 - val_y0: -0.1420 - val_y1: 0.8246 - val_ate_after_scaled: 0.9666 - lr: 1.0000e-05\n",
      "Epoch 26/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 27.5067 — ite: 5.0857  — ate: 3.3984 — pehe: 5.0857 \n",
      " — ite: 5.9230  — ate: 2.8788 — pehe: 5.9230 \n",
      "6/6 [==============================] - 1s 120ms/step - loss: 27.3120 - val_loss: 28.2191 - val_y0: -0.2068 - val_y1: 0.6416 - val_ate_after_scaled: 0.8484 - lr: 1.0000e-05\n",
      "Epoch 27/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 27.4616 — ite: 5.2263  — ate: 3.5341 — pehe: 5.2263 \n",
      " — ite: 5.5651  — ate: 3.3700 — pehe: 5.5651 \n",
      "6/6 [==============================] - 1s 115ms/step - loss: 27.2912 - val_loss: 28.1405 - val_y0: -0.2309 - val_y1: 0.6195 - val_ate_after_scaled: 0.8504 - lr: 1.0000e-05\n",
      "Epoch 28/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 27.3795 — ite: 5.0130  — ate: 3.2323 — pehe: 5.0130 \n",
      " — ite: 5.7684  — ate: 2.9610 — pehe: 5.7684 \n",
      "6/6 [==============================] - 1s 117ms/step - loss: 27.2358 - val_loss: 27.9464 - val_y0: -0.2252 - val_y1: 0.7884 - val_ate_after_scaled: 1.0137 - lr: 1.0000e-05\n",
      "Epoch 29/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 27.3013 — ite: 5.0390  — ate: 3.1454 — pehe: 5.0390 \n",
      " — ite: 5.0232  — ate: 2.0168 — pehe: 5.0232 \n",
      "6/6 [==============================] - 1s 119ms/step - loss: 27.2921 - val_loss: 27.8042 - val_y0: -0.1073 - val_y1: 0.6245 - val_ate_after_scaled: 0.7319 - lr: 1.0000e-05\n",
      "Epoch 30/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 27.1646 — ite: 4.9561  — ate: 3.1183 — pehe: 4.9561 \n",
      " — ite: 5.7207  — ate: 2.9987 — pehe: 5.7207 \n",
      "6/6 [==============================] - 1s 130ms/step - loss: 27.2684 - val_loss: 28.0064 - val_y0: -0.2000 - val_y1: 0.7990 - val_ate_after_scaled: 0.9991 - lr: 1.0000e-05\n",
      "Epoch 31/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 26.9925 — ite: 4.9709  — ate: 2.9996 — pehe: 4.9709 \n",
      " — ite: 5.5827  — ate: 2.6609 — pehe: 5.5827 \n",
      "6/6 [==============================] - 1s 118ms/step - loss: 27.4034 - val_loss: 27.6551 - val_y0: -0.1244 - val_y1: 0.8535 - val_ate_after_scaled: 0.9780 - lr: 1.0000e-05\n",
      "Epoch 32/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 27.1715 — ite: 4.9307  — ate: 3.0710 — pehe: 4.9307 \n",
      " — ite: 5.7350  — ate: 3.2346 — pehe: 5.7350 \n",
      "6/6 [==============================] - 1s 117ms/step - loss: 27.0908 - val_loss: 27.7917 - val_y0: -0.1107 - val_y1: 0.7845 - val_ate_after_scaled: 0.8952 - lr: 1.0000e-05\n",
      "Epoch 33/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 26.9777 — ite: 5.1381  — ate: 3.2371 — pehe: 5.1381 \n",
      " — ite: 5.8561  — ate: 2.8205 — pehe: 5.8561 \n",
      "6/6 [==============================] - 1s 123ms/step - loss: 26.9990 - val_loss: 27.5417 - val_y0: -0.2145 - val_y1: 0.7122 - val_ate_after_scaled: 0.9267 - lr: 1.0000e-05\n",
      "Epoch 34/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 27.0815 — ite: 5.0422  — ate: 3.1385 — pehe: 5.0422 \n",
      " — ite: 4.7647  — ate: 2.5466 — pehe: 4.7647 \n",
      "6/6 [==============================] - 1s 118ms/step - loss: 27.2070 - val_loss: 27.6242 - val_y0: -0.2281 - val_y1: 0.8343 - val_ate_after_scaled: 1.0624 - lr: 1.0000e-05\n",
      "Epoch 35/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 26.8457 — ite: 5.1193  — ate: 3.1837 — pehe: 5.1193 \n",
      " — ite: 4.9952  — ate: 2.4650 — pehe: 4.9952 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 26.7558 - val_loss: 27.6245 - val_y0: -0.1038 - val_y1: 0.7044 - val_ate_after_scaled: 0.8082 - lr: 1.0000e-05\n",
      "Epoch 36/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 26.6636 — ite: 4.7940  — ate: 2.7706 — pehe: 4.7940 \n",
      " — ite: 5.5061  — ate: 2.7224 — pehe: 5.5061 \n",
      "6/6 [==============================] - 1s 116ms/step - loss: 26.8144 - val_loss: 27.4309 - val_y0: -0.0544 - val_y1: 0.8670 - val_ate_after_scaled: 0.9214 - lr: 1.0000e-05\n",
      "Epoch 37/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 26.6409 — ite: 4.9251  — ate: 2.8981 — pehe: 4.9251 \n",
      " — ite: 5.2463  — ate: 2.2565 — pehe: 5.2463 \n",
      "6/6 [==============================] - 1s 117ms/step - loss: 26.6476 - val_loss: 27.3855 - val_y0: -0.2257 - val_y1: 0.9437 - val_ate_after_scaled: 1.1694 - lr: 1.0000e-05\n",
      "Epoch 38/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 26.6542 — ite: 4.9250  — ate: 3.0387 — pehe: 4.9250 \n",
      " — ite: 5.5644  — ate: 2.5931 — pehe: 5.5644 \n",
      "6/6 [==============================] - 1s 118ms/step - loss: 26.6155 - val_loss: 27.1900 - val_y0: -0.2719 - val_y1: 0.9519 - val_ate_after_scaled: 1.2238 - lr: 1.0000e-05\n",
      "Epoch 39/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 26.6388 — ite: 4.8671  — ate: 2.7751 — pehe: 4.8671 \n",
      " — ite: 5.7463  — ate: 2.6670 — pehe: 5.7463 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 26.5143 - val_loss: 27.0732 - val_y0: -0.2405 - val_y1: 0.9393 - val_ate_after_scaled: 1.1798 - lr: 1.0000e-05\n",
      "Epoch 40/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 26.4153 — ite: 4.9000  — ate: 2.8664 — pehe: 4.9000 \n",
      " — ite: 4.9730  — ate: 1.9933 — pehe: 4.9730 \n",
      "6/6 [==============================] - 1s 116ms/step - loss: 26.5656 - val_loss: 26.9352 - val_y0: -0.2128 - val_y1: 0.8183 - val_ate_after_scaled: 1.0311 - lr: 1.0000e-05\n",
      "Epoch 41/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 26.4358 — ite: 4.8172  — ate: 2.7285 — pehe: 4.8172 \n",
      " — ite: 5.1256  — ate: 1.5655 — pehe: 5.1256 \n",
      "6/6 [==============================] - 1s 119ms/step - loss: 26.2780 - val_loss: 26.8219 - val_y0: -0.1332 - val_y1: 0.8882 - val_ate_after_scaled: 1.0214 - lr: 1.0000e-05\n",
      "Epoch 42/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 26.2776 — ite: 4.8280  — ate: 2.7558 — pehe: 4.8280 \n",
      " — ite: 4.8933  — ate: 2.0347 — pehe: 4.8933 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 26.3900 - val_loss: 27.1139 - val_y0: -0.2578 - val_y1: 0.8573 - val_ate_after_scaled: 1.1151 - lr: 1.0000e-05\n",
      "Epoch 43/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 26.3797 — ite: 4.8573  — ate: 2.8399 — pehe: 4.8573 \n",
      " — ite: 5.2636  — ate: 2.5675 — pehe: 5.2636 \n",
      "6/6 [==============================] - 1s 123ms/step - loss: 26.1308 - val_loss: 26.2796 - val_y0: -0.1679 - val_y1: 0.9447 - val_ate_after_scaled: 1.1126 - lr: 1.0000e-05\n",
      "Epoch 44/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 26.1618 — ite: 4.7756  — ate: 2.6800 — pehe: 4.7756 \n",
      " — ite: 5.1356  — ate: 2.0154 — pehe: 5.1356 \n",
      "6/6 [==============================] - 1s 125ms/step - loss: 26.1122 - val_loss: 26.9242 - val_y0: -0.0123 - val_y1: 1.0872 - val_ate_after_scaled: 1.0995 - lr: 1.0000e-05\n",
      "Epoch 45/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 26.2666 — ite: 4.7138  — ate: 2.5563 — pehe: 4.7138 \n",
      " — ite: 4.8887  — ate: 1.7632 — pehe: 4.8887 \n",
      "6/6 [==============================] - 1s 122ms/step - loss: 26.0914 - val_loss: 26.6247 - val_y0: -0.2526 - val_y1: 0.8697 - val_ate_after_scaled: 1.1223 - lr: 1.0000e-05\n",
      "Epoch 46/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 26.0362 — ite: 4.8142  — ate: 2.6016 — pehe: 4.8142 \n",
      " — ite: 5.1584  — ate: 2.4740 — pehe: 5.1584 \n",
      "6/6 [==============================] - 1s 120ms/step - loss: 26.3234 - val_loss: 26.4747 - val_y0: -0.2566 - val_y1: 0.9598 - val_ate_after_scaled: 1.2164 - lr: 1.0000e-05\n",
      "Epoch 47/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 26.0198 — ite: 4.6670  — ate: 2.4602 — pehe: 4.6670 \n",
      " — ite: 4.9397  — ate: 1.8900 — pehe: 4.9397 \n",
      "6/6 [==============================] - 1s 127ms/step - loss: 26.0391 - val_loss: 26.2057 - val_y0: -0.0594 - val_y1: 1.0481 - val_ate_after_scaled: 1.1075 - lr: 1.0000e-05\n",
      "Epoch 48/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 25.8778 — ite: 4.7857  — ate: 2.4979 — pehe: 4.7857 \n",
      " — ite: 5.4820  — ate: 1.7351 — pehe: 5.4820 \n",
      "6/6 [==============================] - 1s 128ms/step - loss: 25.9921 - val_loss: 26.3708 - val_y0: -0.2350 - val_y1: 1.0408 - val_ate_after_scaled: 1.2758 - lr: 1.0000e-05\n",
      "Epoch 49/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 25.9554 — ite: 4.8175  — ate: 2.5316 — pehe: 4.8175 \n",
      " — ite: 5.2194  — ate: 1.9540 — pehe: 5.2194 \n",
      "6/6 [==============================] - 1s 125ms/step - loss: 25.7926 - val_loss: 26.4711 - val_y0: -0.1893 - val_y1: 1.0306 - val_ate_after_scaled: 1.2198 - lr: 1.0000e-05\n",
      "Epoch 50/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 25.7746 — ite: 4.5803  — ate: 2.3211 — pehe: 4.5803 \n",
      " — ite: 4.9123  — ate: 1.0941 — pehe: 4.9123 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 25.9017 - val_loss: 26.0120 - val_y0: -0.2141 - val_y1: 1.0021 - val_ate_after_scaled: 1.2162 - lr: 1.0000e-05\n",
      "Epoch 51/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 25.7375 — ite: 4.6456  — ate: 2.3506 — pehe: 4.6456 \n",
      " — ite: 4.9002  — ate: 1.7730 — pehe: 4.9002 \n",
      "6/6 [==============================] - 1s 122ms/step - loss: 25.5912 - val_loss: 26.1939 - val_y0: -0.1094 - val_y1: 1.0513 - val_ate_after_scaled: 1.1606 - lr: 1.0000e-05\n",
      "Epoch 52/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 25.5858 — ite: 4.5226  — ate: 2.2623 — pehe: 4.5226 \n",
      " — ite: 4.9484  — ate: 1.4648 — pehe: 4.9484 \n",
      "6/6 [==============================] - 1s 120ms/step - loss: 25.7592 - val_loss: 26.2207 - val_y0: -0.2514 - val_y1: 0.9659 - val_ate_after_scaled: 1.2173 - lr: 1.0000e-05\n",
      "Epoch 53/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 25.5463 — ite: 4.5585  — ate: 2.1789 — pehe: 4.5585 \n",
      " — ite: 5.8589  — ate: 2.2680 — pehe: 5.8589 \n",
      "6/6 [==============================] - 1s 124ms/step - loss: 25.3923 - val_loss: 25.8099 - val_y0: -0.2371 - val_y1: 1.0595 - val_ate_after_scaled: 1.2966 - lr: 1.0000e-05\n",
      "Epoch 54/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 25.4436 — ite: 4.6840  — ate: 2.2859 — pehe: 4.6840 \n",
      " — ite: 5.1807  — ate: 2.0763 — pehe: 5.1807 \n",
      "6/6 [==============================] - 1s 120ms/step - loss: 25.5503 - val_loss: 26.3481 - val_y0: -0.1662 - val_y1: 1.2477 - val_ate_after_scaled: 1.4139 - lr: 1.0000e-05\n",
      "Epoch 55/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 25.4003 — ite: 4.4419  — ate: 2.1026 — pehe: 4.4419 \n",
      " — ite: 5.1445  — ate: 1.9622 — pehe: 5.1445 \n",
      "6/6 [==============================] - 1s 124ms/step - loss: 25.3721 - val_loss: 25.8742 - val_y0: -0.3290 - val_y1: 1.0037 - val_ate_after_scaled: 1.3327 - lr: 1.0000e-05\n",
      "Epoch 56/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 25.2829 — ite: 4.5211  — ate: 2.2150 — pehe: 4.5211 \n",
      " — ite: 4.9939  — ate: 1.1596 — pehe: 4.9939 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 25.3868 - val_loss: 25.4352 - val_y0: -0.0678 - val_y1: 0.9369 - val_ate_after_scaled: 1.0047 - lr: 1.0000e-05\n",
      "Epoch 57/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 25.1099 — ite: 4.6025  — ate: 2.0872 — pehe: 4.6025 \n",
      " — ite: 5.0199  — ate: 1.1572 — pehe: 5.0199 \n",
      "6/6 [==============================] - 1s 118ms/step - loss: 25.2686 - val_loss: 25.7446 - val_y0: -0.1224 - val_y1: 0.9838 - val_ate_after_scaled: 1.1062 - lr: 1.0000e-05\n",
      "Epoch 58/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 25.1497 — ite: 4.3816  — ate: 1.6652 — pehe: 4.3816 \n",
      " — ite: 5.6735  — ate: 1.6925 — pehe: 5.6735 \n",
      "6/6 [==============================] - 1s 124ms/step - loss: 25.1003 - val_loss: 25.5271 - val_y0: -0.1250 - val_y1: 1.1107 - val_ate_after_scaled: 1.2357 - lr: 1.0000e-05\n",
      "Epoch 59/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 24.9315 — ite: 4.4782  — ate: 1.8496 — pehe: 4.4782 \n",
      " — ite: 5.1497  — ate: 1.5669 — pehe: 5.1497 \n",
      "6/6 [==============================] - 1s 126ms/step - loss: 24.9756 - val_loss: 25.6430 - val_y0: -0.2008 - val_y1: 1.0651 - val_ate_after_scaled: 1.2659 - lr: 1.0000e-05\n",
      "Epoch 60/300\n",
      "3/6 [==============>...............] - ETA: 0s - loss: 24.8748 — ite: 4.6199  — ate: 1.8961 — pehe: 4.6199 \n",
      " — ite: 5.0854  — ate: 1.4431 — pehe: 5.0854 \n",
      "6/6 [==============================] - 1s 127ms/step - loss: 24.7748 - val_loss: 25.6354 - val_y0: -0.1595 - val_y1: 1.0828 - val_ate_after_scaled: 1.2423 - lr: 1.0000e-05\n",
      "Epoch 61/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 24.9720 — ite: 4.3588  — ate: 1.8119 — pehe: 4.3588 \n",
      " — ite: 5.1867  — ate: 1.0481 — pehe: 5.1867 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 24.8879 - val_loss: 25.3545 - val_y0: -0.0829 - val_y1: 1.0931 - val_ate_after_scaled: 1.1760 - lr: 1.0000e-05\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 24.7928 — ite: 4.3907  — ate: 1.8012 — pehe: 4.3907 \n",
      " — ite: 4.8153  — ate: 0.5214 — pehe: 4.8153 \n",
      "6/6 [==============================] - 1s 131ms/step - loss: 24.7223 - val_loss: 25.4877 - val_y0: -0.1949 - val_y1: 1.2614 - val_ate_after_scaled: 1.4563 - lr: 1.0000e-05\n",
      "Epoch 63/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 24.6549 — ite: 4.5603  — ate: 1.7997 — pehe: 4.5603 \n",
      " — ite: 5.3356  — ate: 0.9642 — pehe: 5.3356 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 24.4740 - val_loss: 25.1575 - val_y0: -0.2339 - val_y1: 1.1415 - val_ate_after_scaled: 1.3754 - lr: 1.0000e-05\n",
      "Epoch 64/300\n",
      "3/6 [==============>...............] - ETA: 0s - loss: 24.5078 — ite: 4.3528  — ate: 1.6644 — pehe: 4.3528 \n",
      " — ite: 4.8692  — ate: 0.7431 — pehe: 4.8692 \n",
      "6/6 [==============================] - 1s 127ms/step - loss: 24.5958 - val_loss: 24.9514 - val_y0: -0.1461 - val_y1: 1.2335 - val_ate_after_scaled: 1.3796 - lr: 1.0000e-05\n",
      "Epoch 65/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 24.5371 — ite: 4.5159  — ate: 1.6653 — pehe: 4.5159 \n",
      " — ite: 4.8368  — ate: 1.1843 — pehe: 4.8368 \n",
      "6/6 [==============================] - 1s 127ms/step - loss: 24.3925 - val_loss: 25.0170 - val_y0: -0.1821 - val_y1: 1.0822 - val_ate_after_scaled: 1.2643 - lr: 1.0000e-05\n",
      "Epoch 66/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 24.5107 — ite: 4.4503  — ate: 1.7069 — pehe: 4.4503 \n",
      " — ite: 5.1043  — ate: 0.6512 — pehe: 5.1043 \n",
      "6/6 [==============================] - 1s 118ms/step - loss: 24.3896 - val_loss: 24.8954 - val_y0: -0.1504 - val_y1: 1.0820 - val_ate_after_scaled: 1.2324 - lr: 1.0000e-05\n",
      "Epoch 67/300\n",
      "3/6 [==============>...............] - ETA: 0s - loss: 24.4303 — ite: 4.2891  — ate: 1.5158 — pehe: 4.2891 \n",
      " — ite: 4.2291  — ate: 0.5618 — pehe: 4.2291 \n",
      "6/6 [==============================] - 1s 130ms/step - loss: 24.3285 - val_loss: 24.8098 - val_y0: -0.0850 - val_y1: 1.1100 - val_ate_after_scaled: 1.1949 - lr: 1.0000e-05\n",
      "Epoch 68/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 24.2995 — ite: 4.3299  — ate: 1.4220 — pehe: 4.3299 \n",
      " — ite: 5.0439  — ate: 0.8880 — pehe: 5.0439 \n",
      "6/6 [==============================] - 1s 126ms/step - loss: 24.5658 - val_loss: 25.2798 - val_y0: -0.1854 - val_y1: 1.1226 - val_ate_after_scaled: 1.3080 - lr: 1.0000e-05\n",
      "Epoch 69/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 24.1008 — ite: 4.2061  — ate: 1.3591 — pehe: 4.2061 \n",
      " — ite: 4.7436  — ate: 0.4290 — pehe: 4.7436 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 24.3867 - val_loss: 24.7925 - val_y0: -0.1193 - val_y1: 1.1666 - val_ate_after_scaled: 1.2859 - lr: 1.0000e-05\n",
      "Epoch 70/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 24.1669 — ite: 4.1789  — ate: 1.0992 — pehe: 4.1789 \n",
      " — ite: 5.4731  — ate: 0.6318 — pehe: 5.4731 \n",
      "6/6 [==============================] - 1s 120ms/step - loss: 24.2048 - val_loss: 24.5277 - val_y0: -0.1746 - val_y1: 1.1651 - val_ate_after_scaled: 1.3397 - lr: 1.0000e-05\n",
      "Epoch 71/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 24.0703 — ite: 4.4518  — ate: 1.4296 — pehe: 4.4518 \n",
      " — ite: 5.1441  — ate: 0.4950 — pehe: 5.1441 \n",
      "6/6 [==============================] - 1s 119ms/step - loss: 23.9726 - val_loss: 24.6159 - val_y0: -0.1240 - val_y1: 1.2303 - val_ate_after_scaled: 1.3543 - lr: 1.0000e-05\n",
      "Epoch 72/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 24.1568 — ite: 4.2208  — ate: 1.2435 — pehe: 4.2208 \n",
      " — ite: 4.8971  — ate: 0.7324 — pehe: 4.8971 \n",
      "6/6 [==============================] - 1s 130ms/step - loss: 24.1909 - val_loss: 24.2487 - val_y0: -0.1686 - val_y1: 1.1745 - val_ate_after_scaled: 1.3431 - lr: 1.0000e-05\n",
      "Epoch 73/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 23.9480 — ite: 4.4928  — ate: 1.3681 — pehe: 4.4928 \n",
      " — ite: 5.3172  — ate: 1.3566 — pehe: 5.3172 \n",
      "6/6 [==============================] - 1s 125ms/step - loss: 23.8337 - val_loss: 24.1591 - val_y0: -0.2092 - val_y1: 1.1801 - val_ate_after_scaled: 1.3894 - lr: 1.0000e-05\n",
      "Epoch 74/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 23.9110 — ite: 4.2551  — ate: 1.2073 — pehe: 4.2551 \n",
      " — ite: 5.2045  — ate: 0.1854 — pehe: 5.2045 \n",
      "6/6 [==============================] - 1s 134ms/step - loss: 24.2659 - val_loss: 24.3310 - val_y0: -0.2106 - val_y1: 1.1676 - val_ate_after_scaled: 1.3782 - lr: 1.0000e-05\n",
      "Epoch 75/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 23.8792 — ite: 4.1347  — ate: 1.1097 — pehe: 4.1347 \n",
      " — ite: 4.9383  — ate: 0.1415 — pehe: 4.9383 \n",
      "6/6 [==============================] - 1s 126ms/step - loss: 23.8204 - val_loss: 23.8730 - val_y0: -0.3717 - val_y1: 1.1959 - val_ate_after_scaled: 1.5676 - lr: 1.0000e-05\n",
      "Epoch 76/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 23.7591 — ite: 4.2944  — ate: 1.2925 — pehe: 4.2944 \n",
      " — ite: 5.0614  — ate: 0.1903 — pehe: 5.0614 \n",
      "6/6 [==============================] - 1s 130ms/step - loss: 23.5485 - val_loss: 24.2251 - val_y0: -0.4462 - val_y1: 1.2037 - val_ate_after_scaled: 1.6500 - lr: 1.0000e-05\n",
      "Epoch 77/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 23.6820 — ite: 4.3479  — ate: 1.2855 — pehe: 4.3479 \n",
      " — ite: 4.9954  — ate: 0.6483 — pehe: 4.9954 \n",
      "6/6 [==============================] - 1s 124ms/step - loss: 23.6770 - val_loss: 24.1882 - val_y0: -0.1539 - val_y1: 1.1463 - val_ate_after_scaled: 1.3002 - lr: 1.0000e-05\n",
      "Epoch 78/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 23.4900 — ite: 4.2544  — ate: 1.1294 — pehe: 4.2544 \n",
      " — ite: 4.5221  — ate: 0.0115 — pehe: 4.5221 \n",
      "6/6 [==============================] - 1s 131ms/step - loss: 23.6237 - val_loss: 24.0783 - val_y0: -0.3522 - val_y1: 1.2915 - val_ate_after_scaled: 1.6437 - lr: 1.0000e-05\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 23.4268 — ite: 4.2951  — ate: 1.0553 — pehe: 4.2951 \n",
      " — ite: 5.3929  — ate: 0.5286 — pehe: 5.3929 \n",
      "6/6 [==============================] - 1s 129ms/step - loss: 23.4297 - val_loss: 24.2640 - val_y0: -0.1907 - val_y1: 1.1190 - val_ate_after_scaled: 1.3097 - lr: 1.0000e-05\n",
      "Epoch 80/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 23.4903 — ite: 4.2685  — ate: 1.1039 — pehe: 4.2685 \n",
      " — ite: 4.6065  — ate: 0.3751 — pehe: 4.6065 \n",
      "6/6 [==============================] - 1s 128ms/step - loss: 23.5029 - val_loss: 23.7885 - val_y0: -0.0955 - val_y1: 1.0958 - val_ate_after_scaled: 1.1913 - lr: 1.0000e-05\n",
      "Epoch 81/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 23.1685 — ite: 4.1516  — ate: 0.8048 — pehe: 4.1516 \n",
      " — ite: 4.8500  — ate: 0.3014 — pehe: 4.8500 \n",
      "6/6 [==============================] - 1s 132ms/step - loss: 23.3555 - val_loss: 23.9488 - val_y0: -0.0762 - val_y1: 1.3016 - val_ate_after_scaled: 1.3778 - lr: 1.0000e-05\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 23.1154 — ite: 4.2952  — ate: 1.0010 — pehe: 4.2952 \n",
      " — ite: 4.8056  — ate: 0.5544 — pehe: 4.8056 \n",
      "6/6 [==============================] - 1s 133ms/step - loss: 23.0130 - val_loss: 23.8532 - val_y0: -0.2454 - val_y1: 1.0885 - val_ate_after_scaled: 1.3338 - lr: 1.0000e-05\n",
      "Epoch 83/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 23.0758 — ite: 4.2272  — ate: 0.8590 — pehe: 4.2272 \n",
      " — ite: 5.2659  — ate: 0.2513 — pehe: 5.2659 \n",
      "6/6 [==============================] - 1s 124ms/step - loss: 23.1404 - val_loss: 23.2594 - val_y0: -0.3020 - val_y1: 1.2679 - val_ate_after_scaled: 1.5699 - lr: 1.0000e-05\n",
      "Epoch 84/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 22.9534 — ite: 4.3463  — ate: 0.8775 — pehe: 4.3463 \n",
      " — ite: 4.6907  — ate: 0.1465 — pehe: 4.6907 \n",
      "6/6 [==============================] - 1s 118ms/step - loss: 23.0421 - val_loss: 23.3456 - val_y0: -0.1938 - val_y1: 1.2370 - val_ate_after_scaled: 1.4308 - lr: 1.0000e-05\n",
      "Epoch 85/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 23.1187 — ite: 4.2657  — ate: 1.0346 — pehe: 4.2657 \n",
      " — ite: 5.2260  — ate: 0.5448 — pehe: 5.2260 \n",
      "6/6 [==============================] - 1s 122ms/step - loss: 22.9856 - val_loss: 23.3960 - val_y0: -0.2208 - val_y1: 1.1282 - val_ate_after_scaled: 1.3491 - lr: 1.0000e-05\n",
      "Epoch 86/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 22.8515 — ite: 4.1666  — ate: 0.8058 — pehe: 4.1666 \n",
      " — ite: 5.0712  — ate: 0.9873 — pehe: 5.0712 \n",
      "6/6 [==============================] - 1s 124ms/step - loss: 22.9059 - val_loss: 23.2851 - val_y0: -0.1779 - val_y1: 1.1711 - val_ate_after_scaled: 1.3490 - lr: 1.0000e-05\n",
      "Epoch 87/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 23.0312 — ite: 4.4295  — ate: 0.7833 — pehe: 4.4295 \n",
      " — ite: 5.2652  — ate: 0.5008 — pehe: 5.2652 \n",
      "6/6 [==============================] - 1s 128ms/step - loss: 22.7733 - val_loss: 23.2563 - val_y0: -0.1787 - val_y1: 1.0976 - val_ate_after_scaled: 1.2764 - lr: 1.0000e-05\n",
      "Epoch 88/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 23.0145 — ite: 4.2418  — ate: 0.8398 — pehe: 4.2418 \n",
      " — ite: 5.2207  — ate: 0.4069 — pehe: 5.2207 \n",
      "6/6 [==============================] - 1s 122ms/step - loss: 22.7592 - val_loss: 23.3297 - val_y0: -0.1730 - val_y1: 1.2484 - val_ate_after_scaled: 1.4214 - lr: 1.0000e-05\n",
      "Epoch 89/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 22.7636 — ite: 4.2321  — ate: 0.8689 — pehe: 4.2321 \n",
      " — ite: 4.3189  — ate: 0.0496 — pehe: 4.3189 \n",
      "6/6 [==============================] - 1s 128ms/step - loss: 22.7690 - val_loss: 23.2543 - val_y0: -0.4301 - val_y1: 1.1145 - val_ate_after_scaled: 1.5446 - lr: 1.0000e-05\n",
      "Epoch 90/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 22.6409 — ite: 4.2333  — ate: 1.0562 — pehe: 4.2333 \n",
      " — ite: 5.0094  — ate: 0.1678 — pehe: 5.0094 \n",
      "6/6 [==============================] - 1s 145ms/step - loss: 22.7382 - val_loss: 23.0054 - val_y0: -0.1206 - val_y1: 1.2374 - val_ate_after_scaled: 1.3579 - lr: 1.0000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 22.6237 — ite: 4.2507  — ate: 0.9866 — pehe: 4.2507 \n",
      " — ite: 5.0633  — ate: 0.0617 — pehe: 5.0633 \n",
      "6/6 [==============================] - 1s 133ms/step - loss: 22.6466 - val_loss: 23.0667 - val_y0: -0.1109 - val_y1: 1.1134 - val_ate_after_scaled: 1.2243 - lr: 1.0000e-05\n",
      "Epoch 92/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 22.3935 — ite: 4.2707  — ate: 0.7988 — pehe: 4.2707 \n",
      " — ite: 5.0314  — ate: 0.2099 — pehe: 5.0314 \n",
      "6/6 [==============================] - 1s 130ms/step - loss: 22.3715 - val_loss: 22.9589 - val_y0: -0.2200 - val_y1: 1.2763 - val_ate_after_scaled: 1.4963 - lr: 1.0000e-05\n",
      "Epoch 93/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 22.3473 — ite: 4.3407  — ate: 0.9217 — pehe: 4.3407 \n",
      " — ite: 4.7067  — ate: 0.1344 — pehe: 4.7067 \n",
      "6/6 [==============================] - 1s 134ms/step - loss: 22.4676 - val_loss: 23.2958 - val_y0: -0.2451 - val_y1: 1.2818 - val_ate_after_scaled: 1.5269 - lr: 1.0000e-05\n",
      "Epoch 94/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 22.3837 — ite: 4.1737  — ate: 0.7903 — pehe: 4.1737 \n",
      " — ite: 4.5653  — ate: 0.0483 — pehe: 4.5653 \n",
      "6/6 [==============================] - 1s 136ms/step - loss: 22.6219 - val_loss: 22.8085 - val_y0: -0.2313 - val_y1: 1.1514 - val_ate_after_scaled: 1.3826 - lr: 1.0000e-05\n",
      "Epoch 95/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 22.3025 — ite: 4.1957  — ate: 0.9476 — pehe: 4.1957 \n",
      " — ite: 4.9110  — ate: 0.1013 — pehe: 4.9110 \n",
      "6/6 [==============================] - 1s 130ms/step - loss: 22.2207 - val_loss: 22.9592 - val_y0: -0.3061 - val_y1: 1.2675 - val_ate_after_scaled: 1.5735 - lr: 1.0000e-05\n",
      "Epoch 96/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 22.2281 — ite: 4.1175  — ate: 0.7838 — pehe: 4.1175 \n",
      " — ite: 4.3453  — ate: 0.1331 — pehe: 4.3453 \n",
      "6/6 [==============================] - 1s 127ms/step - loss: 22.0776 - val_loss: 22.8061 - val_y0: -0.1839 - val_y1: 1.2955 - val_ate_after_scaled: 1.4794 - lr: 1.0000e-05\n",
      "Epoch 97/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 22.0920 — ite: 4.1776  — ate: 0.7928 — pehe: 4.1776 \n",
      " — ite: 5.1153  — ate: 0.3260 — pehe: 5.1153 \n",
      "6/6 [==============================] - 1s 131ms/step - loss: 22.1477 - val_loss: 22.3853 - val_y0: -0.2135 - val_y1: 1.2168 - val_ate_after_scaled: 1.4303 - lr: 1.0000e-05\n",
      "Epoch 98/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 22.0543 — ite: 4.0448  — ate: 0.7646 — pehe: 4.0448 \n",
      " — ite: 5.0037  — ate: 0.2036 — pehe: 5.0037 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 21.9553 - val_loss: 22.5996 - val_y0: -0.2389 - val_y1: 1.2746 - val_ate_after_scaled: 1.5135 - lr: 1.0000e-05\n",
      "Epoch 99/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 21.8902 — ite: 4.1060  — ate: 0.7997 — pehe: 4.1060 \n",
      " — ite: 4.7533  — ate: 0.1178 — pehe: 4.7533 \n",
      "6/6 [==============================] - 1s 123ms/step - loss: 21.9601 - val_loss: 22.4341 - val_y0: -0.2665 - val_y1: 1.0585 - val_ate_after_scaled: 1.3250 - lr: 1.0000e-05\n",
      "Epoch 100/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 21.9163 — ite: 4.1546  — ate: 0.8473 — pehe: 4.1546 \n",
      " — ite: 4.3644  — ate: 0.4054 — pehe: 4.3644 \n",
      "6/6 [==============================] - 1s 136ms/step - loss: 21.8512 - val_loss: 22.0798 - val_y0: -0.2587 - val_y1: 1.1461 - val_ate_after_scaled: 1.4049 - lr: 1.0000e-05\n",
      "Epoch 101/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 21.9047 — ite: 4.1451  — ate: 0.8852 — pehe: 4.1451 \n",
      " — ite: 4.8434  — ate: 0.1515 — pehe: 4.8434 \n",
      "6/6 [==============================] - 1s 138ms/step - loss: 21.7318 - val_loss: 22.1660 - val_y0: -0.2063 - val_y1: 1.0481 - val_ate_after_scaled: 1.2544 - lr: 1.0000e-05\n",
      "Epoch 102/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 21.8133 — ite: 4.0892  — ate: 0.7099 — pehe: 4.0892 \n",
      " — ite: 4.8945  — ate: 0.2261 — pehe: 4.8945 \n",
      "6/6 [==============================] - 1s 135ms/step - loss: 21.8486 - val_loss: 22.3214 - val_y0: -0.3032 - val_y1: 1.2516 - val_ate_after_scaled: 1.5548 - lr: 1.0000e-05\n",
      "Epoch 103/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 21.8350 — ite: 4.1516  — ate: 0.7715 — pehe: 4.1516 \n",
      " — ite: 4.9590  — ate: 0.0921 — pehe: 4.9590 \n",
      "6/6 [==============================] - 1s 147ms/step - loss: 21.9054 - val_loss: 21.7889 - val_y0: -0.1043 - val_y1: 1.2482 - val_ate_after_scaled: 1.3525 - lr: 1.0000e-05\n",
      "Epoch 104/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 21.9055 — ite: 4.1623  — ate: 0.6519 — pehe: 4.1623 \n",
      " — ite: 4.5083  — ate: 0.0180 — pehe: 4.5083 \n",
      "6/6 [==============================] - 1s 140ms/step - loss: 21.6806 - val_loss: 21.6860 - val_y0: -0.1706 - val_y1: 1.2091 - val_ate_after_scaled: 1.3798 - lr: 1.0000e-05\n",
      "Epoch 105/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 21.7795 — ite: 4.1217  — ate: 0.6108 — pehe: 4.1217 \n",
      " — ite: 4.6896  — ate: 0.4646 — pehe: 4.6896 \n",
      "6/6 [==============================] - 1s 136ms/step - loss: 21.7658 - val_loss: 21.7298 - val_y0: -0.2686 - val_y1: 1.0230 - val_ate_after_scaled: 1.2916 - lr: 1.0000e-05\n",
      "Epoch 106/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 21.4988 — ite: 4.2882  — ate: 0.7509 — pehe: 4.2882 \n",
      " — ite: 4.7474  — ate: 0.7243 — pehe: 4.7474 \n",
      "6/6 [==============================] - 1s 139ms/step - loss: 21.5413 - val_loss: 22.0190 - val_y0: -0.0711 - val_y1: 1.2490 - val_ate_after_scaled: 1.3201 - lr: 1.0000e-05\n",
      "Epoch 107/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 21.4893 — ite: 4.0706  — ate: 0.8095 — pehe: 4.0706 \n",
      " — ite: 4.6571  — ate: 0.0776 — pehe: 4.6571 \n",
      "6/6 [==============================] - 1s 139ms/step - loss: 21.4762 - val_loss: 21.7316 - val_y0: -0.0989 - val_y1: 1.2666 - val_ate_after_scaled: 1.3654 - lr: 1.0000e-05\n",
      "Epoch 108/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 21.4401 — ite: 4.2657  — ate: 0.7767 — pehe: 4.2657 \n",
      " — ite: 4.6554  — ate: 0.3713 — pehe: 4.6554 \n",
      "6/6 [==============================] - 1s 137ms/step - loss: 21.5410 - val_loss: 21.7412 - val_y0: -0.3130 - val_y1: 1.1797 - val_ate_after_scaled: 1.4928 - lr: 1.0000e-05\n",
      "Epoch 109/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 21.1466\n",
      "Epoch 00109: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      " — ite: 4.2821  — ate: 0.9026 — pehe: 4.2821 \n",
      " — ite: 4.6907  — ate: 0.3298 — pehe: 4.6907 \n",
      "6/6 [==============================] - 1s 136ms/step - loss: 21.2889 - val_loss: 21.9908 - val_y0: -0.2785 - val_y1: 1.1018 - val_ate_after_scaled: 1.3803 - lr: 1.0000e-05\n",
      "Epoch 110/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 21.3680 — ite: 4.0590  — ate: 0.6221 — pehe: 4.0590 \n",
      " — ite: 4.6353  — ate: 0.0243 — pehe: 4.6353 \n",
      "6/6 [==============================] - 1s 142ms/step - loss: 21.3944 - val_loss: 21.8945 - val_y0: -0.2667 - val_y1: 1.3163 - val_ate_after_scaled: 1.5830 - lr: 5.0000e-06\n",
      "Epoch 111/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 21.4138 — ite: 4.2142  — ate: 0.7310 — pehe: 4.2142 \n",
      " — ite: 4.8455  — ate: 0.3610 — pehe: 4.8455 \n",
      "6/6 [==============================] - 1s 133ms/step - loss: 21.3412 - val_loss: 21.8964 - val_y0: -0.2395 - val_y1: 1.2243 - val_ate_after_scaled: 1.4638 - lr: 5.0000e-06\n",
      "Epoch 112/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 21.1312 — ite: 4.2560  — ate: 0.6734 — pehe: 4.2560 \n",
      " — ite: 5.4858  — ate: 0.1410 — pehe: 5.4858 \n",
      "6/6 [==============================] - 1s 136ms/step - loss: 21.0958 - val_loss: 21.4974 - val_y0: -0.2221 - val_y1: 1.1531 - val_ate_after_scaled: 1.3752 - lr: 5.0000e-06\n",
      "Epoch 113/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 21.1105 — ite: 4.1433  — ate: 0.5120 — pehe: 4.1433 \n",
      " — ite: 4.9150  — ate: 0.0146 — pehe: 4.9150 \n",
      "6/6 [==============================] - 1s 135ms/step - loss: 21.3256 - val_loss: 21.7781 - val_y0: -0.1796 - val_y1: 1.2843 - val_ate_after_scaled: 1.4638 - lr: 5.0000e-06\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 21.0645 — ite: 4.0713  — ate: 0.8353 — pehe: 4.0713 \n",
      " — ite: 5.1423  — ate: 0.0862 — pehe: 5.1423 \n",
      "6/6 [==============================] - 1s 141ms/step - loss: 21.0712 - val_loss: 21.7233 - val_y0: -0.2901 - val_y1: 1.1960 - val_ate_after_scaled: 1.4861 - lr: 5.0000e-06\n",
      "Epoch 115/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 21.1493 — ite: 4.1917  — ate: 0.8208 — pehe: 4.1917 \n",
      " — ite: 4.7726  — ate: 0.2552 — pehe: 4.7726 \n",
      "6/6 [==============================] - 1s 131ms/step - loss: 21.0229 - val_loss: 21.5450 - val_y0: -0.2746 - val_y1: 1.2267 - val_ate_after_scaled: 1.5013 - lr: 5.0000e-06\n",
      "Epoch 116/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 21.1129 — ite: 4.1365  — ate: 0.7333 — pehe: 4.1365 \n",
      " — ite: 4.9427  — ate: 0.1105 — pehe: 4.9427 \n",
      "6/6 [==============================] - 1s 136ms/step - loss: 21.1078 - val_loss: 21.7979 - val_y0: -0.1532 - val_y1: 1.1777 - val_ate_after_scaled: 1.3309 - lr: 5.0000e-06\n",
      "Epoch 117/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 21.0244\n",
      "Epoch 00117: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      " — ite: 4.0788  — ate: 0.6661 — pehe: 4.0788 \n",
      " — ite: 4.6399  — ate: 0.7933 — pehe: 4.6399 \n",
      "6/6 [==============================] - 1s 144ms/step - loss: 20.9607 - val_loss: 21.7784 - val_y0: -0.3743 - val_y1: 1.3139 - val_ate_after_scaled: 1.6881 - lr: 5.0000e-06\n",
      "Epoch 118/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 21.0346 — ite: 4.2320  — ate: 0.8689 — pehe: 4.2320 \n",
      " — ite: 4.7065  — ate: 0.0369 — pehe: 4.7065 \n",
      "6/6 [==============================] - 1s 135ms/step - loss: 20.8286 - val_loss: 21.1575 - val_y0: -0.1857 - val_y1: 1.1580 - val_ate_after_scaled: 1.3436 - lr: 2.5000e-06\n",
      "Epoch 119/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 21.0809 — ite: 4.1046  — ate: 0.7706 — pehe: 4.1046 \n",
      " — ite: 5.1386  — ate: 0.1456 — pehe: 5.1386 \n",
      "6/6 [==============================] - 1s 130ms/step - loss: 20.9860 - val_loss: 21.3926 - val_y0: -0.2475 - val_y1: 1.0891 - val_ate_after_scaled: 1.3366 - lr: 2.5000e-06\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 21.1605 — ite: 4.1934  — ate: 0.7656 — pehe: 4.1934 \n",
      " — ite: 5.4990  — ate: 0.6343 — pehe: 5.4990 \n",
      "6/6 [==============================] - 1s 142ms/step - loss: 21.1186 - val_loss: 21.1154 - val_y0: -0.3404 - val_y1: 1.1337 - val_ate_after_scaled: 1.4741 - lr: 2.5000e-06\n",
      "Epoch 121/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.9658 — ite: 4.2236  — ate: 0.8305 — pehe: 4.2236 \n",
      " — ite: 5.3251  — ate: 0.2738 — pehe: 5.3251 \n",
      "6/6 [==============================] - 1s 128ms/step - loss: 20.9258 - val_loss: 21.4208 - val_y0: -0.1344 - val_y1: 1.1752 - val_ate_after_scaled: 1.3097 - lr: 2.5000e-06\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.9368 — ite: 4.0822  — ate: 0.7621 — pehe: 4.0822 \n",
      " — ite: 4.3802  — ate: 0.4572 — pehe: 4.3802 \n",
      "6/6 [==============================] - 1s 137ms/step - loss: 21.0259 - val_loss: 21.3507 - val_y0: -0.2821 - val_y1: 1.1155 - val_ate_after_scaled: 1.3975 - lr: 2.5000e-06\n",
      "Epoch 123/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.8697 — ite: 4.1488  — ate: 0.8765 — pehe: 4.1488 \n",
      " — ite: 4.7138  — ate: 0.3252 — pehe: 4.7138 \n",
      "6/6 [==============================] - 1s 138ms/step - loss: 20.5117 - val_loss: 21.1132 - val_y0: -0.1692 - val_y1: 1.2396 - val_ate_after_scaled: 1.4088 - lr: 2.5000e-06\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.9840 — ite: 4.2346  — ate: 0.9534 — pehe: 4.2346 \n",
      " — ite: 4.4785  — ate: 0.4159 — pehe: 4.4785 \n",
      "6/6 [==============================] - 1s 135ms/step - loss: 20.9944 - val_loss: 21.8067 - val_y0: -0.0585 - val_y1: 1.2928 - val_ate_after_scaled: 1.3513 - lr: 2.5000e-06\n",
      "Epoch 125/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 21.0562 — ite: 4.2405  — ate: 0.5607 — pehe: 4.2405 \n",
      " — ite: 5.0093  — ate: 0.0852 — pehe: 5.0093 \n",
      "6/6 [==============================] - 1s 133ms/step - loss: 20.7627 - val_loss: 21.5445 - val_y0: -0.2702 - val_y1: 1.1988 - val_ate_after_scaled: 1.4690 - lr: 2.5000e-06\n",
      "Epoch 126/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.7978 — ite: 4.2732  — ate: 0.6944 — pehe: 4.2732 \n",
      " — ite: 4.9264  — ate: 0.0426 — pehe: 4.9264 \n",
      "6/6 [==============================] - 1s 119ms/step - loss: 21.2829 - val_loss: 21.1732 - val_y0: -0.0089 - val_y1: 1.3122 - val_ate_after_scaled: 1.3212 - lr: 2.5000e-06\n",
      "Epoch 127/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.8732 — ite: 4.0171  — ate: 0.7540 — pehe: 4.0171 \n",
      " — ite: 4.7867  — ate: 0.4693 — pehe: 4.7867 \n",
      "6/6 [==============================] - 1s 124ms/step - loss: 20.8189 - val_loss: 21.2991 - val_y0: -0.1446 - val_y1: 1.1314 - val_ate_after_scaled: 1.2760 - lr: 2.5000e-06\n",
      "Epoch 128/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.6745\n",
      "Epoch 00128: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      " — ite: 3.9997  — ate: 0.6567 — pehe: 3.9997 \n",
      " — ite: 4.7291  — ate: 0.0135 — pehe: 4.7291 \n",
      "6/6 [==============================] - 1s 131ms/step - loss: 20.8106 - val_loss: 21.4289 - val_y0: -0.1503 - val_y1: 1.1997 - val_ate_after_scaled: 1.3500 - lr: 2.5000e-06\n",
      "Epoch 129/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 21.0909 — ite: 3.8968  — ate: 0.6455 — pehe: 3.8968 \n",
      " — ite: 4.3155  — ate: 0.2321 — pehe: 4.3155 \n",
      "6/6 [==============================] - 1s 120ms/step - loss: 20.9118 - val_loss: 21.3746 - val_y0: -0.2133 - val_y1: 1.1619 - val_ate_after_scaled: 1.3752 - lr: 1.2500e-06\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.8818 — ite: 4.0686  — ate: 0.8820 — pehe: 4.0686 \n",
      " — ite: 5.1497  — ate: 0.2799 — pehe: 5.1497 \n",
      "6/6 [==============================] - 1s 133ms/step - loss: 20.7994 - val_loss: 21.5714 - val_y0: -0.3288 - val_y1: 1.2128 - val_ate_after_scaled: 1.5416 - lr: 1.2500e-06\n",
      "Epoch 131/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.7371 — ite: 3.9850  — ate: 0.5658 — pehe: 3.9850 \n",
      " — ite: 4.9369  — ate: 0.5539 — pehe: 4.9369 \n",
      "6/6 [==============================] - 1s 118ms/step - loss: 20.8223 - val_loss: 21.5181 - val_y0: -0.1842 - val_y1: 1.1032 - val_ate_after_scaled: 1.2874 - lr: 1.2500e-06\n",
      "Epoch 132/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.6297 — ite: 4.0950  — ate: 0.6743 — pehe: 4.0950 \n",
      " — ite: 5.1268  — ate: 1.0544 — pehe: 5.1268 \n",
      "6/6 [==============================] - 1s 117ms/step - loss: 20.8212 - val_loss: 21.5922 - val_y0: -0.2163 - val_y1: 1.2824 - val_ate_after_scaled: 1.4987 - lr: 1.2500e-06\n",
      "Epoch 133/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.9872 — ite: 4.1449  — ate: 0.6797 — pehe: 4.1449 \n",
      " — ite: 4.4823  — ate: 0.4667 — pehe: 4.4823 \n",
      "6/6 [==============================] - 1s 118ms/step - loss: 20.7392 - val_loss: 21.0020 - val_y0: -0.2316 - val_y1: 1.3275 - val_ate_after_scaled: 1.5592 - lr: 1.2500e-06\n",
      "Epoch 134/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 21.0225 — ite: 4.0238  — ate: 0.7529 — pehe: 4.0238 \n",
      " — ite: 5.0018  — ate: 0.0918 — pehe: 5.0018 \n",
      "6/6 [==============================] - 1s 132ms/step - loss: 20.4900 - val_loss: 21.3674 - val_y0: -0.1218 - val_y1: 1.1845 - val_ate_after_scaled: 1.3064 - lr: 1.2500e-06\n",
      "Epoch 135/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.7876 — ite: 4.1759  — ate: 0.9206 — pehe: 4.1759 \n",
      " — ite: 4.4758  — ate: 1.1098 — pehe: 4.4758 \n",
      "6/6 [==============================] - 1s 127ms/step - loss: 20.8973 - val_loss: 21.0820 - val_y0: -0.1371 - val_y1: 1.1893 - val_ate_after_scaled: 1.3264 - lr: 1.2500e-06\n",
      "Epoch 136/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.6515 — ite: 4.0695  — ate: 0.7848 — pehe: 4.0695 \n",
      " — ite: 4.9950  — ate: 0.1573 — pehe: 4.9950 \n",
      "6/6 [==============================] - 1s 130ms/step - loss: 21.0266 - val_loss: 21.2947 - val_y0: -0.1960 - val_y1: 1.0868 - val_ate_after_scaled: 1.2828 - lr: 1.2500e-06\n",
      "Epoch 137/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.7593 — ite: 4.1116  — ate: 0.6356 — pehe: 4.1116 \n",
      " — ite: 5.2813  — ate: 0.7871 — pehe: 5.2813 \n",
      "6/6 [==============================] - 1s 127ms/step - loss: 20.8913 - val_loss: 21.5260 - val_y0: -0.2269 - val_y1: 1.2692 - val_ate_after_scaled: 1.4961 - lr: 1.2500e-06\n",
      "Epoch 138/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.8696\n",
      "Epoch 00138: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      " — ite: 4.1881  — ate: 0.6647 — pehe: 4.1881 \n",
      " — ite: 4.7487  — ate: 0.3584 — pehe: 4.7487 \n",
      "6/6 [==============================] - 1s 140ms/step - loss: 20.5872 - val_loss: 21.3487 - val_y0: -0.2763 - val_y1: 1.0976 - val_ate_after_scaled: 1.3739 - lr: 1.2500e-06\n",
      "Epoch 139/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.8685 — ite: 4.1179  — ate: 0.7683 — pehe: 4.1179 \n",
      " — ite: 4.8493  — ate: 0.1090 — pehe: 4.8493 \n",
      "6/6 [==============================] - 1s 127ms/step - loss: 20.6718 - val_loss: 21.2446 - val_y0: -0.1956 - val_y1: 1.1306 - val_ate_after_scaled: 1.3262 - lr: 6.2500e-07\n",
      "Epoch 140/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.8314 — ite: 4.0343  — ate: 0.5632 — pehe: 4.0343 \n",
      " — ite: 4.6573  — ate: 0.1459 — pehe: 4.6573 \n",
      "6/6 [==============================] - 1s 130ms/step - loss: 20.8346 - val_loss: 21.2091 - val_y0: -0.0255 - val_y1: 1.1403 - val_ate_after_scaled: 1.1658 - lr: 6.2500e-07\n",
      "Epoch 141/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.8656 — ite: 4.1630  — ate: 0.7212 — pehe: 4.1630 \n",
      " — ite: 4.5413  — ate: 0.0050 — pehe: 4.5413 \n",
      "6/6 [==============================] - 1s 131ms/step - loss: 21.2686 - val_loss: 21.2054 - val_y0: -0.1872 - val_y1: 1.1893 - val_ate_after_scaled: 1.3765 - lr: 6.2500e-07\n",
      "Epoch 142/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.8195 — ite: 4.0882  — ate: 0.8335 — pehe: 4.0882 \n",
      " — ite: 4.5054  — ate: 0.4119 — pehe: 4.5054 \n",
      "6/6 [==============================] - 1s 134ms/step - loss: 20.8953 - val_loss: 21.1351 - val_y0: -0.2148 - val_y1: 1.1205 - val_ate_after_scaled: 1.3354 - lr: 6.2500e-07\n",
      "Epoch 143/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.6556\n",
      "Epoch 00143: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-07.\n",
      " — ite: 4.1812  — ate: 0.8289 — pehe: 4.1812 \n",
      " — ite: 4.8710  — ate: 0.5646 — pehe: 4.8710 \n",
      "6/6 [==============================] - 1s 137ms/step - loss: 20.6180 - val_loss: 21.3681 - val_y0: -0.1387 - val_y1: 1.1780 - val_ate_after_scaled: 1.3167 - lr: 6.2500e-07\n",
      "Epoch 144/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.8627 — ite: 4.1188  — ate: 0.6768 — pehe: 4.1188 \n",
      " — ite: 5.0034  — ate: 0.3671 — pehe: 5.0034 \n",
      "6/6 [==============================] - 1s 130ms/step - loss: 20.9427 - val_loss: 21.3540 - val_y0: -0.2757 - val_y1: 1.1388 - val_ate_after_scaled: 1.4145 - lr: 3.1250e-07\n",
      "Epoch 145/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.7216 — ite: 4.1971  — ate: 0.7662 — pehe: 4.1971 \n",
      " — ite: 4.4668  — ate: 0.2637 — pehe: 4.4668 \n",
      "6/6 [==============================] - 1s 132ms/step - loss: 20.6049 - val_loss: 21.6259 - val_y0: -0.2104 - val_y1: 1.2055 - val_ate_after_scaled: 1.4158 - lr: 3.1250e-07\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.8081 — ite: 4.2316  — ate: 0.7542 — pehe: 4.2316 \n",
      " — ite: 4.9532  — ate: 0.2362 — pehe: 4.9532 \n",
      "6/6 [==============================] - 1s 138ms/step - loss: 20.8325 - val_loss: 20.9321 - val_y0: -0.2025 - val_y1: 1.1596 - val_ate_after_scaled: 1.3621 - lr: 3.1250e-07\n",
      "Epoch 147/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.6456 — ite: 4.1140  — ate: 0.8062 — pehe: 4.1140 \n",
      " — ite: 4.5840  — ate: 0.0796 — pehe: 4.5840 \n",
      "6/6 [==============================] - 1s 141ms/step - loss: 20.6675 - val_loss: 21.4123 - val_y0: -0.0903 - val_y1: 1.0565 - val_ate_after_scaled: 1.1468 - lr: 3.1250e-07\n",
      "Epoch 148/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.6461 — ite: 3.9104  — ate: 0.7773 — pehe: 3.9104 \n",
      " — ite: 4.9734  — ate: 0.3610 — pehe: 4.9734 \n",
      "6/6 [==============================] - 1s 136ms/step - loss: 21.0559 - val_loss: 20.8993 - val_y0: -0.1546 - val_y1: 1.1414 - val_ate_after_scaled: 1.2960 - lr: 3.1250e-07\n",
      "Epoch 149/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.6609 — ite: 4.1884  — ate: 0.8733 — pehe: 4.1884 \n",
      " — ite: 4.6909  — ate: 0.3905 — pehe: 4.6909 \n",
      "6/6 [==============================] - 1s 133ms/step - loss: 20.6514 - val_loss: 20.8947 - val_y0: -0.2538 - val_y1: 1.1763 - val_ate_after_scaled: 1.4301 - lr: 3.1250e-07\n",
      "Epoch 150/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.7907 — ite: 4.0466  — ate: 0.5896 — pehe: 4.0466 \n",
      " — ite: 5.0972  — ate: 0.0130 — pehe: 5.0972 \n",
      "6/6 [==============================] - 1s 133ms/step - loss: 20.7066 - val_loss: 21.1929 - val_y0: -0.1823 - val_y1: 1.2294 - val_ate_after_scaled: 1.4118 - lr: 3.1250e-07\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.8809 — ite: 4.0034  — ate: 0.7287 — pehe: 4.0034 \n",
      " — ite: 4.5166  — ate: 0.4995 — pehe: 4.5166 \n",
      "6/6 [==============================] - 1s 134ms/step - loss: 20.9981 - val_loss: 21.3527 - val_y0: -0.2321 - val_y1: 1.1770 - val_ate_after_scaled: 1.4092 - lr: 3.1250e-07\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.8894 — ite: 4.2363  — ate: 0.6456 — pehe: 4.2363 \n",
      " — ite: 4.6428  — ate: 0.0436 — pehe: 4.6428 \n",
      "6/6 [==============================] - 1s 127ms/step - loss: 20.8889 - val_loss: 21.8278 - val_y0: -0.2075 - val_y1: 1.3433 - val_ate_after_scaled: 1.5508 - lr: 3.1250e-07\n",
      "Epoch 153/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.6465 — ite: 4.1034  — ate: 0.7835 — pehe: 4.1034 \n",
      " — ite: 4.6836  — ate: 0.2238 — pehe: 4.6836 \n",
      "6/6 [==============================] - 1s 123ms/step - loss: 20.5116 - val_loss: 21.2699 - val_y0: -0.2883 - val_y1: 1.0810 - val_ate_after_scaled: 1.3693 - lr: 3.1250e-07\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 21.0053\n",
      "Epoch 00154: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      " — ite: 4.0665  — ate: 0.6860 — pehe: 4.0665 \n",
      " — ite: 4.4289  — ate: 0.1038 — pehe: 4.4289 \n",
      "6/6 [==============================] - 1s 143ms/step - loss: 21.1074 - val_loss: 21.1055 - val_y0: -0.2772 - val_y1: 1.1592 - val_ate_after_scaled: 1.4363 - lr: 3.1250e-07\n",
      "Epoch 155/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.8761 — ite: 4.1791  — ate: 0.7188 — pehe: 4.1791 \n",
      " — ite: 5.1539  — ate: 0.3492 — pehe: 5.1539 \n",
      "6/6 [==============================] - 1s 139ms/step - loss: 20.9342 - val_loss: 21.2114 - val_y0: 0.0080 - val_y1: 1.1766 - val_ate_after_scaled: 1.1686 - lr: 1.5625e-07\n",
      "Epoch 156/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.6388 — ite: 4.2103  — ate: 0.6810 — pehe: 4.2103 \n",
      " — ite: 4.3222  — ate: 0.0189 — pehe: 4.3222 \n",
      "6/6 [==============================] - 1s 130ms/step - loss: 20.6687 - val_loss: 20.9579 - val_y0: -0.1535 - val_y1: 0.9930 - val_ate_after_scaled: 1.1465 - lr: 1.5625e-07\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.7229 — ite: 4.2228  — ate: 0.7343 — pehe: 4.2228 \n",
      " — ite: 4.7252  — ate: 0.1228 — pehe: 4.7252 \n",
      "6/6 [==============================] - 1s 135ms/step - loss: 20.6880 - val_loss: 21.1117 - val_y0: -0.1902 - val_y1: 1.2333 - val_ate_after_scaled: 1.4235 - lr: 1.5625e-07\n",
      "Epoch 158/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.6732 — ite: 4.0904  — ate: 0.7454 — pehe: 4.0904 \n",
      " — ite: 4.5843  — ate: 0.1058 — pehe: 4.5843 \n",
      "6/6 [==============================] - 1s 137ms/step - loss: 20.5138 - val_loss: 21.3899 - val_y0: -0.2257 - val_y1: 1.0768 - val_ate_after_scaled: 1.3025 - lr: 1.5625e-07\n",
      "Epoch 159/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.7968\n",
      "Epoch 00159: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-08.\n",
      " — ite: 4.1796  — ate: 0.7324 — pehe: 4.1796 \n",
      " — ite: 4.4497  — ate: 0.1024 — pehe: 4.4497 \n",
      "6/6 [==============================] - 1s 146ms/step - loss: 20.6971 - val_loss: 21.2685 - val_y0: -0.1793 - val_y1: 1.2435 - val_ate_after_scaled: 1.4229 - lr: 1.5625e-07\n",
      "Epoch 160/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.8884 — ite: 4.1532  — ate: 0.7634 — pehe: 4.1532 \n",
      " — ite: 4.7259  — ate: 0.6221 — pehe: 4.7259 \n",
      "6/6 [==============================] - 1s 138ms/step - loss: 20.7148 - val_loss: 21.2452 - val_y0: -0.2642 - val_y1: 1.2514 - val_ate_after_scaled: 1.5156 - lr: 7.8125e-08\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.7973 — ite: 4.2379  — ate: 0.8454 — pehe: 4.2379 \n",
      " — ite: 4.9211  — ate: 0.3672 — pehe: 4.9211 \n",
      "6/6 [==============================] - 1s 141ms/step - loss: 20.7647 - val_loss: 21.3339 - val_y0: -0.2313 - val_y1: 1.0719 - val_ate_after_scaled: 1.3032 - lr: 7.8125e-08\n",
      "Epoch 162/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.7994 — ite: 4.2970  — ate: 0.8687 — pehe: 4.2970 \n",
      " — ite: 4.4246  — ate: 0.2732 — pehe: 4.4246 \n",
      "6/6 [==============================] - 1s 128ms/step - loss: 20.9041 - val_loss: 21.0735 - val_y0: -0.1101 - val_y1: 1.0838 - val_ate_after_scaled: 1.1940 - lr: 7.8125e-08\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.6158 — ite: 4.1824  — ate: 0.7760 — pehe: 4.1824 \n",
      " — ite: 4.6554  — ate: 0.7982 — pehe: 4.6554 \n",
      "6/6 [==============================] - 1s 138ms/step - loss: 20.6284 - val_loss: 21.4509 - val_y0: -0.2421 - val_y1: 1.1610 - val_ate_after_scaled: 1.4031 - lr: 7.8125e-08\n",
      "Epoch 164/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.8269\n",
      "Epoch 00164: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-08.\n",
      " — ite: 4.1953  — ate: 0.7488 — pehe: 4.1953 \n",
      " — ite: 4.8976  — ate: 0.1763 — pehe: 4.8976 \n",
      "6/6 [==============================] - 1s 143ms/step - loss: 20.7828 - val_loss: 21.2178 - val_y0: -0.1117 - val_y1: 1.0130 - val_ate_after_scaled: 1.1247 - lr: 7.8125e-08\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.7847 — ite: 4.0155  — ate: 0.8362 — pehe: 4.0155 \n",
      " — ite: 5.0279  — ate: 0.5172 — pehe: 5.0279 \n",
      "6/6 [==============================] - 1s 135ms/step - loss: 20.8808 - val_loss: 21.2068 - val_y0: -0.2836 - val_y1: 1.2122 - val_ate_after_scaled: 1.4958 - lr: 3.9062e-08\n",
      "Epoch 166/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.8241 — ite: 4.0692  — ate: 0.7820 — pehe: 4.0692 \n",
      " — ite: 4.7824  — ate: 0.4921 — pehe: 4.7824 \n",
      "6/6 [==============================] - 1s 144ms/step - loss: 20.6481 - val_loss: 21.0121 - val_y0: -0.2619 - val_y1: 1.2769 - val_ate_after_scaled: 1.5388 - lr: 3.9062e-08\n",
      "Epoch 167/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.9274 — ite: 4.1092  — ate: 0.5080 — pehe: 4.1092 \n",
      " — ite: 4.9182  — ate: 0.2307 — pehe: 4.9182 \n",
      "6/6 [==============================] - 1s 129ms/step - loss: 20.5467 - val_loss: 21.3351 - val_y0: -0.4058 - val_y1: 1.2275 - val_ate_after_scaled: 1.6333 - lr: 3.9062e-08\n",
      "Epoch 168/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.6217 — ite: 4.1010  — ate: 0.8133 — pehe: 4.1010 \n",
      " — ite: 4.5866  — ate: 0.1243 — pehe: 4.5866 \n",
      "6/6 [==============================] - 1s 142ms/step - loss: 20.7542 - val_loss: 21.2648 - val_y0: -0.1639 - val_y1: 1.2959 - val_ate_after_scaled: 1.4598 - lr: 3.9062e-08\n",
      "Epoch 169/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.8478\n",
      "Epoch 00169: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-08.\n",
      " — ite: 4.1881  — ate: 0.7665 — pehe: 4.1881 \n",
      " — ite: 4.8455  — ate: 0.0524 — pehe: 4.8455 \n",
      "6/6 [==============================] - 1s 134ms/step - loss: 20.8190 - val_loss: 21.1277 - val_y0: -0.1297 - val_y1: 1.0046 - val_ate_after_scaled: 1.1343 - lr: 3.9062e-08\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.6930 — ite: 4.1456  — ate: 0.6233 — pehe: 4.1456 \n",
      " — ite: 5.0599  — ate: 0.6900 — pehe: 5.0599 \n",
      "6/6 [==============================] - 1s 133ms/step - loss: 20.6972 - val_loss: 21.3055 - val_y0: -0.0225 - val_y1: 1.2146 - val_ate_after_scaled: 1.2371 - lr: 1.9531e-08\n",
      "Epoch 171/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.7744 — ite: 4.2428  — ate: 0.7995 — pehe: 4.2428 \n",
      " — ite: 4.5120  — ate: 0.0974 — pehe: 4.5120 \n",
      "6/6 [==============================] - 1s 135ms/step - loss: 20.7082 - val_loss: 20.8844 - val_y0: -0.2193 - val_y1: 1.2460 - val_ate_after_scaled: 1.4653 - lr: 1.9531e-08\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.7373 — ite: 4.1845  — ate: 0.9439 — pehe: 4.1845 \n",
      " — ite: 4.7130  — ate: 0.2674 — pehe: 4.7130 \n",
      "6/6 [==============================] - 1s 136ms/step - loss: 20.7394 - val_loss: 20.9971 - val_y0: -0.3189 - val_y1: 1.3013 - val_ate_after_scaled: 1.6201 - lr: 1.9531e-08\n",
      "Epoch 173/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.6752 — ite: 4.0726  — ate: 0.8679 — pehe: 4.0726 \n",
      " — ite: 5.0128  — ate: 0.1461 — pehe: 5.0128 \n",
      "6/6 [==============================] - 1s 130ms/step - loss: 20.8312 - val_loss: 21.0930 - val_y0: -0.2651 - val_y1: 1.0239 - val_ate_after_scaled: 1.2889 - lr: 1.9531e-08\n",
      "Epoch 174/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.6540 — ite: 4.3134  — ate: 0.6840 — pehe: 4.3134 \n",
      " — ite: 4.9596  — ate: 0.0620 — pehe: 4.9596 \n",
      "6/6 [==============================] - 1s 134ms/step - loss: 20.9987 - val_loss: 21.2004 - val_y0: -0.1384 - val_y1: 1.2694 - val_ate_after_scaled: 1.4078 - lr: 1.9531e-08\n",
      "Epoch 175/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.7238 — ite: 4.2113  — ate: 0.8028 — pehe: 4.2113 \n",
      " — ite: 5.0117  — ate: 0.3272 — pehe: 5.0117 \n",
      "6/6 [==============================] - 1s 132ms/step - loss: 20.6933 - val_loss: 21.4206 - val_y0: -0.1597 - val_y1: 1.3375 - val_ate_after_scaled: 1.4972 - lr: 1.9531e-08\n",
      "Epoch 176/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.5986\n",
      "Epoch 00176: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-09.\n",
      " — ite: 4.1441  — ate: 0.7029 — pehe: 4.1441 \n",
      " — ite: 5.0883  — ate: 0.0027 — pehe: 5.0883 \n",
      "6/6 [==============================] - 1s 150ms/step - loss: 20.5429 - val_loss: 21.7623 - val_y0: -0.1954 - val_y1: 1.1615 - val_ate_after_scaled: 1.3569 - lr: 1.9531e-08\n",
      "Epoch 177/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.7933 — ite: 4.1977  — ate: 0.7458 — pehe: 4.1977 \n",
      " — ite: 4.8043  — ate: 0.3135 — pehe: 4.8043 \n",
      "6/6 [==============================] - 1s 141ms/step - loss: 20.5771 - val_loss: 21.2945 - val_y0: -0.2393 - val_y1: 1.2295 - val_ate_after_scaled: 1.4688 - lr: 9.7656e-09\n",
      "Epoch 178/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.7721 — ite: 4.3410  — ate: 0.6794 — pehe: 4.3410 \n",
      " — ite: 5.0455  — ate: 0.2555 — pehe: 5.0455 \n",
      "6/6 [==============================] - 1s 137ms/step - loss: 20.8676 - val_loss: 21.7122 - val_y0: -0.4352 - val_y1: 1.2505 - val_ate_after_scaled: 1.6857 - lr: 9.7656e-09\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.6311 — ite: 4.0661  — ate: 0.8393 — pehe: 4.0661 \n",
      " — ite: 5.1102  — ate: 0.1770 — pehe: 5.1102 \n",
      "6/6 [==============================] - 1s 139ms/step - loss: 20.5733 - val_loss: 21.1134 - val_y0: -0.1846 - val_y1: 1.2773 - val_ate_after_scaled: 1.4619 - lr: 9.7656e-09\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.9995 — ite: 4.1520  — ate: 0.7969 — pehe: 4.1520 \n",
      " — ite: 4.7008  — ate: 0.1502 — pehe: 4.7008 \n",
      "6/6 [==============================] - 1s 143ms/step - loss: 21.1308 - val_loss: 21.1773 - val_y0: -0.1134 - val_y1: 1.1949 - val_ate_after_scaled: 1.3083 - lr: 9.7656e-09\n",
      "Epoch 181/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.7258\n",
      "Epoch 00181: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-09.\n",
      " — ite: 4.2200  — ate: 0.7033 — pehe: 4.2200 \n",
      " — ite: 4.4330  — ate: 0.6313 — pehe: 4.4330 \n",
      "6/6 [==============================] - 1s 149ms/step - loss: 20.6752 - val_loss: 21.3292 - val_y0: -0.3106 - val_y1: 1.2575 - val_ate_after_scaled: 1.5681 - lr: 9.7656e-09\n",
      "Epoch 182/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.7165 — ite: 4.1504  — ate: 0.6400 — pehe: 4.1504 \n",
      " — ite: 5.0257  — ate: 0.0388 — pehe: 5.0257 \n",
      "6/6 [==============================] - 1s 126ms/step - loss: 20.6566 - val_loss: 21.1423 - val_y0: -0.2188 - val_y1: 1.1632 - val_ate_after_scaled: 1.3820 - lr: 4.8828e-09\n",
      "Epoch 183/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.7197 — ite: 4.1225  — ate: 0.5874 — pehe: 4.1225 \n",
      " — ite: 4.5481  — ate: 0.3604 — pehe: 4.5481 \n",
      "6/6 [==============================] - 1s 130ms/step - loss: 20.8880 - val_loss: 20.8511 - val_y0: 0.0361 - val_y1: 1.1810 - val_ate_after_scaled: 1.1449 - lr: 4.8828e-09\n",
      "Epoch 184/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.6692 — ite: 4.0808  — ate: 0.8149 — pehe: 4.0808 \n",
      " — ite: 5.1692  — ate: 0.0969 — pehe: 5.1692 \n",
      "6/6 [==============================] - 1s 140ms/step - loss: 20.5910 - val_loss: 21.5075 - val_y0: -0.1085 - val_y1: 1.1810 - val_ate_after_scaled: 1.2895 - lr: 4.8828e-09\n",
      "Epoch 185/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.7931 — ite: 4.1807  — ate: 0.7748 — pehe: 4.1807 \n",
      " — ite: 4.6409  — ate: 0.0766 — pehe: 4.6409 \n",
      "6/6 [==============================] - 1s 147ms/step - loss: 20.6464 - val_loss: 21.7922 - val_y0: -0.2634 - val_y1: 1.0898 - val_ate_after_scaled: 1.3532 - lr: 4.8828e-09\n",
      "Epoch 186/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.5353 — ite: 4.1533  — ate: 0.9357 — pehe: 4.1533 \n",
      " — ite: 4.8040  — ate: 0.1519 — pehe: 4.8040 \n",
      "6/6 [==============================] - 1s 151ms/step - loss: 20.6767 - val_loss: 21.2520 - val_y0: -0.2210 - val_y1: 1.0679 - val_ate_after_scaled: 1.2889 - lr: 4.8828e-09\n",
      "Epoch 187/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.6012 — ite: 4.0371  — ate: 0.5829 — pehe: 4.0371 \n",
      " — ite: 4.1668  — ate: 0.3687 — pehe: 4.1668 \n",
      "6/6 [==============================] - 1s 152ms/step - loss: 20.5139 - val_loss: 21.0541 - val_y0: -0.1471 - val_y1: 1.0950 - val_ate_after_scaled: 1.2421 - lr: 4.8828e-09\n",
      "Epoch 188/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.8342\n",
      "Epoch 00188: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-09.\n",
      " — ite: 4.0694  — ate: 0.7460 — pehe: 4.0694 \n",
      " — ite: 5.2950  — ate: 0.0742 — pehe: 5.2950 \n",
      "6/6 [==============================] - 1s 155ms/step - loss: 20.7895 - val_loss: 21.1504 - val_y0: -0.2855 - val_y1: 1.2904 - val_ate_after_scaled: 1.5759 - lr: 4.8828e-09\n",
      "Epoch 189/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.6604 — ite: 4.1876  — ate: 0.8649 — pehe: 4.1876 \n",
      " — ite: 5.0528  — ate: 0.3781 — pehe: 5.0528 \n",
      "6/6 [==============================] - 1s 143ms/step - loss: 20.6260 - val_loss: 20.8185 - val_y0: -0.1643 - val_y1: 1.1850 - val_ate_after_scaled: 1.3493 - lr: 2.4414e-09\n",
      "Epoch 190/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.7775 — ite: 4.0574  — ate: 0.6095 — pehe: 4.0574 \n",
      " — ite: 4.9325  — ate: 0.3201 — pehe: 4.9325 \n",
      "6/6 [==============================] - 1s 147ms/step - loss: 21.0710 - val_loss: 20.7791 - val_y0: -0.3039 - val_y1: 1.2279 - val_ate_after_scaled: 1.5318 - lr: 2.4414e-09\n",
      "Epoch 191/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.7425 — ite: 4.1778  — ate: 0.8650 — pehe: 4.1778 \n",
      " — ite: 5.1047  — ate: 0.7129 — pehe: 5.1047 \n",
      "6/6 [==============================] - 1s 140ms/step - loss: 21.0526 - val_loss: 21.2945 - val_y0: -0.1591 - val_y1: 1.2503 - val_ate_after_scaled: 1.4094 - lr: 2.4414e-09\n",
      "Epoch 192/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.6178 — ite: 3.9858  — ate: 0.6006 — pehe: 3.9858 \n",
      " — ite: 4.9526  — ate: 0.3757 — pehe: 4.9526 \n",
      "6/6 [==============================] - 1s 141ms/step - loss: 20.6025 - val_loss: 21.7400 - val_y0: -0.2465 - val_y1: 1.0164 - val_ate_after_scaled: 1.2629 - lr: 2.4414e-09\n",
      "Epoch 193/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.8221 — ite: 4.1425  — ate: 0.8187 — pehe: 4.1425 \n",
      " — ite: 4.5804  — ate: 0.2182 — pehe: 4.5804 \n",
      "6/6 [==============================] - 1s 134ms/step - loss: 20.5359 - val_loss: 21.1122 - val_y0: -0.2959 - val_y1: 1.1438 - val_ate_after_scaled: 1.4396 - lr: 2.4414e-09\n",
      "Epoch 194/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.7767 — ite: 4.2238  — ate: 0.7808 — pehe: 4.2238 \n",
      " — ite: 4.7048  — ate: 0.2040 — pehe: 4.7048 \n",
      "6/6 [==============================] - 1s 130ms/step - loss: 20.6740 - val_loss: 21.0845 - val_y0: -0.1563 - val_y1: 1.1058 - val_ate_after_scaled: 1.2621 - lr: 2.4414e-09\n",
      "Epoch 195/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.7957\n",
      "Epoch 00195: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-09.\n",
      " — ite: 4.1538  — ate: 0.6887 — pehe: 4.1538 \n",
      " — ite: 4.8315  — ate: 0.5866 — pehe: 4.8315 \n",
      "6/6 [==============================] - 1s 129ms/step - loss: 20.8164 - val_loss: 20.8958 - val_y0: -0.2808 - val_y1: 1.3352 - val_ate_after_scaled: 1.6160 - lr: 2.4414e-09\n",
      "Epoch 196/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.7773 — ite: 4.2498  — ate: 0.7140 — pehe: 4.2498 \n",
      " — ite: 5.1858  — ate: 0.0231 — pehe: 5.1858 \n",
      "6/6 [==============================] - 1s 125ms/step - loss: 20.6179 - val_loss: 20.8332 - val_y0: -0.1272 - val_y1: 1.2690 - val_ate_after_scaled: 1.3962 - lr: 1.2207e-09\n",
      "Epoch 197/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.7479 — ite: 4.0101  — ate: 0.6290 — pehe: 4.0101 \n",
      " — ite: 4.6960  — ate: 0.5775 — pehe: 4.6960 \n",
      "6/6 [==============================] - 1s 127ms/step - loss: 20.7380 - val_loss: 21.0581 - val_y0: -0.2528 - val_y1: 1.1885 - val_ate_after_scaled: 1.4413 - lr: 1.2207e-09\n",
      "Epoch 198/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.6546 — ite: 4.1915  — ate: 0.8063 — pehe: 4.1915 \n",
      " — ite: 4.8224  — ate: 0.3373 — pehe: 4.8224 \n",
      "6/6 [==============================] - 1s 123ms/step - loss: 20.7231 - val_loss: 21.4129 - val_y0: -0.2137 - val_y1: 1.2342 - val_ate_after_scaled: 1.4479 - lr: 1.2207e-09\n",
      "Epoch 199/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.8709 — ite: 4.1755  — ate: 0.6202 — pehe: 4.1755 \n",
      " — ite: 4.9111  — ate: 0.1429 — pehe: 4.9111 \n",
      "6/6 [==============================] - 1s 126ms/step - loss: 20.9666 - val_loss: 21.5905 - val_y0: -0.3015 - val_y1: 1.1397 - val_ate_after_scaled: 1.4412 - lr: 1.2207e-09\n",
      "Epoch 200/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.7430\n",
      "Epoch 00200: ReduceLROnPlateau reducing learning rate to 6.103515470812226e-10.\n",
      " — ite: 4.0885  — ate: 0.8353 — pehe: 4.0885 \n",
      " — ite: 4.8510  — ate: 0.0348 — pehe: 4.8510 \n",
      "6/6 [==============================] - 1s 136ms/step - loss: 20.7150 - val_loss: 21.1560 - val_y0: -0.2240 - val_y1: 1.0946 - val_ate_after_scaled: 1.3186 - lr: 1.2207e-09\n",
      "Epoch 201/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.7339 — ite: 3.9887  — ate: 0.7509 — pehe: 3.9887 \n",
      " — ite: 4.8584  — ate: 0.0804 — pehe: 4.8584 \n",
      "6/6 [==============================] - 1s 136ms/step - loss: 20.6031 - val_loss: 21.3829 - val_y0: -0.2311 - val_y1: 1.0899 - val_ate_after_scaled: 1.3210 - lr: 6.1035e-10\n",
      "Epoch 202/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.5666 — ite: 3.9337  — ate: 0.7027 — pehe: 3.9337 \n",
      " — ite: 4.7024  — ate: 0.7607 — pehe: 4.7024 \n",
      "6/6 [==============================] - 1s 144ms/step - loss: 20.7042 - val_loss: 21.3783 - val_y0: -0.1132 - val_y1: 1.1682 - val_ate_after_scaled: 1.2814 - lr: 6.1035e-10\n",
      "Epoch 203/300\n",
      "3/6 [==============>...............] - ETA: 0s - loss: 20.6395 — ite: 4.0898  — ate: 0.6417 — pehe: 4.0898 \n",
      " — ite: 5.1218  — ate: 0.5764 — pehe: 5.1218 \n",
      "6/6 [==============================] - 1s 140ms/step - loss: 20.7113 - val_loss: 21.1697 - val_y0: -0.0244 - val_y1: 1.2115 - val_ate_after_scaled: 1.2360 - lr: 6.1035e-10\n",
      "Epoch 204/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.7012 — ite: 4.0458  — ate: 0.8282 — pehe: 4.0458 \n",
      " — ite: 4.8742  — ate: 0.0625 — pehe: 4.8742 \n",
      "6/6 [==============================] - 1s 146ms/step - loss: 20.6369 - val_loss: 21.5098 - val_y0: -0.2145 - val_y1: 1.2450 - val_ate_after_scaled: 1.4595 - lr: 6.1035e-10\n",
      "Epoch 205/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.7235\n",
      "Epoch 00205: ReduceLROnPlateau reducing learning rate to 3.051757735406113e-10.\n",
      " — ite: 4.1728  — ate: 0.8373 — pehe: 4.1728 \n",
      " — ite: 5.0759  — ate: 0.2421 — pehe: 5.0759 \n",
      "6/6 [==============================] - 1s 145ms/step - loss: 21.0688 - val_loss: 21.2280 - val_y0: -0.1067 - val_y1: 1.2117 - val_ate_after_scaled: 1.3183 - lr: 6.1035e-10\n",
      "Epoch 206/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.7404 — ite: 4.1165  — ate: 0.7861 — pehe: 4.1165 \n",
      " — ite: 5.0585  — ate: 0.1792 — pehe: 5.0585 \n",
      "6/6 [==============================] - 1s 142ms/step - loss: 20.7770 - val_loss: 21.2777 - val_y0: -0.2001 - val_y1: 1.1561 - val_ate_after_scaled: 1.3562 - lr: 3.0518e-10\n",
      "Epoch 207/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.8223 — ite: 4.0909  — ate: 0.9496 — pehe: 4.0909 \n",
      " — ite: 4.4355  — ate: 0.2243 — pehe: 4.4355 \n",
      "6/6 [==============================] - 1s 142ms/step - loss: 20.8519 - val_loss: 21.1854 - val_y0: -0.1214 - val_y1: 1.1185 - val_ate_after_scaled: 1.2399 - lr: 3.0518e-10\n",
      "Epoch 208/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.8093 — ite: 4.0858  — ate: 0.7189 — pehe: 4.0858 \n",
      " — ite: 4.8167  — ate: 0.2316 — pehe: 4.8167 \n",
      "6/6 [==============================] - 1s 143ms/step - loss: 20.7879 - val_loss: 20.9712 - val_y0: -0.2401 - val_y1: 1.1963 - val_ate_after_scaled: 1.4364 - lr: 3.0518e-10\n",
      "Epoch 209/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.8181 — ite: 4.0828  — ate: 0.7203 — pehe: 4.0828 \n",
      " — ite: 4.6567  — ate: 0.1372 — pehe: 4.6567 \n",
      "6/6 [==============================] - 1s 157ms/step - loss: 20.7730 - val_loss: 21.3678 - val_y0: -0.1094 - val_y1: 1.1240 - val_ate_after_scaled: 1.2333 - lr: 3.0518e-10\n",
      "Epoch 210/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.6892\n",
      "Epoch 00210: ReduceLROnPlateau reducing learning rate to 1.5258788677030566e-10.\n",
      " — ite: 4.1044  — ate: 0.7939 — pehe: 4.1044 \n",
      " — ite: 4.7468  — ate: 0.2609 — pehe: 4.7468 \n",
      "6/6 [==============================] - 1s 138ms/step - loss: 20.7861 - val_loss: 21.3133 - val_y0: -0.1774 - val_y1: 1.1698 - val_ate_after_scaled: 1.3473 - lr: 3.0518e-10\n",
      "Epoch 211/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.7836 — ite: 4.1540  — ate: 0.7755 — pehe: 4.1540 \n",
      " — ite: 5.1634  — ate: 0.2612 — pehe: 5.1634 \n",
      "6/6 [==============================] - 1s 145ms/step - loss: 20.7251 - val_loss: 21.2287 - val_y0: -0.2746 - val_y1: 1.1778 - val_ate_after_scaled: 1.4524 - lr: 1.5259e-10\n",
      "Epoch 212/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.7493 — ite: 4.1468  — ate: 0.9286 — pehe: 4.1468 \n",
      " — ite: 5.1344  — ate: 0.0137 — pehe: 5.1344 \n",
      "6/6 [==============================] - 1s 139ms/step - loss: 20.7411 - val_loss: 21.2244 - val_y0: -0.2287 - val_y1: 1.1918 - val_ate_after_scaled: 1.4204 - lr: 1.5259e-10\n",
      "Epoch 213/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.5279 — ite: 4.2447  — ate: 0.9093 — pehe: 4.2447 \n",
      " — ite: 5.0250  — ate: 0.1349 — pehe: 5.0250 \n",
      "6/6 [==============================] - 1s 138ms/step - loss: 20.5554 - val_loss: 21.1610 - val_y0: -0.1926 - val_y1: 1.2662 - val_ate_after_scaled: 1.4588 - lr: 1.5259e-10\n",
      "Epoch 214/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.7747 — ite: 4.1070  — ate: 0.8704 — pehe: 4.1070 \n",
      " — ite: 4.9177  — ate: 0.4136 — pehe: 4.9177 \n",
      "6/6 [==============================] - 1s 138ms/step - loss: 20.8666 - val_loss: 20.8735 - val_y0: -0.1475 - val_y1: 1.3390 - val_ate_after_scaled: 1.4865 - lr: 1.5259e-10\n",
      "Epoch 215/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.7557\n",
      "Epoch 00215: ReduceLROnPlateau reducing learning rate to 7.629394338515283e-11.\n",
      " — ite: 4.0522  — ate: 0.7840 — pehe: 4.0522 \n",
      " — ite: 5.2460  — ate: 0.2972 — pehe: 5.2460 \n",
      "6/6 [==============================] - 1s 145ms/step - loss: 20.6299 - val_loss: 20.8135 - val_y0: -0.2393 - val_y1: 1.2922 - val_ate_after_scaled: 1.5314 - lr: 1.5259e-10\n",
      "Epoch 216/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.7476 — ite: 4.0878  — ate: 0.8391 — pehe: 4.0878 \n",
      " — ite: 4.7863  — ate: 0.1937 — pehe: 4.7863 \n",
      "6/6 [==============================] - 1s 139ms/step - loss: 20.7676 - val_loss: 21.6056 - val_y0: -0.2134 - val_y1: 1.3139 - val_ate_after_scaled: 1.5273 - lr: 7.6294e-11\n",
      "Epoch 217/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.7083 — ite: 4.1634  — ate: 0.9113 — pehe: 4.1634 \n",
      " — ite: 4.7046  — ate: 0.1263 — pehe: 4.7046 \n",
      "6/6 [==============================] - 1s 135ms/step - loss: 20.6561 - val_loss: 21.0756 - val_y0: -0.2040 - val_y1: 0.9635 - val_ate_after_scaled: 1.1675 - lr: 7.6294e-11\n",
      "Epoch 218/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.7030 — ite: 4.0822  — ate: 0.6764 — pehe: 4.0822 \n",
      " — ite: 4.5897  — ate: 0.3956 — pehe: 4.5897 \n",
      "6/6 [==============================] - 1s 138ms/step - loss: 20.9334 - val_loss: 21.0235 - val_y0: -0.3255 - val_y1: 1.3551 - val_ate_after_scaled: 1.6806 - lr: 7.6294e-11\n",
      "Epoch 219/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.7762 — ite: 4.2077  — ate: 0.8459 — pehe: 4.2077 \n",
      " — ite: 4.7994  — ate: 0.6212 — pehe: 4.7994 \n",
      "6/6 [==============================] - 1s 135ms/step - loss: 20.7862 - val_loss: 21.4586 - val_y0: -0.2864 - val_y1: 1.1475 - val_ate_after_scaled: 1.4338 - lr: 7.6294e-11\n",
      "Epoch 220/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.7800\n",
      "Epoch 00220: ReduceLROnPlateau reducing learning rate to 3.8146971692576415e-11.\n",
      " — ite: 4.0879  — ate: 0.7715 — pehe: 4.0879 \n",
      " — ite: 5.5834  — ate: 0.3550 — pehe: 5.5834 \n",
      "6/6 [==============================] - 1s 137ms/step - loss: 20.8541 - val_loss: 21.1734 - val_y0: -0.2471 - val_y1: 1.1656 - val_ate_after_scaled: 1.4126 - lr: 7.6294e-11\n",
      "Epoch 221/300\n",
      "3/6 [==============>...............] - ETA: 0s - loss: 20.8372 — ite: 4.2204  — ate: 0.8980 — pehe: 4.2204 \n",
      " — ite: 5.3211  — ate: 0.5524 — pehe: 5.3211 \n",
      "6/6 [==============================] - 1s 140ms/step - loss: 20.5829 - val_loss: 21.3425 - val_y0: -0.2705 - val_y1: 1.1626 - val_ate_after_scaled: 1.4331 - lr: 3.8147e-11\n",
      "Epoch 222/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.7107 — ite: 3.9790  — ate: 0.6404 — pehe: 3.9790 \n",
      " — ite: 4.4703  — ate: 0.3265 — pehe: 4.4703 \n",
      "6/6 [==============================] - 1s 152ms/step - loss: 20.6422 - val_loss: 21.1980 - val_y0: -0.1869 - val_y1: 1.3029 - val_ate_after_scaled: 1.4898 - lr: 3.8147e-11\n",
      "Epoch 223/300\n",
      "3/6 [==============>...............] - ETA: 0s - loss: 20.8504 — ite: 4.2007  — ate: 0.7462 — pehe: 4.2007 \n",
      " — ite: 5.1821  — ate: 0.5260 — pehe: 5.1821 \n",
      "6/6 [==============================] - 1s 133ms/step - loss: 20.7178 - val_loss: 21.3912 - val_y0: -0.1547 - val_y1: 1.3224 - val_ate_after_scaled: 1.4771 - lr: 3.8147e-11\n",
      "Epoch 224/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.7107 — ite: 4.0615  — ate: 0.6929 — pehe: 4.0615 \n",
      " — ite: 4.6928  — ate: 0.1406 — pehe: 4.6928 \n",
      "6/6 [==============================] - 1s 145ms/step - loss: 20.7180 - val_loss: 21.2645 - val_y0: -0.0146 - val_y1: 1.2497 - val_ate_after_scaled: 1.2643 - lr: 3.8147e-11\n",
      "Epoch 225/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.6948\n",
      "Epoch 00225: ReduceLROnPlateau reducing learning rate to 1.9073485846288207e-11.\n",
      " — ite: 4.1763  — ate: 0.7065 — pehe: 4.1763 \n",
      " — ite: 4.9953  — ate: 0.1503 — pehe: 4.9953 \n",
      "6/6 [==============================] - 1s 156ms/step - loss: 20.6168 - val_loss: 21.3568 - val_y0: -0.2736 - val_y1: 1.2592 - val_ate_after_scaled: 1.5328 - lr: 3.8147e-11\n",
      "Epoch 226/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.8864 — ite: 3.9475  — ate: 0.8723 — pehe: 3.9475 \n",
      " — ite: 4.8278  — ate: 0.8136 — pehe: 4.8278 \n",
      "6/6 [==============================] - 1s 141ms/step - loss: 20.4479 - val_loss: 20.7382 - val_y0: -0.1784 - val_y1: 1.1606 - val_ate_after_scaled: 1.3390 - lr: 1.9073e-11\n",
      "Epoch 227/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.7966 — ite: 4.0962  — ate: 0.6171 — pehe: 4.0962 \n",
      " — ite: 4.5691  — ate: 0.1955 — pehe: 4.5691 \n",
      "6/6 [==============================] - 1s 139ms/step - loss: 20.5396 - val_loss: 21.4557 - val_y0: -0.2196 - val_y1: 1.2564 - val_ate_after_scaled: 1.4759 - lr: 1.9073e-11\n",
      "Epoch 228/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.7956 — ite: 4.1262  — ate: 0.6183 — pehe: 4.1262 \n",
      " — ite: 4.9877  — ate: 0.5565 — pehe: 4.9877 \n",
      "6/6 [==============================] - 1s 142ms/step - loss: 21.0541 - val_loss: 21.2327 - val_y0: -0.1555 - val_y1: 1.0995 - val_ate_after_scaled: 1.2550 - lr: 1.9073e-11\n",
      "Epoch 229/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.6630 — ite: 4.0457  — ate: 0.6641 — pehe: 4.0457 \n",
      " — ite: 4.9323  — ate: 0.6277 — pehe: 4.9323 \n",
      "6/6 [==============================] - 1s 141ms/step - loss: 20.5804 - val_loss: 21.0019 - val_y0: -0.1920 - val_y1: 1.0945 - val_ate_after_scaled: 1.2865 - lr: 1.9073e-11\n",
      "Epoch 230/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.8078 — ite: 4.2383  — ate: 0.7046 — pehe: 4.2383 \n",
      " — ite: 4.9026  — ate: 0.3595 — pehe: 4.9026 \n",
      "6/6 [==============================] - 1s 150ms/step - loss: 20.8587 - val_loss: 21.6254 - val_y0: -0.1075 - val_y1: 1.2188 - val_ate_after_scaled: 1.3264 - lr: 1.9073e-11\n",
      "Epoch 231/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.8379\n",
      "Epoch 00231: ReduceLROnPlateau reducing learning rate to 9.536742923144104e-12.\n",
      " — ite: 4.0589  — ate: 0.6461 — pehe: 4.0589 \n",
      " — ite: 4.9338  — ate: 1.1520 — pehe: 4.9338 \n",
      "6/6 [==============================] - 1s 154ms/step - loss: 20.9074 - val_loss: 20.7570 - val_y0: -0.1695 - val_y1: 1.1771 - val_ate_after_scaled: 1.3465 - lr: 1.9073e-11\n",
      "Epoch 232/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.7289 — ite: 3.9805  — ate: 0.6618 — pehe: 3.9805 \n",
      " — ite: 4.9776  — ate: 0.3517 — pehe: 4.9776 \n",
      "6/6 [==============================] - 1s 145ms/step - loss: 20.8574 - val_loss: 21.5271 - val_y0: -0.2615 - val_y1: 1.2098 - val_ate_after_scaled: 1.4713 - lr: 9.5367e-12\n",
      "Epoch 233/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.7668 — ite: 4.1252  — ate: 0.6221 — pehe: 4.1252 \n",
      " — ite: 4.6153  — ate: 0.2800 — pehe: 4.6153 \n",
      "6/6 [==============================] - 1s 144ms/step - loss: 20.8494 - val_loss: 21.1973 - val_y0: -0.2692 - val_y1: 1.1316 - val_ate_after_scaled: 1.4008 - lr: 9.5367e-12\n",
      "Epoch 234/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.7497 — ite: 4.1724  — ate: 0.7439 — pehe: 4.1724 \n",
      " — ite: 4.8167  — ate: 0.4164 — pehe: 4.8167 \n",
      "6/6 [==============================] - 1s 140ms/step - loss: 20.9114 - val_loss: 21.2622 - val_y0: -0.2873 - val_y1: 1.2523 - val_ate_after_scaled: 1.5396 - lr: 9.5367e-12\n",
      "Epoch 235/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.8590 — ite: 4.1418  — ate: 0.7764 — pehe: 4.1418 \n",
      " — ite: 4.7663  — ate: 0.0043 — pehe: 4.7663 \n",
      "6/6 [==============================] - 1s 140ms/step - loss: 20.7597 - val_loss: 21.1430 - val_y0: -0.3043 - val_y1: 1.1104 - val_ate_after_scaled: 1.4147 - lr: 9.5367e-12\n",
      "Epoch 236/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.7269\n",
      "Epoch 00236: ReduceLROnPlateau reducing learning rate to 4.768371461572052e-12.\n",
      " — ite: 4.1849  — ate: 0.6278 — pehe: 4.1849 \n",
      " — ite: 4.4390  — ate: 0.4120 — pehe: 4.4390 \n",
      "6/6 [==============================] - 1s 146ms/step - loss: 20.7900 - val_loss: 21.0342 - val_y0: -0.2795 - val_y1: 1.0224 - val_ate_after_scaled: 1.3019 - lr: 9.5367e-12\n",
      "Epoch 237/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 21.0311 — ite: 4.0883  — ate: 0.6058 — pehe: 4.0883 \n",
      " — ite: 4.5573  — ate: 0.6314 — pehe: 4.5573 \n",
      "6/6 [==============================] - 1s 144ms/step - loss: 20.6471 - val_loss: 21.6157 - val_y0: -0.2275 - val_y1: 1.2763 - val_ate_after_scaled: 1.5039 - lr: 4.7684e-12\n",
      "Epoch 238/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.6976 — ite: 4.1241  — ate: 0.7674 — pehe: 4.1241 \n",
      " — ite: 4.6335  — ate: 0.2769 — pehe: 4.6335 \n",
      "6/6 [==============================] - 1s 151ms/step - loss: 20.4746 - val_loss: 20.7751 - val_y0: -0.1682 - val_y1: 1.2235 - val_ate_after_scaled: 1.3917 - lr: 4.7684e-12\n",
      "Epoch 239/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.6143 — ite: 4.1341  — ate: 0.7446 — pehe: 4.1341 \n",
      " — ite: 4.5433  — ate: 0.1462 — pehe: 4.5433 \n",
      "6/6 [==============================] - 1s 150ms/step - loss: 20.7349 - val_loss: 20.9486 - val_y0: -0.3030 - val_y1: 1.2287 - val_ate_after_scaled: 1.5317 - lr: 4.7684e-12\n",
      "Epoch 240/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.6232 — ite: 4.1562  — ate: 0.7865 — pehe: 4.1562 \n",
      " — ite: 5.0283  — ate: 0.0283 — pehe: 5.0283 \n",
      "6/6 [==============================] - 1s 139ms/step - loss: 20.5707 - val_loss: 21.0641 - val_y0: -0.0901 - val_y1: 1.1586 - val_ate_after_scaled: 1.2488 - lr: 4.7684e-12\n",
      "Epoch 241/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.8065\n",
      "Epoch 00241: ReduceLROnPlateau reducing learning rate to 2.384185730786026e-12.\n",
      " — ite: 4.1912  — ate: 0.7083 — pehe: 4.1912 \n",
      " — ite: 4.7175  — ate: 0.2042 — pehe: 4.7175 \n",
      "6/6 [==============================] - 1s 148ms/step - loss: 20.7177 - val_loss: 21.1578 - val_y0: -0.2007 - val_y1: 1.1835 - val_ate_after_scaled: 1.3842 - lr: 4.7684e-12\n",
      "Epoch 242/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.7091 — ite: 3.9704  — ate: 0.6080 — pehe: 3.9704 \n",
      " — ite: 4.5255  — ate: 0.2432 — pehe: 4.5255 \n",
      "6/6 [==============================] - 1s 143ms/step - loss: 20.9068 - val_loss: 21.2216 - val_y0: -0.2055 - val_y1: 1.1707 - val_ate_after_scaled: 1.3762 - lr: 2.3842e-12\n",
      "Epoch 243/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.6803 — ite: 4.0810  — ate: 0.6516 — pehe: 4.0810 \n",
      " — ite: 5.0468  — ate: 0.1098 — pehe: 5.0468 \n",
      "6/6 [==============================] - 1s 148ms/step - loss: 20.5935 - val_loss: 21.1010 - val_y0: -0.3416 - val_y1: 1.3925 - val_ate_after_scaled: 1.7342 - lr: 2.3842e-12\n",
      "Epoch 244/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.7167 — ite: 4.2212  — ate: 0.7978 — pehe: 4.2212 \n",
      " — ite: 4.9171  — ate: 0.1904 — pehe: 4.9171 \n",
      "6/6 [==============================] - 1s 136ms/step - loss: 20.6757 - val_loss: 21.0244 - val_y0: -0.3599 - val_y1: 1.0513 - val_ate_after_scaled: 1.4112 - lr: 2.3842e-12\n",
      "Epoch 245/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.8333 — ite: 4.1600  — ate: 0.6222 — pehe: 4.1600 \n",
      " — ite: 5.0486  — ate: 0.2963 — pehe: 5.0486 \n",
      "6/6 [==============================] - 1s 147ms/step - loss: 20.8952 - val_loss: 21.3684 - val_y0: -0.2755 - val_y1: 1.3054 - val_ate_after_scaled: 1.5809 - lr: 2.3842e-12\n",
      "Epoch 246/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.6359\n",
      "Epoch 00246: ReduceLROnPlateau reducing learning rate to 1.192092865393013e-12.\n",
      " — ite: 4.0676  — ate: 0.6746 — pehe: 4.0676 \n",
      " — ite: 4.8699  — ate: 0.4269 — pehe: 4.8699 \n",
      "6/6 [==============================] - 1s 141ms/step - loss: 20.4763 - val_loss: 21.3068 - val_y0: -0.0863 - val_y1: 1.1709 - val_ate_after_scaled: 1.2571 - lr: 2.3842e-12\n",
      "Epoch 247/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.8311 — ite: 4.1770  — ate: 0.7057 — pehe: 4.1770 \n",
      " — ite: 4.7245  — ate: 0.0233 — pehe: 4.7245 \n",
      "6/6 [==============================] - 1s 141ms/step - loss: 20.5267 - val_loss: 21.3235 - val_y0: -0.0794 - val_y1: 1.0533 - val_ate_after_scaled: 1.1327 - lr: 1.1921e-12\n",
      "Epoch 248/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.9497 — ite: 3.9355  — ate: 0.6720 — pehe: 3.9355 \n",
      " — ite: 4.4252  — ate: 0.2772 — pehe: 4.4252 \n",
      "6/6 [==============================] - 1s 152ms/step - loss: 20.5095 - val_loss: 21.1483 - val_y0: -0.2965 - val_y1: 1.0630 - val_ate_after_scaled: 1.3595 - lr: 1.1921e-12\n",
      "Epoch 249/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.7925 — ite: 4.0810  — ate: 0.6450 — pehe: 4.0810 \n",
      " — ite: 4.7114  — ate: 0.2041 — pehe: 4.7114 \n",
      "6/6 [==============================] - 1s 138ms/step - loss: 20.9748 - val_loss: 21.4378 - val_y0: -0.2026 - val_y1: 1.1783 - val_ate_after_scaled: 1.3809 - lr: 1.1921e-12\n",
      "Epoch 250/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.5968 — ite: 4.2760  — ate: 0.7718 — pehe: 4.2760 \n",
      " — ite: 4.8519  — ate: 0.1807 — pehe: 4.8519 \n",
      "6/6 [==============================] - 1s 142ms/step - loss: 20.5535 - val_loss: 21.3036 - val_y0: -0.2184 - val_y1: 1.1407 - val_ate_after_scaled: 1.3591 - lr: 1.1921e-12\n",
      "Epoch 251/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.7649\n",
      "Epoch 00251: ReduceLROnPlateau reducing learning rate to 5.960464326965065e-13.\n",
      " — ite: 4.1057  — ate: 0.6125 — pehe: 4.1057 \n",
      " — ite: 4.4269  — ate: 0.3363 — pehe: 4.4269 \n",
      "6/6 [==============================] - 1s 149ms/step - loss: 20.5396 - val_loss: 21.2346 - val_y0: -0.2802 - val_y1: 1.1399 - val_ate_after_scaled: 1.4201 - lr: 1.1921e-12\n",
      "Epoch 252/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.7323 — ite: 4.1481  — ate: 0.6573 — pehe: 4.1481 \n",
      " — ite: 4.8904  — ate: 0.5134 — pehe: 4.8904 \n",
      "6/6 [==============================] - 1s 136ms/step - loss: 20.4938 - val_loss: 21.2321 - val_y0: -0.1577 - val_y1: 1.1899 - val_ate_after_scaled: 1.3475 - lr: 5.9605e-13\n",
      "Epoch 253/300\n",
      "3/6 [==============>...............] - ETA: 0s - loss: 20.6897 — ite: 4.2246  — ate: 0.7170 — pehe: 4.2246 \n",
      " — ite: 4.7177  — ate: 0.4661 — pehe: 4.7177 \n",
      "6/6 [==============================] - 1s 134ms/step - loss: 20.6090 - val_loss: 21.2223 - val_y0: -0.2439 - val_y1: 1.1845 - val_ate_after_scaled: 1.4284 - lr: 5.9605e-13\n",
      "Epoch 254/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.7824 — ite: 3.9581  — ate: 0.5782 — pehe: 3.9581 \n",
      " — ite: 4.5998  — ate: 0.4290 — pehe: 4.5998 \n",
      "6/6 [==============================] - 1s 128ms/step - loss: 20.7980 - val_loss: 21.1336 - val_y0: -0.2566 - val_y1: 1.1932 - val_ate_after_scaled: 1.4498 - lr: 5.9605e-13\n",
      "Epoch 255/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.6657 — ite: 4.0900  — ate: 0.8087 — pehe: 4.0900 \n",
      " — ite: 4.4035  — ate: 0.0316 — pehe: 4.4035 \n",
      "6/6 [==============================] - 1s 148ms/step - loss: 20.6381 - val_loss: 21.0880 - val_y0: -0.1650 - val_y1: 1.2409 - val_ate_after_scaled: 1.4059 - lr: 5.9605e-13\n",
      "Epoch 256/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.6044\n",
      "Epoch 00256: ReduceLROnPlateau reducing learning rate to 2.9802321634825324e-13.\n",
      " — ite: 4.1658  — ate: 0.8037 — pehe: 4.1658 \n",
      " — ite: 4.6592  — ate: 0.4574 — pehe: 4.6592 \n",
      "6/6 [==============================] - 1s 142ms/step - loss: 20.6090 - val_loss: 21.2453 - val_y0: -0.2716 - val_y1: 1.1746 - val_ate_after_scaled: 1.4462 - lr: 5.9605e-13\n",
      "Epoch 257/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.8127 — ite: 4.0294  — ate: 0.6301 — pehe: 4.0294 \n",
      " — ite: 4.6145  — ate: 0.5763 — pehe: 4.6145 \n",
      "6/6 [==============================] - 1s 156ms/step - loss: 20.8759 - val_loss: 21.6580 - val_y0: -0.2255 - val_y1: 1.1962 - val_ate_after_scaled: 1.4218 - lr: 2.9802e-13\n",
      "Epoch 258/300\n",
      "3/6 [==============>...............] - ETA: 0s - loss: 20.6810 — ite: 4.0644  — ate: 0.7904 — pehe: 4.0644 \n",
      " — ite: 4.4317  — ate: 0.2445 — pehe: 4.4317 \n",
      "6/6 [==============================] - 1s 141ms/step - loss: 20.8828 - val_loss: 21.3108 - val_y0: -0.3485 - val_y1: 1.2601 - val_ate_after_scaled: 1.6086 - lr: 2.9802e-13\n",
      "Epoch 259/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.6818 — ite: 3.9575  — ate: 0.6911 — pehe: 3.9575 \n",
      " — ite: 4.3948  — ate: 0.4137 — pehe: 4.3948 \n",
      "6/6 [==============================] - 1s 157ms/step - loss: 20.6799 - val_loss: 20.8982 - val_y0: -0.1397 - val_y1: 1.2163 - val_ate_after_scaled: 1.3560 - lr: 2.9802e-13\n",
      "Epoch 260/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.7815 — ite: 4.1134  — ate: 0.6870 — pehe: 4.1134 \n",
      " — ite: 4.5730  — ate: 0.3907 — pehe: 4.5730 \n",
      "6/6 [==============================] - 1s 155ms/step - loss: 20.8527 - val_loss: 21.1841 - val_y0: -0.0858 - val_y1: 1.1163 - val_ate_after_scaled: 1.2021 - lr: 2.9802e-13\n",
      "Epoch 261/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.8290\n",
      "Epoch 00261: ReduceLROnPlateau reducing learning rate to 1.4901160817412662e-13.\n",
      " — ite: 4.0878  — ate: 0.8936 — pehe: 4.0878 \n",
      " — ite: 4.8776  — ate: 0.5505 — pehe: 4.8776 \n",
      "6/6 [==============================] - 1s 157ms/step - loss: 20.7293 - val_loss: 21.1085 - val_y0: -0.3153 - val_y1: 1.2731 - val_ate_after_scaled: 1.5884 - lr: 2.9802e-13\n",
      "Epoch 262/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.5056 — ite: 4.0744  — ate: 0.7364 — pehe: 4.0744 \n",
      " — ite: 4.5079  — ate: 0.1333 — pehe: 4.5079 \n",
      "6/6 [==============================] - 1s 136ms/step - loss: 20.6033 - val_loss: 21.9489 - val_y0: -0.1755 - val_y1: 1.0716 - val_ate_after_scaled: 1.2471 - lr: 1.4901e-13\n",
      "Epoch 263/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.7152 — ite: 4.1164  — ate: 0.8741 — pehe: 4.1164 \n",
      " — ite: 4.4729  — ate: 0.2707 — pehe: 4.4729 \n",
      "6/6 [==============================] - 1s 131ms/step - loss: 20.8654 - val_loss: 21.2887 - val_y0: -0.1991 - val_y1: 1.4325 - val_ate_after_scaled: 1.6316 - lr: 1.4901e-13\n",
      "Epoch 264/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 20.6901 — ite: 4.1258  — ate: 0.7955 — pehe: 4.1258 \n",
      " — ite: 4.7334  — ate: 0.1142 — pehe: 4.7334 \n",
      "6/6 [==============================] - 1s 147ms/step - loss: 20.7254 - val_loss: 21.3594 - val_y0: -0.3164 - val_y1: 1.1717 - val_ate_after_scaled: 1.4881 - lr: 1.4901e-13\n",
      "Epoch 265/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.7594 — ite: 4.1892  — ate: 0.6667 — pehe: 4.1892 \n",
      " — ite: 4.7331  — ate: 0.4170 — pehe: 4.7331 \n",
      "6/6 [==============================] - 1s 149ms/step - loss: 20.6592 - val_loss: 20.7597 - val_y0: -0.3171 - val_y1: 1.2893 - val_ate_after_scaled: 1.6064 - lr: 1.4901e-13\n",
      "Epoch 266/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.6051\n",
      "Epoch 00266: ReduceLROnPlateau reducing learning rate to 7.450580408706331e-14.\n",
      " — ite: 4.1342  — ate: 0.7032 — pehe: 4.1342 \n",
      " — ite: 5.1824  — ate: 0.1190 — pehe: 5.1824 \n",
      "6/6 [==============================] - 1s 152ms/step - loss: 20.7879 - val_loss: 20.9637 - val_y0: -0.0937 - val_y1: 1.1102 - val_ate_after_scaled: 1.2039 - lr: 1.4901e-13\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Colab command to allow us to run Colab in TF2\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "%reload_ext tensorboard \n",
    "\n",
    "model = CEVAE()\n",
    "### MAIN CODE ####\n",
    "val_split=0.2\n",
    "batch_size=64\n",
    "verbose=True\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    " \n",
    "callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='val_loss', patience=40, min_delta=0), \n",
    "        #40 is Shi's recommendation patience for this dataset, but you should tune for your data \n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0, cooldown=0, min_lr=0),\n",
    "        #This learning rate scheduling is quite agressive which seems good for this dataset\n",
    "        metrics_for_cevae(data_train,'train',verbose),\n",
    "        metrics_for_cevae(data_valid,'valid',verbose),\n",
    "        tensorboard_callback\n",
    "    ]\n",
    "    \n",
    "#optimizer hyperparameters\n",
    "learning_rate = 1e-5\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate))\n",
    "\n",
    "model.fit(\n",
    "    [data_train['x'],data_train['t'],data_train['ys']],\n",
    "    callbacks=callbacks,\n",
    "    validation_data=[[data_valid['x'],data_valid['t'],data_valid['ys']]],\n",
    "    epochs=300,\n",
    "    batch_size=256,\n",
    "    verbose=verbose\n",
    "    )\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 45269), started 2 days, 1:24:53 ago. (Use '!kill 45269' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f53426e7b65ec465\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f53426e7b65ec465\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
