{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 “ihdp_npci_1-100.train.npz” 已经存在；不获取。\n",
      "\n",
      "文件 “ihdp_npci_1-100.test.npz” 已经存在；不获取。\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1344, 25)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.layers as tfkl\n",
    "tfd,tfpl = tfp.distributions,tfp.layers\n",
    "import tensorflow.keras.backend as tfkb\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from evaluation import *\n",
    "from cevae_networks import *\n",
    "################################################\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='')\n",
    "parser.add_argument('--scale_penalize',    type = float, default = 0.001,  help = '')\n",
    "parser.add_argument('--learning_rate',     type = float, default = 0.001,  help = '')\n",
    "parser.add_argument('--default_y_scale',   type = float, default = 1.,  help = '')\n",
    "parser.add_argument('--t_dim',     type = int, default = 1,  help = '')\n",
    "parser.add_argument('--y_dim',     type = int, default = 1,  help = '')\n",
    "parser.add_argument('--x_dim',     type = int, default = 25, help = '')\n",
    "parser.add_argument('--z_dim',     type = int, default = 20, help = '')\n",
    "parser.add_argument('--x_num_dim', type = int, default = 6,  help = '')\n",
    "parser.add_argument('--x_bin_dim', type = int, default = 19, help = '')\n",
    "parser.add_argument('--nh', type = int, default = 3, help = 'number of hidden layers')\n",
    "parser.add_argument('--h',  type = int, default = 200, help = 'number of hidden units')\n",
    "args = parser.parse_args([])\n",
    "################################################\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz \n",
    "\n",
    "def load_IHDP_data(training_data,testing_data,i):\n",
    "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "        train_data=np.load(trf); test_data=np.load(tef)\n",
    "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
    "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
    "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
    "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    "        ycf=np.concatenate((train_data['ycf'][:,i],  test_data['ycf'][:,i])).astype('float32')\n",
    "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "        data['y']=data['y'].reshape(-1,1)\n",
    "        data['ycf'] = ycf.reshape(-1,1)\n",
    "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "    return data\n",
    "\n",
    "ind = 7\n",
    "# rep = 5\n",
    "# rep = 1\n",
    "# data = load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.test.npz',i = ind)\n",
    "# for key in data:\n",
    "#     if key != 'y_scaler':\n",
    "#         data[key] = np.repeat(data[key],repeats = rep, axis = 0)\n",
    "# np.shape(data['x'])\n",
    "data_train = load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.train.npz',i = ind)\n",
    "data_valid = load_IHDP_data(training_data='./ihdp_npci_1-100.test.npz',testing_data='./ihdp_npci_1-100.test.npz',i = ind)\n",
    "np.shape(data_train['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEVAE(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CEVAE, self).__init__()\n",
    "        ########################################\n",
    "        # networks\n",
    "        self.activation = 'elu'\n",
    "        # CEVAE Model \n",
    "        ## (encoder)\n",
    "        self.q_y_tx = q_y_tx(args.x_bin_dim, args.x_num_dim, args.y_dim, args.t_dim, args.nh, args.h)\n",
    "        self.q_t_x = q_t_x(args.x_bin_dim, args.x_num_dim, args.t_dim, args.nh, args.h)\n",
    "        self.q_z_txy = q_z_txy(args.x_bin_dim, args.x_num_dim, args.y_dim, args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        ## (decoder)\n",
    "        self.p_x_z = p_x_z(args.x_bin_dim, args.x_num_dim, args.z_dim, args.nh, args.h)\n",
    "        self.p_t_z = p_t_z(args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        self.p_y_tz = p_y_tz(args.y_dim, args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        \n",
    "\n",
    "    def call(self, data, training=False):\n",
    "        if training:\n",
    "            x_train,t_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "            \n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            \n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.sample()\n",
    "            # decoder\n",
    "            ## p(x|z)\n",
    "            x_num,x_bin = self.p_x_z(z_infer_sample)\n",
    "            ## p(t|z)\n",
    "            t = self.p_t_z(z_infer_sample)\n",
    "            ## p(y|t,z)\n",
    "            y = self.p_y_tz(tf.concat([t_train,z_infer_sample],-1) )\n",
    "            \n",
    "            return y_infer,t_infer,z_infer,y,t,x_num,x_bin\n",
    "        else:\n",
    "            x_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "        \n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            \n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.sample()\n",
    "            # decoder\n",
    "            ## p(x|z)\n",
    "            x_num,x_bin = self.p_x_z(z_infer_sample)\n",
    "            ## p(t|z)\n",
    "            t = self.p_t_z(z_infer_sample)\n",
    "            ## p(y|t,z)\n",
    "            y0 = self.p_y_tz(tf.concat([tf.zeros_like(t_infer_sample),z_infer_sample],-1) )\n",
    "            y1 = self.p_y_tz(tf.concat([tf.ones_like(t_infer_sample),z_infer_sample],-1) )\n",
    "            return [y0,y1],t,z_infer\n",
    "\n",
    "\n",
    "    def cevae_loss(self, data, pred, training = False):\n",
    "        x_train, t_train, y_train = data[0],data[1],data[2]\n",
    "        x_train_num, x_train_bin = x_train[:,:args.x_num_dim],x_train[:,args.x_num_dim:]\n",
    "        y_infer,t_infer,z_infer,y,t,x_num,x_bin = pred\n",
    "        y0,y1 = y_infer\n",
    "        # reconstruct loss\n",
    "        recon_x_num = tfkb.sum(x_num.log_prob(x_train_num), 1)\n",
    "        recon_x_bin = tfkb.sum(x_bin.log_prob(x_train_bin), 1)\n",
    "        recon_y = tfkb.sum(y.log_prob(y_train), 1)\n",
    "        recon_t = tfkb.sum(t.log_prob(t_train), 1)\n",
    "        # kl loss\n",
    "        z_infer_sample = z_infer.sample()\n",
    "        z = tfd.Normal(loc = [0] * 20, scale = [1]*20)\n",
    "        kl_z = tfkb.sum((z.log_prob(z_infer_sample) - z_infer.log_prob(z_infer_sample)), -1)\n",
    "        # aux loss\n",
    "        aux_y = tfkb.sum(y0.log_prob(y_train)*(1-t_train) + y1.log_prob(y_train)* t_train, 1)\n",
    "        aux_t = tfkb.sum(t_infer.log_prob(t_train), 1)\n",
    "        loss = -tfkb.mean(recon_x_bin + recon_x_num + recon_y + recon_t + aux_y + aux_t + kl_z)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, data):\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "            loss = self.cevae_loss(data = data, pred = pred, training = True)\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        metrics = {\"loss\": loss}\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "        y_infer = pred[0]\n",
    "        loss = self.cevae_loss(data = data, pred = pred, training = False)\n",
    "        y0, y1 = y_infer[0].sample(),y_infer[1].sample()\n",
    "        ate = tfkb.mean(y1) - tfkb.mean(y0)\n",
    "        metrics = {\"loss\":loss,\"y0\": tfkb.mean(y0),\"y1\": tfkb.mean(y1),'ate_after_scaled': ate}\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 29.6928  — ite: 5.4995  — ate: 3.8242 — pehe: 5.4995 \n",
      " — ite: 6.1699  — ate: 3.8788 — pehe: 6.1699 \n",
      "6/6 [==============================] - 6s 312ms/step - loss: 29.6176 - val_loss: 30.2740 - val_y0: -0.1265 - val_y1: -0.0523 - val_ate_after_scaled: 0.0742 - lr: 1.0000e-05\n",
      "Epoch 2/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 29.6349 — ite: 5.4663  — ate: 3.7886 — pehe: 5.4663 \n",
      " — ite: 5.6805  — ate: 3.1744 — pehe: 5.6805 \n",
      "6/6 [==============================] - 1s 138ms/step - loss: 29.7420 - val_loss: 30.5334 - val_y0: -0.0286 - val_y1: 0.0238 - val_ate_after_scaled: 0.0524 - lr: 1.0000e-05\n",
      "Epoch 3/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 29.6587 — ite: 5.4587  — ate: 3.8053 — pehe: 5.4587 \n",
      " — ite: 5.1420  — ate: 2.8964 — pehe: 5.1420 \n",
      "6/6 [==============================] - 1s 128ms/step - loss: 29.4698 - val_loss: 30.5152 - val_y0: -0.0348 - val_y1: 0.1335 - val_ate_after_scaled: 0.1684 - lr: 1.0000e-05\n",
      "Epoch 4/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 29.3764 — ite: 5.4683  — ate: 3.7456 — pehe: 5.4683 \n",
      " — ite: 5.6672  — ate: 3.5759 — pehe: 5.6672 \n",
      "6/6 [==============================] - 1s 122ms/step - loss: 29.4386 - val_loss: 30.2659 - val_y0: -0.0961 - val_y1: 0.1195 - val_ate_after_scaled: 0.2156 - lr: 1.0000e-05\n",
      "Epoch 5/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 29.2969 — ite: 5.2798  — ate: 3.6346 — pehe: 5.2798 \n",
      " — ite: 5.4744  — ate: 3.2541 — pehe: 5.4744 \n",
      "6/6 [==============================] - 1s 124ms/step - loss: 29.4378 - val_loss: 30.6047 - val_y0: -0.2876 - val_y1: 0.0666 - val_ate_after_scaled: 0.3542 - lr: 1.0000e-05\n",
      "Epoch 6/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 29.3617 — ite: 5.5127  — ate: 3.8301 — pehe: 5.5127 \n",
      " — ite: 5.8793  — ate: 3.3238 — pehe: 5.8793 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 29.0615 - val_loss: 29.8595 - val_y0: -0.1617 - val_y1: 0.1982 - val_ate_after_scaled: 0.3599 - lr: 1.0000e-05\n",
      "Epoch 7/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 29.1608 — ite: 5.2829  — ate: 3.5391 — pehe: 5.2829 \n",
      " — ite: 5.8330  — ate: 3.4608 — pehe: 5.8330 \n",
      "6/6 [==============================] - 1s 116ms/step - loss: 29.2474 - val_loss: 29.6640 - val_y0: -0.2126 - val_y1: 0.2028 - val_ate_after_scaled: 0.4155 - lr: 1.0000e-05\n",
      "Epoch 8/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 29.0844 — ite: 5.3587  — ate: 3.7039 — pehe: 5.3587 \n",
      " — ite: 5.9855  — ate: 3.7421 — pehe: 5.9855 \n",
      "6/6 [==============================] - 1s 112ms/step - loss: 29.2862 - val_loss: 29.9652 - val_y0: -0.0143 - val_y1: 0.2856 - val_ate_after_scaled: 0.3000 - lr: 1.0000e-05\n",
      "Epoch 9/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 29.0980 — ite: 5.4548  — ate: 3.7245 — pehe: 5.4548 \n",
      " — ite: 5.9635  — ate: 3.7699 — pehe: 5.9635 \n",
      "6/6 [==============================] - 1s 122ms/step - loss: 29.3148 - val_loss: 29.8018 - val_y0: -0.1411 - val_y1: 0.3820 - val_ate_after_scaled: 0.5231 - lr: 1.0000e-05\n",
      "Epoch 10/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 28.9826 — ite: 5.3274  — ate: 3.6318 — pehe: 5.3274 \n",
      " — ite: 5.8726  — ate: 3.5709 — pehe: 5.8726 \n",
      "6/6 [==============================] - 1s 111ms/step - loss: 29.0038 - val_loss: 29.5410 - val_y0: -0.2094 - val_y1: 0.2939 - val_ate_after_scaled: 0.5033 - lr: 1.0000e-05\n",
      "Epoch 11/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 28.7973 — ite: 5.2954  — ate: 3.5146 — pehe: 5.2954 \n",
      " — ite: 5.6318  — ate: 3.1449 — pehe: 5.6318 \n",
      "6/6 [==============================] - 1s 114ms/step - loss: 28.7447 - val_loss: 29.4272 - val_y0: -0.0221 - val_y1: 0.3129 - val_ate_after_scaled: 0.3350 - lr: 1.0000e-05\n",
      "Epoch 12/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 28.8979 — ite: 5.3307  — ate: 3.4919 — pehe: 5.3307 \n",
      " — ite: 5.7593  — ate: 3.5710 — pehe: 5.7593 \n",
      "6/6 [==============================] - 1s 118ms/step - loss: 28.7920 - val_loss: 29.3837 - val_y0: -0.1907 - val_y1: 0.3110 - val_ate_after_scaled: 0.5017 - lr: 1.0000e-05\n",
      "Epoch 13/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 28.7739 — ite: 5.4509  — ate: 3.6403 — pehe: 5.4509 \n",
      " — ite: 5.5730  — ate: 2.9198 — pehe: 5.5730 \n",
      "6/6 [==============================] - 1s 122ms/step - loss: 28.4830 - val_loss: 29.5598 - val_y0: -0.2597 - val_y1: 0.4513 - val_ate_after_scaled: 0.7109 - lr: 1.0000e-05\n",
      "Epoch 14/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 28.8336 — ite: 5.3446  — ate: 3.6389 — pehe: 5.3446 \n",
      " — ite: 5.4511  — ate: 2.5269 — pehe: 5.4511 \n",
      "6/6 [==============================] - 1s 117ms/step - loss: 28.3977 - val_loss: 29.4276 - val_y0: -0.1944 - val_y1: 0.3695 - val_ate_after_scaled: 0.5639 - lr: 1.0000e-05\n",
      "Epoch 15/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 28.3704 — ite: 5.3189  — ate: 3.6011 — pehe: 5.3189 \n",
      " — ite: 5.6303  — ate: 3.1965 — pehe: 5.6303 \n",
      "6/6 [==============================] - 1s 115ms/step - loss: 28.5773 - val_loss: 29.1131 - val_y0: -0.1438 - val_y1: 0.4271 - val_ate_after_scaled: 0.5708 - lr: 1.0000e-05\n",
      "Epoch 16/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 28.4989 — ite: 5.4081  — ate: 3.6476 — pehe: 5.4081 \n",
      " — ite: 5.9422  — ate: 3.2304 — pehe: 5.9422 \n",
      "6/6 [==============================] - 1s 118ms/step - loss: 28.5979 - val_loss: 29.2865 - val_y0: -0.0797 - val_y1: 0.4727 - val_ate_after_scaled: 0.5524 - lr: 1.0000e-05\n",
      "Epoch 17/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 28.4504 — ite: 5.3057  — ate: 3.6256 — pehe: 5.3057 \n",
      " — ite: 5.6169  — ate: 3.2167 — pehe: 5.6169 \n",
      "6/6 [==============================] - 1s 117ms/step - loss: 28.3446 - val_loss: 28.9192 - val_y0: -0.2834 - val_y1: 0.4410 - val_ate_after_scaled: 0.7243 - lr: 1.0000e-05\n",
      "Epoch 18/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 28.3093 — ite: 5.4297  — ate: 3.7646 — pehe: 5.4297 \n",
      " — ite: 6.0570  — ate: 2.7538 — pehe: 6.0570 \n",
      "6/6 [==============================] - 1s 110ms/step - loss: 28.3421 - val_loss: 28.9559 - val_y0: -0.2157 - val_y1: 0.4882 - val_ate_after_scaled: 0.7039 - lr: 1.0000e-05\n",
      "Epoch 19/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 28.2734 — ite: 5.1147  — ate: 3.4237 — pehe: 5.1147 \n",
      " — ite: 5.6303  — ate: 3.2637 — pehe: 5.6303 \n",
      "6/6 [==============================] - 1s 119ms/step - loss: 28.2999 - val_loss: 28.8836 - val_y0: -0.2011 - val_y1: 0.4685 - val_ate_after_scaled: 0.6696 - lr: 1.0000e-05\n",
      "Epoch 20/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 28.0964 — ite: 5.1880  — ate: 3.4606 — pehe: 5.1880 \n",
      " — ite: 5.6238  — ate: 2.9240 — pehe: 5.6238 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 28.2358 - val_loss: 29.1700 - val_y0: -0.3399 - val_y1: 0.6592 - val_ate_after_scaled: 0.9991 - lr: 1.0000e-05\n",
      "Epoch 21/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 28.1400 — ite: 5.1889  — ate: 3.3702 — pehe: 5.1889 \n",
      " — ite: 5.7474  — ate: 3.2947 — pehe: 5.7474 \n",
      "6/6 [==============================] - 1s 116ms/step - loss: 28.4047 - val_loss: 28.6869 - val_y0: -0.3621 - val_y1: 0.4026 - val_ate_after_scaled: 0.7647 - lr: 1.0000e-05\n",
      "Epoch 22/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 28.0279 — ite: 5.2151  — ate: 3.4372 — pehe: 5.2151 \n",
      " — ite: 5.4039  — ate: 2.4306 — pehe: 5.4039 \n",
      "6/6 [==============================] - 1s 111ms/step - loss: 27.9596 - val_loss: 28.6137 - val_y0: -0.2626 - val_y1: 0.5940 - val_ate_after_scaled: 0.8566 - lr: 1.0000e-05\n",
      "Epoch 23/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 27.9812 — ite: 5.1907  — ate: 3.3206 — pehe: 5.1907 \n",
      " — ite: 5.6137  — ate: 3.0359 — pehe: 5.6137 \n",
      "6/6 [==============================] - 1s 117ms/step - loss: 28.0096 - val_loss: 28.7761 - val_y0: -0.4888 - val_y1: 0.5431 - val_ate_after_scaled: 1.0319 - lr: 1.0000e-05\n",
      "Epoch 24/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 27.9696 — ite: 5.1589  — ate: 3.2794 — pehe: 5.1589 \n",
      " — ite: 5.8706  — ate: 3.3747 — pehe: 5.8706 \n",
      "6/6 [==============================] - 1s 112ms/step - loss: 27.7963 - val_loss: 28.5448 - val_y0: -0.1207 - val_y1: 0.5932 - val_ate_after_scaled: 0.7138 - lr: 1.0000e-05\n",
      "Epoch 25/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 27.8076 — ite: 5.0149  — ate: 3.2289 — pehe: 5.0149 \n",
      " — ite: 5.2642  — ate: 2.3999 — pehe: 5.2642 \n",
      "6/6 [==============================] - 1s 120ms/step - loss: 28.0764 - val_loss: 28.6579 - val_y0: -0.2583 - val_y1: 0.7781 - val_ate_after_scaled: 1.0364 - lr: 1.0000e-05\n",
      "Epoch 26/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 27.8122 — ite: 5.1606  — ate: 3.4660 — pehe: 5.1606 \n",
      " — ite: 5.9971  — ate: 2.9437 — pehe: 5.9971 \n",
      "6/6 [==============================] - 1s 118ms/step - loss: 27.5335 - val_loss: 28.4752 - val_y0: -0.3233 - val_y1: 0.5981 - val_ate_after_scaled: 0.9214 - lr: 1.0000e-05\n",
      "Epoch 27/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 27.7568 — ite: 5.2367  — ate: 3.5180 — pehe: 5.2367 \n",
      " — ite: 5.4749  — ate: 3.2993 — pehe: 5.4749 \n",
      "6/6 [==============================] - 1s 119ms/step - loss: 27.5331 - val_loss: 28.4476 - val_y0: -0.3479 - val_y1: 0.5791 - val_ate_after_scaled: 0.9269 - lr: 1.0000e-05\n",
      "Epoch 28/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 27.7088 — ite: 5.0632  — ate: 3.2740 — pehe: 5.0632 \n",
      " — ite: 5.8201  — ate: 2.8938 — pehe: 5.8201 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 27.6202 - val_loss: 28.2688 - val_y0: -0.3426 - val_y1: 0.7507 - val_ate_after_scaled: 1.0932 - lr: 1.0000e-05\n",
      "Epoch 29/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 27.5586 — ite: 5.0560  — ate: 3.1511 — pehe: 5.0560 \n",
      " — ite: 5.0240  — ate: 2.0868 — pehe: 5.0240 \n",
      "6/6 [==============================] - 1s 123ms/step - loss: 27.6188 - val_loss: 28.1817 - val_y0: -0.2248 - val_y1: 0.5899 - val_ate_after_scaled: 0.8147 - lr: 1.0000e-05\n",
      "Epoch 30/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 27.4619 — ite: 4.9943  — ate: 3.2018 — pehe: 4.9943 \n",
      " — ite: 5.7745  — ate: 2.9811 — pehe: 5.7745 \n",
      "6/6 [==============================] - 1s 120ms/step - loss: 27.5038 - val_loss: 28.3010 - val_y0: -0.3175 - val_y1: 0.7682 - val_ate_after_scaled: 1.0857 - lr: 1.0000e-05\n",
      "Epoch 31/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 27.3030 — ite: 5.0050  — ate: 3.0614 — pehe: 5.0050 \n",
      " — ite: 5.6343  — ate: 2.7408 — pehe: 5.6343 \n",
      "6/6 [==============================] - 1s 117ms/step - loss: 27.7520 - val_loss: 28.0350 - val_y0: -0.2429 - val_y1: 0.8259 - val_ate_after_scaled: 1.0689 - lr: 1.0000e-05\n",
      "Epoch 32/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 27.3521 — ite: 5.0153  — ate: 3.1553 — pehe: 5.0153 \n",
      " — ite: 5.6596  — ate: 3.1787 — pehe: 5.6596 \n",
      "6/6 [==============================] - 1s 122ms/step - loss: 27.3171 - val_loss: 28.0099 - val_y0: -0.2298 - val_y1: 0.7602 - val_ate_after_scaled: 0.9901 - lr: 1.0000e-05\n",
      "Epoch 33/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 27.2541 — ite: 5.2015  — ate: 3.3286 — pehe: 5.2015 \n",
      " — ite: 5.7398  — ate: 2.7719 — pehe: 5.7398 \n",
      "6/6 [==============================] - 1s 118ms/step - loss: 27.2822 - val_loss: 27.8041 - val_y0: -0.3333 - val_y1: 0.6913 - val_ate_after_scaled: 1.0246 - lr: 1.0000e-05\n",
      "Epoch 34/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 27.4341 — ite: 5.0913  — ate: 3.2124 — pehe: 5.0913 \n",
      " — ite: 5.0630  — ate: 2.7643 — pehe: 5.0630 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 27.4480 - val_loss: 28.0039 - val_y0: -0.3467 - val_y1: 0.8164 - val_ate_after_scaled: 1.1631 - lr: 1.0000e-05\n",
      "Epoch 35/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 27.1757 — ite: 5.2137  — ate: 3.2482 — pehe: 5.2137 \n",
      " — ite: 5.1667  — ate: 2.5825 — pehe: 5.1667 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 27.0716 - val_loss: 27.7850 - val_y0: -0.2216 - val_y1: 0.6898 - val_ate_after_scaled: 0.9114 - lr: 1.0000e-05\n",
      "Epoch 36/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 26.8932 — ite: 4.8133  — ate: 2.8877 — pehe: 4.8133 \n",
      " — ite: 5.5720  — ate: 2.8406 — pehe: 5.5720 \n",
      "6/6 [==============================] - 1s 130ms/step - loss: 27.0353 - val_loss: 27.9593 - val_y0: -0.1708 - val_y1: 0.8558 - val_ate_after_scaled: 1.0266 - lr: 1.0000e-05\n",
      "Epoch 37/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 26.9585 — ite: 4.9986  — ate: 2.9706 — pehe: 4.9986 \n",
      " — ite: 5.2720  — ate: 2.3893 — pehe: 5.2720 \n",
      "6/6 [==============================] - 1s 125ms/step - loss: 26.9173 - val_loss: 27.6832 - val_y0: -0.3417 - val_y1: 0.9356 - val_ate_after_scaled: 1.2772 - lr: 1.0000e-05\n",
      "Epoch 38/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 26.8532 — ite: 5.0654  — ate: 3.2150 — pehe: 5.0654 \n",
      " — ite: 5.3770  — ate: 2.5418 — pehe: 5.3770 \n",
      "6/6 [==============================] - 1s 120ms/step - loss: 26.8520 - val_loss: 27.5870 - val_y0: -0.3871 - val_y1: 0.9476 - val_ate_after_scaled: 1.3348 - lr: 1.0000e-05\n",
      "Epoch 39/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 26.8839 — ite: 4.9586  — ate: 2.9030 — pehe: 4.9586 \n",
      " — ite: 5.7798  — ate: 2.7756 — pehe: 5.7798 \n",
      "6/6 [==============================] - 1s 116ms/step - loss: 26.7388 - val_loss: 27.3126 - val_y0: -0.3541 - val_y1: 0.9387 - val_ate_after_scaled: 1.2928 - lr: 1.0000e-05\n",
      "Epoch 40/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 26.7380 — ite: 4.9121  — ate: 2.9529 — pehe: 4.9121 \n",
      " — ite: 5.0308  — ate: 2.3475 — pehe: 5.0308 \n",
      "6/6 [==============================] - 1s 114ms/step - loss: 26.9493 - val_loss: 27.4065 - val_y0: -0.3235 - val_y1: 0.8215 - val_ate_after_scaled: 1.1449 - lr: 1.0000e-05\n",
      "Epoch 41/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 26.7851 — ite: 4.9518  — ate: 2.9129 — pehe: 4.9518 \n",
      " — ite: 5.1249  — ate: 1.6412 — pehe: 5.1249 \n",
      "6/6 [==============================] - 1s 112ms/step - loss: 26.6900 - val_loss: 27.2257 - val_y0: -0.2414 - val_y1: 0.8950 - val_ate_after_scaled: 1.1363 - lr: 1.0000e-05\n",
      "Epoch 42/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 26.7008 — ite: 4.9082  — ate: 2.8708 — pehe: 4.9082 \n",
      " — ite: 4.9758  — ate: 2.1037 — pehe: 4.9758 \n",
      "6/6 [==============================] - 1s 123ms/step - loss: 26.7776 - val_loss: 27.3865 - val_y0: -0.3636 - val_y1: 0.8677 - val_ate_after_scaled: 1.2313 - lr: 1.0000e-05\n",
      "Epoch 43/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 26.6261 — ite: 4.9796  — ate: 2.9805 — pehe: 4.9796 \n",
      " — ite: 5.3178  — ate: 2.7184 — pehe: 5.3178 \n",
      "6/6 [==============================] - 1s 122ms/step - loss: 26.3728 - val_loss: 26.9722 - val_y0: -0.2720 - val_y1: 0.9585 - val_ate_after_scaled: 1.2305 - lr: 1.0000e-05\n",
      "Epoch 44/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 26.6562 — ite: 4.8103  — ate: 2.7551 — pehe: 4.8103 \n",
      " — ite: 5.0071  — ate: 2.0946 — pehe: 5.0071 \n",
      "6/6 [==============================] - 1s 119ms/step - loss: 26.5951 - val_loss: 27.0986 - val_y0: -0.1155 - val_y1: 1.1048 - val_ate_after_scaled: 1.2203 - lr: 1.0000e-05\n",
      "Epoch 45/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 26.5885 — ite: 4.8601  — ate: 2.7207 — pehe: 4.8601 \n",
      " — ite: 4.9207  — ate: 1.8720 — pehe: 4.9207 \n",
      "6/6 [==============================] - 1s 131ms/step - loss: 26.4054 - val_loss: 27.1846 - val_y0: -0.3558 - val_y1: 0.8911 - val_ate_after_scaled: 1.2469 - lr: 1.0000e-05\n",
      "Epoch 46/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 26.4004 — ite: 4.8551  — ate: 2.7208 — pehe: 4.8551 \n",
      " — ite: 5.0319  — ate: 2.6856 — pehe: 5.0319 \n",
      "6/6 [==============================] - 1s 123ms/step - loss: 26.6515 - val_loss: 27.0473 - val_y0: -0.3590 - val_y1: 0.9844 - val_ate_after_scaled: 1.3434 - lr: 1.0000e-05\n",
      "Epoch 47/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 26.3551 — ite: 4.7761  — ate: 2.5975 — pehe: 4.7761 \n",
      " — ite: 4.9911  — ate: 2.3091 — pehe: 4.9911 \n",
      "6/6 [==============================] - 1s 124ms/step - loss: 26.3703 - val_loss: 26.7023 - val_y0: -0.1605 - val_y1: 1.0763 - val_ate_after_scaled: 1.2369 - lr: 1.0000e-05\n",
      "Epoch 48/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 26.2204 — ite: 4.8162  — ate: 2.6178 — pehe: 4.8162 \n",
      " — ite: 5.4739  — ate: 1.7331 — pehe: 5.4739 \n",
      "6/6 [==============================] - 1s 127ms/step - loss: 26.2616 - val_loss: 26.6820 - val_y0: -0.3348 - val_y1: 1.0724 - val_ate_after_scaled: 1.4072 - lr: 1.0000e-05\n",
      "Epoch 49/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 26.3882 — ite: 4.8394  — ate: 2.6086 — pehe: 4.8394 \n",
      " — ite: 5.2877  — ate: 2.1388 — pehe: 5.2877 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 26.1754 - val_loss: 26.6563 - val_y0: -0.2875 - val_y1: 1.0655 - val_ate_after_scaled: 1.3530 - lr: 1.0000e-05\n",
      "Epoch 50/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 26.0671 — ite: 4.6200  — ate: 2.5073 — pehe: 4.6200 \n",
      " — ite: 4.9531  — ate: 1.2543 — pehe: 4.9531 \n",
      "6/6 [==============================] - 1s 119ms/step - loss: 26.2327 - val_loss: 26.5403 - val_y0: -0.3118 - val_y1: 1.0405 - val_ate_after_scaled: 1.3523 - lr: 1.0000e-05\n",
      "Epoch 51/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 26.1366 — ite: 4.7014  — ate: 2.4861 — pehe: 4.7014 \n",
      " — ite: 4.9595  — ate: 2.0732 — pehe: 4.9595 \n",
      "6/6 [==============================] - 1s 118ms/step - loss: 26.0138 - val_loss: 26.2954 - val_y0: -0.2047 - val_y1: 1.0936 - val_ate_after_scaled: 1.2983 - lr: 1.0000e-05\n",
      "Epoch 52/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 25.9111 — ite: 4.5989  — ate: 2.3683 — pehe: 4.5989 \n",
      " — ite: 5.2809  — ate: 1.7425 — pehe: 5.2809 \n",
      "6/6 [==============================] - 1s 124ms/step - loss: 26.0699 - val_loss: 26.6463 - val_y0: -0.3436 - val_y1: 1.0113 - val_ate_after_scaled: 1.3549 - lr: 1.0000e-05\n",
      "Epoch 53/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 25.8538 — ite: 4.6585  — ate: 2.3047 — pehe: 4.6585 \n",
      " — ite: 5.6273  — ate: 2.0713 — pehe: 5.6273 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 25.7896 - val_loss: 26.4698 - val_y0: -0.3257 - val_y1: 1.1074 - val_ate_after_scaled: 1.4331 - lr: 1.0000e-05\n",
      "Epoch 54/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 25.8345 — ite: 4.7459  — ate: 2.4163 — pehe: 4.7459 \n",
      " — ite: 5.2177  — ate: 2.0381 — pehe: 5.2177 \n",
      "6/6 [==============================] - 1s 123ms/step - loss: 26.0747 - val_loss: 26.5338 - val_y0: -0.2513 - val_y1: 1.2996 - val_ate_after_scaled: 1.5509 - lr: 1.0000e-05\n",
      "Epoch 55/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 25.6983 — ite: 4.5083  — ate: 2.2700 — pehe: 4.5083 \n",
      " — ite: 5.1807  — ate: 2.1832 — pehe: 5.1807 \n",
      "6/6 [==============================] - 1s 107ms/step - loss: 25.6945 - val_loss: 26.4245 - val_y0: -0.4105 - val_y1: 1.0592 - val_ate_after_scaled: 1.4697 - lr: 1.0000e-05\n",
      "Epoch 56/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 25.6241 — ite: 4.6299  — ate: 2.3906 — pehe: 4.6299 \n",
      " — ite: 5.0119  — ate: 1.4349 — pehe: 5.0119 \n",
      "6/6 [==============================] - 1s 114ms/step - loss: 25.6947 - val_loss: 25.9579 - val_y0: -0.1460 - val_y1: 0.9970 - val_ate_after_scaled: 1.1430 - lr: 1.0000e-05\n",
      "Epoch 57/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 25.5781 — ite: 4.6164  — ate: 2.1814 — pehe: 4.6164 \n",
      " — ite: 5.2228  — ate: 1.4022 — pehe: 5.2228 \n",
      "6/6 [==============================] - 1s 118ms/step - loss: 25.6297 - val_loss: 26.4916 - val_y0: -0.1992 - val_y1: 1.0473 - val_ate_after_scaled: 1.2465 - lr: 1.0000e-05\n",
      "Epoch 58/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 25.4662 — ite: 4.5307  — ate: 1.8255 — pehe: 4.5307 \n",
      " — ite: 5.8568  — ate: 2.0950 — pehe: 5.8568 \n",
      "6/6 [==============================] - 1s 126ms/step - loss: 25.4431 - val_loss: 26.1902 - val_y0: -0.1987 - val_y1: 1.1775 - val_ate_after_scaled: 1.3762 - lr: 1.0000e-05\n",
      "Epoch 59/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 25.4334 — ite: 4.5393  — ate: 1.9397 — pehe: 4.5393 \n",
      " — ite: 5.1640  — ate: 1.5868 — pehe: 5.1640 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 25.4027 - val_loss: 26.1804 - val_y0: -0.2705 - val_y1: 1.1343 - val_ate_after_scaled: 1.4048 - lr: 1.0000e-05\n",
      "Epoch 60/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 25.2893 — ite: 4.6460  — ate: 1.9931 — pehe: 4.6460 \n",
      " — ite: 5.2201  — ate: 1.8736 — pehe: 5.2201 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 25.1962 - val_loss: 25.7062 - val_y0: -0.2246 - val_y1: 1.1540 - val_ate_after_scaled: 1.3786 - lr: 1.0000e-05\n",
      "Epoch 61/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 25.3621 — ite: 4.4528  — ate: 1.9704 — pehe: 4.4528 \n",
      " — ite: 5.4072  — ate: 1.3729 — pehe: 5.4072 \n",
      "6/6 [==============================] - 1s 117ms/step - loss: 25.0948 - val_loss: 25.8814 - val_y0: -0.1450 - val_y1: 1.1652 - val_ate_after_scaled: 1.3102 - lr: 1.0000e-05\n",
      "Epoch 62/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 25.1833 — ite: 4.4207  — ate: 1.9021 — pehe: 4.4207 \n",
      " — ite: 4.7424  — ate: 0.8183 — pehe: 4.7424 \n",
      "6/6 [==============================] - 1s 125ms/step - loss: 25.1360 - val_loss: 25.7742 - val_y0: -0.2536 - val_y1: 1.3347 - val_ate_after_scaled: 1.5883 - lr: 1.0000e-05\n",
      "Epoch 63/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 25.0359 — ite: 4.6325  — ate: 1.8945 — pehe: 4.6325 \n",
      " — ite: 5.1870  — ate: 0.9502 — pehe: 5.1870 \n",
      "6/6 [==============================] - 1s 123ms/step - loss: 24.9993 - val_loss: 25.6184 - val_y0: -0.2901 - val_y1: 1.2153 - val_ate_after_scaled: 1.5054 - lr: 1.0000e-05\n",
      "Epoch 64/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 25.1211 — ite: 4.4005  — ate: 1.8271 — pehe: 4.4005 \n",
      " — ite: 4.9357  — ate: 0.6129 — pehe: 4.9357 \n",
      "6/6 [==============================] - 1s 124ms/step - loss: 25.0030 - val_loss: 25.4824 - val_y0: -0.1994 - val_y1: 1.3075 - val_ate_after_scaled: 1.5069 - lr: 1.0000e-05\n",
      "Epoch 65/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 24.9594 — ite: 4.6153  — ate: 1.7495 — pehe: 4.6153 \n",
      " — ite: 4.7709  — ate: 1.2373 — pehe: 4.7709 \n",
      "6/6 [==============================] - 1s 118ms/step - loss: 24.8490 - val_loss: 25.6064 - val_y0: -0.2345 - val_y1: 1.1571 - val_ate_after_scaled: 1.3916 - lr: 1.0000e-05\n",
      "Epoch 66/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 24.9500 — ite: 4.4719  — ate: 1.7604 — pehe: 4.4719 \n",
      " — ite: 4.8880  — ate: 0.8891 — pehe: 4.8880 \n",
      "6/6 [==============================] - 1s 124ms/step - loss: 24.9614 - val_loss: 25.5805 - val_y0: -0.2027 - val_y1: 1.1575 - val_ate_after_scaled: 1.3603 - lr: 1.0000e-05\n",
      "Epoch 67/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 24.8632 — ite: 4.4082  — ate: 1.6208 — pehe: 4.4082 \n",
      " — ite: 4.3905  — ate: 0.7818 — pehe: 4.3905 \n",
      "6/6 [==============================] - 1s 130ms/step - loss: 24.8985 - val_loss: 25.5519 - val_y0: -0.1338 - val_y1: 1.1861 - val_ate_after_scaled: 1.3199 - lr: 1.0000e-05\n",
      "Epoch 68/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 24.7282 — ite: 4.4303  — ate: 1.6075 — pehe: 4.4303 \n",
      " — ite: 5.0135  — ate: 1.1103 — pehe: 5.0135 \n",
      "6/6 [==============================] - 1s 127ms/step - loss: 25.0289 - val_loss: 25.2854 - val_y0: -0.2302 - val_y1: 1.1990 - val_ate_after_scaled: 1.4292 - lr: 1.0000e-05\n",
      "Epoch 69/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 24.6409 — ite: 4.2387  — ate: 1.4723 — pehe: 4.2387 \n",
      " — ite: 5.0491  — ate: 0.6299 — pehe: 5.0491 \n",
      "6/6 [==============================] - 1s 126ms/step - loss: 24.9786 - val_loss: 25.0953 - val_y0: -0.1635 - val_y1: 1.2432 - val_ate_after_scaled: 1.4068 - lr: 1.0000e-05\n",
      "Epoch 70/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 24.6891 — ite: 4.2341  — ate: 1.1510 — pehe: 4.2341 \n",
      " — ite: 5.5136  — ate: 0.6301 — pehe: 5.5136 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 24.6338 - val_loss: 25.1018 - val_y0: -0.2183 - val_y1: 1.2409 - val_ate_after_scaled: 1.4593 - lr: 1.0000e-05\n",
      "Epoch 71/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 24.3622 — ite: 4.5297  — ate: 1.5420 — pehe: 4.5297 \n",
      " — ite: 5.0599  — ate: 0.6859 — pehe: 5.0599 \n",
      "6/6 [==============================] - 1s 127ms/step - loss: 24.3550 - val_loss: 25.1723 - val_y0: -0.1670 - val_y1: 1.3059 - val_ate_after_scaled: 1.4729 - lr: 1.0000e-05\n",
      "Epoch 72/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 24.4676 — ite: 4.2251  — ate: 1.2745 — pehe: 4.2251 \n",
      " — ite: 4.8755  — ate: 0.4650 — pehe: 4.8755 \n",
      "6/6 [==============================] - 1s 113ms/step - loss: 24.6482 - val_loss: 24.6575 - val_y0: -0.2087 - val_y1: 1.2508 - val_ate_after_scaled: 1.4596 - lr: 1.0000e-05\n",
      "Epoch 73/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 24.4063 — ite: 4.4790  — ate: 1.3743 — pehe: 4.4790 \n",
      " — ite: 5.3941  — ate: 1.1898 — pehe: 5.3941 \n",
      "6/6 [==============================] - 1s 114ms/step - loss: 24.2511 - val_loss: 24.7819 - val_y0: -0.2447 - val_y1: 1.2576 - val_ate_after_scaled: 1.5024 - lr: 1.0000e-05\n",
      "Epoch 74/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 24.2420 — ite: 4.2895  — ate: 1.2118 — pehe: 4.2895 \n",
      " — ite: 5.0081  — ate: 0.1870 — pehe: 5.0081 \n",
      "6/6 [==============================] - 1s 117ms/step - loss: 24.4726 - val_loss: 24.9969 - val_y0: -0.2412 - val_y1: 1.2472 - val_ate_after_scaled: 1.4883 - lr: 1.0000e-05\n",
      "Epoch 75/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 24.2185 — ite: 4.1967  — ate: 1.1674 — pehe: 4.1967 \n",
      " — ite: 4.7527  — ate: 0.2326 — pehe: 4.7527 \n",
      "6/6 [==============================] - 1s 116ms/step - loss: 24.2111 - val_loss: 24.4630 - val_y0: -0.3990 - val_y1: 1.2776 - val_ate_after_scaled: 1.6766 - lr: 1.0000e-05\n",
      "Epoch 76/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 24.0861 — ite: 4.2647  — ate: 1.3979 — pehe: 4.2647 \n",
      " — ite: 4.7062  — ate: 0.4026 — pehe: 4.7062 \n",
      "6/6 [==============================] - 1s 113ms/step - loss: 23.9061 - val_loss: 24.7004 - val_y0: -0.4702 - val_y1: 1.2879 - val_ate_after_scaled: 1.7581 - lr: 1.0000e-05\n",
      "Epoch 77/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 24.1062 — ite: 4.3125  — ate: 1.3668 — pehe: 4.3125 \n",
      " — ite: 5.1460  — ate: 0.8462 — pehe: 5.1460 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 24.1858 - val_loss: 24.5872 - val_y0: -0.1763 - val_y1: 1.2325 - val_ate_after_scaled: 1.4088 - lr: 1.0000e-05\n",
      "Epoch 78/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 24.0537 — ite: 4.2781  — ate: 1.1726 — pehe: 4.2781 \n",
      " — ite: 4.7196  — ate: 0.0286 — pehe: 4.7196 \n",
      "6/6 [==============================] - 1s 118ms/step - loss: 24.1812 - val_loss: 24.3616 - val_y0: -0.3733 - val_y1: 1.3784 - val_ate_after_scaled: 1.7518 - lr: 1.0000e-05\n",
      "Epoch 79/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 23.8907 — ite: 4.3094  — ate: 1.0651 — pehe: 4.3094 \n",
      " — ite: 5.4544  — ate: 0.6680 — pehe: 5.4544 \n",
      "6/6 [==============================] - 1s 118ms/step - loss: 23.8271 - val_loss: 24.4910 - val_y0: -0.2096 - val_y1: 1.2072 - val_ate_after_scaled: 1.4168 - lr: 1.0000e-05\n",
      "Epoch 80/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 23.9246 — ite: 4.3070  — ate: 1.2539 — pehe: 4.3070 \n",
      " — ite: 4.7262  — ate: 0.4837 — pehe: 4.7262 \n",
      "6/6 [==============================] - 1s 113ms/step - loss: 23.9103 - val_loss: 24.2563 - val_y0: -0.1113 - val_y1: 1.1849 - val_ate_after_scaled: 1.2962 - lr: 1.0000e-05\n",
      "Epoch 81/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 23.6272 — ite: 4.1706  — ate: 0.8989 — pehe: 4.1706 \n",
      " — ite: 4.6971  — ate: 0.4047 — pehe: 4.6971 \n",
      "6/6 [==============================] - 1s 116ms/step - loss: 23.6565 - val_loss: 24.0981 - val_y0: -0.0898 - val_y1: 1.3903 - val_ate_after_scaled: 1.4801 - lr: 1.0000e-05\n",
      "Epoch 82/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 23.8009 — ite: 4.2197  — ate: 1.0336 — pehe: 4.2197 \n",
      " — ite: 4.8406  — ate: 0.6042 — pehe: 4.8406 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 23.8099 - val_loss: 24.2574 - val_y0: -0.2552 - val_y1: 1.1768 - val_ate_after_scaled: 1.4320 - lr: 1.0000e-05\n",
      "Epoch 83/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 23.5318 — ite: 4.1880  — ate: 0.9953 — pehe: 4.1880 \n",
      " — ite: 5.2030  — ate: 0.1737 — pehe: 5.2030 \n",
      "6/6 [==============================] - 1s 124ms/step - loss: 23.7560 - val_loss: 23.9457 - val_y0: -0.3091 - val_y1: 1.3559 - val_ate_after_scaled: 1.6650 - lr: 1.0000e-05\n",
      "Epoch 84/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 23.5092 — ite: 4.3634  — ate: 0.8839 — pehe: 4.3634 \n",
      " — ite: 4.4602  — ate: 0.1146 — pehe: 4.4602 \n",
      "6/6 [==============================] - 1s 127ms/step - loss: 23.4639 - val_loss: 23.7347 - val_y0: -0.1984 - val_y1: 1.3248 - val_ate_after_scaled: 1.5232 - lr: 1.0000e-05\n",
      "Epoch 85/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 23.5431 — ite: 4.2425  — ate: 1.1107 — pehe: 4.2425 \n",
      " — ite: 5.1592  — ate: 0.5152 — pehe: 5.1592 \n",
      "6/6 [==============================] - 1s 125ms/step - loss: 23.3751 - val_loss: 23.8388 - val_y0: -0.2239 - val_y1: 1.2150 - val_ate_after_scaled: 1.4389 - lr: 1.0000e-05\n",
      "Epoch 86/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 23.3900 — ite: 4.2312  — ate: 0.8526 — pehe: 4.2312 \n",
      " — ite: 4.7666  — ate: 0.9055 — pehe: 4.7666 \n",
      "6/6 [==============================] - 1s 127ms/step - loss: 23.3126 - val_loss: 23.7525 - val_y0: -0.1809 - val_y1: 1.2575 - val_ate_after_scaled: 1.4384 - lr: 1.0000e-05\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 23.3231 — ite: 4.3662  — ate: 0.8270 — pehe: 4.3662 \n",
      " — ite: 5.1628  — ate: 0.1350 — pehe: 5.1628 \n",
      "6/6 [==============================] - 1s 167ms/step - loss: 23.3478 - val_loss: 23.9973 - val_y0: -0.1820 - val_y1: 1.1836 - val_ate_after_scaled: 1.3657 - lr: 1.0000e-05\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 23.1196 — ite: 4.3291  — ate: 0.9003 — pehe: 4.3291 \n",
      " — ite: 5.1224  — ate: 0.3249 — pehe: 5.1224 \n",
      "6/6 [==============================] - 1s 200ms/step - loss: 23.0525 - val_loss: 23.6097 - val_y0: -0.1765 - val_y1: 1.3332 - val_ate_after_scaled: 1.5097 - lr: 1.0000e-05\n",
      "Epoch 89/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 23.2674 — ite: 4.2361  — ate: 0.8633 — pehe: 4.2361 \n",
      " — ite: 4.4052  — ate: 0.0596 — pehe: 4.4052 \n",
      "6/6 [==============================] - 1s 190ms/step - loss: 23.3582 - val_loss: 23.6446 - val_y0: -0.4313 - val_y1: 1.1978 - val_ate_after_scaled: 1.6291 - lr: 1.0000e-05\n",
      "Epoch 90/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 23.1391 — ite: 4.2586  — ate: 1.0756 — pehe: 4.2586 \n",
      " — ite: 5.0467  — ate: 0.3595 — pehe: 5.0467 \n",
      "6/6 [==============================] - 1s 133ms/step - loss: 23.0428 - val_loss: 23.6665 - val_y0: -0.1189 - val_y1: 1.3201 - val_ate_after_scaled: 1.4390 - lr: 1.0000e-05\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 23.1172 — ite: 4.3143  — ate: 0.9376 — pehe: 4.3143 \n",
      " — ite: 5.0516  — ate: 0.0471 — pehe: 5.0516 \n",
      "6/6 [==============================] - 1s 141ms/step - loss: 23.2267 - val_loss: 23.8194 - val_y0: -0.1059 - val_y1: 1.1954 - val_ate_after_scaled: 1.3013 - lr: 1.0000e-05\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 22.8722 — ite: 4.1762  — ate: 0.8707 — pehe: 4.1762 \n",
      " — ite: 5.0028  — ate: 0.1159 — pehe: 5.0028 \n",
      "6/6 [==============================] - 1s 132ms/step - loss: 22.9221 - val_loss: 22.9578 - val_y0: -0.2125 - val_y1: 1.3553 - val_ate_after_scaled: 1.5678 - lr: 1.0000e-05\n",
      "Epoch 93/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 22.6680 — ite: 4.4276  — ate: 0.9492 — pehe: 4.4276 \n",
      " — ite: 4.7140  — ate: 0.0504 — pehe: 4.7140 \n",
      "6/6 [==============================] - 1s 125ms/step - loss: 22.8787 - val_loss: 23.1222 - val_y0: -0.2369 - val_y1: 1.3576 - val_ate_after_scaled: 1.5945 - lr: 1.0000e-05\n",
      "Epoch 94/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 22.8065 — ite: 4.1869  — ate: 0.8839 — pehe: 4.1869 \n",
      " — ite: 4.4546  — ate: 0.2091 — pehe: 4.4546 \n",
      "6/6 [==============================] - 1s 129ms/step - loss: 22.9887 - val_loss: 23.7222 - val_y0: -0.2217 - val_y1: 1.2266 - val_ate_after_scaled: 1.4483 - lr: 1.0000e-05\n",
      "Epoch 95/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 22.6925 — ite: 4.1597  — ate: 0.9400 — pehe: 4.1597 \n",
      " — ite: 4.7037  — ate: 0.0435 — pehe: 4.7037 \n",
      "6/6 [==============================] - 1s 133ms/step - loss: 22.5586 - val_loss: 23.0872 - val_y0: -0.2952 - val_y1: 1.3422 - val_ate_after_scaled: 1.6374 - lr: 1.0000e-05\n",
      "Epoch 96/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 22.9384 — ite: 4.0478  — ate: 0.7571 — pehe: 4.0478 \n",
      " — ite: 4.2092  — ate: 0.3529 — pehe: 4.2092 \n",
      "6/6 [==============================] - 1s 125ms/step - loss: 22.8135 - val_loss: 23.0371 - val_y0: -0.1733 - val_y1: 1.3707 - val_ate_after_scaled: 1.5439 - lr: 1.0000e-05\n",
      "Epoch 97/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 22.6174 — ite: 4.2727  — ate: 0.8523 — pehe: 4.2727 \n",
      " — ite: 5.0161  — ate: 0.1297 — pehe: 5.0161 \n",
      "6/6 [==============================] - 1s 145ms/step - loss: 22.5131 - val_loss: 22.7348 - val_y0: -0.2053 - val_y1: 1.2925 - val_ate_after_scaled: 1.4978 - lr: 1.0000e-05\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 22.4872 — ite: 4.1462  — ate: 0.7533 — pehe: 4.1462 \n",
      " — ite: 5.0583  — ate: 0.2192 — pehe: 5.0583 \n",
      "6/6 [==============================] - 1s 143ms/step - loss: 22.4449 - val_loss: 22.9294 - val_y0: -0.2317 - val_y1: 1.3498 - val_ate_after_scaled: 1.5815 - lr: 1.0000e-05\n",
      "Epoch 99/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 22.3302 — ite: 4.1696  — ate: 0.8997 — pehe: 4.1696 \n",
      " — ite: 4.8320  — ate: 0.1081 — pehe: 4.8320 \n",
      "6/6 [==============================] - 1s 123ms/step - loss: 22.4107 - val_loss: 23.1065 - val_y0: -0.2597 - val_y1: 1.1336 - val_ate_after_scaled: 1.3933 - lr: 1.0000e-05\n",
      "Epoch 100/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 22.4674 — ite: 4.1356  — ate: 0.8419 — pehe: 4.1356 \n",
      " — ite: 4.4898  — ate: 0.4860 — pehe: 4.4898 \n",
      "6/6 [==============================] - 1s 134ms/step - loss: 22.3154 - val_loss: 22.9378 - val_y0: -0.2522 - val_y1: 1.2202 - val_ate_after_scaled: 1.4725 - lr: 1.0000e-05\n",
      "Epoch 101/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 22.3556 — ite: 4.0839  — ate: 0.7961 — pehe: 4.0839 \n",
      " — ite: 4.9058  — ate: 0.0585 — pehe: 4.9058 \n",
      "6/6 [==============================] - 1s 132ms/step - loss: 22.0586 - val_loss: 22.4879 - val_y0: -0.1981 - val_y1: 1.1216 - val_ate_after_scaled: 1.3197 - lr: 1.0000e-05\n",
      "Epoch 102/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 22.2111 — ite: 4.0983  — ate: 0.7587 — pehe: 4.0983 \n",
      " — ite: 4.9414  — ate: 0.0990 — pehe: 4.9414 \n",
      "6/6 [==============================] - 1s 131ms/step - loss: 22.3021 - val_loss: 23.0124 - val_y0: -0.2903 - val_y1: 1.3241 - val_ate_after_scaled: 1.6145 - lr: 1.0000e-05\n",
      "Epoch 103/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 22.1016 — ite: 4.2234  — ate: 0.7426 — pehe: 4.2234 \n",
      " — ite: 4.9058  — ate: 0.1644 — pehe: 4.9058 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 22.3421 - val_loss: 22.5892 - val_y0: -0.0886 - val_y1: 1.3188 - val_ate_after_scaled: 1.4074 - lr: 1.0000e-05\n",
      "Epoch 104/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 22.2006 — ite: 4.1637  — ate: 0.6852 — pehe: 4.1637 \n",
      " — ite: 4.4910  — ate: 0.0853 — pehe: 4.4910 \n",
      "6/6 [==============================] - 1s 145ms/step - loss: 21.9584 - val_loss: 22.2451 - val_y0: -0.1528 - val_y1: 1.2786 - val_ate_after_scaled: 1.4314 - lr: 1.0000e-05\n",
      "Epoch 105/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 21.9672 — ite: 4.1561  — ate: 0.5687 — pehe: 4.1561 \n",
      " — ite: 4.8796  — ate: 0.4241 — pehe: 4.8796 \n",
      "6/6 [==============================] - 1s 125ms/step - loss: 22.0835 - val_loss: 22.4652 - val_y0: -0.2488 - val_y1: 1.0924 - val_ate_after_scaled: 1.3412 - lr: 1.0000e-05\n",
      "Epoch 106/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 21.9135 — ite: 4.2454  — ate: 0.6369 — pehe: 4.2454 \n",
      " — ite: 4.9032  — ate: 0.7461 — pehe: 4.9032 \n",
      "6/6 [==============================] - 1s 130ms/step - loss: 21.9796 - val_loss: 22.2409 - val_y0: -0.0481 - val_y1: 1.3176 - val_ate_after_scaled: 1.3657 - lr: 1.0000e-05\n",
      "Epoch 107/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 21.7934 — ite: 4.1622  — ate: 0.8322 — pehe: 4.1622 \n",
      " — ite: 4.6905  — ate: 0.1473 — pehe: 4.6905 \n",
      "6/6 [==============================] - 1s 127ms/step - loss: 21.9165 - val_loss: 22.1214 - val_y0: -0.0727 - val_y1: 1.3338 - val_ate_after_scaled: 1.4065 - lr: 1.0000e-05\n",
      "Epoch 108/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 21.7928 — ite: 4.2542  — ate: 0.7902 — pehe: 4.2542 \n",
      " — ite: 4.5975  — ate: 0.1749 — pehe: 4.5975 \n",
      "6/6 [==============================] - 1s 128ms/step - loss: 21.7906 - val_loss: 22.0888 - val_y0: -0.2862 - val_y1: 1.2468 - val_ate_after_scaled: 1.5330 - lr: 1.0000e-05\n",
      "Epoch 109/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 21.6400 — ite: 4.2424  — ate: 0.8764 — pehe: 4.2424 \n",
      " — ite: 4.9827  — ate: 0.4988 — pehe: 4.9827 \n",
      "6/6 [==============================] - 1s 117ms/step - loss: 21.7917 - val_loss: 22.2295 - val_y0: -0.2538 - val_y1: 1.1699 - val_ate_after_scaled: 1.4237 - lr: 1.0000e-05\n",
      "Epoch 110/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 21.6151 — ite: 4.1028  — ate: 0.5969 — pehe: 4.1028 \n",
      " — ite: 4.7725  — ate: 0.0359 — pehe: 4.7725 \n",
      "6/6 [==============================] - 1s 127ms/step - loss: 21.5133 - val_loss: 22.2044 - val_y0: -0.2449 - val_y1: 1.3850 - val_ate_after_scaled: 1.6299 - lr: 1.0000e-05\n",
      "Epoch 111/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 21.6347 — ite: 4.2263  — ate: 0.7375 — pehe: 4.2263 \n",
      " — ite: 4.9922  — ate: 0.1207 — pehe: 4.9922 \n",
      "6/6 [==============================] - 1s 119ms/step - loss: 21.6811 - val_loss: 22.0913 - val_y0: -0.2191 - val_y1: 1.2925 - val_ate_after_scaled: 1.5116 - lr: 1.0000e-05\n",
      "Epoch 112/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 21.6057 — ite: 4.1707  — ate: 0.6583 — pehe: 4.1707 \n",
      " — ite: 5.3178  — ate: 0.4338 — pehe: 5.3178 \n",
      "6/6 [==============================] - 1s 119ms/step - loss: 21.4474 - val_loss: 22.0156 - val_y0: -0.2035 - val_y1: 1.2220 - val_ate_after_scaled: 1.4255 - lr: 1.0000e-05\n",
      "Epoch 113/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 21.4979 — ite: 4.1204  — ate: 0.5477 — pehe: 4.1204 \n",
      " — ite: 5.0198  — ate: 0.0969 — pehe: 5.0198 \n",
      "6/6 [==============================] - 1s 115ms/step - loss: 21.5364 - val_loss: 21.9500 - val_y0: -0.1581 - val_y1: 1.3539 - val_ate_after_scaled: 1.5120 - lr: 1.0000e-05\n",
      "Epoch 114/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 21.3508 — ite: 4.0803  — ate: 0.7750 — pehe: 4.0803 \n",
      " — ite: 5.1480  — ate: 0.2335 — pehe: 5.1480 \n",
      "6/6 [==============================] - 1s 118ms/step - loss: 21.4557 - val_loss: 21.9873 - val_y0: -0.2675 - val_y1: 1.2649 - val_ate_after_scaled: 1.5323 - lr: 1.0000e-05\n",
      "Epoch 115/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 21.2311 — ite: 4.2821  — ate: 0.8539 — pehe: 4.2821 \n",
      " — ite: 4.6850  — ate: 0.1504 — pehe: 4.6850 \n",
      "6/6 [==============================] - 1s 118ms/step - loss: 21.0167 - val_loss: 21.8375 - val_y0: -0.2526 - val_y1: 1.2929 - val_ate_after_scaled: 1.5455 - lr: 1.0000e-05\n",
      "Epoch 116/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 21.3116 — ite: 4.1193  — ate: 0.7270 — pehe: 4.1193 \n",
      " — ite: 5.0167  — ate: 0.1210 — pehe: 5.0167 \n",
      "6/6 [==============================] - 1s 114ms/step - loss: 21.4423 - val_loss: 21.5543 - val_y0: -0.1335 - val_y1: 1.2417 - val_ate_after_scaled: 1.3753 - lr: 1.0000e-05\n",
      "Epoch 117/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 21.2090 — ite: 4.0893  — ate: 0.6900 — pehe: 4.0893 \n",
      " — ite: 4.6579  — ate: 0.7477 — pehe: 4.6579 \n",
      "6/6 [==============================] - 1s 118ms/step - loss: 21.2743 - val_loss: 21.8786 - val_y0: -0.3565 - val_y1: 1.3756 - val_ate_after_scaled: 1.7321 - lr: 1.0000e-05\n",
      "Epoch 118/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 21.0692 — ite: 4.2074  — ate: 0.8757 — pehe: 4.2074 \n",
      " — ite: 4.6515  — ate: 0.0789 — pehe: 4.6515 \n",
      "6/6 [==============================] - 1s 111ms/step - loss: 21.1000 - val_loss: 21.3200 - val_y0: -0.1656 - val_y1: 1.2173 - val_ate_after_scaled: 1.3829 - lr: 1.0000e-05\n",
      "Epoch 119/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 21.0304 — ite: 4.0729  — ate: 0.7670 — pehe: 4.0729 \n",
      " — ite: 4.9450  — ate: 0.1827 — pehe: 4.9450 \n",
      "6/6 [==============================] - 1s 112ms/step - loss: 20.9840 - val_loss: 21.6449 - val_y0: -0.2286 - val_y1: 1.1475 - val_ate_after_scaled: 1.3761 - lr: 1.0000e-05\n",
      "Epoch 120/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 21.0914 — ite: 4.1770  — ate: 0.7620 — pehe: 4.1770 \n",
      " — ite: 5.3879  — ate: 0.7258 — pehe: 5.3879 \n",
      "6/6 [==============================] - 1s 113ms/step - loss: 20.9902 - val_loss: 21.6623 - val_y0: -0.3223 - val_y1: 1.1920 - val_ate_after_scaled: 1.5143 - lr: 1.0000e-05\n",
      "Epoch 121/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.8039 — ite: 4.2022  — ate: 0.7665 — pehe: 4.2022 \n",
      " — ite: 5.2199  — ate: 0.4005 — pehe: 5.2199 \n",
      "6/6 [==============================] - 1s 114ms/step - loss: 20.7257 - val_loss: 21.3432 - val_y0: -0.1160 - val_y1: 1.2308 - val_ate_after_scaled: 1.3468 - lr: 1.0000e-05\n",
      "Epoch 122/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.9173 — ite: 4.0699  — ate: 0.7574 — pehe: 4.0699 \n",
      " — ite: 4.5311  — ate: 0.5776 — pehe: 4.5311 \n",
      "6/6 [==============================] - 1s 118ms/step - loss: 21.0869 - val_loss: 21.2602 - val_y0: -0.2639 - val_y1: 1.1671 - val_ate_after_scaled: 1.4311 - lr: 1.0000e-05\n",
      "Epoch 123/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.8842 — ite: 4.1394  — ate: 0.9203 — pehe: 4.1394 \n",
      " — ite: 4.7017  — ate: 0.3456 — pehe: 4.7017 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 20.5000 - val_loss: 21.1033 - val_y0: -0.1484 - val_y1: 1.2893 - val_ate_after_scaled: 1.4377 - lr: 1.0000e-05\n",
      "Epoch 124/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.8855 — ite: 4.2392  — ate: 0.9438 — pehe: 4.2392 \n",
      " — ite: 4.2221  — ate: 0.0053 — pehe: 4.2221 \n",
      "6/6 [==============================] - 1s 100ms/step - loss: 20.8837 - val_loss: 21.1392 - val_y0: -0.0363 - val_y1: 1.3395 - val_ate_after_scaled: 1.3758 - lr: 1.0000e-05\n",
      "Epoch 125/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.8748 — ite: 4.1911  — ate: 0.5847 — pehe: 4.1911 \n",
      " — ite: 5.0482  — ate: 0.0816 — pehe: 5.0482 \n",
      "6/6 [==============================] - 1s 111ms/step - loss: 20.7998 - val_loss: 20.7703 - val_y0: -0.2452 - val_y1: 1.2420 - val_ate_after_scaled: 1.4872 - lr: 1.0000e-05\n",
      "Epoch 126/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.4742 — ite: 4.2630  — ate: 0.7273 — pehe: 4.2630 \n",
      " — ite: 4.7322  — ate: 0.1864 — pehe: 4.7322 \n",
      "6/6 [==============================] - 1s 110ms/step - loss: 21.0450 - val_loss: 21.0685 - val_y0: 0.0146 - val_y1: 1.3541 - val_ate_after_scaled: 1.3395 - lr: 1.0000e-05\n",
      "Epoch 127/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.6722 — ite: 3.9974  — ate: 0.7481 — pehe: 3.9974 \n",
      " — ite: 4.6977  — ate: 0.4804 — pehe: 4.6977 \n",
      "6/6 [==============================] - 1s 120ms/step - loss: 20.6838 - val_loss: 21.5485 - val_y0: -0.1211 - val_y1: 1.1737 - val_ate_after_scaled: 1.2948 - lr: 1.0000e-05\n",
      "Epoch 128/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.4555 — ite: 3.9970  — ate: 0.7178 — pehe: 3.9970 \n",
      " — ite: 4.7997  — ate: 0.0417 — pehe: 4.7997 \n",
      "6/6 [==============================] - 1s 114ms/step - loss: 20.7154 - val_loss: 21.0063 - val_y0: -0.1277 - val_y1: 1.2416 - val_ate_after_scaled: 1.3693 - lr: 1.0000e-05\n",
      "Epoch 129/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.5625 — ite: 3.8771  — ate: 0.6481 — pehe: 3.8771 \n",
      " — ite: 4.1567  — ate: 0.1460 — pehe: 4.1567 \n",
      "6/6 [==============================] - 1s 108ms/step - loss: 20.6618 - val_loss: 20.8566 - val_y0: -0.1901 - val_y1: 1.2019 - val_ate_after_scaled: 1.3920 - lr: 1.0000e-05\n",
      "Epoch 130/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.4057 — ite: 4.0888  — ate: 0.8498 — pehe: 4.0888 \n",
      " — ite: 5.1157  — ate: 0.1072 — pehe: 5.1157 \n",
      "6/6 [==============================] - 1s 114ms/step - loss: 20.2399 - val_loss: 20.5689 - val_y0: -0.3056 - val_y1: 1.2521 - val_ate_after_scaled: 1.5576 - lr: 1.0000e-05\n",
      "Epoch 131/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.3503 — ite: 4.0017  — ate: 0.5968 — pehe: 4.0017 \n",
      " — ite: 4.7163  — ate: 0.5449 — pehe: 4.7163 \n",
      "6/6 [==============================] - 1s 102ms/step - loss: 20.3722 - val_loss: 20.6137 - val_y0: -0.1605 - val_y1: 1.1408 - val_ate_after_scaled: 1.3013 - lr: 1.0000e-05\n",
      "Epoch 132/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.1328 — ite: 4.0610  — ate: 0.5839 — pehe: 4.0610 \n",
      " — ite: 5.1120  — ate: 0.8884 — pehe: 5.1120 \n",
      "6/6 [==============================] - 1s 110ms/step - loss: 20.2661 - val_loss: 20.7360 - val_y0: -0.1893 - val_y1: 1.3191 - val_ate_after_scaled: 1.5085 - lr: 1.0000e-05\n",
      "Epoch 133/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 20.4836 — ite: 4.1372  — ate: 0.7131 — pehe: 4.1372 \n",
      " — ite: 4.6539  — ate: 0.2918 — pehe: 4.6539 \n",
      "6/6 [==============================] - 1s 113ms/step - loss: 20.2855 - val_loss: 20.7752 - val_y0: -0.2015 - val_y1: 1.3652 - val_ate_after_scaled: 1.5667 - lr: 1.0000e-05\n",
      "Epoch 134/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.1595 — ite: 3.9718  — ate: 0.6600 — pehe: 3.9718 \n",
      " — ite: 5.1047  — ate: 0.0782 — pehe: 5.1047 \n",
      "6/6 [==============================] - 1s 107ms/step - loss: 19.7371 - val_loss: 20.8064 - val_y0: -0.0933 - val_y1: 1.2232 - val_ate_after_scaled: 1.3166 - lr: 1.0000e-05\n",
      "Epoch 135/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.0434 — ite: 4.1581  — ate: 0.8463 — pehe: 4.1581 \n",
      " — ite: 4.5659  — ate: 0.9261 — pehe: 4.5659 \n",
      "6/6 [==============================] - 1s 104ms/step - loss: 20.0962 - val_loss: 20.4244 - val_y0: -0.1114 - val_y1: 1.2275 - val_ate_after_scaled: 1.3389 - lr: 1.0000e-05\n",
      "Epoch 136/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 19.9384 — ite: 3.9690  — ate: 0.7442 — pehe: 3.9690 \n",
      " — ite: 4.9840  — ate: 0.0119 — pehe: 4.9840 \n",
      "6/6 [==============================] - 1s 103ms/step - loss: 20.3927 - val_loss: 20.4636 - val_y0: -0.1694 - val_y1: 1.1235 - val_ate_after_scaled: 1.2929 - lr: 1.0000e-05\n",
      "Epoch 137/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.0997 — ite: 4.0795  — ate: 0.5759 — pehe: 4.0795 \n",
      " — ite: 5.1444  — ate: 0.7336 — pehe: 5.1444 \n",
      "6/6 [==============================] - 1s 106ms/step - loss: 20.1491 - val_loss: 20.3363 - val_y0: -0.1997 - val_y1: 1.3054 - val_ate_after_scaled: 1.5051 - lr: 1.0000e-05\n",
      "Epoch 138/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.0058 — ite: 4.1060  — ate: 0.5805 — pehe: 4.1060 \n",
      " — ite: 4.8708  — ate: 0.2484 — pehe: 4.8708 \n",
      "6/6 [==============================] - 1s 106ms/step - loss: 20.0189 - val_loss: 20.2521 - val_y0: -0.2460 - val_y1: 1.1329 - val_ate_after_scaled: 1.3789 - lr: 1.0000e-05\n",
      "Epoch 139/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 20.0104 — ite: 4.0922  — ate: 0.7079 — pehe: 4.0922 \n",
      " — ite: 5.0922  — ate: 0.1637 — pehe: 5.0922 \n",
      "6/6 [==============================] - 1s 122ms/step - loss: 19.6579 - val_loss: 20.6771 - val_y0: -0.1645 - val_y1: 1.1664 - val_ate_after_scaled: 1.3309 - lr: 1.0000e-05\n",
      "Epoch 140/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 19.8680 — ite: 3.9960  — ate: 0.4971 — pehe: 3.9960 \n",
      " — ite: 4.6770  — ate: 0.1091 — pehe: 4.6770 \n",
      "6/6 [==============================] - 1s 105ms/step - loss: 19.9900 - val_loss: 19.8871 - val_y0: 0.0013 - val_y1: 1.1755 - val_ate_after_scaled: 1.1742 - lr: 1.0000e-05\n",
      "Epoch 141/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 19.8316 — ite: 4.1842  — ate: 0.6565 — pehe: 4.1842 \n",
      " — ite: 4.4757  — ate: 0.0430 — pehe: 4.4757 \n",
      "6/6 [==============================] - 1s 102ms/step - loss: 20.2056 - val_loss: 20.5478 - val_y0: -0.1641 - val_y1: 1.2245 - val_ate_after_scaled: 1.3886 - lr: 1.0000e-05\n",
      "Epoch 142/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 19.7767 — ite: 3.9485  — ate: 0.7321 — pehe: 3.9485 \n",
      " — ite: 4.6075  — ate: 0.4887 — pehe: 4.6075 \n",
      "6/6 [==============================] - 1s 106ms/step - loss: 19.7804 - val_loss: 19.7894 - val_y0: -0.1921 - val_y1: 1.1542 - val_ate_after_scaled: 1.3463 - lr: 1.0000e-05\n",
      "Epoch 143/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 19.5932 — ite: 4.1306  — ate: 0.7789 — pehe: 4.1306 \n",
      " — ite: 4.5914  — ate: 0.5237 — pehe: 4.5914 \n",
      "6/6 [==============================] - 1s 106ms/step - loss: 19.8114 - val_loss: 20.0107 - val_y0: -0.1176 - val_y1: 1.2103 - val_ate_after_scaled: 1.3279 - lr: 1.0000e-05\n",
      "Epoch 144/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 19.5983 — ite: 4.0718  — ate: 0.6630 — pehe: 4.0718 \n",
      " — ite: 4.8861  — ate: 0.3461 — pehe: 4.8861 \n",
      "6/6 [==============================] - 1s 111ms/step - loss: 19.5435 - val_loss: 19.7888 - val_y0: -0.2544 - val_y1: 1.1705 - val_ate_after_scaled: 1.4249 - lr: 1.0000e-05\n",
      "Epoch 145/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 19.5540 — ite: 4.1922  — ate: 0.7398 — pehe: 4.1922 \n",
      " — ite: 4.3452  — ate: 0.2940 — pehe: 4.3452 \n",
      "6/6 [==============================] - 1s 112ms/step - loss: 19.3240 - val_loss: 20.2920 - val_y0: -0.1916 - val_y1: 1.2357 - val_ate_after_scaled: 1.4274 - lr: 1.0000e-05\n",
      "Epoch 146/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 19.4956 — ite: 4.2223  — ate: 0.6223 — pehe: 4.2223 \n",
      " — ite: 4.9609  — ate: 0.6055 — pehe: 4.9609 \n",
      "6/6 [==============================] - 1s 110ms/step - loss: 19.6536 - val_loss: 19.7928 - val_y0: -0.1793 - val_y1: 1.1886 - val_ate_after_scaled: 1.3679 - lr: 1.0000e-05\n",
      "Epoch 147/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 19.4476 — ite: 4.0002  — ate: 0.6855 — pehe: 4.0002 \n",
      " — ite: 4.3395  — ate: 0.0450 — pehe: 4.3395 \n",
      "6/6 [==============================] - 1s 113ms/step - loss: 19.3392 - val_loss: 20.0394 - val_y0: -0.0616 - val_y1: 1.0848 - val_ate_after_scaled: 1.1464 - lr: 1.0000e-05\n",
      "Epoch 148/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 19.2593 — ite: 3.9259  — ate: 0.7147 — pehe: 3.9259 \n",
      " — ite: 4.9577  — ate: 0.4597 — pehe: 4.9577 \n",
      "6/6 [==============================] - 1s 129ms/step - loss: 19.5659 - val_loss: 19.6866 - val_y0: -0.1248 - val_y1: 1.1687 - val_ate_after_scaled: 1.2935 - lr: 1.0000e-05\n",
      "Epoch 149/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 19.2691 — ite: 4.1497  — ate: 0.7209 — pehe: 4.1497 \n",
      " — ite: 4.7976  — ate: 0.4868 — pehe: 4.7976 \n",
      "6/6 [==============================] - 1s 119ms/step - loss: 19.4328 - val_loss: 19.9468 - val_y0: -0.2257 - val_y1: 1.2018 - val_ate_after_scaled: 1.4274 - lr: 1.0000e-05\n",
      "Epoch 150/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 19.2057 — ite: 4.0230  — ate: 0.5534 — pehe: 4.0230 \n",
      " — ite: 4.9638  — ate: 0.0366 — pehe: 4.9638 \n",
      "6/6 [==============================] - 1s 108ms/step - loss: 19.1710 - val_loss: 19.8088 - val_y0: -0.1585 - val_y1: 1.2541 - val_ate_after_scaled: 1.4125 - lr: 1.0000e-05\n",
      "Epoch 151/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 19.0853 — ite: 3.9724  — ate: 0.6418 — pehe: 3.9724 \n",
      " — ite: 4.4622  — ate: 0.2953 — pehe: 4.4622 \n",
      "6/6 [==============================] - 1s 105ms/step - loss: 19.5123 - val_loss: 19.5787 - val_y0: -0.2074 - val_y1: 1.2005 - val_ate_after_scaled: 1.4079 - lr: 1.0000e-05\n",
      "Epoch 152/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 19.2784 — ite: 4.1784  — ate: 0.6107 — pehe: 4.1784 \n",
      " — ite: 4.3244  — ate: 0.2025 — pehe: 4.3244 \n",
      "6/6 [==============================] - 1s 106ms/step - loss: 19.2681 - val_loss: 19.9862 - val_y0: -0.1792 - val_y1: 1.3658 - val_ate_after_scaled: 1.5450 - lr: 1.0000e-05\n",
      "Epoch 153/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 19.1142 — ite: 4.0008  — ate: 0.6608 — pehe: 4.0008 \n",
      " — ite: 4.5622  — ate: 0.2317 — pehe: 4.5622 \n",
      "6/6 [==============================] - 1s 113ms/step - loss: 19.1366 - val_loss: 20.1696 - val_y0: -0.2590 - val_y1: 1.1020 - val_ate_after_scaled: 1.3611 - lr: 1.0000e-05\n",
      "Epoch 154/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 19.0264 — ite: 4.0231  — ate: 0.5965 — pehe: 4.0231 \n",
      " — ite: 4.5332  — ate: 0.4641 — pehe: 4.5332 \n",
      "6/6 [==============================] - 1s 129ms/step - loss: 19.1686 - val_loss: 19.2866 - val_y0: -0.2480 - val_y1: 1.1799 - val_ate_after_scaled: 1.4279 - lr: 1.0000e-05\n",
      "Epoch 155/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 18.8178 — ite: 4.1284  — ate: 0.6579 — pehe: 4.1284 \n",
      " — ite: 4.9565  — ate: 0.1999 — pehe: 4.9565 \n",
      "6/6 [==============================] - 1s 114ms/step - loss: 19.1967 - val_loss: 19.1506 - val_y0: 0.0351 - val_y1: 1.1972 - val_ate_after_scaled: 1.1621 - lr: 1.0000e-05\n",
      "Epoch 156/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 18.8326 — ite: 4.1368  — ate: 0.6124 — pehe: 4.1368 \n",
      " — ite: 4.3854  — ate: 0.1824 — pehe: 4.3854 \n",
      "6/6 [==============================] - 1s 111ms/step - loss: 19.0863 - val_loss: 19.2087 - val_y0: -0.1255 - val_y1: 1.0116 - val_ate_after_scaled: 1.1372 - lr: 1.0000e-05\n",
      "Epoch 157/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 19.0556 — ite: 4.1707  — ate: 0.6224 — pehe: 4.1707 \n",
      " — ite: 4.7375  — ate: 0.1915 — pehe: 4.7375 \n",
      "6/6 [==============================] - 1s 151ms/step - loss: 18.6372 - val_loss: 19.2638 - val_y0: -0.1600 - val_y1: 1.2532 - val_ate_after_scaled: 1.4132 - lr: 1.0000e-05\n",
      "Epoch 158/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 18.8931 — ite: 4.0644  — ate: 0.6451 — pehe: 4.0644 \n",
      " — ite: 4.6387  — ate: 0.3242 — pehe: 4.6387 \n",
      "6/6 [==============================] - 1s 153ms/step - loss: 18.6382 - val_loss: 19.4845 - val_y0: -0.2003 - val_y1: 1.0976 - val_ate_after_scaled: 1.2979 - lr: 1.0000e-05\n",
      "Epoch 159/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 18.6348 — ite: 4.1337  — ate: 0.6150 — pehe: 4.1337 \n",
      " — ite: 4.5747  — ate: 0.1834 — pehe: 4.5747 \n",
      "6/6 [==============================] - 1s 139ms/step - loss: 18.5087 - val_loss: 19.9469 - val_y0: -0.1558 - val_y1: 1.2652 - val_ate_after_scaled: 1.4210 - lr: 1.0000e-05\n",
      "Epoch 160/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 18.9073 — ite: 4.1604  — ate: 0.7343 — pehe: 4.1604 \n",
      " — ite: 4.7241  — ate: 0.5850 — pehe: 4.7241 \n",
      "6/6 [==============================] - 1s 142ms/step - loss: 18.7617 - val_loss: 18.7207 - val_y0: -0.2386 - val_y1: 1.2721 - val_ate_after_scaled: 1.5107 - lr: 1.0000e-05\n",
      "Epoch 161/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 18.4628 — ite: 4.1838  — ate: 0.7235 — pehe: 4.1838 \n",
      " — ite: 4.7948  — ate: 0.4002 — pehe: 4.7948 \n",
      "6/6 [==============================] - 1s 137ms/step - loss: 18.5087 - val_loss: 19.1291 - val_y0: -0.2039 - val_y1: 1.0915 - val_ate_after_scaled: 1.2954 - lr: 1.0000e-05\n",
      "Epoch 162/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 18.6096 — ite: 4.2371  — ate: 0.7430 — pehe: 4.2371 \n",
      " — ite: 4.4549  — ate: 0.2748 — pehe: 4.4549 \n",
      "6/6 [==============================] - 1s 134ms/step - loss: 18.8243 - val_loss: 19.2833 - val_y0: -0.0851 - val_y1: 1.1012 - val_ate_after_scaled: 1.1863 - lr: 1.0000e-05\n",
      "Epoch 163/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 18.2314 — ite: 4.1726  — ate: 0.6577 — pehe: 4.1726 \n",
      " — ite: 4.6779  — ate: 0.6622 — pehe: 4.6779 \n",
      "6/6 [==============================] - 1s 126ms/step - loss: 18.3300 - val_loss: 19.1402 - val_y0: -0.2170 - val_y1: 1.1792 - val_ate_after_scaled: 1.3962 - lr: 1.0000e-05\n",
      "Epoch 164/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 18.6318 — ite: 4.1778  — ate: 0.7002 — pehe: 4.1778 \n",
      " — ite: 4.6483  — ate: 0.4501 — pehe: 4.6483 \n",
      "6/6 [==============================] - 1s 125ms/step - loss: 18.1001 - val_loss: 18.9826 - val_y0: -0.0809 - val_y1: 1.0306 - val_ate_after_scaled: 1.1116 - lr: 1.0000e-05\n",
      "Epoch 165/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 18.3640\n",
      "Epoch 00165: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      " — ite: 3.9369  — ate: 0.7159 — pehe: 3.9369 \n",
      " — ite: 4.9755  — ate: 0.8079 — pehe: 4.9755 \n",
      "6/6 [==============================] - 1s 119ms/step - loss: 18.4029 - val_loss: 18.8575 - val_y0: -0.2523 - val_y1: 1.2295 - val_ate_after_scaled: 1.4819 - lr: 1.0000e-05\n",
      "Epoch 166/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 18.3038 — ite: 3.9746  — ate: 0.6462 — pehe: 3.9746 \n",
      " — ite: 4.6112  — ate: 0.6451 — pehe: 4.6112 \n",
      "6/6 [==============================] - 1s 101ms/step - loss: 18.2203 - val_loss: 18.6052 - val_y0: -0.2292 - val_y1: 1.2945 - val_ate_after_scaled: 1.5237 - lr: 5.0000e-06\n",
      "Epoch 167/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 18.4369 — ite: 4.0357  — ate: 0.4311 — pehe: 4.0357 \n",
      " — ite: 5.0158  — ate: 0.0989 — pehe: 5.0158 \n",
      "6/6 [==============================] - 1s 118ms/step - loss: 18.0474 - val_loss: 19.1814 - val_y0: -0.3744 - val_y1: 1.2460 - val_ate_after_scaled: 1.6204 - lr: 5.0000e-06\n",
      "Epoch 168/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 18.2594 — ite: 4.1247  — ate: 0.7263 — pehe: 4.1247 \n",
      " — ite: 4.5957  — ate: 0.2236 — pehe: 4.5957 \n",
      "6/6 [==============================] - 1s 117ms/step - loss: 18.2973 - val_loss: 18.7210 - val_y0: -0.1339 - val_y1: 1.3141 - val_ate_after_scaled: 1.4480 - lr: 5.0000e-06\n",
      "Epoch 169/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 18.2535 — ite: 4.0946  — ate: 0.6914 — pehe: 4.0946 \n",
      " — ite: 4.9203  — ate: 0.2177 — pehe: 4.9203 \n",
      "6/6 [==============================] - 1s 144ms/step - loss: 18.0256 - val_loss: 18.6470 - val_y0: -0.1006 - val_y1: 1.0214 - val_ate_after_scaled: 1.1220 - lr: 5.0000e-06\n",
      "Epoch 170/300\n",
      "3/6 [==============>...............] - ETA: 0s - loss: 18.2675 — ite: 3.9874  — ate: 0.5765 — pehe: 3.9874 \n",
      " — ite: 4.7740  — ate: 0.5303 — pehe: 4.7740 \n",
      "6/6 [==============================] - 1s 126ms/step - loss: 18.1120 - val_loss: 18.9932 - val_y0: 0.0058 - val_y1: 1.2309 - val_ate_after_scaled: 1.2251 - lr: 5.0000e-06\n",
      "Epoch 171/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 18.1565\n",
      "Epoch 00171: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      " — ite: 4.0629  — ate: 0.6595 — pehe: 4.0629 \n",
      " — ite: 4.5615  — ate: 0.0066 — pehe: 4.5615 \n",
      "6/6 [==============================] - 1s 135ms/step - loss: 18.0630 - val_loss: 19.0431 - val_y0: -0.1922 - val_y1: 1.2616 - val_ate_after_scaled: 1.4538 - lr: 5.0000e-06\n",
      "Epoch 172/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 18.1237 — ite: 4.1958  — ate: 0.8703 — pehe: 4.1958 \n",
      " — ite: 4.5996  — ate: 0.0521 — pehe: 4.5996 \n",
      "6/6 [==============================] - 1s 101ms/step - loss: 18.0281 - val_loss: 18.4469 - val_y0: -0.2925 - val_y1: 1.3168 - val_ate_after_scaled: 1.6093 - lr: 2.5000e-06\n",
      "Epoch 173/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 17.9919 — ite: 4.0867  — ate: 0.8077 — pehe: 4.0867 \n",
      " — ite: 4.5946  — ate: 0.1552 — pehe: 4.5946 \n",
      "6/6 [==============================] - 1s 104ms/step - loss: 18.0298 - val_loss: 18.2021 - val_y0: -0.2402 - val_y1: 1.0391 - val_ate_after_scaled: 1.2792 - lr: 2.5000e-06\n",
      "Epoch 174/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 17.9308 — ite: 4.2468  — ate: 0.6150 — pehe: 4.2468 \n",
      " — ite: 4.9425  — ate: 0.0921 — pehe: 4.9425 \n",
      "6/6 [==============================] - 1s 128ms/step - loss: 18.0716 - val_loss: 18.6106 - val_y0: -0.1136 - val_y1: 1.2844 - val_ate_after_scaled: 1.3979 - lr: 2.5000e-06\n",
      "Epoch 175/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 18.0143 — ite: 4.1475  — ate: 0.7465 — pehe: 4.1475 \n",
      " — ite: 4.8847  — ate: 0.3364 — pehe: 4.8847 \n",
      "6/6 [==============================] - 1s 102ms/step - loss: 17.8767 - val_loss: 18.0880 - val_y0: -0.1352 - val_y1: 1.3525 - val_ate_after_scaled: 1.4877 - lr: 2.5000e-06\n",
      "Epoch 176/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 17.9794 — ite: 4.1661  — ate: 0.6660 — pehe: 4.1661 \n",
      " — ite: 4.8554  — ate: 0.0222 — pehe: 4.8554 \n",
      "6/6 [==============================] - 1s 110ms/step - loss: 17.9544 - val_loss: 19.0690 - val_y0: -0.1712 - val_y1: 1.1767 - val_ate_after_scaled: 1.3479 - lr: 2.5000e-06\n",
      "Epoch 177/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 18.0158 — ite: 4.1523  — ate: 0.6299 — pehe: 4.1523 \n",
      " — ite: 5.0167  — ate: 0.4242 — pehe: 5.0167 \n",
      "6/6 [==============================] - 1s 133ms/step - loss: 17.8456 - val_loss: 18.7143 - val_y0: -0.2152 - val_y1: 1.2447 - val_ate_after_scaled: 1.4599 - lr: 2.5000e-06\n",
      "Epoch 178/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 17.8667 — ite: 4.2913  — ate: 0.6298 — pehe: 4.2913 \n",
      " — ite: 4.9001  — ate: 0.1999 — pehe: 4.9001 \n",
      "6/6 [==============================] - 1s 112ms/step - loss: 18.0623 - val_loss: 18.4475 - val_y0: -0.4111 - val_y1: 1.2659 - val_ate_after_scaled: 1.6769 - lr: 2.5000e-06\n",
      "Epoch 179/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 17.9346 — ite: 4.0165  — ate: 0.7965 — pehe: 4.0165 \n",
      " — ite: 5.4079  — ate: 0.3936 — pehe: 5.4079 \n",
      "6/6 [==============================] - 1s 132ms/step - loss: 17.9201 - val_loss: 18.3437 - val_y0: -0.1601 - val_y1: 1.2929 - val_ate_after_scaled: 1.4529 - lr: 2.5000e-06\n",
      "Epoch 180/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 17.8896\n",
      "Epoch 00180: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      " — ite: 4.1297  — ate: 0.7518 — pehe: 4.1297 \n",
      " — ite: 4.6013  — ate: 0.1713 — pehe: 4.6013 \n",
      "6/6 [==============================] - 1s 105ms/step - loss: 18.3121 - val_loss: 18.8035 - val_y0: -0.0885 - val_y1: 1.2106 - val_ate_after_scaled: 1.2991 - lr: 2.5000e-06\n",
      "Epoch 181/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 17.8998 — ite: 4.0937  — ate: 0.5969 — pehe: 4.0937 \n",
      " — ite: 4.4983  — ate: 0.6958 — pehe: 4.4983 \n",
      "6/6 [==============================] - 1s 101ms/step - loss: 17.7813 - val_loss: 18.2805 - val_y0: -0.2852 - val_y1: 1.2733 - val_ate_after_scaled: 1.5584 - lr: 1.2500e-06\n",
      "Epoch 182/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 18.0665 — ite: 4.1095  — ate: 0.6048 — pehe: 4.1095 \n",
      " — ite: 5.1069  — ate: 0.1006 — pehe: 5.1069 \n",
      "6/6 [==============================] - 1s 100ms/step - loss: 17.9151 - val_loss: 18.2652 - val_y0: -0.1929 - val_y1: 1.1791 - val_ate_after_scaled: 1.3720 - lr: 1.2500e-06\n",
      "Epoch 183/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 17.9913 — ite: 4.1017  — ate: 0.4964 — pehe: 4.1017 \n",
      " — ite: 4.2897  — ate: 0.4878 — pehe: 4.2897 \n",
      "6/6 [==============================] - 1s 104ms/step - loss: 18.1376 - val_loss: 18.0628 - val_y0: 0.0617 - val_y1: 1.1966 - val_ate_after_scaled: 1.1349 - lr: 1.2500e-06\n",
      "Epoch 184/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 17.8345 — ite: 4.0542  — ate: 0.7712 — pehe: 4.0542 \n",
      " — ite: 5.2487  — ate: 0.2671 — pehe: 5.2487 \n",
      "6/6 [==============================] - 1s 104ms/step - loss: 17.9756 - val_loss: 18.4853 - val_y0: -0.0827 - val_y1: 1.1963 - val_ate_after_scaled: 1.2791 - lr: 1.2500e-06\n",
      "Epoch 185/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 17.8190 — ite: 4.1532  — ate: 0.7332 — pehe: 4.1532 \n",
      " — ite: 4.7878  — ate: 0.0435 — pehe: 4.7878 \n",
      "6/6 [==============================] - 1s 105ms/step - loss: 17.9349 - val_loss: 18.7516 - val_y0: -0.2381 - val_y1: 1.1045 - val_ate_after_scaled: 1.3425 - lr: 1.2500e-06\n",
      "Epoch 186/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 17.8475 — ite: 4.0828  — ate: 0.8458 — pehe: 4.0828 \n",
      " — ite: 4.6502  — ate: 0.3747 — pehe: 4.6502 \n",
      "6/6 [==============================] - 1s 105ms/step - loss: 17.9082 - val_loss: 18.1092 - val_y0: -0.1965 - val_y1: 1.0821 - val_ate_after_scaled: 1.2787 - lr: 1.2500e-06\n",
      "Epoch 187/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 18.0736 — ite: 4.0073  — ate: 0.4774 — pehe: 4.0073 \n",
      " — ite: 4.2894  — ate: 0.2578 — pehe: 4.2894 \n",
      "6/6 [==============================] - 1s 99ms/step - loss: 17.8671 - val_loss: 18.2598 - val_y0: -0.1229 - val_y1: 1.1091 - val_ate_after_scaled: 1.2321 - lr: 1.2500e-06\n",
      "Epoch 188/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 17.9188\n",
      "Epoch 00188: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      " — ite: 4.0275  — ate: 0.7099 — pehe: 4.0275 \n",
      " — ite: 5.1702  — ate: 0.0495 — pehe: 5.1702 \n",
      "6/6 [==============================] - 1s 109ms/step - loss: 18.0325 - val_loss: 18.6463 - val_y0: -0.2610 - val_y1: 1.3044 - val_ate_after_scaled: 1.5654 - lr: 1.2500e-06\n",
      "Epoch 189/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 17.7580 — ite: 4.1704  — ate: 0.7922 — pehe: 4.1704 \n",
      " — ite: 4.9641  — ate: 0.4065 — pehe: 4.9641 \n",
      "6/6 [==============================] - 1s 125ms/step - loss: 17.7354 - val_loss: 18.8224 - val_y0: -0.1399 - val_y1: 1.1990 - val_ate_after_scaled: 1.3388 - lr: 6.2500e-07\n",
      "Epoch 190/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 18.0279 — ite: 4.0151  — ate: 0.5878 — pehe: 4.0151 \n",
      " — ite: 5.0675  — ate: 0.3316 — pehe: 5.0675 \n",
      "6/6 [==============================] - 1s 104ms/step - loss: 17.9889 - val_loss: 18.5261 - val_y0: -0.2795 - val_y1: 1.2418 - val_ate_after_scaled: 1.5212 - lr: 6.2500e-07\n",
      "Epoch 191/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 17.9283 — ite: 4.1598  — ate: 0.8192 — pehe: 4.1598 \n",
      " — ite: 4.8612  — ate: 0.4878 — pehe: 4.8612 \n",
      "6/6 [==============================] - 1s 127ms/step - loss: 17.9827 - val_loss: 18.6428 - val_y0: -0.1346 - val_y1: 1.2644 - val_ate_after_scaled: 1.3990 - lr: 6.2500e-07\n",
      "Epoch 192/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 17.8770 — ite: 4.0518  — ate: 0.5391 — pehe: 4.0518 \n",
      " — ite: 5.0926  — ate: 0.5548 — pehe: 5.0926 \n",
      "6/6 [==============================] - 1s 108ms/step - loss: 17.9339 - val_loss: 18.3494 - val_y0: -0.2221 - val_y1: 1.0305 - val_ate_after_scaled: 1.2526 - lr: 6.2500e-07\n",
      "Epoch 193/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 18.0163\n",
      "Epoch 00193: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-07.\n",
      " — ite: 4.1722  — ate: 0.7877 — pehe: 4.1722 \n",
      " — ite: 4.4784  — ate: 0.2363 — pehe: 4.4784 \n",
      "6/6 [==============================] - 1s 114ms/step - loss: 17.8956 - val_loss: 18.1286 - val_y0: -0.2713 - val_y1: 1.1579 - val_ate_after_scaled: 1.4292 - lr: 6.2500e-07\n",
      "Epoch 194/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 17.8079 — ite: 4.1604  — ate: 0.7614 — pehe: 4.1604 \n",
      " — ite: 4.6686  — ate: 0.2845 — pehe: 4.6686 \n",
      "6/6 [==============================] - 1s 116ms/step - loss: 17.6947 - val_loss: 18.9241 - val_y0: -0.1318 - val_y1: 1.1199 - val_ate_after_scaled: 1.2517 - lr: 3.1250e-07\n",
      "Epoch 195/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 18.0814 — ite: 4.0918  — ate: 0.6123 — pehe: 4.0918 \n",
      " — ite: 4.8083  — ate: 0.6246 — pehe: 4.8083 \n",
      "6/6 [==============================] - 1s 137ms/step - loss: 18.1566 - val_loss: 18.5718 - val_y0: -0.2564 - val_y1: 1.3493 - val_ate_after_scaled: 1.6056 - lr: 3.1250e-07\n",
      "Epoch 196/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 17.7550 — ite: 4.2794  — ate: 0.6486 — pehe: 4.2794 \n",
      " — ite: 5.3192  — ate: 0.1347 — pehe: 5.3192 \n",
      "6/6 [==============================] - 1s 116ms/step - loss: 17.6632 - val_loss: 18.6188 - val_y0: -0.1027 - val_y1: 1.2830 - val_ate_after_scaled: 1.3857 - lr: 3.1250e-07\n",
      "Epoch 197/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 17.7801 — ite: 3.8924  — ate: 0.5663 — pehe: 3.8924 \n",
      " — ite: 4.5505  — ate: 0.5389 — pehe: 4.5505 \n",
      "6/6 [==============================] - 1s 131ms/step - loss: 17.7154 - val_loss: 18.7428 - val_y0: -0.2282 - val_y1: 1.2025 - val_ate_after_scaled: 1.4307 - lr: 3.1250e-07\n",
      "Epoch 198/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 17.8426\n",
      "Epoch 00198: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      " — ite: 4.1436  — ate: 0.7206 — pehe: 4.1436 \n",
      " — ite: 4.7738  — ate: 0.3087 — pehe: 4.7738 \n",
      "6/6 [==============================] - 1s 140ms/step - loss: 17.7969 - val_loss: 18.6954 - val_y0: -0.1889 - val_y1: 1.2483 - val_ate_after_scaled: 1.4373 - lr: 3.1250e-07\n",
      "Epoch 199/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 18.0677 — ite: 4.1504  — ate: 0.5172 — pehe: 4.1504 \n",
      " — ite: 4.7986  — ate: 0.0768 — pehe: 4.7986 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 18.0925 - val_loss: 18.5902 - val_y0: -0.2768 - val_y1: 1.1537 - val_ate_after_scaled: 1.4305 - lr: 1.5625e-07\n",
      "Epoch 200/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 17.9590 — ite: 4.1208  — ate: 0.7471 — pehe: 4.1208 \n",
      " — ite: 4.7387  — ate: 0.1361 — pehe: 4.7387 \n",
      "6/6 [==============================] - 1s 142ms/step - loss: 17.9312 - val_loss: 18.6905 - val_y0: -0.1992 - val_y1: 1.1087 - val_ate_after_scaled: 1.3079 - lr: 1.5625e-07\n",
      "Epoch 201/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 17.7829 — ite: 3.9226  — ate: 0.7134 — pehe: 3.9226 \n",
      " — ite: 4.7518  — ate: 0.0357 — pehe: 4.7518 \n",
      "6/6 [==============================] - 1s 122ms/step - loss: 17.5772 - val_loss: 18.3218 - val_y0: -0.2063 - val_y1: 1.1039 - val_ate_after_scaled: 1.3103 - lr: 1.5625e-07\n",
      "Epoch 202/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 17.8358 — ite: 3.8833  — ate: 0.5976 — pehe: 3.8833 \n",
      " — ite: 4.6201  — ate: 0.6296 — pehe: 4.6201 \n",
      "6/6 [==============================] - 1s 119ms/step - loss: 18.0639 - val_loss: 18.5858 - val_y0: -0.0883 - val_y1: 1.1823 - val_ate_after_scaled: 1.2706 - lr: 1.5625e-07\n",
      "Epoch 203/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 17.8335\n",
      "Epoch 00203: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-08.\n",
      " — ite: 4.0401  — ate: 0.5433 — pehe: 4.0401 \n",
      " — ite: 4.9080  — ate: 0.6001 — pehe: 4.9080 \n",
      "6/6 [==============================] - 1s 122ms/step - loss: 17.8671 - val_loss: 18.6490 - val_y0: 5.6723e-04 - val_y1: 1.2256 - val_ate_after_scaled: 1.2250 - lr: 1.5625e-07\n",
      "Epoch 204/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 17.6177 — ite: 4.0166  — ate: 0.7146 — pehe: 4.0166 \n",
      " — ite: 4.7474  — ate: 0.2512 — pehe: 4.7474 \n",
      "6/6 [==============================] - 1s 128ms/step - loss: 17.9945 - val_loss: 18.4920 - val_y0: -0.1895 - val_y1: 1.2590 - val_ate_after_scaled: 1.4485 - lr: 7.8125e-08\n",
      "Epoch 205/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 17.9072 — ite: 4.0766  — ate: 0.7459 — pehe: 4.0766 \n",
      " — ite: 4.9396  — ate: 0.5131 — pehe: 4.9396 \n",
      "6/6 [==============================] - 1s 129ms/step - loss: 18.0845 - val_loss: 18.8163 - val_y0: -0.0816 - val_y1: 1.2257 - val_ate_after_scaled: 1.3073 - lr: 7.8125e-08\n",
      "Epoch 206/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 17.6897 — ite: 4.0957  — ate: 0.7202 — pehe: 4.0957 \n",
      " — ite: 5.0284  — ate: 0.2109 — pehe: 5.0284 \n",
      "6/6 [==============================] - 1s 123ms/step - loss: 18.0790 - val_loss: 18.4434 - val_y0: -0.1750 - val_y1: 1.1701 - val_ate_after_scaled: 1.3451 - lr: 7.8125e-08\n",
      "Epoch 207/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 17.8452 — ite: 4.0296  — ate: 0.7927 — pehe: 4.0296 \n",
      " — ite: 4.4649  — ate: 0.3251 — pehe: 4.4649 \n",
      "6/6 [==============================] - 1s 122ms/step - loss: 17.7036 - val_loss: 18.3429 - val_y0: -0.0963 - val_y1: 1.1325 - val_ate_after_scaled: 1.2288 - lr: 7.8125e-08\n",
      "Epoch 208/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 17.8662\n",
      "Epoch 00208: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-08.\n",
      " — ite: 4.0041  — ate: 0.5828 — pehe: 4.0041 \n",
      " — ite: 4.7378  — ate: 0.1607 — pehe: 4.7378 \n",
      "6/6 [==============================] - 1s 137ms/step - loss: 17.8122 - val_loss: 18.3717 - val_y0: -0.2150 - val_y1: 1.2104 - val_ate_after_scaled: 1.4253 - lr: 7.8125e-08\n",
      "Epoch 209/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 17.8066 — ite: 4.0312  — ate: 0.6919 — pehe: 4.0312 \n",
      " — ite: 4.7347  — ate: 0.0240 — pehe: 4.7347 \n",
      "6/6 [==============================] - 1s 124ms/step - loss: 17.8866 - val_loss: 18.7153 - val_y0: -0.0842 - val_y1: 1.1380 - val_ate_after_scaled: 1.2223 - lr: 3.9062e-08\n",
      "Epoch 210/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 17.8768 — ite: 4.0669  — ate: 0.6863 — pehe: 4.0669 \n",
      " — ite: 4.6408  — ate: 0.2852 — pehe: 4.6408 \n",
      "6/6 [==============================] - 1s 121ms/step - loss: 17.9747 - val_loss: 18.7924 - val_y0: -0.1523 - val_y1: 1.1839 - val_ate_after_scaled: 1.3362 - lr: 3.9062e-08\n",
      "Epoch 211/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 17.7534 — ite: 4.1400  — ate: 0.6824 — pehe: 4.1400 \n",
      " — ite: 4.9323  — ate: 0.0042 — pehe: 4.9323 \n",
      "6/6 [==============================] - 1s 138ms/step - loss: 17.8413 - val_loss: 18.2977 - val_y0: -0.2494 - val_y1: 1.1918 - val_ate_after_scaled: 1.4413 - lr: 3.9062e-08\n",
      "Epoch 212/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 17.8340 — ite: 4.0322  — ate: 0.8265 — pehe: 4.0322 \n",
      " — ite: 4.9707  — ate: 0.0006 — pehe: 4.9707 \n",
      "6/6 [==============================] - 1s 112ms/step - loss: 17.6555 - val_loss: 18.5566 - val_y0: -0.2035 - val_y1: 1.2058 - val_ate_after_scaled: 1.4093 - lr: 3.9062e-08\n",
      "Epoch 213/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 17.6647\n",
      "Epoch 00213: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-08.\n",
      " — ite: 4.2043  — ate: 0.8042 — pehe: 4.2043 \n",
      " — ite: 4.8101  — ate: 0.1026 — pehe: 4.8101 \n",
      "6/6 [==============================] - 1s 111ms/step - loss: 17.9096 - val_loss: 18.2680 - val_y0: -0.1674 - val_y1: 1.2803 - val_ate_after_scaled: 1.4477 - lr: 3.9062e-08\n",
      "Epoch 214/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 17.7284 — ite: 4.0291  — ate: 0.7288 — pehe: 4.0291 \n",
      " — ite: 4.7148  — ate: 0.4168 — pehe: 4.7148 \n",
      "6/6 [==============================] - 1s 122ms/step - loss: 17.9579 - val_loss: 18.2113 - val_y0: -0.1223 - val_y1: 1.3531 - val_ate_after_scaled: 1.4754 - lr: 1.9531e-08\n",
      "Epoch 215/300\n",
      "6/6 [==============================] - ETA: 0s - loss: 17.6244 — ite: 3.9410  — ate: 0.6645 — pehe: 3.9410 \n",
      " — ite: 5.2116  — ate: 0.2137 — pehe: 5.2116 \n",
      "6/6 [==============================] - 1s 126ms/step - loss: 17.4303 - val_loss: 18.2972 - val_y0: -0.2141 - val_y1: 1.3063 - val_ate_after_scaled: 1.5203 - lr: 1.9531e-08\n",
      "Epoch 216/300\n",
      "3/6 [==============>...............] - ETA: 0s - loss: 17.6532 — ite: 4.0243  — ate: 0.6985 — pehe: 4.0243 \n",
      " — ite: 4.7084  — ate: 0.2905 — pehe: 4.7084 \n",
      "6/6 [==============================] - 1s 133ms/step - loss: 18.0435 - val_loss: 18.2216 - val_y0: -0.1882 - val_y1: 1.3280 - val_ate_after_scaled: 1.5162 - lr: 1.9531e-08\n",
      "Epoch 217/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 17.8445 — ite: 4.0787  — ate: 0.7985 — pehe: 4.0787 \n",
      " — ite: 4.5014  — ate: 0.0279 — pehe: 4.5014 \n",
      "6/6 [==============================] - 1s 128ms/step - loss: 17.6110 - val_loss: 18.7201 - val_y0: -0.1788 - val_y1: 0.9776 - val_ate_after_scaled: 1.1564 - lr: 1.9531e-08\n",
      "Epoch 218/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 17.8364\n",
      "Epoch 00218: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-09.\n",
      " — ite: 4.0383  — ate: 0.6188 — pehe: 4.0383 \n",
      " — ite: 4.5898  — ate: 0.5310 — pehe: 4.5898 \n",
      "6/6 [==============================] - 1s 118ms/step - loss: 18.3448 - val_loss: 19.1631 - val_y0: -0.3003 - val_y1: 1.3692 - val_ate_after_scaled: 1.6694 - lr: 1.9531e-08\n",
      "Epoch 219/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 17.7153 — ite: 4.1391  — ate: 0.7447 — pehe: 4.1391 \n",
      " — ite: 4.6892  — ate: 0.7667 — pehe: 4.6892 \n",
      "6/6 [==============================] - 1s 120ms/step - loss: 17.7302 - val_loss: 18.6178 - val_y0: -0.2612 - val_y1: 1.1615 - val_ate_after_scaled: 1.4227 - lr: 9.7656e-09\n",
      "Epoch 220/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 17.7669 — ite: 4.0340  — ate: 0.6969 — pehe: 4.0340 \n",
      " — ite: 5.5424  — ate: 0.3723 — pehe: 5.5424 \n",
      "6/6 [==============================] - 1s 128ms/step - loss: 17.9309 - val_loss: 18.6850 - val_y0: -0.2218 - val_y1: 1.1797 - val_ate_after_scaled: 1.4015 - lr: 9.7656e-09\n",
      "Epoch 221/300\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 17.7849 — ite: 4.1556  — ate: 0.7774 — pehe: 4.1556 \n",
      " — ite: 5.1745  — ate: 0.3865 — pehe: 5.1745 \n",
      "6/6 [==============================] - 1s 117ms/step - loss: 17.7725 - val_loss: 18.3518 - val_y0: -0.2453 - val_y1: 1.1767 - val_ate_after_scaled: 1.4220 - lr: 9.7656e-09\n",
      "Epoch 222/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 17.7363 — ite: 3.8985  — ate: 0.5430 — pehe: 3.8985 \n",
      " — ite: 4.6890  — ate: 0.1954 — pehe: 4.6890 \n",
      "6/6 [==============================] - 1s 110ms/step - loss: 17.7365 - val_loss: 18.9142 - val_y0: -0.1617 - val_y1: 1.3170 - val_ate_after_scaled: 1.4786 - lr: 9.7656e-09\n",
      "Epoch 223/300\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 17.8662\n",
      "Epoch 00223: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-09.\n",
      " — ite: 4.1577  — ate: 0.7305 — pehe: 4.1577 \n",
      " — ite: 5.2435  — ate: 0.5437 — pehe: 5.2435 \n",
      "6/6 [==============================] - 1s 120ms/step - loss: 17.8845 - val_loss: 18.2377 - val_y0: -0.1295 - val_y1: 1.3365 - val_ate_after_scaled: 1.4660 - lr: 9.7656e-09\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Colab command to allow us to run Colab in TF2\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "%reload_ext tensorboard \n",
    "\n",
    "model = CEVAE()\n",
    "### MAIN CODE ####\n",
    "val_split=0.2\n",
    "batch_size=64\n",
    "verbose=True\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    " \n",
    "callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='val_loss', patience=40, min_delta=0), \n",
    "        #40 is Shi's recommendation patience for this dataset, but you should tune for your data \n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0, cooldown=0, min_lr=0),\n",
    "        #This learning rate scheduling is quite agressive which seems good for this dataset\n",
    "        metrics_for_cevae(data_train,'train',verbose),\n",
    "        metrics_for_cevae(data_valid,'valid',verbose),\n",
    "        tensorboard_callback\n",
    "    ]\n",
    "    \n",
    "#optimizer hyperparameters\n",
    "learning_rate = 1e-5\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate))\n",
    "\n",
    "model.fit(\n",
    "    [data_train['x'],data_train['t'],data_train['ys']],\n",
    "    callbacks=callbacks,\n",
    "    validation_data=[[data_valid['x'],data_valid['t'],data_valid['ys']]],\n",
    "    epochs=300,\n",
    "    batch_size=256,\n",
    "    verbose=verbose\n",
    "    )\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 45269), started 2 days, 2:51:12 ago. (Use '!kill 45269' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-91f3b9a42ba6a850\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-91f3b9a42ba6a850\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
