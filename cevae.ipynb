{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 “ihdp_npci_1-100.train.npz” 已经存在；不获取。\n",
      "\n",
      "文件 “ihdp_npci_1-100.test.npz” 已经存在；不获取。\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([5.26691518]), array([2.59847927]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras.layers as tfkl\n",
    "tfd,tfpl = tfp.distributions,tfp.layers\n",
    "import tensorflow.keras.backend as tfkb\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from evaluation import *\n",
    "from cevae_networks import *\n",
    "################################################\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='')\n",
    "parser.add_argument('--scale_penalize',    type = float, default = 0.001,  help = '')\n",
    "parser.add_argument('--learning_rate',     type = float, default = 0.001,  help = '')\n",
    "parser.add_argument('--default_y_scale',   type = float, default = 1.,  help = '')\n",
    "parser.add_argument('--t_dim',     type = int, default = 1,  help = '')\n",
    "parser.add_argument('--y_dim',     type = int, default = 1,  help = '')\n",
    "parser.add_argument('--x_dim',     type = int, default = 25, help = '')\n",
    "parser.add_argument('--z_dim',     type = int, default = 20, help = '')\n",
    "parser.add_argument('--x_num_dim', type = int, default = 6,  help = '')\n",
    "parser.add_argument('--x_bin_dim', type = int, default = 19, help = '')\n",
    "parser.add_argument('--nh', type = int, default = 3, help = 'number of hidden layers')\n",
    "parser.add_argument('--h',  type = int, default = 200, help = 'number of hidden units')\n",
    "args = parser.parse_args([])\n",
    "################################################\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
    "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz \n",
    "\n",
    "def load_IHDP_data(training_data,testing_data,i):\n",
    "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
    "        train_data=np.load(trf); test_data=np.load(tef)\n",
    "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
    "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
    "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
    "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
    "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
    "        ycf=np.concatenate((train_data['ycf'][:,i],  test_data['ycf'][:,i])).astype('float32')\n",
    "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
    "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
    "        data['y']=data['y'].reshape(-1,1)\n",
    "        data['ycf'] = ycf.reshape(-1,1)\n",
    "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
    "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
    "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
    "    return data\n",
    "\n",
    "ind = 7\n",
    "rep = 1\n",
    "data = load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.test.npz',i = ind)\n",
    "for key in data:\n",
    "    if key != 'y_scaler':\n",
    "        data[key] = np.repeat(data[key],repeats = rep, axis = 0)\n",
    "data['y_scaler'].mean_, data['y_scaler'].scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEVAE(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CEVAE, self).__init__()\n",
    "        ########################################\n",
    "        # networks\n",
    "        self.activation = 'elu'\n",
    "        # CEVAE Model \n",
    "        ## (encoder)\n",
    "        self.q_y_tx = q_y_tx(args.x_bin_dim, args.x_num_dim, args.y_dim, args.t_dim, args.nh, args.h)\n",
    "        self.q_t_x = q_t_x(args.x_bin_dim, args.x_num_dim, args.t_dim, args.nh, args.h)\n",
    "        self.q_z_txy = q_z_txy(args.x_bin_dim, args.x_num_dim, args.y_dim, args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        ## (decoder)\n",
    "        self.p_x_z = p_x_z(args.x_bin_dim, args.x_num_dim, args.z_dim, args.nh, args.h)\n",
    "        self.p_t_z = p_t_z(args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        self.p_y_tz = p_y_tz(args.y_dim, args.t_dim, args.z_dim, args.nh, args.h)\n",
    "        \n",
    "\n",
    "    def call(self, data, training=False):\n",
    "        if training:\n",
    "            x_train,t_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "            \n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            \n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.sample()\n",
    "            # decoder\n",
    "            ## p(x|z)\n",
    "            x_num,x_bin = self.p_x_z(z_infer_sample)\n",
    "            ## p(t|z)\n",
    "            t = self.p_t_z(z_infer_sample)\n",
    "            ## p(y|t,z)\n",
    "            y = self.p_y_tz(tf.concat([t_train,z_infer_sample],-1) )\n",
    "            \n",
    "            return y_infer,t_infer,z_infer,y,t,x_num,x_bin\n",
    "        else:\n",
    "            x_train = data\n",
    "            # encoder\n",
    "            t_infer = self.q_t_x(x_train)\n",
    "            t_infer_sample = tf.cast(t_infer.sample(), tf.float32)\n",
    "        \n",
    "            y_infer = self.q_y_tx(x_train)\n",
    "            y0_infer, y1_infer = y_infer\n",
    "            y_infer_sample = y0_infer.sample() * (1-t_infer_sample) + y1_infer.sample() * t_infer_sample\n",
    "            \n",
    "            txy = tf.concat([tf.cast(t_infer_sample,tf.float32), y_infer_sample, x_train],-1)\n",
    "            z_infer = self.q_z_txy(txy)\n",
    "            z_infer_sample = z_infer.sample()\n",
    "            return y_infer,t_infer,z_infer\n",
    "\n",
    "\n",
    "    def cevae_loss(self, data, pred, training = False):\n",
    "        x_train, t_train, y_train = data[0],data[1],data[2]\n",
    "        x_train_num, x_train_bin = x_train[:,:args.x_num_dim],x_train[:,args.x_num_dim:]\n",
    "        y_infer,t_infer,z_infer,y,t,x_num,x_bin = pred\n",
    "        y0,y1 = y_infer\n",
    "        # reconstruct loss\n",
    "        recon_x_num = tfkb.sum(x_num.log_prob(x_train_num), 1)\n",
    "        recon_x_bin = tfkb.sum(x_bin.log_prob(x_train_bin), 1)\n",
    "        recon_y = tfkb.sum(y.log_prob(y_train), 1)\n",
    "        recon_t = tfkb.sum(t.log_prob(t_train), 1)\n",
    "        # kl loss\n",
    "        z_infer_sample = z_infer.sample()\n",
    "        z = tfd.Normal(loc = [0] * 20, scale = [1]*20)\n",
    "        kl_z = tfkb.sum((z.log_prob(z_infer_sample) - z_infer.log_prob(z_infer_sample)), -1)\n",
    "        # aux loss\n",
    "        aux_y = tfkb.sum(y0.log_prob(y_train)*(1-t_train) + y1.log_prob(y_train)* t_train, 1)\n",
    "        aux_t = tfkb.sum(t_infer.log_prob(t_train), 1)\n",
    "        loss = -tfkb.mean(recon_x_bin + recon_x_num + recon_y + recon_t + aux_y + aux_t + kl_z)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, data):\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "            loss = self.cevae_loss(data = data, pred = pred, training = True)\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        metrics = {\"loss\": loss}\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        data = data[0]\n",
    "        x,t,y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = self([x,t], training=True)  # Forward pass\n",
    "        y_infer = pred[0]\n",
    "        loss = self.cevae_loss(data = data, pred = pred, training = False)\n",
    "        y0, y1 = y_infer[0].sample(),y_infer[1].sample()\n",
    "        ate = tfkb.mean(y1) - tfkb.mean(y0)\n",
    "        metrics = {\"loss\":loss,\"y0\": tfkb.mean(y0),\"y1\": tfkb.mean(y1),'ate_after_scaled': ate}\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/137\n",
      "1/3 [=========>....................] - ETA: 10s - loss: 29.5666 — ite: 4.6589  — ate: 3.5713 — pehe: 5.5123 \n",
      "3/3 [==============================] - 7s 674ms/step - loss: 29.3668 - val_loss: 30.2301 - val_y0: -0.0844 - val_y1: -0.0142 - val_ate_after_scaled: 0.0702 - lr: 5.0000e-05\n",
      "Epoch 2/137\n",
      "3/3 [==============================] - ETA: 0s - loss: 29.2373 — ite: 4.6778  — ate: 3.4235 — pehe: 5.3033 \n",
      "3/3 [==============================] - 1s 331ms/step - loss: 29.1704 - val_loss: 30.1985 - val_y0: -0.0031 - val_y1: 0.0987 - val_ate_after_scaled: 0.1018 - lr: 5.0000e-05\n",
      "Epoch 3/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.6669 — ite: 4.5036  — ate: 3.0135 — pehe: 5.0856 \n",
      "3/3 [==============================] - 1s 297ms/step - loss: 28.9573 - val_loss: 29.9381 - val_y0: -0.0254 - val_y1: 0.2446 - val_ate_after_scaled: 0.2700 - lr: 5.0000e-05\n",
      "Epoch 4/137\n",
      "3/3 [==============================] - ETA: 0s - loss: 29.0384 — ite: 4.3273  — ate: 2.8596 — pehe: 4.9398 \n",
      "3/3 [==============================] - 1s 273ms/step - loss: 29.0137 - val_loss: 29.5499 - val_y0: -0.1041 - val_y1: 0.2650 - val_ate_after_scaled: 0.3691 - lr: 5.0000e-05\n",
      "Epoch 5/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.9555 — ite: 4.3237  — ate: 2.6227 — pehe: 4.7147 \n",
      "3/3 [==============================] - 1s 296ms/step - loss: 28.6463 - val_loss: 29.7161 - val_y0: -0.3093 - val_y1: 0.2466 - val_ate_after_scaled: 0.5558 - lr: 5.0000e-05\n",
      "Epoch 6/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.2925 — ite: 4.1442  — ate: 2.1854 — pehe: 4.3629 \n",
      "3/3 [==============================] - 1s 320ms/step - loss: 28.3352 - val_loss: 29.1836 - val_y0: -0.1955 - val_y1: 0.4118 - val_ate_after_scaled: 0.6073 - lr: 5.0000e-05\n",
      "Epoch 7/137\n",
      "3/3 [==============================] - ETA: 0s - loss: 28.4174 — ite: 4.1616  — ate: 1.9951 — pehe: 4.5314 \n",
      "3/3 [==============================] - 1s 352ms/step - loss: 28.3354 - val_loss: 28.7319 - val_y0: -0.2547 - val_y1: 0.4501 - val_ate_after_scaled: 0.7048 - lr: 5.0000e-05\n",
      "Epoch 8/137\n",
      "3/3 [==============================] - ETA: 0s - loss: 28.0303 — ite: 4.1402  — ate: 2.0170 — pehe: 4.3302 \n",
      "3/3 [==============================] - 0s 239ms/step - loss: 28.0253 - val_loss: 29.0106 - val_y0: -0.0632 - val_y1: 0.5654 - val_ate_after_scaled: 0.6286 - lr: 5.0000e-05\n",
      "Epoch 9/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.3763 — ite: 4.0514  — ate: 1.7004 — pehe: 4.2046 \n",
      "3/3 [==============================] - 0s 231ms/step - loss: 27.9011 - val_loss: 28.7025 - val_y0: -0.1951 - val_y1: 0.6954 - val_ate_after_scaled: 0.8905 - lr: 5.0000e-05\n",
      "Epoch 10/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.5811 — ite: 3.9833  — ate: 1.8262 — pehe: 4.3330 \n",
      "3/3 [==============================] - 0s 226ms/step - loss: 27.9043 - val_loss: 28.6832 - val_y0: -0.2661 - val_y1: 0.6401 - val_ate_after_scaled: 0.9062 - lr: 5.0000e-05\n",
      "Epoch 11/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 28.0868 — ite: 4.0264  — ate: 1.7076 — pehe: 4.3957 \n",
      "3/3 [==============================] - 0s 230ms/step - loss: 27.6696 - val_loss: 28.2838 - val_y0: -0.0800 - val_y1: 0.6908 - val_ate_after_scaled: 0.7708 - lr: 5.0000e-05\n",
      "Epoch 12/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.8748 — ite: 3.9036  — ate: 1.2310 — pehe: 4.1523 \n",
      "3/3 [==============================] - 0s 219ms/step - loss: 27.7297 - val_loss: 28.3263 - val_y0: -0.2480 - val_y1: 0.7195 - val_ate_after_scaled: 0.9675 - lr: 5.0000e-05\n",
      "Epoch 13/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.4964 — ite: 3.8977  — ate: 0.9084 — pehe: 4.0167 \n",
      "3/3 [==============================] - 0s 213ms/step - loss: 27.3162 - val_loss: 28.2609 - val_y0: -0.3145 - val_y1: 0.8881 - val_ate_after_scaled: 1.2026 - lr: 5.0000e-05\n",
      "Epoch 14/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.3986 — ite: 3.8505  — ate: 0.9016 — pehe: 4.1144 \n",
      "3/3 [==============================] - 1s 272ms/step - loss: 27.0933 - val_loss: 27.8799 - val_y0: -0.2477 - val_y1: 0.8305 - val_ate_after_scaled: 1.0782 - lr: 5.0000e-05\n",
      "Epoch 15/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.5080 — ite: 3.8340  — ate: 0.9437 — pehe: 4.0454 \n",
      "3/3 [==============================] - 1s 269ms/step - loss: 27.1849 - val_loss: 27.6957 - val_y0: -0.1948 - val_y1: 0.9111 - val_ate_after_scaled: 1.1059 - lr: 5.0000e-05\n",
      "Epoch 16/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 27.4831 — ite: 3.8028  — ate: 0.7501 — pehe: 3.9101 \n",
      "3/3 [==============================] - 1s 261ms/step - loss: 26.7646 - val_loss: 27.6432 - val_y0: -0.1280 - val_y1: 0.9750 - val_ate_after_scaled: 1.1029 - lr: 5.0000e-05\n",
      "Epoch 17/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 26.9746 — ite: 3.7811  — ate: 0.9480 — pehe: 3.9479 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: 26.7492 - val_loss: 27.3934 - val_y0: -0.3275 - val_y1: 0.9576 - val_ate_after_scaled: 1.2852 - lr: 5.0000e-05\n",
      "Epoch 18/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 26.9008 — ite: 3.8298  — ate: 0.8246 — pehe: 3.9777 \n",
      "3/3 [==============================] - 1s 314ms/step - loss: 26.6226 - val_loss: 27.0073 - val_y0: -0.2581 - val_y1: 1.0143 - val_ate_after_scaled: 1.2724 - lr: 5.0000e-05\n",
      "Epoch 19/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 26.4897 — ite: 3.8555  — ate: 0.3755 — pehe: 3.9323 \n",
      "3/3 [==============================] - 0s 221ms/step - loss: 26.3329 - val_loss: 26.9327 - val_y0: -0.2395 - val_y1: 0.9988 - val_ate_after_scaled: 1.2383 - lr: 5.0000e-05\n",
      "Epoch 20/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 26.8457 — ite: 3.8141  — ate: 0.4269 — pehe: 3.8699 \n",
      "3/3 [==============================] - 1s 309ms/step - loss: 26.3882 - val_loss: 26.6973 - val_y0: -0.3756 - val_y1: 1.1911 - val_ate_after_scaled: 1.5667 - lr: 5.0000e-05\n",
      "Epoch 21/137\n",
      "3/3 [==============================] - ETA: 0s - loss: 25.8968 — ite: 3.8167  — ate: 0.1178 — pehe: 3.8718 \n",
      "3/3 [==============================] - 1s 295ms/step - loss: 25.8701 - val_loss: 26.4836 - val_y0: -0.3944 - val_y1: 0.9328 - val_ate_after_scaled: 1.3272 - lr: 5.0000e-05\n",
      "Epoch 22/137\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 25.7861 — ite: 3.7102  — ate: 0.2763 — pehe: 3.8849 \n",
      "3/3 [==============================] - 1s 259ms/step - loss: 25.9156 - val_loss: 26.3517 - val_y0: -0.2909 - val_y1: 1.1186 - val_ate_after_scaled: 1.4096 - lr: 5.0000e-05\n",
      "Epoch 23/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.8225 — ite: 3.8096  — ate: 0.0816 — pehe: 3.9101 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: 25.6391 - val_loss: 26.3492 - val_y0: -0.5090 - val_y1: 1.0607 - val_ate_after_scaled: 1.5697 - lr: 5.0000e-05\n",
      "Epoch 24/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.5369 — ite: 3.7596  — ate: 0.2916 — pehe: 3.7389 \n",
      "3/3 [==============================] - 0s 228ms/step - loss: 25.6866 - val_loss: 26.1500 - val_y0: -0.1310 - val_y1: 1.1009 - val_ate_after_scaled: 1.2319 - lr: 5.0000e-05\n",
      "Epoch 25/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.4152 — ite: 3.8153  — ate: 0.1835 — pehe: 4.0873 \n",
      "3/3 [==============================] - 0s 240ms/step - loss: 25.3734 - val_loss: 25.7810 - val_y0: -0.2630 - val_y1: 1.2720 - val_ate_after_scaled: 1.5351 - lr: 5.0000e-05\n",
      "Epoch 26/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.3173 — ite: 3.7697  — ate: 0.3333 — pehe: 3.8797 \n",
      "3/3 [==============================] - 0s 227ms/step - loss: 24.9818 - val_loss: 26.0267 - val_y0: -0.3240 - val_y1: 1.0776 - val_ate_after_scaled: 1.4015 - lr: 5.0000e-05\n",
      "Epoch 27/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 25.4150 — ite: 3.7379  — ate: 0.1633 — pehe: 3.6831 \n",
      "3/3 [==============================] - 0s 218ms/step - loss: 25.1781 - val_loss: 25.3034 - val_y0: -0.3465 - val_y1: 1.0399 - val_ate_after_scaled: 1.3864 - lr: 5.0000e-05\n",
      "Epoch 28/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.7883 — ite: 3.8258  — ate: 0.1438 — pehe: 3.8307 \n",
      "3/3 [==============================] - 1s 263ms/step - loss: 24.7186 - val_loss: 25.3169 - val_y0: -0.3404 - val_y1: 1.1930 - val_ate_after_scaled: 1.5334 - lr: 5.0000e-05\n",
      "Epoch 29/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.7577 — ite: 3.7872  — ate: 0.1297 — pehe: 3.6951 \n",
      "3/3 [==============================] - 0s 235ms/step - loss: 24.7129 - val_loss: 25.1934 - val_y0: -0.2222 - val_y1: 1.0141 - val_ate_after_scaled: 1.2363 - lr: 5.0000e-05\n",
      "Epoch 30/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.3268 — ite: 3.7934  — ate: 0.1200 — pehe: 3.9526 \n",
      "3/3 [==============================] - 0s 231ms/step - loss: 24.3512 - val_loss: 25.0670 - val_y0: -0.3108 - val_y1: 1.1731 - val_ate_after_scaled: 1.4839 - lr: 5.0000e-05\n",
      "Epoch 31/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.2284 — ite: 3.7265  — ate: 0.0776 — pehe: 3.8537 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: 24.1150 - val_loss: 25.0317 - val_y0: -0.2303 - val_y1: 1.2114 - val_ate_after_scaled: 1.4417 - lr: 5.0000e-05\n",
      "Epoch 32/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.2961 — ite: 3.7415  — ate: 0.1424 — pehe: 3.8773 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: 24.0090 - val_loss: 24.3795 - val_y0: -0.2124 - val_y1: 1.1270 - val_ate_after_scaled: 1.3393 - lr: 5.0000e-05\n",
      "Epoch 33/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.1228 — ite: 3.8454  — ate: 0.3224 — pehe: 3.8669 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: 23.6880 - val_loss: 24.1489 - val_y0: -0.3091 - val_y1: 1.0403 - val_ate_after_scaled: 1.3494 - lr: 5.0000e-05\n",
      "Epoch 34/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 24.1487 — ite: 3.7413  — ate: 0.1835 — pehe: 3.8177 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: 23.4931 - val_loss: 24.2827 - val_y0: -0.3126 - val_y1: 1.1495 - val_ate_after_scaled: 1.4621 - lr: 5.0000e-05\n",
      "Epoch 35/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.9949 — ite: 3.7570  — ate: 0.2726 — pehe: 3.6296 \n",
      "3/3 [==============================] - 0s 222ms/step - loss: 23.3803 - val_loss: 23.9182 - val_y0: -0.1813 - val_y1: 1.0092 - val_ate_after_scaled: 1.1905 - lr: 5.0000e-05\n",
      "Epoch 36/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.6101 — ite: 3.7950  — ate: 0.2045 — pehe: 3.9663 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: 23.2748 - val_loss: 23.7707 - val_y0: -0.1325 - val_y1: 1.1630 - val_ate_after_scaled: 1.2954 - lr: 5.0000e-05\n",
      "Epoch 37/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.7092 — ite: 3.8038  — ate: 0.1022 — pehe: 3.6734 \n",
      "3/3 [==============================] - 0s 226ms/step - loss: 23.3378 - val_loss: 23.5987 - val_y0: -0.3011 - val_y1: 1.2318 - val_ate_after_scaled: 1.5329 - lr: 5.0000e-05\n",
      "Epoch 38/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 23.1370 — ite: 3.7967  — ate: 0.1015 — pehe: 3.9242 \n",
      "3/3 [==============================] - 0s 221ms/step - loss: 22.9819 - val_loss: 22.9784 - val_y0: -0.3482 - val_y1: 1.2322 - val_ate_after_scaled: 1.5804 - lr: 5.0000e-05\n",
      "Epoch 39/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.5949 — ite: 3.7701  — ate: 0.0646 — pehe: 3.8275 \n",
      "3/3 [==============================] - 0s 223ms/step - loss: 22.7337 - val_loss: 22.8618 - val_y0: -0.3121 - val_y1: 1.2136 - val_ate_after_scaled: 1.5256 - lr: 5.0000e-05\n",
      "Epoch 40/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.7032 — ite: 3.8081  — ate: 0.1116 — pehe: 3.8772 \n",
      "3/3 [==============================] - 0s 206ms/step - loss: 22.3859 - val_loss: 22.8678 - val_y0: -0.2817 - val_y1: 1.0860 - val_ate_after_scaled: 1.3677 - lr: 5.0000e-05\n",
      "Epoch 41/137\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 21.9482 — ite: 3.7663  — ate: 0.1223 — pehe: 3.8123 \n",
      "3/3 [==============================] - 1s 250ms/step - loss: 22.1719 - val_loss: 22.5049 - val_y0: -0.1998 - val_y1: 1.1457 - val_ate_after_scaled: 1.3454 - lr: 5.0000e-05\n",
      "Epoch 42/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.1290 — ite: 3.8684  — ate: 0.0506 — pehe: 3.8425 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: 22.2641 - val_loss: 22.4580 - val_y0: -0.3244 - val_y1: 1.1005 - val_ate_after_scaled: 1.4249 - lr: 5.0000e-05\n",
      "Epoch 43/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.1841 — ite: 3.7451  — ate: 0.0807 — pehe: 3.8513 \n",
      "3/3 [==============================] - 0s 205ms/step - loss: 21.9558 - val_loss: 21.9319 - val_y0: -0.2277 - val_y1: 1.1756 - val_ate_after_scaled: 1.4032 - lr: 5.0000e-05\n",
      "Epoch 44/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 22.1197 — ite: 3.6841  — ate: 0.1367 — pehe: 3.7405 \n",
      "3/3 [==============================] - 0s 213ms/step - loss: 21.5254 - val_loss: 22.0167 - val_y0: -0.0690 - val_y1: 1.3062 - val_ate_after_scaled: 1.3753 - lr: 5.0000e-05\n",
      "Epoch 45/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.2414 — ite: 3.7612  — ate: 0.1812 — pehe: 3.8915 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: 21.3741 - val_loss: 21.8964 - val_y0: -0.3092 - val_y1: 1.0727 - val_ate_after_scaled: 1.3819 - lr: 5.0000e-05\n",
      "Epoch 46/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.6688 — ite: 3.7804  — ate: 0.1918 — pehe: 3.7140 \n",
      "3/3 [==============================] - 0s 210ms/step - loss: 21.2959 - val_loss: 22.0259 - val_y0: -0.3080 - val_y1: 1.1477 - val_ate_after_scaled: 1.4557 - lr: 5.0000e-05\n",
      "Epoch 47/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.2164 — ite: 3.7899  — ate: 0.0444 — pehe: 3.8452 \n",
      "3/3 [==============================] - 0s 214ms/step - loss: 21.2009 - val_loss: 21.4374 - val_y0: -0.1130 - val_y1: 1.2253 - val_ate_after_scaled: 1.3383 - lr: 5.0000e-05\n",
      "Epoch 48/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.0570 — ite: 3.8074  — ate: 0.1784 — pehe: 3.8526 \n",
      "3/3 [==============================] - 0s 213ms/step - loss: 20.9723 - val_loss: 21.5464 - val_y0: -0.2883 - val_y1: 1.2054 - val_ate_after_scaled: 1.4937 - lr: 5.0000e-05\n",
      "Epoch 49/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.1821 — ite: 3.8210  — ate: 0.0013 — pehe: 3.7128 \n",
      "3/3 [==============================] - 0s 217ms/step - loss: 20.5241 - val_loss: 21.2197 - val_y0: -0.2428 - val_y1: 1.1786 - val_ate_after_scaled: 1.4214 - lr: 5.0000e-05\n",
      "Epoch 50/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 21.0904 — ite: 3.7552  — ate: 0.1635 — pehe: 3.8633 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: 20.7324 - val_loss: 21.1806 - val_y0: -0.2643 - val_y1: 1.1303 - val_ate_after_scaled: 1.3946 - lr: 5.0000e-05\n",
      "Epoch 51/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.3204 — ite: 3.8092  — ate: 0.3631 — pehe: 3.7066 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: 20.3940 - val_loss: 21.0474 - val_y0: -0.1597 - val_y1: 1.1655 - val_ate_after_scaled: 1.3252 - lr: 5.0000e-05\n",
      "Epoch 52/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.2087 — ite: 3.8013  — ate: 0.0855 — pehe: 3.8083 \n",
      "3/3 [==============================] - 0s 207ms/step - loss: 20.2735 - val_loss: 20.8659 - val_y0: -0.3024 - val_y1: 1.0677 - val_ate_after_scaled: 1.3701 - lr: 5.0000e-05\n",
      "Epoch 53/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.4347 — ite: 3.8025  — ate: 0.1674 — pehe: 3.8302 \n",
      "3/3 [==============================] - 0s 207ms/step - loss: 20.3789 - val_loss: 20.7800 - val_y0: -0.2885 - val_y1: 1.1497 - val_ate_after_scaled: 1.4382 - lr: 5.0000e-05\n",
      "Epoch 54/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 20.2723 — ite: 3.7660  — ate: 0.1990 — pehe: 3.6872 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: 20.0991 - val_loss: 21.0128 - val_y0: -0.2224 - val_y1: 1.3307 - val_ate_after_scaled: 1.5531 - lr: 5.0000e-05\n",
      "Epoch 55/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.5343 — ite: 3.6922  — ate: 0.1861 — pehe: 3.7583 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: 19.9262 - val_loss: 20.3751 - val_y0: -0.3832 - val_y1: 1.0782 - val_ate_after_scaled: 1.4614 - lr: 5.0000e-05\n",
      "Epoch 56/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.6053 — ite: 3.8101  — ate: 0.1379 — pehe: 3.8591 \n",
      "3/3 [==============================] - 0s 219ms/step - loss: 19.6325 - val_loss: 19.3814 - val_y0: -0.1176 - val_y1: 1.0058 - val_ate_after_scaled: 1.1234 - lr: 5.0000e-05\n",
      "Epoch 57/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.2100 — ite: 3.8827  — ate: 0.0730 — pehe: 3.8152 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: 19.4235 - val_loss: 20.2234 - val_y0: -0.1716 - val_y1: 1.0467 - val_ate_after_scaled: 1.2183 - lr: 5.0000e-05\n",
      "Epoch 58/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 19.3140 — ite: 3.7798  — ate: 0.1534 — pehe: 3.6843 \n",
      "3/3 [==============================] - 1s 249ms/step - loss: 19.5196 - val_loss: 20.0270 - val_y0: -0.1799 - val_y1: 1.1648 - val_ate_after_scaled: 1.3447 - lr: 5.0000e-05\n",
      "Epoch 59/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.9373 — ite: 3.8263  — ate: 0.1696 — pehe: 3.7470 \n",
      "3/3 [==============================] - 1s 372ms/step - loss: 18.9557 - val_loss: 19.6311 - val_y0: -0.2553 - val_y1: 1.1098 - val_ate_after_scaled: 1.3651 - lr: 5.0000e-05\n",
      "Epoch 60/137\n",
      "3/3 [==============================] - ETA: 0s - loss: 19.1658 — ite: 3.8036  — ate: 0.0381 — pehe: 3.8656 \n",
      "3/3 [==============================] - 1s 255ms/step - loss: 19.1989 - val_loss: 20.0921 - val_y0: -0.2063 - val_y1: 1.1169 - val_ate_after_scaled: 1.3232 - lr: 5.0000e-05\n",
      "Epoch 61/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.3995 — ite: 3.8500  — ate: 0.1183 — pehe: 3.8721 \n",
      "3/3 [==============================] - 0s 233ms/step - loss: 18.6772 - val_loss: 19.3115 - val_y0: -0.1256 - val_y1: 1.1181 - val_ate_after_scaled: 1.2436 - lr: 5.0000e-05\n",
      "Epoch 62/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.6440 — ite: 3.7673  — ate: 0.0995 — pehe: 3.8252 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: 18.4901 - val_loss: 19.3294 - val_y0: -0.2293 - val_y1: 1.2799 - val_ate_after_scaled: 1.5091 - lr: 5.0000e-05\n",
      "Epoch 63/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.2477 — ite: 3.8042  — ate: 0.0836 — pehe: 3.7241 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: 18.5555 - val_loss: 18.9715 - val_y0: -0.2692 - val_y1: 1.1524 - val_ate_after_scaled: 1.4215 - lr: 5.0000e-05\n",
      "Epoch 64/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.5815 — ite: 3.9246  — ate: 0.0657 — pehe: 4.0158 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: 18.2673 - val_loss: 18.8309 - val_y0: -0.1757 - val_y1: 1.2385 - val_ate_after_scaled: 1.4143 - lr: 5.0000e-05\n",
      "Epoch 65/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.6629 — ite: 3.8033  — ate: 0.2842 — pehe: 3.7942 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: 18.0693 - val_loss: 18.3903 - val_y0: -0.2139 - val_y1: 1.0871 - val_ate_after_scaled: 1.3010 - lr: 5.0000e-05\n",
      "Epoch 66/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.4253 — ite: 3.9092  — ate: 0.1363 — pehe: 3.8381 \n",
      "3/3 [==============================] - 0s 194ms/step - loss: 17.7112 - val_loss: 18.8960 - val_y0: -0.1885 - val_y1: 1.0873 - val_ate_after_scaled: 1.2757 - lr: 5.0000e-05\n",
      "Epoch 67/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17.9896 — ite: 3.8305  — ate: 0.1294 — pehe: 3.7634 \n",
      "3/3 [==============================] - 0s 195ms/step - loss: 18.1222 - val_loss: 18.1762 - val_y0: -0.1267 - val_y1: 1.1124 - val_ate_after_scaled: 1.2391 - lr: 5.0000e-05\n",
      "Epoch 68/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.2534 — ite: 3.7967  — ate: 0.1213 — pehe: 3.5512 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: 17.6434 - val_loss: 18.2898 - val_y0: -0.2287 - val_y1: 1.1192 - val_ate_after_scaled: 1.3479 - lr: 5.0000e-05\n",
      "Epoch 69/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 18.3755 — ite: 3.9105  — ate: 0.2592 — pehe: 3.9758 \n",
      "3/3 [==============================] - 0s 204ms/step - loss: 17.4401 - val_loss: 18.4268 - val_y0: -0.1628 - val_y1: 1.1530 - val_ate_after_scaled: 1.3159 - lr: 5.0000e-05\n",
      "Epoch 70/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16.9356 — ite: 3.8044  — ate: 0.0514 — pehe: 3.8714 \n",
      "3/3 [==============================] - 0s 226ms/step - loss: 17.5480 - val_loss: 17.6242 - val_y0: -0.2173 - val_y1: 1.1430 - val_ate_after_scaled: 1.3603 - lr: 5.0000e-05\n",
      "Epoch 71/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17.5914 — ite: 3.7516  — ate: 0.0782 — pehe: 3.7499 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: 17.0801 - val_loss: 17.7495 - val_y0: -0.1702 - val_y1: 1.2008 - val_ate_after_scaled: 1.3709 - lr: 5.0000e-05\n",
      "Epoch 72/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17.4125 — ite: 3.8070  — ate: 0.2994 — pehe: 3.7382 \n",
      "3/3 [==============================] - 0s 205ms/step - loss: 16.8410 - val_loss: 17.3118 - val_y0: -0.2192 - val_y1: 1.1371 - val_ate_after_scaled: 1.3562 - lr: 5.0000e-05\n",
      "Epoch 73/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17.1946 — ite: 3.8239  — ate: 0.0165 — pehe: 3.9500 \n",
      "3/3 [==============================] - 0s 224ms/step - loss: 16.5783 - val_loss: 18.0352 - val_y0: -0.2625 - val_y1: 1.1378 - val_ate_after_scaled: 1.4003 - lr: 5.0000e-05\n",
      "Epoch 74/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 17.4103 — ite: 3.9544  — ate: 0.1635 — pehe: 3.9267 \n",
      "3/3 [==============================] - 0s 220ms/step - loss: 16.3852 - val_loss: 16.9860 - val_y0: -0.2572 - val_y1: 1.1204 - val_ate_after_scaled: 1.3776 - lr: 5.0000e-05\n",
      "Epoch 75/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16.6415 — ite: 3.8328  — ate: 0.4479 — pehe: 3.5829 \n",
      "3/3 [==============================] - 1s 268ms/step - loss: 16.1720 - val_loss: 16.9364 - val_y0: -0.4135 - val_y1: 1.1488 - val_ate_after_scaled: 1.5623 - lr: 5.0000e-05\n",
      "Epoch 76/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16.4554 — ite: 3.8358  — ate: 0.0272 — pehe: 3.7465 \n",
      "3/3 [==============================] - 1s 242ms/step - loss: 16.0930 - val_loss: 16.8699 - val_y0: -0.4810 - val_y1: 1.1582 - val_ate_after_scaled: 1.6392 - lr: 5.0000e-05\n",
      "Epoch 77/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16.6142 — ite: 3.8120  — ate: 0.0642 — pehe: 3.6869 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: 16.2271 - val_loss: 16.2805 - val_y0: -0.1829 - val_y1: 1.1046 - val_ate_after_scaled: 1.2875 - lr: 5.0000e-05\n",
      "Epoch 78/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 16.0685 — ite: 3.7659  — ate: 0.2184 — pehe: 3.7374 \n",
      "3/3 [==============================] - 0s 200ms/step - loss: 15.6049 - val_loss: 16.2029 - val_y0: -0.3828 - val_y1: 1.2487 - val_ate_after_scaled: 1.6315 - lr: 5.0000e-05\n",
      "Epoch 79/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15.7326 — ite: 3.9010  — ate: 0.0954 — pehe: 3.6728 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: 15.4087 - val_loss: 16.2938 - val_y0: -0.2265 - val_y1: 1.0782 - val_ate_after_scaled: 1.3047 - lr: 5.0000e-05\n",
      "Epoch 80/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 15.3157 — ite: 3.9089  — ate: 0.0738 — pehe: 3.8558 \n",
      "3/3 [==============================] - 0s 232ms/step - loss: 15.1501 - val_loss: 16.2520 - val_y0: -0.1428 - val_y1: 1.0606 - val_ate_after_scaled: 1.2034 - lr: 5.0000e-05\n",
      "Epoch 81/137\n",
      "3/3 [==============================] - ETA: 0s - loss: 14.9398 — ite: 3.7705  — ate: 0.2423 — pehe: 3.8375 \n",
      "3/3 [==============================] - 1s 280ms/step - loss: 14.7886 - val_loss: 15.8165 - val_y0: -0.1292 - val_y1: 1.2673 - val_ate_after_scaled: 1.3965 - lr: 5.0000e-05\n",
      "Epoch 82/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 14.4754 — ite: 3.7748  — ate: 0.2195 — pehe: 3.8708 \n",
      "3/3 [==============================] - 1s 272ms/step - loss: 14.5148 - val_loss: 15.0709 - val_y0: -0.2965 - val_y1: 1.0488 - val_ate_after_scaled: 1.3452 - lr: 5.0000e-05\n",
      "Epoch 83/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13.4376 — ite: 3.8558  — ate: 0.1201 — pehe: 3.6225 \n",
      "3/3 [==============================] - 0s 220ms/step - loss: 14.5808 - val_loss: 14.5729 - val_y0: -0.3495 - val_y1: 1.2230 - val_ate_after_scaled: 1.5725 - lr: 5.0000e-05\n",
      "Epoch 84/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13.2445 — ite: 3.8829  — ate: 0.0825 — pehe: 3.7992 \n",
      "3/3 [==============================] - 0s 222ms/step - loss: 13.8184 - val_loss: 13.9675 - val_y0: -0.2362 - val_y1: 1.1881 - val_ate_after_scaled: 1.4243 - lr: 5.0000e-05\n",
      "Epoch 85/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13.0242 — ite: 3.8002  — ate: 0.3693 — pehe: 3.8390 \n",
      "3/3 [==============================] - 0s 227ms/step - loss: 13.4794 - val_loss: 14.1769 - val_y0: -0.2559 - val_y1: 1.0788 - val_ate_after_scaled: 1.3348 - lr: 5.0000e-05\n",
      "Epoch 86/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13.2760 — ite: 3.9614  — ate: 0.2151 — pehe: 3.8467 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: 13.1011 - val_loss: 13.6493 - val_y0: -0.2086 - val_y1: 1.1185 - val_ate_after_scaled: 1.3271 - lr: 5.0000e-05\n",
      "Epoch 87/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 13.7723 — ite: 3.8697  — ate: 0.2188 — pehe: 3.7773 \n",
      "3/3 [==============================] - 0s 205ms/step - loss: 13.0539 - val_loss: 13.7648 - val_y0: -0.2197 - val_y1: 1.0411 - val_ate_after_scaled: 1.2608 - lr: 5.0000e-05\n",
      "Epoch 88/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 12.3832 — ite: 3.8445  — ate: 0.1267 — pehe: 3.7223 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: 12.6880 - val_loss: 13.5024 - val_y0: -0.2234 - val_y1: 1.1877 - val_ate_after_scaled: 1.4112 - lr: 5.0000e-05\n",
      "Epoch 89/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11.2485 — ite: 3.9156  — ate: 0.0096 — pehe: 3.8564 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: 12.0625 - val_loss: 12.6229 - val_y0: -0.4898 - val_y1: 1.0495 - val_ate_after_scaled: 1.5393 - lr: 5.0000e-05\n",
      "Epoch 90/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10.3914 — ite: 3.8163  — ate: 0.1058 — pehe: 3.7707 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: 11.7059 - val_loss: 11.7188 - val_y0: -0.1840 - val_y1: 1.1641 - val_ate_after_scaled: 1.3480 - lr: 5.0000e-05\n",
      "Epoch 91/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10.8761 — ite: 3.8083  — ate: 0.2205 — pehe: 3.8888 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: 11.1305 - val_loss: 11.7742 - val_y0: -0.1634 - val_y1: 1.0386 - val_ate_after_scaled: 1.2020 - lr: 5.0000e-05\n",
      "Epoch 92/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 11.1397 — ite: 3.7868  — ate: 0.0969 — pehe: 3.7860 \n",
      "3/3 [==============================] - 0s 217ms/step - loss: 10.6949 - val_loss: 11.1415 - val_y0: -0.2638 - val_y1: 1.1991 - val_ate_after_scaled: 1.4629 - lr: 5.0000e-05\n",
      "Epoch 93/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10.5389 — ite: 3.8052  — ate: 0.2167 — pehe: 3.7342 \n",
      "3/3 [==============================] - 0s 214ms/step - loss: 10.0414 - val_loss: 10.3072 - val_y0: -0.2850 - val_y1: 1.1986 - val_ate_after_scaled: 1.4836 - lr: 5.0000e-05\n",
      "Epoch 94/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.8020 — ite: 3.8481  — ate: 0.3684 — pehe: 3.7595 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: 9.2746 - val_loss: 9.2935 - val_y0: -0.2705 - val_y1: 1.0668 - val_ate_after_scaled: 1.3373 - lr: 5.0000e-05\n",
      "Epoch 95/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 8.4003 — ite: 3.9855  — ate: 0.3302 — pehe: 3.8302 \n",
      "3/3 [==============================] - 0s 220ms/step - loss: 8.6313 - val_loss: 9.0848 - val_y0: -0.3463 - val_y1: 1.1827 - val_ate_after_scaled: 1.5290 - lr: 5.0000e-05\n",
      "Epoch 96/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 10.8586 — ite: 4.0340  — ate: 0.3760 — pehe: 4.0036 \n",
      "3/3 [==============================] - 0s 223ms/step - loss: 7.9239 - val_loss: 8.6804 - val_y0: -0.2311 - val_y1: 1.2134 - val_ate_after_scaled: 1.4445 - lr: 5.0000e-05\n",
      "Epoch 97/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.2955 — ite: 3.8371  — ate: 0.1960 — pehe: 4.0317 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: 7.9146 - val_loss: 8.1282 - val_y0: -0.2665 - val_y1: 1.1336 - val_ate_after_scaled: 1.4001 - lr: 5.0000e-05\n",
      "Epoch 98/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.0717 — ite: 3.9174  — ate: 0.0874 — pehe: 3.7049 \n",
      "3/3 [==============================] - 0s 213ms/step - loss: 7.3188 - val_loss: 7.6716 - val_y0: -0.2951 - val_y1: 1.1929 - val_ate_after_scaled: 1.4879 - lr: 5.0000e-05\n",
      "Epoch 99/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.4683 — ite: 3.8592  — ate: 0.0884 — pehe: 3.6732 \n",
      "3/3 [==============================] - 0s 220ms/step - loss: 5.3682 - val_loss: 5.9672 - val_y0: -0.3155 - val_y1: 0.9733 - val_ate_after_scaled: 1.2888 - lr: 5.0000e-05\n",
      "Epoch 100/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 7.1666 — ite: 3.8030  — ate: 0.1005 — pehe: 3.7365 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: 4.5582 - val_loss: 5.0898 - val_y0: -0.3070 - val_y1: 1.0627 - val_ate_after_scaled: 1.3697 - lr: 5.0000e-05\n",
      "Epoch 101/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 5.7913 — ite: 3.9165  — ate: 0.3352 — pehe: 3.8365 \n",
      "3/3 [==============================] - 0s 242ms/step - loss: 4.4960 - val_loss: 3.7943 - val_y0: -0.2532 - val_y1: 0.9689 - val_ate_after_scaled: 1.2221 - lr: 5.0000e-05\n",
      "Epoch 102/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.9433 — ite: 3.8654  — ate: 0.1443 — pehe: 3.9121 \n",
      "3/3 [==============================] - 1s 248ms/step - loss: 3.1874 - val_loss: 3.9556 - val_y0: -0.3541 - val_y1: 1.1791 - val_ate_after_scaled: 1.5332 - lr: 5.0000e-05\n",
      "Epoch 103/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 3.6904 — ite: 3.8087  — ate: 0.1559 — pehe: 3.6260 \n",
      "3/3 [==============================] - 0s 244ms/step - loss: 2.2640 - val_loss: 2.0571 - val_y0: -0.1506 - val_y1: 1.1832 - val_ate_after_scaled: 1.3339 - lr: 5.0000e-05\n",
      "Epoch 104/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.3903 — ite: 3.9573  — ate: 0.0553 — pehe: 3.8657 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: 1.4662 - val_loss: 0.8157 - val_y0: -0.2222 - val_y1: 1.1466 - val_ate_after_scaled: 1.3688 - lr: 5.0000e-05\n",
      "Epoch 105/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 1.4324 — ite: 3.8894  — ate: 0.0257 — pehe: 3.7743 \n",
      "3/3 [==============================] - 0s 225ms/step - loss: -0.2630 - val_loss: -0.0990 - val_y0: -0.3235 - val_y1: 0.9538 - val_ate_after_scaled: 1.2773 - lr: 5.0000e-05\n",
      "Epoch 106/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -0.7435 — ite: 3.9375  — ate: 0.1834 — pehe: 3.8854 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: -1.4220 - val_loss: -2.2680 - val_y0: -0.1175 - val_y1: 1.1729 - val_ate_after_scaled: 1.2904 - lr: 5.0000e-05\n",
      "Epoch 107/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -2.8145 — ite: 3.8339  — ate: 0.4012 — pehe: 3.8266 \n",
      "3/3 [==============================] - 0s 222ms/step - loss: -3.0984 - val_loss: -3.1783 - val_y0: -0.1440 - val_y1: 1.1835 - val_ate_after_scaled: 1.3274 - lr: 5.0000e-05\n",
      "Epoch 108/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -6.1351 — ite: 3.8945  — ate: 0.3279 — pehe: 3.7840 \n",
      "3/3 [==============================] - 0s 222ms/step - loss: -3.8145 - val_loss: -4.0118 - val_y0: -0.3502 - val_y1: 1.0884 - val_ate_after_scaled: 1.4386 - lr: 5.0000e-05\n",
      "Epoch 109/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -7.0504 — ite: 3.9322  — ate: 0.1911 — pehe: 3.8143 \n",
      "3/3 [==============================] - 0s 213ms/step - loss: -5.3419 - val_loss: -5.2931 - val_y0: -0.3235 - val_y1: 1.0091 - val_ate_after_scaled: 1.3326 - lr: 5.0000e-05\n",
      "Epoch 110/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -4.5436 — ite: 3.9710  — ate: 0.2949 — pehe: 3.8652 \n",
      "3/3 [==============================] - 0s 214ms/step - loss: -7.3474 - val_loss: -7.8910 - val_y0: -0.3126 - val_y1: 1.2211 - val_ate_after_scaled: 1.5337 - lr: 5.0000e-05\n",
      "Epoch 111/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -9.0704 — ite: 3.9220  — ate: 0.5286 — pehe: 3.8367 \n",
      "3/3 [==============================] - 0s 209ms/step - loss: -8.7595 - val_loss: -9.2825 - val_y0: -0.2821 - val_y1: 1.1338 - val_ate_after_scaled: 1.4159 - lr: 5.0000e-05\n",
      "Epoch 112/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -13.3951 — ite: 3.9245  — ate: 0.1109 — pehe: 3.8819 \n",
      "3/3 [==============================] - 0s 219ms/step - loss: -11.6540 - val_loss: -10.6336 - val_y0: -0.2559 - val_y1: 1.0721 - val_ate_after_scaled: 1.3280 - lr: 5.0000e-05\n",
      "Epoch 113/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -13.0695 — ite: 3.9870  — ate: 0.1298 — pehe: 3.8234 \n",
      "3/3 [==============================] - 0s 215ms/step - loss: -13.6240 - val_loss: -13.4140 - val_y0: -0.2184 - val_y1: 1.2148 - val_ate_after_scaled: 1.4332 - lr: 5.0000e-05\n",
      "Epoch 114/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -10.7459 — ite: 3.9123  — ate: 0.0097 — pehe: 3.7714 \n",
      "3/3 [==============================] - 0s 216ms/step - loss: -16.7527 - val_loss: -15.7116 - val_y0: -0.3443 - val_y1: 1.1313 - val_ate_after_scaled: 1.4757 - lr: 5.0000e-05\n",
      "Epoch 115/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -14.9661 — ite: 3.9044  — ate: 0.1853 — pehe: 3.9545 \n",
      "3/3 [==============================] - 0s 225ms/step - loss: -18.6737 - val_loss: -19.7239 - val_y0: -0.3341 - val_y1: 1.1746 - val_ate_after_scaled: 1.5087 - lr: 5.0000e-05\n",
      "Epoch 116/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -19.1060 — ite: 4.0288  — ate: 0.4379 — pehe: 3.9312 \n",
      "3/3 [==============================] - 0s 210ms/step - loss: -21.3752 - val_loss: -20.8359 - val_y0: -0.2095 - val_y1: 1.1251 - val_ate_after_scaled: 1.3346 - lr: 5.0000e-05\n",
      "Epoch 117/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -26.1973 — ite: 3.9907  — ate: 0.0476 — pehe: 3.8401 \n",
      "3/3 [==============================] - 0s 214ms/step - loss: -22.5404 - val_loss: -24.2146 - val_y0: -0.4308 - val_y1: 1.2533 - val_ate_after_scaled: 1.6841 - lr: 5.0000e-05\n",
      "Epoch 118/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -24.8016 — ite: 3.9638  — ate: 0.1170 — pehe: 3.8222 \n",
      "3/3 [==============================] - 0s 211ms/step - loss: -26.1407 - val_loss: -29.0205 - val_y0: -0.2388 - val_y1: 1.0816 - val_ate_after_scaled: 1.3204 - lr: 5.0000e-05\n",
      "Epoch 119/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -28.9794 — ite: 3.9453  — ate: 0.3312 — pehe: 3.9607 \n",
      "3/3 [==============================] - 0s 203ms/step - loss: -30.9878 - val_loss: -30.9012 - val_y0: -0.2923 - val_y1: 1.0014 - val_ate_after_scaled: 1.2938 - lr: 5.0000e-05\n",
      "Epoch 120/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -38.3203 — ite: 3.8310  — ate: 0.1979 — pehe: 3.8137 \n",
      "3/3 [==============================] - 0s 213ms/step - loss: -32.7294 - val_loss: -34.8847 - val_y0: -0.3773 - val_y1: 1.0450 - val_ate_after_scaled: 1.4223 - lr: 5.0000e-05\n",
      "Epoch 121/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -35.6681 — ite: 4.0786  — ate: 0.3362 — pehe: 3.8311 \n",
      "3/3 [==============================] - 0s 227ms/step - loss: -37.7909 - val_loss: -38.5659 - val_y0: -0.1792 - val_y1: 1.0705 - val_ate_after_scaled: 1.2497 - lr: 5.0000e-05\n",
      "Epoch 122/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -39.4868 — ite: 3.9623  — ate: 0.3019 — pehe: 4.0156 \n",
      "3/3 [==============================] - 0s 206ms/step - loss: -41.1563 - val_loss: -42.7947 - val_y0: -0.3350 - val_y1: 0.9970 - val_ate_after_scaled: 1.3320 - lr: 5.0000e-05\n",
      "Epoch 123/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -49.4317 — ite: 4.0653  — ate: 0.5128 — pehe: 3.8970 \n",
      "3/3 [==============================] - 0s 225ms/step - loss: -45.3674 - val_loss: -49.6552 - val_y0: -0.2361 - val_y1: 1.0999 - val_ate_after_scaled: 1.3360 - lr: 5.0000e-05\n",
      "Epoch 124/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -48.6022 — ite: 4.0891  — ate: 0.0669 — pehe: 4.0036 \n",
      "3/3 [==============================] - 0s 224ms/step - loss: -52.2287 - val_loss: -52.1113 - val_y0: -0.1293 - val_y1: 1.1559 - val_ate_after_scaled: 1.2852 - lr: 5.0000e-05\n",
      "Epoch 125/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -56.3339 — ite: 4.2273  — ate: 0.2624 — pehe: 3.9769 \n",
      "3/3 [==============================] - 0s 238ms/step - loss: -55.6360 - val_loss: -58.6748 - val_y0: -0.3224 - val_y1: 1.0955 - val_ate_after_scaled: 1.4179 - lr: 5.0000e-05\n",
      "Epoch 126/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -58.5944 — ite: 4.0772  — ate: 0.3133 — pehe: 3.9055 \n",
      "3/3 [==============================] - 0s 208ms/step - loss: -62.2129 - val_loss: -62.8900 - val_y0: -0.0341 - val_y1: 1.2307 - val_ate_after_scaled: 1.2648 - lr: 5.0000e-05\n",
      "Epoch 127/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -58.9766 — ite: 4.1526  — ate: 0.4132 — pehe: 3.8261 \n",
      "3/3 [==============================] - 0s 210ms/step - loss: -66.7420 - val_loss: -69.3886 - val_y0: -0.1436 - val_y1: 1.0495 - val_ate_after_scaled: 1.1931 - lr: 5.0000e-05\n",
      "Epoch 128/137\n",
      "3/3 [==============================] - ETA: 0s - loss: -71.9845 — ite: 4.2058  — ate: 0.1267 — pehe: 3.8227 \n",
      "3/3 [==============================] - 1s 245ms/step - loss: -73.6101 - val_loss: -75.0159 - val_y0: -0.1454 - val_y1: 1.1158 - val_ate_after_scaled: 1.2612 - lr: 5.0000e-05\n",
      "Epoch 129/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -79.6265 — ite: 4.3752  — ate: 0.3850 — pehe: 4.0988 \n",
      "3/3 [==============================] - 0s 219ms/step - loss: -79.5162 - val_loss: -82.4868 - val_y0: -0.2252 - val_y1: 1.0613 - val_ate_after_scaled: 1.2866 - lr: 5.0000e-05\n",
      "Epoch 130/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -83.5035 — ite: 4.2727  — ate: 0.2968 — pehe: 3.9793 \n",
      "3/3 [==============================] - 0s 212ms/step - loss: -85.3968 - val_loss: -90.7222 - val_y0: -0.3446 - val_y1: 1.0841 - val_ate_after_scaled: 1.4287 - lr: 5.0000e-05\n",
      "Epoch 131/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -88.1833 — ite: 4.3594  — ate: 0.5715 — pehe: 4.1328 \n",
      "3/3 [==============================] - 0s 207ms/step - loss: -95.6692 - val_loss: -96.6976 - val_y0: -0.1999 - val_y1: 0.9588 - val_ate_after_scaled: 1.1587 - lr: 5.0000e-05\n",
      "Epoch 132/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -98.5834 — ite: 4.3671  — ate: 0.0799 — pehe: 3.9641 \n",
      "3/3 [==============================] - 0s 213ms/step - loss: -102.3756 - val_loss: -107.4274 - val_y0: -0.2456 - val_y1: 1.1780 - val_ate_after_scaled: 1.4236 - lr: 5.0000e-05\n",
      "Epoch 133/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -97.9329 — ite: 4.4611  — ate: 0.1859 — pehe: 3.9344 \n",
      "3/3 [==============================] - 0s 214ms/step - loss: -108.3948 - val_loss: -116.5249 - val_y0: -0.2896 - val_y1: 1.2266 - val_ate_after_scaled: 1.5162 - lr: 5.0000e-05\n",
      "Epoch 134/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -108.8734 — ite: 4.4388  — ate: 0.0712 — pehe: 4.0994 \n",
      "3/3 [==============================] - 0s 218ms/step - loss: -115.5960 - val_loss: -127.7054 - val_y0: -0.2274 - val_y1: 1.0522 - val_ate_after_scaled: 1.2797 - lr: 5.0000e-05\n",
      "Epoch 135/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -137.6904 — ite: 4.5604  — ate: 0.0120 — pehe: 3.9366 \n",
      "3/3 [==============================] - 1s 244ms/step - loss: -125.0777 - val_loss: -136.3976 - val_y0: -0.2715 - val_y1: 1.0676 - val_ate_after_scaled: 1.3391 - lr: 5.0000e-05\n",
      "Epoch 136/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -147.4845 — ite: 4.6775  — ate: 0.1643 — pehe: 3.9825 \n",
      "3/3 [==============================] - 0s 236ms/step - loss: -140.0223 - val_loss: -145.3302 - val_y0: -0.2933 - val_y1: 0.9986 - val_ate_after_scaled: 1.2919 - lr: 5.0000e-05\n",
      "Epoch 137/137\n",
      "1/3 [=========>....................] - ETA: 0s - loss: -128.9102 — ite: 4.6570  — ate: 0.1263 — pehe: 4.0925 \n",
      "3/3 [==============================] - 0s 229ms/step - loss: -151.5474 - val_loss: -157.6884 - val_y0: -0.2551 - val_y1: 1.2508 - val_ate_after_scaled: 1.5059 - lr: 5.0000e-05\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Colab command to allow us to run Colab in TF2\n",
    "!rm -rf ./logs/ \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "%reload_ext tensorboard \n",
    "\n",
    "model = CEVAE()\n",
    "### MAIN CODE ####\n",
    "val_split=0.2\n",
    "batch_size=64\n",
    "verbose=True\n",
    "i = 0\n",
    "tf.random.set_seed(i)\n",
    "np.random.seed(i)\n",
    " \n",
    "callbacks = [\n",
    "        TerminateOnNaN(),\n",
    "        EarlyStopping(monitor='val_loss', patience=40, min_delta=0), \n",
    "        #40 is Shi's recommendation patience for this dataset, but you should tune for your data \n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
    "                          min_delta=0, cooldown=0, min_lr=0),\n",
    "        #This learning rate scheduling is quite agressive which seems good for this dataset\n",
    "        metrics_for_cevae(data,verbose),\n",
    "        tensorboard_callback\n",
    "    ]\n",
    "    \n",
    "#optimizer hyperparameters\n",
    "learning_rate = 5e-5\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate))\n",
    "\n",
    "model.fit(\n",
    "    [data['x'],data['t'],data['ys']],\n",
    "    callbacks=callbacks,\n",
    "    validation_split=val_split,\n",
    "    epochs=137,\n",
    "    batch_size=200,\n",
    "    verbose=verbose\n",
    "    )\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 45269), started 0:27:50 ago. (Use '!kill 45269' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-aba92ab7c795520c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-aba92ab7c795520c\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
